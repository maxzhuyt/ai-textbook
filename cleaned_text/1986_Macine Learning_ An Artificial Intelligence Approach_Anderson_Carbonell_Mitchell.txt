--- PAGE 1 ---
MACHINE
LEARNING
Artificial Intelligence Approach
VOLUME
Ryszard S. Michalski
Jaime G. Carbonell
Tom M.
Mitchell
--- PAGE 2 ---
RICOH CORPORATION.
Research Center
1321 Ridder Park Drive
San Jose, CA 95131-2306
--- PAGE 3 ---
SSft
--- PAGE 4 ---
Digitized by the Internet Archive
http://archive.org/details/machinelearningOOrysz
--- PAGE 5 ---
MACHINE LEARNING
An Artificial Intelligence Approach
Volume
--- PAGE 6 ---
--- PAGE 7 ---
MACHINE LEARNING
An Artificial Intelligence Approach
Volume
Contributors:
Saul Amarel Michael Lebowitz
John R. Anderson Douglas B. Lenat
Ranan B. Banerji Sridhar Mahadevan
Robert C. Berwick Ryszard S. Michalski
Gary L. Bradshaw Donald Michie
Mark H. Burstein Allen Newell
Jaime G. Carbonell J. Ross Quinlan
Gerald DeJong Paul S. Rosenbloom
Nachum Dershowitz Claude Sammut
Thomas G. Dietterich Bernard Silver
Kenneth D. Forbus Herbert A. Simon
Jean-Gabriel Ganascia Robert E. Stepp III
Dedre Gentner Gail E. Thornburg
John H.Holland Paul E. Utgoff
SmadarT. Kedar-Cabelli Patrick H. Winston
Yves Kodratoff Jan M. Zytkow
Patrick Langley
Editors:
Ryszard S. Michalski Jaime G. Carbonell Tom M. Mitchell
UniversityofIllinois Carnegie-Mellon University Rutgers University
at Urbana-Champaign, IL Pittsburgh,PA NewBrunswick, NJ
Morgan Kaufmann
Publishers, Inc.
95 First Street, Los Altos, California 94022
--- PAGE 8 ---
Copyright © 1986by Morgan Kaufmann Publishers, Inc. All rights reserved, including the
right of reproduction in whole or in part in any form. For further information, write to
Permissions Department, Morgan Kaufmann Publishers, Inc., 95 First Street, Los Altos,
California94022.
Printed inthe United StatesofAmerica
98765432
Library ofCongress Cataloging-in-PublicationData
(Revised forvol. 2)
Mainentry undertitle:
Machinelearning.
Vol. 2- preparedby Saul Amarel . . . [etal.]
andpublishedby M. Kaufmann Publishers, LosAltos, Calif.
Bibliography; p. 511-549.
Includes indexes.
1. Machinelearning. 2. Artificial intelligence.
I. Anderson, JohnRobert, date. II. Michalski,
Ryszard Stanislaw, date. III. Carbonell, JaimeG.
(JaimeGuillermo) IV. Mitchell, TomM. (Tom Michael), date.
V. Amarel, Saul
Q325.M32 1983 006.3'1 82-10654
ISBN0-935382-05-4 (Volume 1)
ISBN 0-934613-00-1 (Volume2)
Acknowledgments
The paper appearing as chapter4 ofthis book originally appeared in thejournal Artificial
Intelligence, Volume25, Number 2, pages 187-232, and isreprintedherewithpermissionof
North Holland Publishing, Amsterdam.
Chapter 10 ofthis book is a portion ofa book to be released soon by Kluwer Academic
Publishers and is published here with the permission ofKluwer Academic Publishers,
Hingham, Massachusetts.
Chapter 17 is a modified version ofanarticle published inArtificialIntelligenceJournal.
Volume 20, Number 10, and is published here with the permission of North Holland
Publishing, Amsterdam.
--- PAGE 9 ---
CONTENTS
Preface ix
PART ONE GENERAL ISSUES
Chapter 1 Understanding the Nature of Learning:
Issues and Research Directions
RyszardS. MichaIski
Chapter2 Machine Learning: Challenges of the
Eighties 27
RyszardS. Michalski, SaulAmarel, Douglas B.
Lenat, Donald Michie, and Patrick H. Winston
(Editedby Gail Thornburg and Ryszard Michalski)
PART TWO LEARNING CONCEPTS AND RULES FROM
EXAMPLES 43
Chapter 3 Learning by Augmenting Rules and
Accumulating Censors 45
Patrick H. Winston
Chapter4 Learning to Predict Sequences 63
Thomas G. Dietterich and RyszardS. Michalski
Chapter 5 Shift of Bias for Inductive Concept Learning 107
Paul E. Utgoff
Chapter6 The Effect of Noise on Concept Learning 149
J. Ross Quintan
Chapter 7 Learning Concepts by Asking Questions 167
Claude Sammutand Ranan B. Banerji
Chapter8 Concept Learning in a Rich Input Domain:
Generalization-Based Memory 193
Michael Lebowitz
--- PAGE 10 ---
CONTENTS
Chapter9 Improving the Generalization Step in
Learning 215
Yves KodratoffandJean-GabrielGanascia
PART THREE COGNITIVE ASPECTS OF LEARNING 245
Chapter 10 The Chunking of Goal Hierarchies: A
Generalized Model of Practice 247
PaulS. Rosenbloom andAllen Newell
Chapter 11 Knowledge Compilation: The General
Learning Mechanism 289
John R. Anderson
Chapter 12 Learning Physical Domains: Toward a
Theoretical Framework 311
Kenneth D. Forbus andDedre Gentner
PART FOUR LEARNING BY ANALOGY 349
Chapter 13 Concept Formation by Incremental
Analogical Reasoning and Debugging 351
Mark H. Burstein
Chapter 14 Derivational Analogy: A Theory of
Reconstructive Problem Solving and
Expertise Acquisition 371
Jaime G. Carbonell
Chapter15 Programming by Analogy 393
Nachum Dershowitz
PART FIVE LEARNING BYOBSERVATION AND 423
DISCOVERY
Chapter 16 The Search for Regularity: Four Aspects of
Scientific Discovery 425
Pat Langley, Jan M. Zytkow, HerbertA. Simon,
and Gary L. Bradshaw
Chapter 17 Conceptual Clustering: Inventing
Goal-Oriented Classifications of Structured
Objects 471
Robert E. Stepp III and RyszardS. Michalski
--- PAGE 11 ---
CONTENTS
VII
Chapter 18 Program Synthesis as a Theory Formation
Task: Problem Representations and Solution
Methods 499
SaulAmarei
Chapter 19 An Approach to Learning from Observation 571
Gerald DeJong
PART SIX AN EXPLORATION OF GENERAL ASPECTS 591
OF LEARNING
Chapter 20 Escaping Brittleness: The Possibilities of
General-Purpose Learning Algorithms
Applied to Parallel Rule-Based Systems 593
John H. Holland
Chapter 21 Learning from Positive-Only Examples: The
Subset Principle and Three Case Studies 625
Robert C. Berwick
Chapter 22 Precondition Analysis: Learning Control
Information 647
BernardSilver
Bibliography of Recent Machine Learning Research 671
Smadar
Kedar-CabelliandSridharMahadevan
Updated Glossary of Selected Terms in Machine Learning 707
About the Authors 715
Author Index 725
Subject Index 729
--- PAGE 12 ---
--- PAGE 13 ---
PREFACE
The recent extraordinary growth ofartificial intelligence and its applications
hasbeenparalleledbyasurgeofinterestinmachinelearning, afieldconcernedwith
the developing computational theories oflearning processes and building learning
machines. Because the ability to learn is clearly fundamental to any intelligent
behavior, the concerns and goals ofmachine learning are central to the progress of
artificial intelligence.
Thisbookpresents tutorial overviews and a selection ofworks representative
ofthe state oftheartin machinelearning. Itisthe secondbasic textonthis subject,
following MachineLearning: AnArtificialIntelligenceApproach (Michalski, Carbonell, andMitchell, 1983). Thesuccessofthefirstbookandtherapidexpansionof
thefieldinrecentyearshaveencouragedtheeditorstoputtogetherthecurrentcollection. The individual chapters represent contributions ofleading researchers in the
field. Initial shorter versions ofmost ofthe chapters were presented at the Second
International Workshop on Machine Learning, held June 22-24, 1983, at Allerton
House in Monticello, the residential conference centerofthe University ofIllinois.
The authors ofthechapters havemade a special efforttowritetheircontributions in a readable tutorial fashion. It is hoped that this will make the book more
accessibletoawidespectrumofreaderswithaninterestinunderstandingthecurrent
state ofmachinelearning. These might include researchers in artificial intelligence
and cognitive science, knowledge engineers, computer scientists, data analysts,
information scientists, philosophers, psychologists and linguists. The book is
intendedto serve both as aconvenient source ofinformation forthem, and as atext
forstudentstakingcoursesinartificialintelligence,particularlyinmachinelearning
and knowledge acquisition.
Topics covered include
• A road-map and aclassification ofmachine learning research
• Theoreticalissuesandmethodsforlearningconceptsandrulesfromexamples
• Cognitive models ofhuman learning
• Using analogy to learn concepts, to program, and to solveproblems
• Quantitative discovery systems, theory formation, and conceptual clustering
--- PAGE 14 ---
x PREFACE
• Learning in parallel rule-based systems
• Aspects ofnatural language learning
• Learning to solve equations
To facilitateinstructionandfurtherstudies, anextensive indexedbibliography
ofrecent contributions and landmarks ofearlier research is provided, as well as an
updated glossary ofbasic terms in machine learning.
Itisourpleasantdutytothankmanyindividualswhohelpedinvariouswaysto
makethisbookareality. OurgratitudegoestoAlanL. MeyrowitzfromtheOfficeof
Naval ResearchandJames N. Snyderand Richard Canaday fromthe Departmentof
Computer Science at the University ofIllinois at Urbana-Champaign for their supportandcontributiontotheSecondInternationalMachineLearningWorkshop. This
Workshop provided the real impetus forthe genesis ofthis book.
Wethanktheauthorsfortheirspecialeffortstomaketheirchapterseasytoread
andconsistentwithotherchapters. Crucialtothequalityofthisbookwerecomments
and criticism from the reviewers ofindividual chapters: JeffBecker, Kaihu Chen,
Jerry DeJong, Brian Falkenhainer, Bruce Katz, Heedong Ko, Steve Minton, Bob
Reinke, Robert Stepp, and Bryan Stout. We are indebted to the copy editor Kate
Engelberg forhelpingto smooth some sharpedges inthetext, andto Debi Place for
heradeptsecretarialassistance. ThanksgoalsotoMIT'sArtificialIntelligence Laboratory for providing support forthe firsteditor while he worked on this book.
Wepresentthisnewtextonmachinelearningtoits communityofreadersinthe
hopethatitwillprovetobeavaluablemessengerofprogress inthis importantfield.
-RyszardS. Michalski, Jaime G. Carbonell, Tom M. Mitchell
--- PAGE 15 ---
PART
ONE
GENERAL
ISSUES
--- PAGE 16 ---
--- PAGE 17 ---
UNDERSTANDING THE NATURE
OF LEARNING:
Issues and Research Directions
Ryszard S. Michalski*
Massachusetts Institute ofTechnology
Abstract
Thischapterpresents an overview ofgoals anddirections in machine learning
research and serves as a conceptual road map to other chapters. It investigates
intrinsic aspects of the learning process, classifies current lines of research, and
presentsthe author'sviewoftherelationshipamong learningparadigms, strategies,
and orientations.
DO WE NEED LEARNING MACHINES?
1.1
Artificial intelligence (AI) is now experiencing extraordinary growth, and
applications ofits ideas and methods are appearing in many fields. Among its most
visible and important successes are the development of expert systems, practical
implementationsofnaturallanguage-understandingsystems, significantadvancesin
computervision andspeech understanding, and new insights intobuilding powerful
inference systems. This rapid expansion ofactivities in AI leads one to believe that
new successes are forthcoming.
'OnleaveofabsencefromtheUniversityofIllinoisat Urbana-Champaign.
--- PAGE 18 ---
4 CHAPTER 1: UNDERSTANDINGTHE NATURE OF LEARNING
In this context, it is important to ask what the limitations of the current
methods are and what new directions research in this field should take. One ofthe
obvious limitations, and hence a direction for further research, relates to machine
learning-afieldconcerned withdevelopingcomputational theories oflearning and
constructing learning systems.
Exceptforexperimentalprogramsdevelopedinthecourseofmachinelearning
research, currentAIsystemshaveverylimitedlearningabilitiesornoneatall. Allof
their knowledge must be programmed into them. When they contain an error, they
cannotcorrectit ontheirown;theywillrepeatitendlessly, nomatterhowmanytimes
the procedure is executed. They can neither improve gradually with experience nor
learn domain knowledge by experimentation. They cannot automatically generate
their algorithms, formulate new abstractions, or develop new solutions by drawing
analogies to old ones, orthrough discovery. Generally speaking, these systems lack
the ability to draw inductive inferences from information given to them. One might
saythatalmostall currentAI systemsaredeductive, asthey are abletodrawconclusionsfromknowledgeincorporatedand/orsuppliedtothem, buttheycannotacquire
or generate new knowledge on theirown.
By contrast, when we look at human intelligence we see that among its most
strikingaspectsaretheabilitiestoacquirenewknowledge,tolearnnewskills, andto
improvewithpractice. Intime, useoftheselearningabilitiescanturnayoung, inexperienced person into a journeyman engineer, educator, artist, or physician. Our
commonperceptionisthatapersonwhowouldrepeatthesameerroragainandagain
could hardly be called intelligent. The ability to learn from error is considered
fundamental tothe individual andtothe society at large (Popper, 1959, 1981 ; Kuhn,
1970; Lakatos, 1970; Berkson and Wettersten, 1984; Hayes-Roth, 1983-Machine
Learning I, chap. 8, see below).
Becauselearningabilityissointimatelyentwinedwithintelligentbehaviorand
research inAIgivesusnew insightsandpowerfultoolstostudy it, many researchers
postulate that one of the new central goals for research in artificial intelligence
should be understandingthe nature oflearning and implementing learning capabilitiesinmachines(McCarthy, 1983; Schank, 1983). Overcomingtheabove-mentioned
limitations sets an agenda ofresearch tasks.
Questionsthenariseaboutwhetherthisgoal isachievable, and ifso, whether it
isdesirable. Let us start with the question ofachievability. Answering it involves us
immediately in questions ofdefinition. Can we identify some general criteria such
that, ifsatisfiedbyamachine, wewouldagreetocallthismachinealearning system?
Asmachinelearningresearchhasshown, learningabilitymanifests itselfnotas
an all-or-nothing quality but as a spectrum of information-processing activities,
ranging from theduvet memorizationoffactsandacquisitionofsimpleskillsby imitation to very intricate inferential processes leading to creation ofnew concepts and
discovery ofnew knowledge. Italways involvesachange inasystem, whetherhuman
Of machine, that makes it better in some sense.
--- PAGE 19 ---
MICHALSKI 5
For now, let us put the question ofthe definition oflearning aside (it is discussed in more detail in the next section ofthis chapter) and observe that machine
learningisexperiencingarenaissanceafteritspaststeadybutslowgrowth. Effortsto
developprogramsexhibitingsomeformsoflearninghavemultipliedinrecentyears.
Thisyoungfieldhasalreadyachievedanumberofsuccesses. Asummaryofsomeof
these efforts is found in Machine Learning: An Artificial Intelligence Approach
(Michalski, Carbonell, and Mitchell, 1983), henceforth referred to as Machine
LearningI. Thecurrentbookisasequel; itreportssomekeysubsequenteffortscharacteristic ofthe state-of-the-art in machine learning.
On the basis ofthe results achieved so far, it is clear that some rudimentary
machine learning abilities arepossible. Alreadythereexistprograms abletoformulate new concepts and discover previously unknown regularities in data; develop
decisionrulesthatcanoutperformhumanrules;drawinterestinganalogies; automaticallylearnproblem-solvingheuristics; ordevelopgeneralizedplansforachievinga
goal. ManyoftheseprogramsarediscussedinMachineLearningI. What islessclear
is the level ofprogress that can be achieved in machine learning using conventional
computer hardware and present programming methods. As always in science, such
questions can be answered only by conducting further research and continuing to
develop experimental learning systems.
New dimensions ofresearch in machine learning will open with the development of connection machines, fifth generation computer systems, and other novel
computerarchitectures, currentlyunderdevelopment(e.g., Hillis, 1981
Kawanobe,
1984). For example, Hinton, Sejnowski, and Ackley (1984) describe how learning
may occurinBoltzmannmachines. Theknowledgeacquiredby such systems is represented by the strengths ofthe connections between simple, neuron-like elements.
The research in this direction should address theproblemofovercoming the limitationsofearlysystemsofthistype, suchasthePerceptron(MinskyandPapert, 1969).
New potential forresearch in machinelearningalsoemerges inconnection withthe
developmentofnewprogramming systems, inparticular, logicprogrammingandits
firstembodiment in PROLOG (Robinson, 1983).
Why is itdesirabletodevelop learning machines? It appears thatthe developmentofsuch systemsisnecessarytoensurefurtherprogressinartificial intelligence
or closely related disciplines. This seems to be particularly true in areas such as
expert systems or any large-scale, knowledge-based systems; computer vision and
speech understanding; natural languageunderstanding; intelligenttutoring systems;
and(truly) friendlyhuman-machineinterfaces. Asmoreandmorecomplextasksare
set for AI systems, more and more knowledge must be represented in them. Such
knowledge must encompass domain-specific facts and rules, commonsense heuristicsandconstraints, andgeneralconceptsandtheoriesabouttheworld. Thescopeof
knowledge in any system must be widened to avoid a common problem with the
--- PAGE 20 ---
6 CHAPTER 1: UNDERSTANDING THE NATUREOF LEARNING
current systems, sometimes referred to as falling off the knowledge cliff(Feigenbaum, 1984) or brittleness (Holland, 1975, chap. 20; see also Larkin et al., 1985).
Theproblem isthatasystem performs well withinthe scopeofknowledge provided
to it, but any slight move outside its narrow competence causes the performance to
deteriorate rapidly.
Introducingalltherequiredknowledgeintoanynewsystemisaverycomplex,
time-consuming, anderror-proneprocess, requiring specialexpertise. Forexample,
buildinganexpertsysteminvolvesacollaborativeeffortofhighlytrainedexperts-at
leastonedomainexpertandoneknowledgeengineer(DavisandLenat, 1982; HayesRoth, Waterman, andLenat, 1983; BuchananandShortliffe, 1984). Thistaskcanbe
simplified by using machine learning techniques. Such techniques would enable a
systemtodevelopdecisionrulesfromexamplesofexperts' decisionsandthroughthe
automated analysis offacts in a database.
With the rapid increase in the amount ofdata and knowledge that the society
generates, there is a growing need not only for storing, organizing, and delivering
thisinformationbutalsoforusingitinnew, creativeways. Knowledgecanbeviewed
as compressed information (Rendell, 1983), and we now need machines that can
compressdatabasesandinformationsystemsintoknowledgebasesautomaticallyvia
conceptual analysis of their contents. As envisioned by Michie (1982), "the most
technicallygrippingchallenge, evenifnotimmediatelythemosteconomicallyimportant, willbehowtospreadthecomputerwavefromthefrontendofthescientificprocess, thetelescopes, microscopes,
. . .
sparkchambersandthelike, backtotherecognitionandreasoningprocessesbywhichthechaosofdataisfinallyconsolidatedinto
orderlydiscovery.
This chapter's author mightaddthatthe computerwill have a role not only as
scientists'and technologists' intelligent assistant but also as an intelligentpersonal
assistant. Individuals intheexpanding information society will need such assistants
to cope with the overwhelming amounts ofavailable information and the complexitiesofeverydaydecision making. Inorderforsuchassistantstoplay thedesignated
roles, their function and knowledge should by dynamic. These assistants should be
abletoadapttochangingdemandsandbeself-modifiable; thatis, theyshouldbeable
to learn.
A similarneed for learning abilitiesexists in the areas ofcomputervision and
speechunderstanding. Tobuildacomputervisionsystem, onehastoincorporate into
it a variety of vision-specific transformations; concepts ofgeometry; physical and
functionaldescriptionsofvisualobjectsthesystemistorecognize; and related information (e.g., Winstonetal., 1983; Winston, 1984). To "handcraft" all this information intoasystem isdifficult. It wouldbemucheasiertoteachthe systemby showing
it examples ofgiven concepts and have it learn the appropriate generalizations and
descriptions,just as we teach visual concepts to humans.
--- PAGE 21 ---
MICHALSKI 7
A systemcapableofunderstandingandinteractingwithhumansinnaturallanguage has to be equipped with knowledge of syntactic properties of language
(Marcus, 1980), as well as with many concepts and concept structures (such as
frames, scripts, and schemata) capturing semantic andpragmatic aspects ofthe language(Winograd, 1981 Schank, 1982; seealsochaps. 19and21 ofthisvolume). One
may estimatethatinanadvancednaturallanguageunderstander, thenumberofsuch
concepts and concept structures may easily reach tens ofthousands or more. Programmingall thisknowledge intoacomputerisamonumentaltask. Itisverydesirabletosimplifythistaskbyemployingalearningsystem. Inaddition,evenifatsome
point all this knowledge were incorporated in a machine, a language understander
wouldnotworkwell forlongwithoutlearningabilities. Themeaningofhumanconcepts isdynamic; itchangeswithtimeandadaptstonewcontextsandrequirements.
Novel concepts are continuously being created and developed, and some are being
outgrown. Therefore, as in the cases above, we need a learning system capable of
acquiring new concepts and concept structures by generalization from examples or
byanalogytopriorknowledge. Suchasystemshouldbeabletomodify, specialize,or
generalize old concepts in a flexible fashion.
Intelligenttutoring systems mustbeabletopresentmaterial atalevel ofdifficulty anddetail suitedtothe state ofknowledgeofthe student. Inordertodo so, the
system mustknowandfollowthe student'schangingknowledge. A desirablewayof
acquiring this information is not by repeated direct testing but by learning from
clues, behavior, andthe implicit model ofthe studentduring tutorial sessions. Thus
learning abilities are required not only from the student but from the tutor as well
(Sleeman and Brown, 1982; Sleeman, 1983-MachineLearningI, chap. 16).
Through learning capabilities future computers should be able to acquire
knowledgedirectlybyusingdocumentsandbooks, byconversingwithhumans, and
bygeneralizingobservationsoftheirenvironment, whichtheymakewiththeirartificial senses. Theyshouldbecapableofimprovingthroughpracticeandexperience. It
ispossiblethatfuturemachinelearningsystemswill sufferlittle, ifatall, fromsome
human limitations, such as poor memory, distracted attention, low efficiency, and
thedifficulty oftransferring acquiredknowledge fromone learnertoanother. Once
onelearning systemisdeveloped, atheoreticallylimitlessnumberofcopiesofitcan
be built, which, one hopes, can be employed to learn new knowledge in diverse
domains. In addition, any new knowledge acquired by a learning system can be
copied to other systems rapidly and relatively inexpensively (unlike human knowledge, which mustbe painstakingly reacquired by each new student).
Of course, we are still far away from such idealized vision, but it has now
becomeconceivablethatsuchlearningsystemsmightbedevelopedinthefuture.
Itis
thendesirabletoconsidernotonlyexpectedadvantagesbutalsopossibleundesirable
consequences. The latter issue could be dismissed by observing that any new technologybrings new opportunities formisuse, andthatthishas neverstoppedus from
developingit. Moreover, suchaspectsareusuallyconsideredanissueoutsidescientific
--- PAGE 22 ---
8 CHAPTER 1: UNDERSTANDING THE NATUREOF LEARNING
ortechnical research. Yet we need to examine this particular issue carefully, forthe
creation ofmachines that can self-acquire knowledge brings about new dimensions
ofcomplexity in the development oftechnology and reflects on the way the field of
machine learning should be developed.
The first dimension of complexity is the predictive opacity of self-changing
systems. Predicting the behaviorofmachines that can learn inductively is considerably moredifficultthanpredictingthebehaviorofmachineswithoutsuchanability.
The key idea behind learning machines is that they should be able to create knowledge that can surprise their human creators. This might cause unexpected difficulties, orevendangers, ifsomeonewouldapply suchasystemtosolveimportantproblems without understanding the system's limitations. In addition, the increased
unpredictability of learning machines implies increased possibilities for their
misuse.
Some experts argue that predicting behavior ofcomplex computer systems is
very difficultalready. They look at the addition oflearning capabilities to ourcomputers as further amplification ofthese difficulties, but not as a quantum leap to a
new state. Whether we see a leap or merely an amplification ofunpredictability, a
strongexpectationisthatpotentialbenefitsfromthistechnologywillamplycompensate for such undesirable consequences. And with regard to the increased potential
foritsmisuse, whynotusethesesmartlearningmachinesto "police" othermachines
to prevent or combat attempted misapplication?
In addition to the difficulty ofpredicting the behavior oflearning machines,
another dimension must be considered, which stems from the very nature of any
knowledgeotherthanfactualobservation. AshasbeenobservedbyHume (see, e.g.
1888) and later by Popper (1959) and others, such knowledge is inherently conjectural;thatis, any knowledgecreatedbygeneralizationfromspecificobservationsor
by analogy to known facts cannot in principle be proven correct, though it may be
disproven.
This results from the fact that inductive inference is not truthpreserving but
onlyfalsitypreserving(Michalski, 1983). Asanillustration, considerthisstatement:
"All scientistsatMIT'sAILaboratoryarebright." Adeductiveconclusion fromthis
statementcanbethatRogerLight, whoworksattheAILaboratory, mustbebright. If
the original premise is true, then this conclusion must be true also. An example of
inductive inferencefromtheinitialpremisemightbethisstatement: *'A11 scientistsat
MITare bright." In thiscase, even iftheoriginal premise weretrue, such an inductive conclusion might not be. However, if the original premise is false, then this
inductive conclusion must be false also. Thus, in contrast with a deductive system,
correct inputstoan inductive systemdonot guarantee thecorrectness ofthe outputs.
Moreover, for any given input there is theoretically an infinite number of possible
inductive conclusions. The ones we actually make reflect the preferences, assumptions, andconstraintsthat we use in formulatingourgeneralizations(Medin. Wattenmaker, and Michalski. 1985; Utgoff, chap. 5),
--- PAGE 23 ---
MICHALSKI 9
Fortheabovereasons, iflearningmachinesaretogenerateknowledgeusefulto
us, it is important that they be equipped with knowledge ofall the relevant human
constraintsandassumptions. As it isunlikelythatallsubtlehumanandsocietalconstraintsandpreferenceswilleverbemadeknowntomachines, thereisthepossibility
that machine-generated knowledge will violate some human constraints. A quote
from Hofstadter (1980) is pertinent here: "Unless [the program] hadan amazingly
faithfulreplicaofhuman body . . . it wouldprobablyhaveenormouslydifferentperspectiveson whatisimportant, whatwasinteresting, etc."Becausetheperceptionof
what is important and what is interesting is anecessary component inguiding creation ofnew knowledge (Lenat, 1983), such differences are significant. Thus when
suchmachine-createdknowledgeisused,
mayleadtosolutionsthataretechnically
flawless but socially undesirable.
A related concern is that people may give too much credibility to the knowledge created by machines. This phenomenon has already been observed in related
contexts, forexample, whenpeopleareundulyinfluencedbyresultsofcomputerstatistical analysis without clearly understanding its assumptions, or when people
ascribe personality to a computer consultation system, as in the case of ELIZA
(Weizenbaum, 1976). Furthermore, even ifit may be well known to scientists that
inductively generated knowledge is inherently error-prone, this fact may be less
obvious to nonexperts.
An important implication ofthe above discussion is that any new knowledge
generatedby machinesshouldbesubjectedtoclosehumanscrutinybeforeitisused.
This suggests an important goal for research in machine learning: Ifpeople have to
understand and validate machine-generated knowledge, then machine learning systems shouldbe equippedwith adequate explanationfacilities. Furthermore, knowledge created by machines should be expressed in forms closely corresponding to
human descriptions and mental models ofthis knowledge; that is, such knowledge
should satisfy what this author calls the comprehensibilityprinciple (Michalski,
1983). When designing explanation capabilities for learning systems, one should
strivetofacilitatehumanunderstandingnotonlyofthesurfaceresultsbutalsoofthe
underlying principles, assumptions, andtheories that lead to these results.
Onemayhypothesizethatalthoughtheexistenceofadvancedlearningmachines
would eliminate the current knowlege acquisition bottleneck, it could ultimately
createaknowledgeratificationbottleneck. Inthis situation somuchnewknowledge
might be generated by machines that itcould become difficult forhuman experts to
test and approve it. Shouldthis happen well, future researchers will have an interesting problem with which to while away their idle hours. One may envision these
researchers inventing sophisticated learning machines that would design experiments to test knowledge created by other sophisticated learning machines.
Withthesenotesofconcern, mixedwithargumentsstressingtheimportanceof
machine learning, let us now look more closely at the intrinsic properties of the
learning process.
--- PAGE 24 ---
: "
10 CHAPTER 1: UNDERSTANDING THE NATURE OF LEARNING
WHAT LEARNING?
1.2 IS
As mentioned earlier, a common view holds that learning involves making
changes in the system that will improve it in some way. In this description, the term
improveneeds moreprecision. Clearly, wineimproves with time, butnobody would
call such an improvement learning.' Simon (1983-Machine Learning I, chap. 2)
gives a more precise characterization:
''Learningdenoteschangesinthesystemthatareadaptiveinthesensethatthey
enable thesystem todothesametaskortasksdrawnfrom thesamepopulation
moreeffectivelythenexttime.
The requirementthatasystem improve performance forlearning totake place
iswidelyaccepted. Thereare, however, activitiesthatcanbecategorizedaslearning,
inwhichtheimprovementcriterionisdifficulttoapply(aswillbeseeninadiscussion
below). Minsky (1985) in his insightful theory of thinking, The Society ofMind,
replaces this criterion with a more general one requiring that changes are merely
useful
"Learningismaking usefulchanges in ourminds."
He subsequently observes that such a definition is too broad to be ofany use.
Letusthenapproachtheproblemofcapturingthefundamentalaspectsoflearningin
anotherway. It maybeobservedthatlearningisoftenequated simply withacquiring
newknowledge, as in the statement: "As the satellite burned in the atmosphere, the
spacelab astronaut learnedthat the satellite had an auxiliary antenna." In this case,
theastronaut simply acquiredapieceofinformation, butthis will never improve his
performance with this satellite. Theknowledgeacquisition aspect oflearning seems
to be the essence ofmost learning acts. Those acts where it appears to play only a
small role are cases ofwhat is usually termed skill acquisition. The latter refers to
gradual improvementofmotororcognitiveskillsthroughrepeatedeffort, sometimes
involving littleornoconscious thought (Carbonell, Michalski and Mitchell. 1983MachineLearningI , chap. 1). Inthisdiscussion, however, wewillconcentrateonthe
knowledge acquisition aspect oflearning, a theme that recurs throughout the book.
Inordertoacquireknowledgeofanything, one, obviously, hastorepresentthis
knowledge in some form, whether as declarative statements, procedures, a mixture
ofthe two, orassomethingelse (McCarthy, 1968). This fact and the aboveconsiderations lead us to the following characterization oflearning:
Learning is constructing <>r modifying representations ofwhat is being
experienced.
Iinscounterexample was suggested b) SteveTanimoto from the University ofWashington in Seattle
--- PAGE 25 ---
MICHALSKI 11
„ Theconceptofexperienceincludeshereanysensorystimuli, aswellasinternal
Gedanken processes. These stimuli and internal processes are the vehicles through
which the learning system perceives the reality that it is trying to represent. The
internal thought processes can themselves be a subject oflearning.
Thus, fromtheaboveview, thecentral aspectoflearning istheprocessofconstructing a representation ofsome reality, ratherthan improving performance. Performance improvement is considered to be a consequence and often the purpose of
buildingthe representation, butitcanbeassertedonly inthecontextofthe learner's
goals. Because most learning acts indeed involve improvement ofperformance and
because itiseasiertomeasureperformancethantoreadminds, naturallywelinkthe
two. Yet, performance improvementdoes not seemtobe an invariable condition for
everyactoflearning. Therearesituationsinwhich itdoesnotappeartobeofprimary
relevance, as in learning to appreciate beauty. There are also situations in which it
may evenbe misleading. Thelattersituationsoccurwhen itis difficulttoaccurately
assess the learner's goal. For example, workers in a labor camp may want to learn
how todolessandappeartodomore, yetthey keepthis goal secret. Fromtheviewpoint ofan external observer, these workers will appear not to be learning, as their
performance will be decreasing with practice. Thus it seems clearthattodetermine
learning by measuring performance may notbe possible without knowing the goals
ofthe learner.
Three dimensions seem to be particularly important for evaluating the constructed representations: validity, effectiveness, and abstraction level. Validity (or
truthfulness) refers to the degree ofaccuracy with which the representation fits the
reality. Itcharacterizestheprecisionofthemappingbetweentherealityandtherepresentation. Thesecondcriterion,effectiveness, attemptstocapturetheperformance
aspectoflearning. Itcharacterizestheusefulnessoftherepresentationforachieving
agivenpurposeorgoal. Themoreeffectivetherepresentation, thebettertheperformance ofthe system. Thusthiscriterion iscentral fortasks inwhichperformanceis
ofprimaryconcern. Thethirdcriterion, abstractionlevel, reflectsthe scope, detail,
andprecisionofconceptsusedinthedescription. Itdefinestheexplanatorypowerof
the representation. These three dimensions together determine what may be called
the qualityoflearning.
The representations can be in the form ofsymbolic descriptions, algorithms,
simulation models, controlprocedures, plans, images, orgeneral formaltheories. If
one stretches the concept of representation to include physical or physiological
imprints occurring in the nervous system when one is acquiring a skill, the above
view oflearning seems alsotocover skill acquisition.
From this viewpoint, a fundamental problem in any research on machine
learningconcernsthe formandmethodusedtorepresentandmodifytheknowledge
ortheskillbeingacquired. Withregardtothequestionofmodifyingknowledge, itis
importanttoidentifythecomponentsandthepropertiesoftherepresentationthatare
modifiable by the system and those that are not.
--- PAGE 26 ---
12 CHAPTER 1: UNDERSTANDINGTHE NATUREOF LEARNING
In the taxonomy ofmachine learning research given in chapter 1 (Carbonell,
Michalski, and Mitchell) ofMachine Learning I, three criteria were suggested as
especially useful for classifying and comparing machine learning investigations:
learningstrategy, knowledge representation, andapplication domain. The learning
strategyreferstothetypeofinferenceemployedbythesystemduringlearning. Some
additional ideas reflecting recent progress on this topic are presented in section 1.4
below. The criteria ofknowledge representation and application domain were well
covered in the above-mentioned reference and will be omitted here. Instead, two
other classification criteria will be discussed in some detail: research paradigms
(section 1.3)andlearningorientations(section 1.5). Theresearchparadigmcriterion
refers heretothe approach takentoconstructa system, andthe learning orientation
refers to the scope and the subject ofstudy.
RESEARCH PARADIGMS
1.3
Since the inception of machine learning in the fifties, research efforts have
placedtheemphasisatdifferenttimesondifferentapproachesandgoals. Onecandistinguishthreemajorresearchparadigmsorapproaches inthis area: neuralmodeling
and decision theoretic techniques; symbolic concept acquisition; and knowledgeintensive, domain-specificlearning. These research approaches differchiefly in the
amounts ofa priori knowledge builtinto the learning system and in the way knowledge is representedand modifiedin the system.
Theneuralmodelingapproachstrivestodevelopgeneral-purposelearningsystems that start with little initial knowledge. Such systems are usually referred to as
neuralnetsorself-organizingsystems. A systemofthistypeconsistsofanetworkof
interconnected elements, typically neuron-like, that perform some simple logical
function, usually a threshold logic function. Such a system learns by incrementally
modifying the connection strengths between the elements, typically by changing
continuous (i.e., non-discrete) weights associated with these connections. The system's initial knowledgeisprovidedbythechoiceofthe inputelementsthat represent
selected attributes of objects under consideration and by the structure and initial
strength oftheconnections inthe network. Thiscanbe a random structure, one prearrangedby thedesigner, ora mixtureofthetwo. Such learning systems include the
Perceptron (Rosenblatt, 1958), Pandemonium (Selfridge, 1959), and any learning
machine usingdiscriminantfunctions (Nilsson, 1965). More recent examples stemming from this paradigm are various adaptive control systems (Tsypkin, 1972:
Caianiello and Musso, 1984). Research in this area has led to the decision-theoretic
approach inpattern recognition. Relatedtothisapproachisresearchonevolutionary
learning (Fogel, Owens, andWalsh, 1966; Conrad, 1983) andongeneticalgorithms
(Holland, 1975; see also chap. 20). As mentioned earlier, there is a resurgence of
interest in this learning paradigm with the recent efforts to develop connection
machines (Hinton, Sejnowski, and Ackley, 1984).
--- PAGE 27 ---
MICHALSKI 13
Characteristic featuresofsystemsbuiltunderthisparadigm includelowlevels
ofapriori built-in knowledge andthe use ofcontinuously changeable parametersto
achieve learning. A related feature is the numerical character oflearning methods
and algorithms. This strongly contrasts this paradigm with the next two paradigms,
inwhichthemainemphasisisoncreatingandmanipulatingcomplexsymbolicstructures during the process oflearning.
In symbolic concept acquisition (SCA), the system learns by constructing a
symbolic representation ofa given set ofconcepts through the analysis ofexamples
andcounterexamplesoftheseconcepts. Therepresentationstypicallyareintheform
of a logical expression, a decision tree, production rules, or a semantic network.
Some ofthe systemsdevelopedunderthis paradigmhave multipurpose applicability
andhavedemonstratedpracticalusefulness. ExamplesofsuchsystemsareWinston's
ARCH program (Winston, 1975), the AQVAL program (Michalski, 1975), and ID3
(Quinlan, 1979). Inthisparadigm, theattributesorpredicatesrelevanttotheconcept
are provided to the system by the teacher.
Inknowledge-intensive, domain-specificlearning (KDL), the systemcontains
numerouspredefinedconcepts, knowledge structures, domainconstraints, heuristic
rules, and built-in transformations relevant to the specific domain for which the
system is built. Not all the relevant attributes or concepts are proved initially; the
systemisexpectedtoderivenewones intheprocessoflearning (thisauthorrefersto
such a process as constructive induction). Thus the main differences between the
KDL and SCA paradigms lie in the amount and the kind ofbackground knowledge
supplied to the system and the richness of knowledge structures generated by the
system. Learning systems based on this approach are typically developed for a
specific domainandcannotbeuseddirectly in anotherdomain. The research inthis
paradigmhasexplorednotonlythestrategyoflearningfromexamples, butalsostrategies such as learning by analogy, and learning by observation anddiscovery (see
section 1.4). Examples of systems based on this approach are Meta-DENDRAL
(Buchanan and Feigenbaum, 1978) and AM (Lenat, 1983-Machine Learning I,
chap. 9).
Many systems developed in the past represent a certain mixture ofthe abovementionedapproaches. AninterestingcombinationoftheSCAandKDL approaches
is a system based on the idea ofan exchangeable knovAedge module. Such a system
combines general-purpose learning mechanisms with built-in facilities fordefining
and using domain-specific knowledge. When such a system is applied to a given
problem, domain-specific knowledge is suppliedtoitbytheteacherviathe system's
knowledge representation facilities. By separating general inference capabilities
fromthedomain-specificknowledge, suchalearningsystemcanbeappliedtoawide
spectrumofdifferentdomainsandstilltakeadvantageofdomain-specificknowledge
in the process oflearning. This philosophy underlies the INDUCE system, which
learnsstructuraldescriptionsofobjectsfromexamples(Michalski, 1980). Winston's
program for learning by analogy is another example (Winston, 1982). The LEX
--- PAGE 28 ---
14 CHAPTER 1: UNDERSTANDINGTHE NATUREOF LEARNING
systemforacquiring andrefiningproblem-solvingheuristics (Mitchell, Utgoff, and
Banerji, 1983-MachineLearningI, chap. 6)andtheEURISKOprogramfordiscovering new heuristics (Lenat, 1983) are other examples. Several chapters in this
volume describe learning methods that also fall into this category.
Forahistorical reviewofthesethree researchparadigmsthereaderis referred
to chapter 1 in Machine Learning I. A sample of contemporary research on selforganizing systems is found in Caianiello and Musso (1984). A recent review of
approaches to machine learning has been made by Langley and Carbonell (1984).
Theprimaryconcernsofthisbookaresymbolicconceptsacquisitionandknowledgeintensive, domain-specific paradigms oflearning.
1.4 LEARNING STRATEGIES
In every learning situation, the learner transforms information provided by a
teacher(orenvironment)intosomenewforminwhich itisstoredforfutureuse. The
nature ofthistransformationdetermines the type oflearning strategy used. Several
basic strategies have been distinguished: rote learning, learning by instruction,
learning by deduction, learning by analogy, and learning by induction. The latter
subdivides into learningfrom examples and learning byobservation anddiscovery.
These strategies are ordered by the increasing complexity of the transformation
(inference) from the information initially provided to the knowledge ultimately
acquired. Theirorderthusreflectsincreasingeffortonthepartofthestudentandcorrespondingly decreasing effort on the part of the teacher. In any act of human
learning, a mixture ofthese strategies is usually involved. It is useful to distinguish
these strategies not only for tutorial purposes but for the purpose of designing
learning systems as well. Though most current systems focus on a single learning
strategy, one may expect that machine learning research will give increasing attention to multistrategy systems. Chapter 1 of Machine Learning I describes these
learningstrategiesindetail. Becauseoftheirimportancetothisbookandbecauseof
some changes in their classification brought about by recent research, they will be
reviewed briefly here.
In rote learningthere isbasically notransformation; the information from the
teacherismoreorlessdirectlyacceptedandmemorizedbythelearner. Amajorconcern here is how to index the stored knowledge for future retrieval. In learning by
instruction (or learning by being told), the basic transformations performed by a
learner are selection and reformulation (mainly at a syntactic level) of information
provided by the teacher. In deductive learning, the learner draws deductive, truthpreserving inferences from the knowledge given and stores useful conclusions (this
strategy was identified as a separate category only recently; see Michalski. 1983,
1985). Deductive learning includes knowledge reformulation, knowledge compilation, creation of macro-operators, caching, chunking, equivalence-preserving
operationalization, and other truth-preserving transformations (see Glossary).
--- PAGE 29 ---
MICHALSKI 15
Ifthetransformationprocess involvesgeneralization ofinput informationand
selection ofthe most plausible or desirable result, that is, the inductive inference,
then we have inductive learning. Learning by analogy is deductive and inductive
learningcombined. Here, descriptionsfromdifferentdomainsarematchedtodetermine a common substructure, which serves as the basis for analogical mapping.
Findingthecommonsubstructureinvolvesinductiveinference, whereasperforming
analogical mapping is a form ofdeduction. Learning bybeing reminded, described
by Schank (1982), can be viewed as a form of learning by analogy. Learning by
analogy isdiscussedinchapters 13 (Burstein), 14 (Carbonell), and 15 (Dershowitz).
Inductivelearningcanbesubdividedintolearningfromexamplesandlearning
by observationanddiscovery. In learningfromexamples (alsocalledconceptacquisition), thetaskistodetermineageneraldescriptionexplainingallpositiveexamples
andexcludingallnegativeexamplesofthetargetconcept. Theexamplesareprovided
by a source ofinformation, which can be a teacher who knows the concept or the
environmenton whichthestudentperformsexperimentsand from which itreceives
feedback. The latter case is called learning by experimentation (this includes
learningbydoingand learningbyproblemsolving). Stimulus-responselearningcan
also be classified as a form oflearning from examples.
Recent research has revealed two interesting subdivisions within this form of
learning: instance-to-class and part-to-whole generalization. In instance-to-class
generalization,thesystemisgivenindependentinstances(examples)ofsomeclassof
objects, and the goal is to induce a general description ofthe class. Most research
doneonlearning fromexampleshasconcentratedon such instance-to-classgeneralization. The objects can be structured blocks, geometrical shapes, descriptions of
diseases, stories, problemsolutions, controloperators, andsoforth. Variousaspects
of this problem are discussed in chapters 3 (Winston), 5 (Utgoff), 6 (Quinlan),
7(SammutandBanerji), 8(Lebowitz), and9(KodratoffandGanascia). Forareview
of earlier methods for such generalization, see Dietterich and Michalski (1983MachineLearning I, chap. 3) and Cohen and Feigenbaum (1982).
In part-to-whole generalization, the task is to hypothesize a description ofa
whole object (scene, situation, process), given selected parts of it. For example,
givenacollectionofsnapshotsofselectedpartsofaroom, reconstructthetotalviewof
thatroom. Anotherexampleistodeterminearule(atheory)characterizingasequence
ofobjectsoraprocessfromseeingonlyapartofthissequenceorprocess. Thistypeof
learning problem is considered in chapter 4 (Dietterich and Michalski). A closely
relatedareaofresearchconcernsthequalitativeprocessprediction(Michalski,Ko, and
Chen, 1985).
In learning by observation and discovery (also called descriptive generalization), one searches, without the help ofa teacher, for regularities and general rules
explaining all orat least most observations. This form oflearning includes conceptualclustering(formingobjectclassesdescribablebysimpleconcepts), constructing
classifications, fitting equations to data, discovering laws explaining a set of
--- PAGE 30 ---
16 CHAPTER 1: UNDERSTANDING THE NATURE OF LEARNING
observations, and formulating theories accounting for the behavior of a system.
Geneticalgorithms (Holland, chap. 20) and empiricalprediction algorithms (Zagoruiko, 1976)canbeviewedasvariantsofthislearningstrategy. Variousaspectsofthis
strategy are discussed in chapters 16 (Langley et al.), 17 (Stepp and Michalski),
18 (Amarel), and 19 (DeJong).
Theprimaryfocusofthisbookisonlearningbyinductionandanalogy. Therefore, a few additional comments may be useful about inductive inference, which
is at the heart of these strategies. Inductive inference starts with a set of facts
(observations)-and optionally with an a priori hypothesis about these facts-and
producesapreferredgeneralizationexplainingthesefacts. Asmentionedbefore, itis
a falsity-preserving inference accomplished by the application ofgeneralization
inference rules (Michalski, 1983a). As noted by Popper (1981) and others, "pure"
induction, that is, direct inference from facts to theories without any interpretive
(explanatory) concepts, is impossible. These concepts are needed to describe the
observations and are part ofthe learner's backgroundknowledge. This background
knowledgeisanecessary componentofany inductiveprocess. Italsoincludes goals
oflearning, domain-specific constraints, causal relationships, heuristics and biases
that guide the generalization process, and the criteria for evaluating competing
hypotheses.
One can distinguish two techniques for guiding and constraining generalization: thesimilarity-basedandtheconstraint-basedtechniques. The similarity-based
techniqueexploresinter-examplerelationships; thatis, it examinestheexamplesand
counterexamplesofaconceptinordertocreateaconceptdescription. Itsearches for
features shared among facts or examples in the same class and looks for common
causesandexplanationsofwhydifferentexamplesbelongtothesameclass. Itgeneralizesoverthedifferencesbetweenexampleseitherbyignoringthedifferingfeatures
or by formulating concepts that encompass the differences. Some early learning
methods using this technique are reviewed by Dietterich and Michalski in chapter 3
ofMachineLearning
The constraint-based technique exploits the intra-example relationships,
whichconstrainthe interpretiveorexplanatoryconceptsappliedtooneormorefacts
or examples. Any generalization ofthese facts or examples must satisfy these constraints. Forexample, whengeneralizingthefactthataboxisonthetable, oneshould
satisfy the constraint that whatever is on the table cannot be so heavy that it would
break the table or so large that it could not be placed on the table. A variant ofthis
technique isdescribedby Andreae (1984), whousestheconceptofjustification fora
hypothesis. Another important variant, called an explanation-basedgeneralization,
putstheemphasisontheroleofexplanatory knowledge(Mitchell. Keller, and KedarCabelli, 1986). Itappliesasystem'sbackground knowledgeto formulate a high-level
conceptual explanation or interpretation of a given tact or event. In chapter 19.
DeJong discusses a method implementing such a technique in the context ofStOT)
--- PAGE 31 ---
MICHALSKI 17
understanding. The similarity-based and constraint-based techniques are complementary and can be used simultaneously in learning systems.
1.5 LEARNING ORIENTATIONS
The previous two sections discussed two important classifying criteria for
machinelearningresearch: learningparadigmsandlearningstrategies, respectively.
To recapitulate, the first criterion concerns the type ofknowledge represented and
manipulated in the system, andthe secondcriteriondeals with the type ofinference
performedonthe knowledge. This section will briefly discuss one more classifying
criterion,theresearchorientation, whichconcernsthescopeandsubjectofstudy. By
analogy, aparadigmcorrespondstoone'spointofdeparture andtheterrainthrough
which one travels, astrategy specifies the means oflocomotion, and an orientation
indicates the destination.
Asdescribedinchapter
ofMachineLearningI, researchinmachinelearning
encompasses three interconnected orientations:
• Theoretical analysis and development ofgeneral learning algorithms
• The development ofcomputational models ofhuman learning processes (also
called cognitive modeling)
• Task-oriented studies concerned with building learning systems for specific
applications (alsocalled an engineering orientation)
Research in the first orientation investigates theoretical learning tasks, or
simplifiedpracticalones, andtriestodevelopalgorithmsthataccomplishthesetasks
independently ofapplication. There is no restrictiononthe type ofalgorithmdeveloped. Thealgorithmneednotbesimilartotheoneahumanmightusetoperformthe
giventask. As avariation, some authors postulatethatat leastthe knowledge structuresgeneratedasanendresultoflearning shouldbe similartothoseahumanbeing
might create, although the process oftheir creation can be different (Michalski,
1983a). Inthisorientationresearchersstrivetochartthetheoreticalspaceofpossible
learning algorithms. Chapters 3 (Winston), 5 (Utgoff), 7 (Sammut and Banerji),
and 9 (Kodratoff and Ganascia) represent a sample of work representative of this
orientation.
Inthe secondorientation, humanlearningisthefocus, andthedevelopmentof
computationaltheoriesandexperimentalmodelsofhumanlearningisthegoal. This
research will likely have important influence on human education as well as on the
techniques of implementing machine learning systems. Chapters 10 (Rosenbloom
andNewell), 11 (Anderson), and 14(Carbonell)arecharacteristicofthisorientation.
Finally, work in the third orientation undertakes specific practical learning
tasksandtriestodevelopengineeringsystemscapableofperformingthesetasks. An
example here would be a program that learns to recognize dangerous conditions for
aircraft in flight. Such efforts usually have to address a host ofother problems not
--- PAGE 32 ---
18 CHAPTER 1: UNDERSTANDING THE NATURE OF LEARNING
directly relatedtolearning, suchastheappropriateinterpretationoftheinputsignals
orthedevelopmentofproblem-specifictransformationsofthedata. Anyuseful ideas
fromtheothertwoorientationsarereadilyadoptedinthisorientation. Often, whena
solution to a specific problem is found, it is generalized to a method for solving a
classofsimilarproblems. AnexampleofsuchresearchisdescribedbyDietterichand
Michalski in chapter4.
Theabovethreeresearchorientationsmakeupatrichotomyofmutuallydependent and supportive efforts that fuel the machinery oflearning research. Such atrichotomy has come to pervade the whole ofartificial intelligence.
1.6 READER'S GUIDE TO THIS BOOK
Asindicatedinsections 1.3and 1.4above, thisbookisconcernedwiththe SCA
(symbolic concept acquisition) and the KDL (knowledge-intensive domain-specific
learning) paradigms and concentrates on inductive and analogical learning strategies. Both major types of inductive learning-that is, learning from examples and
learning by observation and discovery-are represented. The chapters are grouped
into six parts reflecting the major learning strategy or the research orientation
employed in the work.
PartOneprovidesanintroductionanddiscussionofgeneral issues inthe field
ofmachinelearning. Aftertheoverviewpresentedinthischapter, viewsfromseveral
researchersonimportantproblemsinthisfieldforthedecadeoftheeightiesarepresented in chapter 2. These topics emerged from a panel discussion held at the 2nd
International MachineLearningWorkshopatthe University ofIllinois inJune 1983
(Michalski, 1983b).
Part Two describes a selection of results on learningfrom examples. In
chapter3, Winston integrates ideas about several interrelated topics: learning from
precedents and exercises, using near misses in learning, generalizing if-then rules,
andemployingunlessconditionstopreventincorrectruleapplication. The roleofan
unless condition is to block a given if-then rule whenever facts at hand satisfy this
condition. Such a condition facilitates an incremental improvement ofrules.
In chapter 4, Dietterich and Michalski present a theoretical framework and
methodology foracertaintypeofpart-to-wholegeneralization. Theydescribeageneral methodusingthreemodelsfordiscoveringarulethatcharacterizesasequenceof
objectsandpredictsaplausiblesequencecontinuation. Eachobject inthe sequence is
described by discrete attributes, which are either given a priori or derived b)
applying various inference rules and sequence transformations.
Utgoffillchapter5 investigatesthe roleofbiasorpreferencecriterion indeterminingaplausiblehypothesis in inductive learning. Hepresentsamethodology anda
program STABB- for shifting bias in the course o\'learning from examples.
--- PAGE 33 ---
MICHALSKI 19
Inchapter6, Quinlanexaminestheeffectofnoise intrainingexamplesonthe
discovery of classification rules and their accuracy. He makes several interesting
conjectures about how to formulate the learning task when training examples are
expected to contain noise.
Next, in chapter 7, Sammut and Banerji investigate the role of previously
learned concepts in the learning ofnew ones and the problem ofinductive learning
withanactivelearner. Suchalearnerisnotjustpassivelyacceptingexamplesfroma
teacher but is also generating examples on its own and asking the teacher whether
they represent the concept being learned.
Inchapter8, Lebowitzdiscussesasomewhatrelatedproblem. Heexploresthe
use ofconcepts stored in the memory for generalizing complex structural descriptions. His Generalization-BasedMemorymethoddetermineswhatconceptstolearn
andformulatesdefinitionsoftheconceptslearned. Theideasareexemplifiedbytwo
programs, oneforconceptevaluation, theotherforgeneralizationofcomplex structural descriptions.
Next, inchapter9, KodratoffandGanasciadiscussvarioustheoreticalaspects
of the generalization process. They show how generalization is accomplished by
creating links among training examples. These links are represented as variable
bindings.
PartThreetakesupcognitiveaspectsoflearning. Inchapter 10, Rosenbloom
and Newell present ideas about modeling processes that underlie improvement of
performancebypractice. Theirmodelofpracticeisbasedontheconceptofchunking,
that is, grouping subgoals intohighergoals. They show thatthis model explains the
knownpowerlawofhuman practice.
Next, in chapter 11, Anderson discusses learning mechanisms involved in
knowledgecompilation, thatis, intheprocessbywhichsubjectsmovefromadeclarative representation ofa skill to aprocedural representation. He shows how mechanismsofcomposition (collapsing multiple productions into a singleproduction) and
proceduralization (building intoproductions information that resides in declarative
form in the long-term memory) can simulate the initial stages ofskill acquisition in
the domain oflearning how to program.
In chapter 12, Forbus and Gentner present their work on a computational
model ofhuman learning ofphysical domains. They use QualitativeProcess Theory
to model human physical knowledge and Structure Mapping Theory, which characterizesanalogyandothercomparisons,todescribeprocessesofchangingknowledge
representations.
PartFourfocusesonthetopicoflearningbyanalogy. Burstein, inchapter 13,
presentsamodeloflearningbyanalogical reasoning. Hedescribesitinthecontextof
acquiring the semantics ofassignment statements in the BASIC programming language. According to his model, the use of analogies to learn concepts in a new
--- PAGE 34 ---
20 CHAPTER 1: UNDERSTANDING THE NATUREOF LEARNING
domain depends strongly on causal abstractions previously formed in a familiar
domain. These analogies are extended incrementally to handle related situations.
In chapter 14, Carbonell presents his theory ofderivational analogy and its
implicationsforcase-basedreasoningandexpertiseacquisition. Inessence, thederivation of solutions to related problems is replayed and modified to solve new and
increasingly more complex problems. The method is proposed as a means ofautomating knowledge and skill acquisition forexpert systems.
Dershowitz, inchapter15,focusesonanalogyasatoolforautomaticprogramming. He shows how analogies between program specifications (as well as between
theirderivations)canbeusedtodebugaprogramortomodifyanexistingprogramto
performanewtask. Theseanalogiescanalsobeusedtoderiveanabstractschemaof
a set ofprograms andto instantiate a schema in orderto yield aparticular program.
Part Five covers learning by observation and discovery. In chapter 16,
Langley, Zytkow, Simon, and Bradshaw describe four systems addressing different
aspects ofscientific discovery. BACON.6 formulates empirical laws characterizing
any numerical observational data. GLAUBER takesondiscoveryofqualitative laws
ofchemical reactions. STAHL undertakes the problem ofdetermining components
ofsubstances involved in such reactions. Finally, DALTON focuses onthe formulation ofstructural models forthese reactions.
Inchapter 17, Steppand Michalski reportontheir recentworkon conceptual
clustering, that is, creatingaclassificationofobservationsby identifying subclasses
that correspond to simple concepts. Unlike previous work on generating goal-free
classificationsofunstructuredobjects, thenew researchtakesontheconstructionof
goal-oriented classifications of structured objects. The authors describe and illustrate by examples how a learner's concepts and inference rules are used in constructing such purposive classifications.
Inchapter 18,Amareldiscussesproblemsoftheoryformation inthecontextof
program synthesis. He illustrates his method and ideas by a problem of inferring a
programfrominput-outputdataassociationsinthedomainofpartiallyorderedstructures. His method emphasizes the role ofalgebraic and geometric models and the
importance ofshifting problem representations in the program synthesis task.
Taking a different tack, DeJong in chapter 19 discusses a method o\'learning
from observation that exploits the inner constraints among explanatory concepts in
the system's background knowledge to guide the process of generalization from a
single example. His examples are stories about people's problem-solving behavior.
This knowledge-based generalization process is used to propose new schemata.
I*art Sixexploressomegeneral aspectsoflearning. Inchapter20, Hollanddiscusses general-purpose learning algorithms based on a parallel rule-based system
architecture. He advances the theme that inductive processes in such rule-based
--- PAGE 35 ---
MICHALSKI 21
systemsareawayofovercomingthebrittlenessofcurrentAIsystems, whichisdueto
the narrow scope oftheir domain-specific knowledge.
In chapter 21, Berwick explores the issues ofgeneral constraints underlying
processes ofnatural language acquisition. He discusses the relative importance of
general, domain-independent learning principles versus domain-specific learning,
and presents the subset principle for guiding generalization from positive-only
examples.
Finally, in chapter22, Silverdescribes alearningtechnique calledPreconditionAnalysisthatallows aprogramtolearn strategies forproblem solving. He illustrates his method with examples from the domain ofalgebraic equations.
Thebookconcludeswith abibliography ofresearch inmachinelearning since
1980, with a few major landmarks representing earlier research. (A comprehensive
bibliography ofprevious research inthis fieldcanbe found inMachineLearningI.)
Thebibliography is indexedby underlying learning strategy, domain ofapplication,
andresearchmethodology. Anupdatedglossaryofterms inmachinelearningisalso
provided, as well as a bibliographical note about each author.
ACKNOWLEDGMENTS
This research wasdoneattheArtificial Intelligence Laboratory ofthe MassachusettsInstituteofTechnology, wherethe authorworked while onleave ofabsence
from the University ofIllinois at Urbana-Champaign. The author wishes to thank
PatrickWinston for inviting him to the Laboratory, for many fruitful and enjoyable
discussions, and forcomments on anearlierversion ofthis chapter. The unique and
stimulating environment ofthe MIT AI Laboratory and discussions at the Learning
Groupwerehelpfulinshapingtheideaspresentedhere. SupportfortheLaboratory's
research is provided in part by the Advanced Research Projects Agency under the
Office ofNaval Research Contract N00014-80-C-0505.
The author thanks his coeditors, Jaime Carbonell and Tom Mitchell, for the
collaboration and contributive comments. The discussion and remarks of Randy
DavisfromtheMITAILaboratory wereavaluablechallengeandhelpedtoimprove
this paper. He suggested the idea ofusing learning machines for protection against
the misuse oflearning machines. Richard Doyle, Michael Kashket, Boris Katz, and
David Kirsh fromthe MIT AI Laboratory andAllan Collins from Bolt Beranekand
Newman, Inc. providedimportantfeedbackandthoughtfulcommentsontheearlier
draft. TheauthorexpressesgratitudetoBobStepp, LarryRendell,JeffBecker, Bruce
Katz, and Brian Stout from the AI Laboratory at the University ofIllinois for valuable suggestions andcriticism. Useful comments and suggestions were provided by
Ken Forbus fromtheUI DepartmentofComputerScience. Theauthorisindebtedto
GailThornburgfromtheUIGraduateSchoolofLibraryandInformationSciencefor
hermany insightful remarksandcriticisms. Importantsuggestionswereprovidedby
--- PAGE 36 ---
22 CHAPTER 1: UNDERSTANDINGTHE NATUREOF LEARNING
Jan Gorecki from the UI DepartmentofSociology. This work was supported in part
bytheNational ScienceFoundationunderGrantNo. DCR-8406801 andtheOfficeof
Naval Research under Contract N00014-82-K-0186.
References
Andreae, P. M., "ConstraintLimitedGeneralization: AcquiringProcedures from Examples," ProceedingsofAAAI-84, Austin, Tex., pp. 6-10, 1984.
Berkson, W. andWettersten,J.,LearningfromError, OpenCourt, LaSalle, 111., 1984.
Buchanan, B. G., and Feigenbaum, E. A., "DENDRAL and Meta-DENDRAL: Their Applications
Dimension,"ArtificialIntelligence, Vol. 11, pp. 5-24, 1978.
Buchanan, B. G., and Shortliffe, E. H. (Eds.), Rule-basedExpert Systems, Addison-Wesley, Reading.
Mass., 1984.
Caianiello, E. R., and Musso, G., Cybernetic Systems: Recognition, Learning, Self-Organization,
ResearchStudiesPress, Ltd., Letchworth, Hertfordshire, England; Wiley, NewYork, 1984.
Carbonell,J.G.,Michalski,R.S.,andMitchell,T.M.,"AnOverviewofMachineLearning,"inMachine
Learning: An Artificial IntelligenceApproach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.),Tioga, PaloAlto,Calif., 1983.
Cohen,P. R.,andFeigenbaum,E. A. (Eds.), TheHandbookofArtificialIntelligence,Vol. 3.Kaufmann.
LosAltos, Calif., 1982.
Conrad, M.,Adaptability, PlenumPress, NewYork, 1983.
Davis, R., and Lenat, D. B., Knowledge-basedSystems in Artificial Intelligence, McGraw-Hill. NewYork, 1982.
Dietterich,T. G.,andMichalski, R. S., "AComparativeReviewofSelectedMethodsforLearningfrom
Examples,"inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski,J.G.Carbonell. andT. M. Mitchell (Eds.),Tioga, PaloAlto, Calif., 1983.
Feigenbaum,A.E., LectureattheFirstU.S.-ChinaJointSeminaronAutomationandIntelligentSystems.
Beijing, China, May28-June 1, 1984.
Fogel, L., Owens, A.,andWalsh, M.,ArtificialIntelligence TliroughSimulatedEvolution. Wiley, New
York, 1966.
Hayes-Roth,F., "UsingProofsandRefutationstoLearnfromExperience,"inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski,J. G. Carbonell, andT. M. Mitchell (Eds >. Tiogi,
PaloAlto, Calif., 1983.
Hayes Roth, I . Waterman, D. A., and Lenat. D. B. (Eds.), Building Expert System.-.. Addisoo-Wesley,
Reading, Mass.. 1983.
Hillis.w. D.. "TheConnectionMachine(ComputerArchitectureformeNew Wave)" \l MemoNo 646^
MIT. September L981.
--- PAGE 37 ---
MICHALSKI 23
Hinton, G. E., Sejnowski,T.J.,andAckley, D. H., "BoltzmannMachines: ConstraintSatisfactionNetworks That Learn," Technical Report CMU-CS-84-119, Department ofComputer Science,
Carnegie-MellonUniversity, 1984.
Hofstadter, D. R., Godel, Escher, Bach:AnEternalGoldenBraid, Vintage, NewYork, 1980.
Holland,J., AdaptationinNaturalandArtificialSystems,UniversityofMichiganPress,AnnArbor, 1975.
Hume, D.,A TreatiseofHumanNature, L. S. Selby-Bigge(Ed.), ClarendonPress, Oxford, 1888.
Kawanobe, K., "Current Status and Future Plans ofthe Fifth Generation Computer Systems Project,"
ProceedingsoftheInternationalConferenceonFifthGenerationComputerSystems,COT,Tokyo,
pp. 3-36, 1984.
Kuhn, T S., TheStructureofScientificRevolutions, 2ded. en., UniversityofChicagoPress, Chicago,
1970.
Lakatos,I., "FalsificationandtheMethodologyofScientificResearchProgrammes,"inCriticismandthe
Growth ofKnowledge, A. Musgrave and I. Lakatos (Eds.), Cambridge University Press, Cambridge, 1970.
Langley,P., andCarbonell,J.G., "ApproachestoMachineLearning,"JournaloftheAmericanSocietyfor
InformationScience, Vol. 35, No. 5, pp. 306-331, 1984.
Larkin, J., Reif, F, Carbonell, J., andGugliotta, A., "FERMI: FlexibleExpertReasoningwith MultiDomainInferencing," submittedtoCognitiveScience, 1985.
Lenat, D. G., "The Role ofHeuristics in Learning by Discovery: Three Case Studies," in Machine
Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.),Tioga, PaloAlto, Calif., 1983.
Marcus, M. P.,A TheoryofSyntacticRecognitionforNaturalLanguage, MITPress, Cambridge, 1980.
McCarthy,J., "ProgramswithCommonSense," inSemanticInformationProcessing, M. Minsky(Ed.),
MITPress,Cambridge, 1968.
, "President'sQuarterly Message: AI NeedsMoreEmphasisonBasicResearch,"AIMagazine,
Vol. 4, pp. 4-5, 1983.
Medin,D.L., Wattenmaker,W. D
andMichalski,R.S., "ConstraintsinInductiveLearning:AnExperimental Study Comparing Human and Machine Performance," submitted to Cognitive Science,
1985.
Michalski, R. S., "Variable-Valued Logic and Its Applications to Pattern Recognition and Machine
Learning,"inComputerScienceandMultiple-ValuedLogic: TheoryandApplications
C. Rine
(Ed.), North-Holland, 1975.
-, "PatternRecognitionasRule-GuidedInductiveInference,"IEEETransactionsonPatternAnalysisandMachineIntelligence, Vol. PAMI-2, No. 4, pp. 349-61,July 1980.
-, "A Theory and Methodology ofInductive Learning," Artificial Intelligence, Vol. 20, No. 2,
pp. 111-161, 1983, 1983a.
-, "Learning StrategiesandAutomated Knowledge Acquisition: An Overview,;" in KnowledgeBasedLearningSystems, L. Bole, (Ed.), Springer-Verlag, 1985.
--- PAGE 38 ---
24 CHAPTER 1: UNDERSTANDING THE NATUREOF LEARNING
Michalski, R. S., Carbonell,J. G., andMitchell, T. M. (Eds.),MachineLearning:AnArtificialIntelligenceApproach, Tioga, PaloAlto,Calif., 1983.
Michalski, R. S., Ko, H., and Chen, K., "Qualitative Process Prediction: A Method and a Program
SPARC/G,"ReportsoftheIntelligentSystemsGroup, ISG-12, DepartmentofComputerScience,
UniversityofIllinois, Urbana, 1985.
Michie, D.,MachineIntelligenceandRelatedTopics, GordonandBreach, NewYork, 1982.
Minsky, M. TheSocietyofMind, MITPress, Cambridge(draft, April 1985), forthcoming.
Minsky, M., andPapert, S.,Perceptrons, MITPress, Cambridge, 1969.
Mitchell, T M., Keller, R. M., andKedar-Cabelli, S. T., "Explanation-Based Generalization: A UnifyingView,"MachineLearning, Vol. 1, No. 1 (Jan 1986): inpress.
Mitchell,T. M., Utgoff,P. E.,andBanerji,R., "LearningbyExperimentation: AcquiringandRefining
Problem-Solving Heuristics," in MachineLearning: An ArtificialIntelligenceApproach, R. S.
Michalski,J. G. Carbonell,andT. M. Mitchell(Eds.), Tioga, PaloAlto, Calif., 1983.
Nilsson, N. J.,LearningMachines , McGraw-Hill, NewYork, 1965.
Popper, K. R., TheLogicofScientificDiscovery, BasicBooks, NewYork, 1959.
, ObjectiveKnowledge:An EvolutionaryApproach, rev. ed., Oxford University Press, Oxford,
1981.
Quinlan,J. R., "DiscoveringRulesfromLargeCollectionsofExamples: ACaseStudy," inExpertSystemsintheMicroelectronicsAge, D. Michie(Ed.), EdinburgUniversityPress, Edinburgh, 1979.
Rendell, L. A., "Toward a Unified Approach to Conceptual Knowledge Acquisition," AIMagazine,
Vol. 4,pp. 19-27,Winter, 1983.
Robinson,J. A., "LogicProgramming-Past, PresentandFuture,"NewGeneration Computing, Vol. 1,
No. 2,pp. 107-24, 1983.
Rosenblatt,F, "ThePerceptron:AProbabilistic ModelforInformationStorageandOrganizationinthe
Brain,"PsychologicalReview, Vol. 65,pp. 386-407, 1958.
Schank,R.C., DynamicMemory:A TheoryofRemindingandLearningin ComputersandPeople, CambridgeUniversity Press, Cambridge, 1982.
, "TheCurrentStateofAI: One Man'sOpinion,"AIMagazine, Vol. 4. No. 1, pp. 3-8, Winter
Spring 1983.
Selfridge,M.
"Pandemonium:AParadigmforLearning,"ProceedingsoftheSymposiumonMechanizationofThoughtProcesses, D. Blake, andA. Uttley (Eds.), HMSO. London, pp. 511-29. 1959.
Simon, H. A., "Why Should Machines Learn?" in Machine Learning: An Artificial Intelligence
Approach, R. S. Michalski,J. G. Carbonell, andT. M. Mitchell(Eds.). Tioga. PaloAlto. Calif..
1983.
Sleeman, D. H., "Inferring Student Models for Intelligent Computer-Aided Instruction.'* in Machine
Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell. and T. M.
Mitchell (Eds.), Tioga. PaloAlto. Calif., 1983.
Sleeman, D. H .. and Brown,J. s. (Eds.), Intelligent TutoringSystems. Academic Press, Now York, 1982.
--- PAGE 39 ---
MICHALSKI 25
Tsypkin,J. Z.,Foundationsofthe TheoryofLearningSystems(inRussian), PublisherNauka, Moscow,
1972.
Weizenbaum,J., ComputerPowerandHumanReason, Freeman, SanFrancisco, 1976.
Winograd, T., "What Does It Mean to Understand Language?" inPerspectives on Cognitive Science,
D. A. Norman(Ed.),Ablex, Norwood,N. J., 1981.
Winston, P. H., "Learning Structural Descriptions from Examples," in The Psychology ofComputer
Vision, P. H. Winston(Ed.), McGraw-Hill, NewYork, 1975.
, "LearningandReasoningbyAnalogy," CommunicationsoftheACM, Vol. 19, No. 3, 1982.
,ArtificialIntelligence, 2ded., Addison-Wesley, Reading, Mass., 1984.
Winston,P.H.;Binford,T.O.;Katz,B.;andLowry,M.R., "LearningPhysicalDescriptionsfromFunctional Definitions, Examples and Precedents," Proceedings oftheAAAI-83, Washington, DC,
pp.433-39, 1983.
Zagoruiko, N., "Empirical Prediction Algorithms," in Computer OrientedLearning Processes, J. C.
Simon(Ed.), Noordhoff, Leyden, 1976.
--- PAGE 40 ---
--- PAGE 41 ---
MACHINE LEARNING:
Challenges of the Eighties
Edited transcript of a panel discussion held at
the 2nd International Machine Learning Workshop,
Allerton House, University of Illinois, June 22-24, 1983
Ryszard S. Michalski
(Moderator)
Saul Amarel
Douglas B. Lenat
Donald Michie
PatrickH. Winston
Transcribedandeditedby
Gail Thornburg and
Ryszard S. Michalski
Michalski:
Now that our Workshop is coming to a close, it is time to summarize our
thoughts anddiscuss someofthe issues importanttoour field. To start with, let me
raise some questions particularly suitable for discussion. First ofall, what are the
most important tasks for machine learning research for the near and not-so-near
future? Next, what is the role ofmachine learning in AI? How important and how
feasible isautomated knowledgeacquisition forexpertsystems? Shouldn'twe stress
this area much more than we stressed it in the past?
Another interesting issue is the role ofdomain-specific versus general approaches to machine learning. As you know, for a long time many researchers
avoided research that could be called general methods oflearning. It was believed
--- PAGE 42 ---
28 CHAPTER 2: CHALLENGES OFTHE EIGHTIES
thatsuchresearchwasnotgoingtobringanyinterestingresults,thattheresultingsystems would be very inefficient, and that the whole area ofgeneral learning was a
hopeless task.
Asaresultofsuchattitudes,manyresearchersstartedtoexplorelearningissues
in the contextofspecific problems. Thisdomain-specific research ledto interesting
resultsandimpressivelearningsystems. Therewas, however, abadsidetoit; itledto
asituationinwhichcertaingroupsworkedintheirlittleniches, deeplyinvolvedwith
their favorite domain-specific problems and not communicating sufficiently with
othergroups. Theyoftendevelopedtheirownterminology, unawarethatit wasmore
orlessisomorphictotheterminologyofsomeothergroups, andthishamperedinteractionandtheprogressofthefield. Moreover, thatkindofdomain-specific research
didn'tleadtoany moregeneralunderstandingoftheproblems ofthe fieldanddidn't
leadtoanynewtheoriesorprinciples. Certainlythetimehascome forustoidentify
more general principles in our field.
A related issue for discussion is the question ofpure versus applied machine
learning research. Should we stress the theoretical research, or should we be more
oriented toward designing and implementing practical learning systems?
Anotherissuetoconsideriswhetherweshouldcontinuetostudy indepthsingle
learningstrategies, orwhetherweshouldnowattempttobuildintegratedlearningsystems that employ several strategies. Clearly the existing embodiment of a learning
system, a human being, can learn using a variety ofstrategies simultaneously. Moreover, the strategies that peopleuse change with theiraccumulation ofknowledge. We
knowthatchildrenlearndifferentlythanadultsdo. Themajordifference isthatadults
already have a large store ofknowledge about the external world and so can use this
knowledge whenthey learnnewthings. Therefore, using strategies involving analogy
is generally more appropriate for them than for children, who don't yet have much
knowledge and thus cannotuse analogical reasoningtothe same extent.
Anotherimportanttopic fordiscussionconcernstheterminology anddescription languages useful for machine learning research. I have already mentioned that
oneoftheproblemswefaceisthatresearchersinthisfielduseavarietyofterminologies, someofthem isomorphic. Identificationofsome ofthose isomorphisms would
be very useful forthe furtherprogress ofthe field and would make it easier for new
researchers to enter the field. An interesting problem related to this is the role of
formal languages suchas predicatecalculus inourfield. What istheirusefulness for
representing program-generated knowledge versus their usefulness as well-defined
formalisms fordescribing learning algorithms.
Finally, are we already a field? Ifso, what are ourunifying principles and our
goals? And one more issue: We are now facing the development of new computer
architectures, sowe may study learning problems using machinestobe developedconnectionmachines, Boltzmannmachines, andsoforth. Willthis new development
in computer architectures bring us some important new tools that will help us in
machine Learning research?
--- PAGE 43 ---
MICHALSKI, AMAREL, LENAT, MICHIE, ANDWINSTON 29
I will end on this question and introduce to you our panelists: Saul Amarel,
RutgersUniversity; DonaldMichie, UniversityofEdinburgh; DougLenat, Stanford
University; and Patrick Winston, MIT. They will air their own views on these and
othertopicsinthisorder. Attheend, usingmyprerogativeasmoderator, Iwillmake
some final comments.
Amarel:
Iunderstandthatwhatwe'resupposedtotalkaboutisaviewofthefieldfiveto
tenyears from now. Ryszardhas putmany questionstous, andI'mgoingto address
some ofthem.
First ofall, on the issue ofpure research versus applied research, I think we
needboth. Ithinkweneedbothinparallel, andIthinkweshoulddobothevenonthe
same project. This has been my ownphilosophy for some time. We have to develop
both parts ofthe activity. We need applications, specific explorations, to generate
ideas about how to approach new problems and new challenges. We need the basic
researchpartinordertotiethingstogetherandtoseewhatisequivalenttowhat, what
is superiorto what, and why. We also needto see how things relate to each otheras
well as to otherareas ofAI and computer science, orto psychology.
My own sense is that problems oftheory formation-and some problems of
learning-are nodifferent from other issues inthe area ofproblem solving. I would
very much like to see an integration and an overall framework to encompass problems in all domains, including domains oftheory formation. To date, we haveput a
tremendous amount ofemphasis on derivation problems, on pathfinding problems,
on interpretation problems, and on other areas that we call problem solving. There
aresomeareasofproblemsolvingthathavetodowithconstraintsatisfaction, where
thenumberofconstraintstobesatisfiedistypicallylarge; also, wehaveproblemsin
whichwemusthandlealargenumberofinteractinggoals. Ifyoulookcarefullyatthe
problem methodologies used in these areas, you will find them not very different
from methodologiesthat we use in some areas ofhypothesis generation, theory formation, andlearning. Ithinkweshoulddoalittlemoreworkonrelatingthesevarious
areas to each other, from two points ofview: what the problem formulation is and
what the problem-solving processes are.
Idon'tthinkweshouldsplittheseissues. Wecouldofcoursesplittheproblemsolving domain intosubdomains, inaccordancewithcertain ideashavingtodowith
the methodology of solution, complexity, or the degree of dependence between
problem conditions. In my mind the degree ofdependence between problem conditions is probably the most fundamental parameter for thinking about different
methodsofproblemsolving. Inmanyofthederivationproblems-thatis, manyofthe
problems that are usually conceived as conventional problem solving-we usually
workinonespace. Herewetalkabou* moving from statetostate, ofusingoperators,
--- PAGE 44 ---
30 CHAPTER 2: CHALLENGES OFTHE EIGHTIES
inference rules, and soon. The more you delve intotheory formation problems, the
more you see that you have to work in more than one space. In most instances we
workintwospaces: inthespaceofsolutionsand inthe spaceofproblem conditions.
And mostofthe time the difficulty arises because thelanguages andthe sets ofconcepts that are available to us in these two spaces are different.
Thebest strategy we can thenpursue, which indeed wepursue in avery intuitiveway, istotrytoestablishearlyenoughalinkbetweenonespaceandtheotherand
toformulatethe problem injustone spaceasmuch aspossible, inordertobe ableto
solve it with well-known methods. The entire issue of how to handle problems
involvingbothasolutionspaceandaproblemconditionspace, aswell ashowtolink
the spaces and how to coordinate the two-space search, has been with us for some
time. Thiswas recognized intheearly 1970sby severalpeople; one mightciteavery
good paper by Simon and Lea in this area.1 I think this issue will require more
thinking.
As Isaid, Ithinkthebasicissueinchoosingaproblem-solvingmethodisreally
the question ofproblem decomposability. Ifwe can decompose a problem, we can
assignamethodtoagoal, independentlyofothergoals, makingourproblem-solving
activity relatively easy. In most ofthe problem-solving efforts in AI we have taken
this kind of approach. As soon as goals become very interdependent, we cannot
reason very clearly from problem conditions to goals, and we have a difficult
problem. This iswhatcreatesamajordifficulty inproblemsoftheory formationand
learning: it'sverydifficulttodecomposetheproblems. Totheextentthatweareable
todecompose,ortotheextentthatwearegoingtobegoodinhandlingmethodologies
ofdecomposition, we will also improve our ability to develop effective methodologies for formation problems.
I was very interested in the many comments and the considerable amount of
work being done in the area ofanalogical reasoning. Personally, I would very much
like to have a moratorium on the term analogy, because it is regarded in somewhat
different ways by different people. That's a problem that we have in the field,
anyway-tryingtodeterminehowtousesuchtermsasintelligenceorlearning. Since
these terms havedifferentconnotations fordifferentpeople, agreement isextremely
difficult to accomplish. We have to be a little more precise in operational terms, in
definingwhatwemeanwhenwetalkaboutaparticularanalogical reasoningmethodology, and soon. As I was listeningtovariousapproaches, my own sense was as follows: AsRyszardwassaying, studyinglearninginanenvironmentwithatremendous
amount ofrelevant knowledge is very important. Most ofour learning is being done
'H. A. Simon and (i I ea, "Problem Solving and Rule Induction: A Unified View" in Knowledge
Cognition, I Gregg (Ed i, Erlbaum, Hillsdale, N J.. 1974.
--- PAGE 45 ---
MICHALSKI, AMAREL, LENAT, MICHIE, ANDWINSTON 31
in a situation in which we know ofmany problems and ofways ofsolving them; we
knowthestructuresandmethodsinotherdomains, andinsomewaywetrytoimport
that knowledge and bring it to bearonthe problem at hand.
Possibly the entire issue of analogy could be subsumed under the following
mechanism: Givenaproblem, we mustfind some otherproblem knowntousthat is
somehow similartothe initialproblem. We usethe "similar" problemasafocusing
mechanism for selecting schemata that are promising, to start at least a part ofthe
problem-solving activity. We importthe "similar" problem, weusethemethodthat
workedforthatproblem, andthenwegoontodoadifferentkindofproblemsolving
in order to complete ourtask. I cannot possibly conceive how analogical reasoning
alone can do the entirejob. The most difficult part is not the identification ofthe
analogy but the assimilation of the analogy, the repair, and the additional work
needed in orderto finish thejob, afterthe analogy has been imported.
And Iwouldverymuchliketoseemoreworkdoneontheuseofanalogicalreasoning and the use ofrepair strategies to finish a piece ofanalogical reasoning as a
basisforsolvinganewproblem. Also, intermsoftheoryformation, my recentwork
shows that the most difficult aspect ofthe problem is not encountered in the early
stagesoftheoryformation. Thedifficultyisattheendoftheprocess, whenyouhave
"almost correct" theories and would like to converge on a solution. The amount of
reasoning that is needed then is enormous, and the techniques used are much more
demanding, which is precisely where I think domain knowledge in large quantities
must come into play. I don't see how we can form theories in certain areas without
alreadyknowingquiteabitabouttheareaitself. ThisisessentiallyBillMartin'spostulate, thatyoumustknowquiteabitaboutadomainifyouwanttodosomelearning
in it. I think it's absolutely essential.
Asforapplications, Icertainlythinkthatweneedmuchmoreworkinthearea
oflearning, inthe creation ofknowledge bases forexpert systems. There's nodoubt
thatthis isthe only way wecango. The field is asking forit, notonly intermsofthe
expertsystemsoftoday, wherewehaveathousandrulesormore, butalsointermsof
situations-and these are both fundamental problems and application problemswhere a system is already working, butwe would like to identify subdomains ofthe
system where, onthebasisoftheactiveexperienceofasystem, thingscouldbedone
evenbetter. Wewouldliketobeabletoidentifyspecializedmethodsinanygivensubdomain, ashumanexpertsdo, andmakethemavailabletothatparticularsubdomain.
The entire movement from novice or average performance to expert performance
requires this identification of special characteristics of subdomains and special
methodologies that could be applied to them, so that the result will be increased
expertise. Theentire areaofexpertise acquisition inthe contextofexpert systems is
extremely interesting for learning, both in terms ofbasic kinds ofproblems and in
terms ofimpact on the building ofexpert systems.
TherearemorespecificthingsIwanttosayaboutcurrentprojects. Itconcerns
me thatI don't see any projects ofthe Meta-DENDRALtype around us today. What
--- PAGE 46 ---
32 CHAPTER 2: CHALLENGES OFTHE EIGHTIES
are we going to have after Meta-DENDRAL? This was a major, interesting, wellchosen domain, with very interesting challenges, where one could develop many
ideas about theory formation. I would like us to find ways whereby any given group
thatdidnothavetheinterestandthestaffingtocontinuesuchaprojectcoulddevelop
arrangementssothatsomeothergroupcouldcontinuetheproject(I know allthedifficulties involved) and perhaps build on the experience ofthe first group, to try to
move beyond the stage at which, for instance, Meta-DENDRAL was left. I would
verymuchliketoseemorescientifictheoryformationgoingon, maybeinthebiological sciences, orperhaps in areas ofphysics. This wouldbe extremely important for
us to pursue.
Now thereisanotherareaoflearningthatIfindmightbeveryusefulforusand
mightrelatealittlemorecloselytopsychological investigations. Thishas todowith
developing environments for problem solving with appropriate graphics and monitoringcapabilities, whereonecould watchtheoperationofanexpertinadomain. It
couldbeadesigner(e.g., adesignerofanengineering system), anotherkindofprofessional such as a manager, and at the same time it could give various aids to that
professional. I am not talking yet about an automatic system. Rather, those professionals will have aids, and at the same time they will have monitoring facilities to
recordwhattheyaredoing. Itwouldbeanexcellentthingtocapture, insomegradual
fashion, some ofthe ways in which those managers orprofessionals ordesigners do
things-and learn how people actually do things-by using these kinds ofenvironments. This has implications for experimental environments and facilities. That's
where I can see those psychologists who are interested in human-machine interactions or in learning generally interacting with involved nonpsychologists.
A finalthought: Ihaveafeelingthatif wewanttoadvanceintheareaofconcept
discoveryand intheareaoftheory formation inadeepway, thenwe shouldbedoing
more things ofthe kind that Doug Lenat did in the AM system. This means that we
shouldbethinkingnotonlyaboutonespecificlearningproblembutaboutaclusterof
interconnected learning problems. The output ofone can in some way be utilized as
an input to another and can in some cases enable us to come back and revise ideas
about concepts that we have been using in a component problem. Isolated, very
simple formation or learning activities are very important to our understanding of
someofthebasic methodology. Yethow muchmore interesting it would be tohavea
set ofactivities in a specific domain-such as mathematics or physics-in which we
could see how the various activities interact. No scientists work exclusively on one
problem at one time. They always work on several problems, and they transfer
knowledge from problem to problem; in addition, of course, they bring to each
problem considerable knowledge from outside the problem domain.
At this point I think I shouldend my commentsand let my othercolleagues on
the panel speak. Thank you.
--- PAGE 47 ---
MICHALSKI, AMAREL, LENAT, MICHIE, ANDWINSTON 33
Michie:
I should like to endorse a theme that I take from Saul Amarel's remarks,
namely, theanchoringeffectofagoodchoiceofproblem. AIwork is nowatacrossroads. More accurately, it is at a Y-junction ofthe kind that experimental psychologists like to use to test animals. One arm ofthe Y-junction leads to a philosophy of
free-floatingwork. Theotherleadstoasenseofdirectionderivedfromwell-defined,
hard problems.
Iseeananalogyheretotheearlydaysofaeronautics. Theuseofballoonsoffers
ausefulcaricatureofthefree-floatingschool. Inaballoononeishappytofloatwherever the wind blows and to exchange anecdotes with other balloonists about interesting glimpses ofwhateverterrain one happens to pass over.
The really hard problem in aeronautics was that ofdirectional flight, which
confrontedtheheavier-than-airschool. Unfortunately, ineverybranchofsystematic
inquirythe free-floatingapproachhasafatal attractionforthe administratorsofscience. They feel they really understandthat kind ofthing. Sothe first peopleto venture intothedirectional styles mustnotbetoosurprised ifthepolitical andadministrative leadersofsociety givethemahardtimeandseektocoaxordeflectthem into
unstructured explorations in which all concerned can relax.
Letme tellyouaboutthisas it workedoutinthecaseofpoweredflight. Early
experimentsmetsuccessinthehandsoftwoveryhard-headed, scientificallytrained
engineers, theWrightbrothers, followedalmostimmediatelyafterwardsby Cody in
the United Kingdom. By about 1908-1909 an infant technology had taken root, in
many ways comparable to the infant technology ofintelligent, computer-controlled
roboticsthatcharacterizedthelate 1960sandtheearly 1970sintheAIfield. Theballoonistswerestillpotteringon, andtheyweremoresuccessfulthantheheavier-thanairpeople in the higher reaches ofscience/political wisdom.
The British prime minister set up a subcommittee ofthe Imperial Defense
Committee, chairedbyLordEscher. Thiscommitteeworkedforafewmonthsinlate
1908 and reported early in 1909. They took a variety ofevidence from officers and
politiciansofthedefenseestablishmentonwhethertheheavier-than-airprinciplehad
a future.
Afterfinelysiftingtheevidence,theycametotheunanimousconclusionthatit
did not. The subcommittee's recommendation to the Imperial Defense Committee
was that all work on heavier-than-air flight should be canceled and government
resources redeployed to the study ofballoons.
The prime ministerofthe time, H. H. Asquith, is on record in the minutes of
the Imperial Defense Committee as pronouncing himselfhighly satisfied with this
decision. Shortly after this, Bleriot flew the Channel, attracting a great deal of
publicity. By good chance, Lord Escherwas an intelligent man ofhigh integrity. He
began to worry that possibly his committee had made aterrible error. After further
--- PAGE 48 ---
34 CHAPTER 2: CHALLENGESOFTHE EIGHTIES
thought and study, he put a heavily documented case to the prime minister, to the
effect that his committee had made a mistake and that it was in the national interest
for Britain to arm herselfwith an effective fleet ofheavier-than-air machines.
There is a moral in this story forartificial intelligence. The kind ofwork that
wasbeingdoneatSRIonthe SHAKEY projectwasatypical hardproblemon which
all the intellectual and other resources ofthe AI craft had to be brought to bear to
establishsuccess. Alongwith similarprojectsatStanford, MIT, and Edinburgh, this
investigation intoworldmodeling, recognition, andplanning hadtobediscontinued
becausetheworldarounduscanunderstandcoffeetabletalkaboutthesetopics, butit
is repelled and mystified by sustained and detailed experiment.
Yetas faras ourprofessional criteriaareconcerned, there is noway out butto
selecthardproblemstoactas forcing functions. The factthata free-floating, liberal
artsapproachcanwarmtheheartsofadministrators shouldbetakennotasapositive
rallying point but as a point against.
Interms ofpractice, what does this mean? Our field, which is infant still and
lacks a hardened skeleton on which to hang a definite morphology, needs a style of
practicedeterminedbytheprofessionalstandardsandrulesofevidencecustomaryin
experimental and theoretical science. It should aim to supersede the standards and
rulesofevidencecustomaryintheliberalartsandinsomeofthelessdevelopedengineering disciplines such as computer science.
For the future, let me humbly suggest that our next meeting be restricted to
papers that report on completed results. Any philosophizing about future work that
theymayadditionallycontainwillthenatleastbeaccompaniedbyadirectionalpoint
ofreference. In well-established branches ofscience, no one would consider operating to any more permissive criteria.
Lenat:
Now let mestartoffbyagreeing, inaway, withsomethingthatDonaldsaidearlier, thatthe fieldofmachinelearningcanbeanchoredbyworkingonhard, specific,
very well-defined problems. In fact, I thinkthat's the reason the field has lookedatanchor for so long.
Assuming that we wanttoprogress from our relatively primitive state oftechnology, I think we will have to send out small craft and hope that some ofthem do
make it back safely. More seriously, though, ifwe do want anchors ofthe kind that
Donald was talking about, something that we can use productively, then those
anchorsshouldbethesourcesofpowerthatourprogramstapintoandthat wetapinto
in our research.
The first sourceofpowerissynergy. Synergy means gettingout more than you
put in, in dealings between programs and human beings. In the work we do with
EURISKO- for example, the toy naval ship design-are things that neither we nor
the program alone could do, and it's the human-machine s>nerg\ that I think we're
--- PAGE 49 ---
MICHALSKI, AMAREL, LENAT, MICHIE, ANDWINSTON 35
reallytapping. We'reexploitingandtechnically combiningthe differentcapabilities
ofeachofus. Then, ofcourse,there'ssynergybetweentheprogramswebuildandthe
work that other AI researchers do in areas other than machine learning. Putting
learningmodulesontothefrontofexpertsystems, forinstance, isthatsortofsynergy.
Andfinally, thereissynergywithothermachinelearningresearchers, sothatwecan
getourprogramstocooperate, worktogether, arguetogether. That's something that
byandlargehasnotbeentapped, butIthinkitisasourceofpowerjustwaitingthere
tobe exploited.
Thenextsourceisanalogy, withtwotypesofuses, thoughAmarelwantstosee
that word banned. One use is to generate plausible, potentially true conjectures,
ideas,conceptualizations,andwaysoflookingattheworld,aswellaswaystoexplore
them (perhaps through other means) to see ifthey really aretrue.
Theseconduseforanalogyisinknowledgeacquisition, forinstance, ingetting
material into the knowledge base ofan expert system. We do this all the time, by
lookingaroundforaunitoraframeorarulethatissimilartowhatwewanttoenter,
getting the unit, copying it, and editing it. While the "copy and edit" process is a
trivial kind ofanalogizing, less trivial analogizing would presumably provide less
trivial kinds ofknowledge acquisition aids.
Thenextsourceofpowerisheuristics,andIhavenothingmoretosayaboutthat
right now. In case you aren't familiar with this, you can see Jerry DeJong's puzzle
[awordpuzzledistributedtoallconferenceparticipants]foracleardefinitionofwhat
it's good for, or my 1984AIJournalarticles.
Next is representation. Again, there are two issues here. One is having and
findingnaturalrepresentations, whichIthinkisveryimportant. Theotherischanging
representations, alsoveryimportant, oneofthekindsofthingsthatSaul mentioned.
Finally there isacertaincatchall category, inwhichwe findthings likeparallelism, morphological analysis, sources ofpowerwe haven'tdiscovered yet.
What I really want to focus on is what we can do to exploit these sources of
powerinthecomingdecade. Ifwecarrythisexercisefurther, then somewhereupat
theverytoplevel wouldbethegoalsthatwearetryingtoachieve-butI'mnotinterestedinthathighalevel. Atanintermediatelevel,oneconcernisthehuman-machine
interface. Thisissomethingthatcantapintothehuman-machinesynergy, obviously.
Inthehuman-machine interfacethereare several aspectsofconcern. Oneofthem is
I/O, butthat's notreally partofourbusiness-that's forpeople inhardwareorother
areasofAItoworryabout. ThesearethingslikehavingsnazzyformsofIvanSutherland'soldhelmetyoucanwear-thatis, sunglassesthatprojectseparateCRTimages
on the insideofeach eye; accelerometersthat senseyourhead, neck, andeye movements, sothatas you turn yourheadthe scene changes in real time; nice things like
that mean you're not limitedby small screen sizes in whatyourdisplay areacanbe.
Ofcourse with the hat it's natural to wantaccoutrements likeglovesthat sense your
hands. Anyway, we're not going to worry aboutthat-but somebody else will.
. . .
Then, obviously, we'll want things like natural language and speech recognitionand-rememberingthatwe'll havetheseglovesandthese funny glasses on-we
--- PAGE 50 ---
36 CHAPTER 2: CHALLENGESOFTHE EIGHTIES
mightaswell startusing nonverbalcuesaswell. Again, let'sletsomeoneelseworry
about that, but keepin mind, it's going to happen.
The thing we can do to exploit the synergy with human beings is to start
thinking about models ofsessions at a terminal between a person and the program
that's running, or in fact, models ofindividual people. One way todothat is to start
taxonomizing sessions and taxonomizing groups of individuals; so, for instance,
you know that if a user starts a sentence with the word let that user must be a
mathematician-and we treat mathematicians differently from human beings.
Michie: The usercould be apriest.
Lenat: Yes, but I suspect we would treat him as a phenomenon similar to a
mathematician.
Next we have the synergy with the other AI researchers and their programs.
Thewaywefosterthatistobuildoursystemsasportablemodulesthatcanbeplugged
into various other sorts ofsystems. Similarly, if we build those modules in a very
clean way so that they can plug into each other, then we can start getting synergy
among various learning programs. This is one ofthe main directions that I see the
workin EURISKOtakinginthefuture. We'regoingtotrytocleanitupandgetitinto
a formI can giveto the world to lookatoruse, depending on the audience.
As for you-know-what [analogy], we could do the generation of plausible
hypotheses thatwe'dlike using it, ifonly wehad abroadenough knowledge base. I
thinkthethingthat'sheldusbackisthatiftheprogramonlyknowsaboutplanegeometryandhastocomeupwithananalogy,
mustbeananalogy fromplanegeometry.
What lets people do analogizing, or generation and exploitation of metaphors, so
effectively is the enormous range ofknowledge we have. This is not so much the
depth butthebreadthofknowledge, several orders ofmagnitude more than any system's program has. The kinds oftasks to work on there include putting an encyclopediaon-line, notinatextualsense,butactuallyinaknowledgebase, sothatitcanbe
used. There isagroupatAtariworkingwithAlanKay andmyselfdoingthat kindof
projectwith roughlya 13-to 15-yeartimeframe. There'sanarticleon itatIJCAIthis
summer [1983], ifyou're interested.
Besides putting in encyclopedic facts about the world, you want to add commonsense factsabouttheworld. Let'ssaythereareathousand "factwords" inbasic
English, maybe anotherthousand ortwo that should have been there. And ifyou're
going to do such a project in any finite amount oftime, you won't want to take the
approach that Pat Hayes took with water, spending several years and doing it right.
Insteadyoumusttakeadayorso, thinkaboutwhatatwo-year-oldchild knowsabout
water, writeit down,andgoontothenextword. Ithinkthat'stherighttacktotake for
ten years, tosee what happens. Imagine-weget a littlebit ofknowledge about each
of several thousand commonsense concepts into a program, and then, parenthetically,technical knowledge, thekindofknowledegeonewould find inexpert systems.
--- PAGE 51 ---
MICHALSKI, AMAREL, LENAT, MICHIE, ANDWINSTON 37
If we had that for several different domains in one place, that might also lead to
exploiting analogy.
Just as we wantto have abroad knowledge base to exploit analogy, we wanta
broadheuristicbasetoexploitthepowerofheuristicsandthewaytheyorganize. You
need to consider all the world's heuristics, which you might do by looking at thousands ofspecific heuristics and starting to generalize them very slowly, to build up
some huge tree ofheuristics.
Once you have that kind ofheuristic base you can tap into analogy and other
things. And again, I think that this is something that's doable, just barely, in the
coming decade-a moderatejob, not acompletejob.
And then I even have the nerve to put representation bases on our "list of
knowledge to accumulate," though we only have about eight representations we
know about-so you might as well have a program that knows about them and can
chooseamongthemandoccasionallyevenaugmentthem. Iseethatasoneofthereal
opportunities for work in the future.
Anotherthing-ifyoutookalookatthepaperIhaveintheproceedingsforthis
conference, you'll have noticedthatItalkalotaboutcognitiveeconomy-programs
thatmodel, monitor, andmodifythemselves. That'sanotherwayoftappingintoboth
change ofrepresentation and ofheuristics.
Nowhereinmytalkisthereanythingabouttheory,andso-sincesomeonewill
probably ask-wecan fittheory inhereifyouwant. Youcantalkaboutworkonthe
natureoflearning, andifyoudo,thenwhatyou'rereallylookingintoisakindofsynthesis ofall the things thataregoing on elsewhere inthe picture, plus perhaps some
idea of what's happening with human beings. And notice all the work on human
cognition-just a very narrow fragment ofwhat people could be working on. Why
is that? Why not worry about societies, organizations, and machines instead of
just organisms? Why not worry about organisms, whyjust worry about cognition?
Learning goes on, on different time scales: by hours rather than minutes at the
immune system level, by years at the corporate level, or even over millenia via
evolution.
The final kind oflearning theory that I think is worth doing is dimensions of
learning, the kinds ofthings that Michalski, Carbonell, and Mitchell talk about in
theirchapter inMachineLearning I. It's very useful because itlets you do morphological analysis. You can start by saying, here are the ten dimensions along which
learning systemscan be categorized. Observethatall the systems thatwe'vebuiltso
farclusterhereandhereandhere,yettherearevastareasofthespacethataren'tpopulated by any system. Why is that?
Thinking aboutthose sortsofissuescanleadyoutonew insightsaboutwhat's
lesseasy and hard and why. Or, occasionally, they can lead youto say that someone
ought to build a system that has these properties.
Beforeclosing, let merespondtoacoupleofthingsthatotherpeoplehave said
earlier. One ofthem related to the role ofmachine learning in AI. I see a kind of
--- PAGE 52 ---
38 CHAPTER 2: CHALLENGESOFTHE EIGHTIES
coroutinerole,justasifIsaid, "Gee,I'dreallyliketohavenaturallanguageandspeech
recognitionmodulesin mylearningsystem."Thensupposethenaturallanguagepeople
say, "Gee, I'd really like to have a learning module in my natural language system."
Those are both reasonable things to say, and I envision a kind of symbiosis-maybe
synergy-developing there.
Asforpureversusappliedresearch, Ithinklabelslikethatareakindofredherring. When we build expert systems, for instance, the real problem in getting consensusamongexperts isthatthey have slightly differentmeanings forthe termsthey
use. All the time gets wasted in arguments that involve mere syntactical disagreement. Ithinkthesamethingislikelytohappeninthepure/appliedissue, withalotof
time wasted arguing about various categories ofwhat should and shouldn't be done.
Let's not worry so much about terminology.
Asforindividualversusintegratedlearningsystems, Ithinkthatintegratedsystemsare almostgoingtobe anecessity. Again, looking backoverAI, we see lots of
individual mechanismsthatwereoriginally thechiefsourceofpowerin programsPerceptrons, automaticbacktracking, unification, and resolution. Inall those cases,
peoplegotrealexcitedandtheydevelopedprogramsthathadthisorthatmechanism
as their sole source ofpower, and they had some initial early successes. Then they
"lost big" three to five years down the road, and everyone got turned offand went
intootherfields. Seventotenyearslater, westartedcomingbackwithanewperspective, saying "Hey, these are really neat things to use as sub-sub-submechanisms all
through our programs." The same thing is going to happen here, I think; we should
start integrating before we find ourselves at the wrong end of another seven-year
backlash.
Winston:
I proposeto make aclaimthat we are faced with one dangerand one opportunity, butfirstIwanttosaythatIthinkthingsarebasically inpretty good shape. This
was a splendid meeting, an uplifting one incomparison with most I attend. It seems
to me that there are four reasons why we should be happy to be in the position we
are in.
Reason numberone is that the research that we're doing is well balanced. The
variousdimensions-practical, applied, special, general, formal, informal-all look
good to me in the sense that the dimensions are generally well balanced.
The second point is that we're not diverted much, in comparison with other
parts of the field. I don't see very many people doing what I would consider silly
recursions into noncritical problems. We don't see very many papers claiming that a
new control structure is essential before we can do any work on learning, or that
we have to invent new programming languages before we can get started. Some
--- PAGE 53 ---
MICHALSKI, AMAREL, LENAT, MICHIE, ANDWINSTON 39
saywe needthesethings, ofcourse, butthere'snogeneral senseoffutility forlackof
some tool.
The third thing is that there are very few, if any, "snake oil salespeople"
making ridiculouspromises. Ifyoulookatotherpartsofartificial intelligence, that's
notthecase. Wehaveanobligationtocontinuetoinsurethatourlittle subfieldmaintains that kind ofdistinction.
Fourth, I think we're doing basically the right things. It's notjust a matter of
generalized balance, but the fact that there are now some new things that weren't
being done, that should have been done, andthat are, in fact, beginning to get done.
We now see papers on guessing and confirming structure, work on quality andprocess, and new reflections on what we can say about the educational process. Those
are all important pursuits that happily are now under way.
Now let megobackto my original two-pointlistofonedangerandoneopportunity. Thedangeristhatourfieldisabouttobecometoosuccessful. IfI mayinvokea
precedentandtrytobuildan analogy, I would notbe atall surprisedto findthat our
specialtyisgoingthewayofexpertsystems. Thatis, vastpublic interest, hundredsof
spin-off companies, depletion of university resources, industrial raids-the whole
works. The usual corruptionthatall ofthatbrings is aserious dangerthatIthink we
have to face. And I don't know what to do about it.
WecouldverywellbecomethebannerpartofAIinthenextfewyears,
seems
to me, displacing expert systems. Again we needtogobackto a kind ofsocial pressure, I suppose, to insure that we're notcorrupted by that popularity.
Theopportunityhastodowithambitionfueledbyhardware. When
wasdoing
computervision,
wasunthinkabletoconsideranythingotherthanrunninga3-by-3
operator overa 256-by-256 image. That was aprocrustean bed on which to lie, and
we didn'tgetvery farasaconsequence. So, what'sthe analog ofthattoday? Itmight
be fooling around with a single story instead ofhundreds ofstories.
WhatI'mdrivingatisthatwehaveallthis interestinsupercomputerslyingout
there, and I don't think we've thought much about how to exploit it. I'm willing to
arguebothsidesofthisquestion, ofcourse. Idon'tthinkweshoulddashoffandbuild
machinesforlearning. Ontheotherhand, Ithinkit'sworthalittlemorethoughtthan
has gone into it so far-that is, thought about what we could do with supermachines
by way ofsuperuses. Doug Lenatofcoursethinks about using lots ofmachines, but
very few of us, if any, have thought about how we might use unbelievably fast
machines to do unbelievably quick matching and operations ofthat sort.
So, I thinkthat'stheopportunity. Ifthings workoutastheydid invision, then
we'llbethinkingmuchbetterthoughts intenyearsasaconsequence. Now it'slaughable to run a 3-by-3 operator overanything; you view vision problems much differently. When youcan run 30-by-30operatorsover 1000-by-1000pictures inaquarter
ofa second, you change the nature ofyour thinking. Similarly, when we can make
hundredsofanalogies, notjustafew, ourperspectivewill shiftincreasinglytowarda
more global view.
--- PAGE 54 ---
40 CHAPTER 2: CHALLENGESOFTHE EIGHTIES
Michalski:
Thankyou, Patrick. Ifindmyselfverymuchinagreementwithotherpanelists;
maybe I should feel a little disappointed.
Winston: I'll argue with you.
Michalski:
But there are a few issues that I feel were insufficiently discussed. One is
theory: Areweanywhereclosetobuildingsomethingthatcouldbecalledthecomputational theory oflearning? Well, certainly we are at anearly stage in our research,
and we cannot say that we are in aposition to build any complete general theory of
learning.
Still, I think that work toward a computational theory of learning has some
goodpoints. I amnotsayingthatweshouldallgotoworkonatheoryoflearning, naturally, butIthinkafewofusshouldgivesomethoughttothisissue. Theorycangive
usabetterunderstandingoftherelationshipsexistingamongdifferentdirectionsand
differenttechniques, cancleanupterminology, and, mostofall, canhelpustoteach
thesubjectofmachinelearning. Whenwehaveacleantheory,althoughnotcompleteand actually not even "clean"-we can more easily discuss our problems and build
upon what we have already done. We can also identify isomorphisms and relationships between different concepts and different methods.
Icould identify several groupsinthepastwhosemethods were almostequivalent except for the terminologies they used. So while there was the appearance of
something new and different, in substance it was not necessarily so. Another thing
that I would like to argue-and I probably will be a minority here-is that there is a
need for research on what could be termed multi-purpose methods (or generic task
methods).
What do I mean by such multi-purpose methods? I don't mean methods that
are, shall we say, quintessentially general techniques oflearning, applicable to all
problems. Rather, I mean the following: A certain sufficiently broad yet cohesive
subdomainofproblems isidentified, andthenaneffortis madetodevelopalearning
method applicable to any problem that falls into such a domain.
These multi-purpose methodscanbeequipped with knowledge ofa particular
domain to which the method applies, so this would not be a "knowledge-tree"
method. However, ifthe method issufficiently robust, itcouldbeadaptedeasilytoa
range oftasks, so we would not havejust one method per task. That's m\ idea ofa
multi-purpose method.
Youmayaskforexamples, andthereareexamplesofsuchmethods. Theeasiest
thing for me to say is that some ofthe methods that we developed in Illinois, like
those Implemented in INDUCE, AQ11, or CLUSTER programs, represent, in my
opinion, that kindofwork. Theyarenotgeneral inthesensethatthe) can solveever)
--- PAGE 55 ---
MICHALSKI, AMAREL, LENAT, MICHIE, ANDWINSTON 41
kindofproblem, buttheyaremulti-purposeastheycanbeusefulforarangeofproblemsofaspecifictype, occurring inmanyapplications. Inotherwords, ifaproblem
satisfies certain constraints and criteria, then the method can be applied.
Finally, I was somewhat surprised that so few papers in this workshop were
devoted to the area of knowledge acquisition for expert systems. We know that
knowledgeacquisitionisthebottleneckinthedevelopmentofAIsystems-inparticular, ofexpert systems. Using current methods, this painstaking process may take
years, and I believe that we as researchers in machine learning should devote new
efforts to this area. Although we had few talks on this subject, it is certainly an
important research direction to explore.
Withtheseremarks, Iproposetocloseourdiscussion. ToallpanelistsIextend
the warmest thanks fortheircontribution.
--- PAGE 56 ---
--- PAGE 57 ---
PART
TWO
LEARNING CONCEPTS AND
RULES FROM EXAMPLES
--- PAGE 58 ---
--- PAGE 59 ---
LEARNING BY AUGMENTING RULES
AND ACCUMULATING CENSORS
PatrickH. Winston
MassachusettsInstitute ofTechnology
Abstract
Thischapterisasynthesisofseveralsetsofideas: ideasaboutlearningfromprecedentsandexercises, ideasaboutlearningusing nearmisses, ideas aboutgeneralizing
if-then rules, and ideas aboutusing censorstopreventprocedure misapplication.
The synthesis enables two extensions to an implemented system that solves
problems involvingprecedentsandexercisesandthatgenerates if-thenrulesasabyproduct. These extensions are as follows:
• If-then rules are augmented by unless conditions, creating augmented if-then
rules. An augmented if-then rule is blocked whenever an unless condition is
easilyshowntobetrue. Whenoneif-thenruleisusedtoblockanotherthrough
an unless condition, the blocking rule is called a censor. Like ordinary augmented if-then rules, censors canbe learned.
• Definition rulesare introducedthatfacilitategraceful refinement. Thedefinition rules are also augmented if-then rules. They work by virtue of unless
entries that capture certain nuances ofmeaning that are different from those
expressible by necessary conditions. Like ordinary augmented if-then rules,
definition rules can be learned.
The strength ofthe ideas is illustrated by representative experiments. All ofthese
experiments have been performed with an implemented system.
--- PAGE 60 ---
46 CHAPTER 3: LEARNING BY AUGMENTING RULES
3.1 KEY IDEAS
This work builds primarily on atheory oflearning from precedents and exercises using constraint transfer (Winston, 1981). The theory addresses the analogy
processatworkwhenweexploitpastexperienceinfieldslikemanagement, political
science, economics, medicine, and law, as well as experience from everyday life.
Two extensions to the theory are described. Work on the first extension was
motivatedby someoftheapparentblundersoftheextantsystem. Workonthesecond
extension was motivated by some problems encountered in making definitions.
First, abriefreviewoftheoverall theory ispresented, followedbyanexample
thatshowshowtherulesgeneratedbytheunextendedlearning systemcanbe misapplied. Next, varioussolutionstothemisapplicationproblemarediscussed, including
theintroductionofcensors. Atthispointaugmentedif-thenrulesareexplained. Each
augmented if-then rule contains not only ifand then parts, but also an unless part.
Beforearuleacts,censorsdetermineifanyexistingfactsdirectlydemonstratethatan
unless relation is true. Ifso, the rule is blocked.
This leads to the development ofdefinition rules based on augmented if-then
rules and a discussion oftheir relevance to the problem ofconcise definition versus
unlimited nuance.
Next, it is shown that censors can block censors and that censors can be
learned, both by precedent and exercise and by near miss.
Finally, precedentsforthisworkitselfaredescribed, includingideasthatstimulated what this author has done, such as Minsky's ideas on the role ofcensors in
problem solving (Minsky, 1980), as well as other ideas that were reinvented or borrowed by the authoras his workprogressed, such as Goldstein and Grimson's ideas
on generalizing if-then rules (1977).
There are references throughout to an implemented system that acquires and
usescensors. This implemented system inheritsthe following key ingredients, all of
which are explained in detail in previously published papers:
• Analogy-basedreasoning using constraint transfer. Reasoning by analogy requires the ability to determine how two situations that are similar in some
respects may be similar in other respects as well. Here the determination is
done by transferring constraining cause relations from the precedent situation
to the exercise situation.
• Learned if-then rules. In contrast to current practice in knowledge engineering, if-then rules emerge automatically as problems are solved. Teachers
supply precedents and exercises, leaving the work of formulating the it-then
rules to the system.
• Rule-basedreasoning. Oncelearned, rulescanbeused. Since rulesare\iewedas
simple situations, the constraint-transfer programs that work with the precedent
--- PAGE 61 ---
WINSTON 47
situationalsoworkwithrules, doingsimplerule-basedreasoning. Thuslearning
and reasoning reside together harmoniously in the same system.
• Actor-object representation. Situations are represented using relations. Each
relationhastrue, false, orunknownasitstruthvalue, andanyrelationcanbean
object involved in another relation.
• Importance-dominated matching. The similarity between two situations is
measuredby findingthebestpossiblematchaccordingtowhatisimportantin
the precedent situation. A precedent relation is considered important if it is
connectedtoanotherrelationbyanimportance-determiningconstraint. Atthe
moment, cause-relation connection is the only importance-determining constraint recognized.
3.2 WHAT IS TO BE UNDERSTOOD
Letusbeginbyreviewingthesortoftaskaddressedbythetheoryaspreviously
reported. Consider the following precis of Macbeth, given by a teacher as a
precedent:
MA isastoryaboutMacbeth,Lady-Macbeth,Duncan, andMacduff. Macbethis
an evilnoble, Lady-Macbeth isa greedy, ambitious woman, Duncan isa king,
andMacduffisanoble.
Lady-MacbethpersuadesMacbethtowanttobekingbecausesheisgreedy. Sheis
abletoinfluencehimbecauseheismarriedtoherandbecauseheisweak. Macbeth murdersDuncan withaknife. MacbethmurdersDuncan becauseMacbeth
wantstobekingandbecauseMacbeth isevil. Lady-Macbethkillsherself. Macduffis angry. Macduffkills Macbeth because Macbeth murderedDuncan and
becauseMacduffisloyaltoDuncan.
Next, considerthe following exercise:
LetEbeanexerciseaboutaweaknobleandagreedylady. Theladyismarriedto
thenoble. InEshowthatthenoblemaywanttobeking.
Told by a teacher that Macbeth is to be considered a precedent, the implemented system announces thatthe precedent suggests thatthe noble may want to be
king. Thenthesystemcreatesaprinciple-capturingif-thenrulethatsuggeststhatthe
weaknessofanobleandthegreedofhiswifecancausethenobletowanttobeking.
The rule looks like this, printed as an if-then rule:
Rule
RULE-1
[LADY-4 IS GREEDY]
[NOBLE-4 IS WEAK]
[[NOBLE-4 IS MARRIED] TO LADY-4]
--- PAGE 62 ---
48 CHAPTER 3: LEARNING BYAUGMENTING RULES
then
[NOBLE-4 WANT [NOBLE-4 A-KIND-OF KING]]
case
Internally,theruleactuallycontainsmoreinformationbecausetheinternalrepresentationpreservesthewaycauserelationstieeverythingtogether, whichismerelysummarized by the if-then form ofthe rule. Figure 3-1 illustrates this.
Here,however, itsuitsourpurposebesttoparaphrasetherulesinEnglish, asin
the following rendition ofthe sample rule:
RULE-1 IF There is agreedy lady
and there is a weak noble
and the noble is married to the lady
THEN the noble may wanttobe king.
Theexerciseproblemcouldhavebeenhandledbythisruledirectly, withoutrecourse
to the Macbeth precedent, were it available when the problem was posed. Thus the
ruleaddspower. Unfortunately, italsoaddsblunder, as whenthe followingexercise
is given:
LetEbeanexerciseaboutaweaknobleandagreedylady. Theladyismarriedto
thenoble. Hedoesnotlikeher. InEshowthatthenoblemaywanttobeking.
King
/ A
want
Noble a-kind-of
A A
Noble Greedy
persuade
Lady Lady
influence
Noble Able Noble
Lady
Weak Married
Figure3-1: The internal representation(ifa rul<
--- PAGE 63 ---
WINSTON 49
Thissituationisdifferentbecauseweknowthatitisdifficultforapersontoinfluence
someonewhodoesnotlikehimorher. Evidently, theruleisoverlygeneral, readyto
reach conclusions when it should not.
This paper introduces extensions to the existing theory such that the implementedsystemactscorrectlyonthegivenexampleandmanyothers. Tobeconsidered
asuccess,however,asystemshouldnotjustwork, itshouldworkbecauseitembodies
arguable ideas. The arguable ideas embodied in the improved system are the
following:
• The blockingprinciple. Suppose a rule, derived from a precedent, seems to
apply to a problem. Consider all the relations in that part ofthe precedent's
causal structure involvedin formingtherule. Ifany such relationcorresponds
toa relationthat is eitherfalse ormanifestly implausible intheproblem situation, then the rule based on theprecedent does not apply.
• Theprimafacieconjecture. A relationismanifestly implausibleifitsnegation
can be shownby adirect, one-step inference from relations already inplace.
3.3 REASONING AND CREATING RULES USING ANALOGY
Letus reviewhowrulesaregenerated. A previouspaperexplainsthis indetail
(Winston, 1981), as does thetextbooktreatment (Winston, 1984).
ConsidertheMacbethprecedent,givenearlier,togetherwiththeexercise,both
expressedinsemantic-networkform, asshowninfigures3-2and3-3. Whenaskedto
demonstrate that the man may want to be king, given the Macbeth precedent, the
system proceeds as follows:
• Thepeopleintheprecedentarematchedwiththepeople intheexercise. More
generally, precedentparts are matched with exercise parts.
• The causal structure ofthe precedent is mapped ontothe exercise.
• Itisdeterminedthatthemappedcausalstructuretiestherelationtobeshownto
relations known tobe true.
• rule is constructed, with generalizations ofthe exercise relations used becoming ifparts and ageneralization ofthe relationtobe shown becoming the
then part.
When asingleprecedentcannotsupply thetotal causal structure needed, the system
attempts tochain several together. In theexample, ifit were not known already that
the woman was greedy, as required forapplication oftheMacbeth precedent, greed
might be established through anotherprecedent oralready-learned rule.
--- PAGE 64 ---
50 CHAPTER 3: LEARNING BYAUGMENTING RULES
Macbeth
Macbeth
cause
Greedy
t I I I I I I 1 t) I ) I I I I i\
persuade « h is
l I I I I I I cause
Lady-Macbeth Lady-Macbeth
influence
Macbeth nunAbbllee \I t cause Macbeth
I I I I
Lady-Macbeth ^- to
Weak Married
Figure 3-2: Problems are solved by transferring the existing cause relations ofa precedent (crossed
lines)ontotheproblemtobesolved(figure3-3).
/' King
Man
want(?)
a-kind-of(?)
Man Greedy
cause V
Woman
Man
Man
Woman
^I~ to
Married
Weak
Figure3-3: Problemsaresolvedbytransferringtheexistingcauserelationsofaprecedent(figure 3-2)
ontotheproblemtobesolved (dottedlines).
--- PAGE 65 ---
WINSTON 51
3.4 IMPROVING PERFORMANCE BY ENABLING CENSORS
Sofarwehaveestablishedthat rulescanbe generatedandthatthey needtobe
blocked in certain circumstances. There are three obvious ways to arrange for
blocking.
First, expandthe ifpart ofan offending rule, restricting its use. One problem
with this idea is that rules can become bloated with endless tests for increasingly
unlikely minutiae. Such bloat makes rules obscure and hardto criticize, debug, and
improve, both for us people and for reasoning programs.
Second, attachcensorstoeach rule. Havethecensorschecktheproblemtobe
solved for contraindications to the rules to which the censors are attached. One
problemisthattherulescanbecomebloatedwithcensornames; thesecensornames
would give noexplicit insight into whenthe rules do not apply.
Third,havecensorswatchforparticularrelations. Forbidanyruleorprecedent
toworktowardestablishingarelationtowhichacensorobjects. Oneproblemisthat
the rules continue to look silly, containing no hint about when they do notapply.
3.4.1 Censors Can Block Augmented If-Then Rules
A better, less obvious idea is this:
• Augment each rule at the time it is generated with entries that correspond to
negations of all the relations in the transferred causal structure, except of
course for those relations that become the ifand then parts ofthe rule. These
augmenting entries constitute the unless part ofthe rule. According to the
blocking principle, ifany entry in the unless part ofthe rule corresponds to
something that is manifestly true, thenthe ruledoes not apply.
Notethatentries intheunlesspartofthe rule aredistinguishedby the way thatthey
are used, not by the fact that the truth value ofmany is false. In our examples, the
unless entries usually will be false relations because they are negations ofrelations
that are usually true. But a precedent's intermediate relations, those from which
unlessentriesaremade, maybefalserelations, leadingtounlessentriesthataretrue
relations.
Clearly, a relation is manifestly true iftheexisting facts indicatethatthe relation is true. But introspectively, it seems unreasonable to go deeply into reasoning
aboutunlessentries. Hencetheimplementedsystemadherestothefollowingspecialization ofthe prima facie conjecture:
• Ifany entry in the unless part ofa rule corresponds to a relation that can be
shown to be true by another rule working directly from relations already in
place, without further inference, then blockthe rule.
Notethatthe restrictiontoone-stepinferenceisanattempttotranslatemanifestlytrue
intoacomputationallyprecisemechanism. Doubtlesstherewillbebettertranslations.
--- PAGE 66 ---
52 CHAPTER 3: LEARNING BYAUGMENTING RULES
Letusconsideranexample. Supposethatarule'sunlesspartistriggeredwhen
someoneisunabletoinfluenceanother. Sucharulewillbeblockedifthepersontobe
influenced does not like the other. The augmented form ofRULE-1 is as follows:
RULE-1 IF There is a greedy lady
and there is a weak noble
and the noble is married to the lady
THEN the noble may want to be king
UNLESS the lady does notpersuade the noble to wantto be king
orthe lady is not able to influence the noble.
The blocking rule is as follows:
RULE-2 IF ThereisapersonXwhodoesnotlikeanotherperson Y
THEN Y is not able to influence X.
A rulebecomesacensorwhen itblockstheapplicationofanotherrule. Becausecensors lookjust like any other rules, they can be learned, stored, and retrieved in the
same ways.
Notethatwhenthe illustratedruleisusedtoblockanother, the illustrated rule
worksonlyifitis knownatthetimeofusethatthereisdislike. Thereisnoattemptto
demonstrate dislike when it is not already known.
Notethattheviabilityoftheprimafacieconjecturedependsontheexistenceof
a rich vocabulary ofrelations. It would be difficult to demonstrate anything in one
step if all relations were reduced to canonical constellations of small-vocabulary
primitives. Thisopensthequestionofjusthowrichthevocabularyshouldbe, aquestion answered operationally by the free use of those relations for which there are
common natural language words.
The viability ofthe prima facie conjecture also depends on the availability of
all solidfactsbeforebackward-chainingproblemsolvingbegins. Thismeansthat all
solid factsareeithergivenordeduced already by forwardchaining from given facts
using reliable, potentially relevant rules. Reliability is insured by forward chaining
onlywithrulesthatreachunassailableconclusions. Relevancecannotbeinsured, but
itcanberenderedmorelikely. Oneway istousethecontext mechanismdescribed in
an earlier paper (Winston, 1981).
3.4.2 Censors Can Block Censors
Actually, it is possible to be influenced by someone you dislike if for some
reason you trust the person in spite ofthe dislike. Perhaps the real able-to-influence
censor should look like this:
RULE-2 IF There is a personX whodoes not like another person Y
THEN Y is not able to influence X
UNLESS X Y
trusts
--- PAGE 67 ---
WINSTON 53
Suchacensorcouldbeblockedbyanothercensorthatstatesthatyoubelieveaperson
ifhe or she has the ability to convince you:
CENSOR-1 IF There is a person X who is able to convince another
person Y
THEN Y trusts X.
To illustrate how these can interact, considerthe following situation:
LetEbeanexerciseaboutaweaknobleandagreedylady. Theladyismarriedto
thenoble. Hedoesnotlikeher. However,theladyisabletoconvincethenoble. In
Eshowthatthenoblemaywanttobeking.
This produces the following scenario:
• Firstthe problem is posed and RULE-1 is fetched. Its ifparts are satisfied.
• Next, the unless part of RULE-1 is examined. The condition involving the
ability to influence causes RULE-2 to be fetched. Its if parts are satisfied.
RULE-1 is aboutto be blocked.
• But RULE-2 's unless part must be examined. The line involving believing
causes CENSOR-1 to be fetched. Its ifparts are satisfied. Thus CENSOR-1
blocks RULE-2, preventing RULE-2 fromblocking RULE-1.
• Finally, RULE-1 succeeds, establishing the relation originally in question.
3.4.3 Augmented If-Then Rules Are Not Rules of Inference
It is tempting to write censors in the following way:
A & & A & ~ (B V V B => C
x . . . n x • • • n)
or alternatively,
A & & A & - B & & ~ B =» C
x . . . n x . . . n
whereA's are in the ifpart ofthe rule andB 's are inthe unless part.
Logical notation is deceptive, however, for in the use of augmented if-then
rules, theA's andB 's are treated differently from each other, in contrast to the conventions oftraditional logic: unlimited effort is to be put into showing the A's are
true; only one-step effort is put into showing that the B's are true, with the B's
assumed false on failure.
Note that rules used as censors are not permitted to create new objects. This
insuresthattheamountofcomputationaddedby theapplicationofcensorstounless
entries is bounded even though censors have their own unless parts that must be
checked by censors. This author believes that it is likely that censor computations
willproveinpracticetobebroadandshallowaswellasbounded, suggestingparallel
implementation.
--- PAGE 68 ---
54 CHAPTER 3: LEARNING BYAUGMENTING RULES
3.4.4 Augmented Rules Suggest an Approach to Certain Definition
Problems
Winograd has discussed the difficulty ofdefinition using the word bachelor
(1976). To be sure, a bachelor is an unmarried adult man, but Winograd notes
that such definition can cause trouble if used when someone says, "Please invite
some nice bachelors to my party," for it would be strange to invite certain kinds of
bachelors. Forexample, Catholic priests and misogamists, although they satisfy the
dictionary definition, are clearly not what a party giver has in mind.
Since the exception possibilities seem limitless, Winograd feels it is inappropriateto rest a definition ofbacheloron aclearly defined, small set ofprimitive
propositions. He argues that it is better to think ofusing some abstract measure of
closenesstoanextensiblesetofexemplars. Woodstakes issuewithWinograd 'sview,
arguing that correct understanding must involve an explicit selection ofa particular
word sense, rather than closeness to a generally applicable exemplar set (Woods,
1981).
Theaugmented-ruleideamayofferaslightlydifferentapproachtotheproblem.
Considerthe following definition ofbachelor, statedas an augmented if-then rule:
RULE-2 IF There is a man
and the man is not married
and the man is an adult
THEN the man is abachelor
UNLESS the man is not expected to be married
orthe man is not able to be married.
Withthisdefinitiontheconclusioncanbeavoided, eventhoughthe ifpartofthe rule
is fully satisfied, providedthatthe individual involved is notabletobe marriedoris
not expected to be married. This takes care ofthe priest and the misogamist problems, given the following censors:
CENSOR-1 IF A man is a misogamist
THEN the man is not expected to be married.
CENSOR-2 IF A man is a priest
THEN the man is not able to be married.
Evidentlytherecanbeasimple, stabledefinitionofbachelorandat the same time an
interaction between growing knowledge about bachelors and the definition, when
appropriate, as the knowledge is accumulated. The definition is used more intelligently as more is learned, and in a sense, the definition is never closed.
How does capturing the meaning ofbachelor with an augmented it-then rule
comparewithotherapproaches'Onepointofview isthat Winogntd'sexemplars correspond to rule-generating precedents, and learned augmented if-then rules correspond to Woods's selectable word senses. Learning about bachelors from precedents
will be discussed in section 3.5.1.
--- PAGE 69 ---
WINSTON 55
3.4.5 Censors Can Improve Precedent Reasoning
Althoughcensorswereoriginally investigated inthisworkinordertocurethe
apparentsillinessofsomelearnedrules, theyhelpinanothercontexttoo. Whenordinary precedent-exercise problem solving is in progress, the analogy part of the
system works back through the causal structure in the precedent, looking for relationsthatcorrespondtorelations intheexercise. Eachtimethe systemfailstofinda
corresponding relation, it does a censor check before moving further through the
causal structure.
Work with a precedent stops ifacensorcheckexposes a relation inthe precedentthatcorrespondstoarelation intheexercisethat is manifestly improbable. The
precedent's intended conclusion isjudged inoperable because the exercise supports
the censor's blocking conclusion.
LEARNING AUGMENTED RULES
3.5
Sincecensorrulesanddefinitionrulesarejustrulesusedinaspecialway, they
canbe learnedjustlikeanyotherrules. Thismaybebydirecttelling, orit maybeby
precedent and exercise, or it may be by near miss.
3.5.1 Augmented Rules Can Be Learned by Precedent and Exercise
Here is aprecedentand anexercise for learning the bachelordefinition rule:
LetSbeastoryaboutCasanova. Casanovaisabachelorbecauseheisamanand
becauseheisexpectedtobemarried. He isexpectedtobemarriedbecauseheis
abletobemarried. He isabletobemarriedbecauseheisanadultandbecausehe
isnotmarried.
LetEbeanexerciseaboutHenry. He isamanandanadult. He isnotmarried. In
EshowthatHenryisabachelor.
Ofcourse, onemightarguethatprovidingtheprecedentinvolvingCasanovais
unrealisticspoonfeeding. Indeedit maywellbe, soitisimportanttounderstandthat
thesamebachelorrulecanbelearnedusingseveralindependentprecedentstogether:
LetSbeastoryaboutaman. He isabachelorbecauseheisexpectedtobemarried. He isabachelorbecauseheisaman.
LetSbeastoryaboutaman. He isexpectedtobemarriedbecauseheisabletobe
married.
LetSbeastoryaboutaman. He isabletobemarriedbecauseheisanadultand
becauseheisnotmarried.
--- PAGE 70 ---
56 CHAPTER 3: LEARNING BYAUGMENTING RULES
Alternatively, the bachelor rule can be learned using several previously learned
rules:
STORY-1 IF There is a man
and the man is an adult
THEN the man is expected to be married.
STORY-2 IF A man is able tobe married
THEN the man is expected tobe married.
STORY-3 IF A man is an adult
and the man is not married
THEN the man is able to be married.
ItisalsopossibletolearnarulethatallowsamarriedMuslimwho isseekinganadditional wife tobe considered abachelor.
3.5.2 Augmented Rules Can Be Learned by Near Miss
Ofcourse, there shouldbe some way ofrecovering ifan impoverished definition is acquired early on. The near-miss idea seems useful in such situations. Considerthis scenario:
• A teachertellsthesystemthatabachelorisanunmarried, adultman. Thisproduces an impoverished definition ofbachelor, one without anything in the
unless part.
• The teacher complains when the system identifies a Catholic priest as a
bachelor.
• Thesystemnoticesthattheonly robustdifferencebetweenthepriestandother
people whoarecorrectly identifiedasbachelors isthatthepriest is notable to
be married.
• The system guesses that bachelors must be able to be married and puts an
appropriate entry in the unless part ofthe bachelordefinition.
Ofcourse, this isaparticularly simple situationbecausethere isbutoneobject
involvedandthedescriptionsaresuchthattherelationthatcausesthenearmiss isthe
only relation that is caused by something and not deemed plausible in a situation
wheretheruledoesnotapply. Itisnotknownhowdifficult it wouldbeto identif) the
right difference in general, but recent work on near-miss groups suggests that the
rightdifferencecanbeidentified, giventhatthereareseveral situations forwhichthe
rule works and should, as well as several for which the rule works but should not
(Winston, 1984).
In theevent that there is no way tonarrow down the possibilities conclusively,
there are two approaches, both ofwhich are under study and deserve attention. One
approach is to do search, perhaps massive search. The other alternative is to do
noil ng. Work by Berwick on syntax acquisition (1C)S2) and by Minsk) on concept
--- PAGE 71 ---
WINSTON 57
learning (draft) both suggest that ifit is difficult to identify the right difference, a
learning system should simply give up, waiting for more transparent examples to
come along.
THE IMPLEMENTED SYSTEM
3.6
The example precedents, exercises, rules, andcensors inthispaperare shown
intheexactEnglishformusedbytheimplementedsystem. TranslationfromEnglish
intothesemanticnetrepresentationusedbythesystemisdonebyaparserdeveloped
and implementedby Katz (Katz, 1980; KatzandWinston, 1982). Thegrammarused
by the parser is also used by a generator, which produces English versions of
the rules.
So far, the system knows a few dozen censors, most ofwhich it has been told
and ofall ofwhich it can learn from precedents or rules and exercises. Clearly, the
numberisenoughtodosurface-scratchingexperimentsandtoillustratetheideas,but
an order ofmagnitude ortwo more will be requiredtodemonstrate the ideas.
OPEN QUESTIONS
3.7
It is plain that this work is only a beginning. Work is in progress on several
related fronts:
• Exploiting several situations forwhich arule worksand should, togetherwith
several forwhich the same rule works but should not, in orderto improve the
rule.
• In collaboration with Ryszard Michalski: generalizing the notion manifestly
improbable in order to devise a variable-precision logic (Michalski and Winston, 1985).
• In collaboration with Thomas O. Binford (Stanford University), Michael
Lowry (Stanford University), and Boris Katz: creating appearance descriptions from functional descriptions, precedents, and examples (Winston etal.,
1983).
• In collaboration with Peter Andreae: using abstractions in matching and in
indexing and retrieving.
• In collaboration with Richard Doyle: the problem ofincorporating time into
the representation.
• IncollaborationwithBorisKatzandothers: retrievingprecedentsfromadatabase sothat they need notbe given by ateacher.
• In responsetoasuggestionbyJ. MichaelBrady: augmentingthe ruleswithan
if-relevantpartinadditiontotheunlesspartdescribedinthispaper. Theideais
thattheif-relevantpartwill somehowkeeptrackoftheultimategoalstowhich
arulemayberelevant, sothattheruleisusedinforwardchainingonlyifoneof
--- PAGE 72 ---
58 CHAPTER 3: LEARNING BYAUGMENTING RULES
thepotentialultimategoalsisinvolvedintheproblemtobesolved. Thiswould
make the rules look like this in logical notation:
A x & ... & A„ & - (Bi V • • . V B n) & (G, V • • - V ft) => C
wheretheA'sareintheifpartoftherule, theB 'sareintheunlesspart, andthe
G's are in the if-relevant part; and where it is understood that only one-step
effortistobeputintotheB
andG's. Thiswouldcomplementtheexistingcontext mechanism explained previously (Winston, 1981).
In addition, the following questions, enumerated in a previous paper, remain open
(Winston, 1981):
• There is no way to handle subcategories of cause such as those sketched by
Rieger(1978).
• There is no way to summarize an episode in a story so as to make a general
precisleadingtomoreabstractrules. Lehnert'ssummarizationworkshouldbe
tried (1981).
3.8 CONCLUSION: SIMPLE IDEAS HAVE PROMISE
Thischapterisaboutasetofideasthatenableimprovementinthereliabilityof
learned rules. Theextendedtheoryenables improvedperformance inthose domains
subject to problem solving by analogy. Such domains satisfy several restrictions:
• The situations in the domain can be represented by the relations between the
parts together and the classes and properties ofthose parts.
• The importance ofa part ofa description is determined by the constraints in
which it participates.
• Constraints that determine something once will tend to do so again.
Things that involve spatial, visual, and aural reasoning do not seem to satisfy
all restrictions. Things that involve management, political science, economics, law.
medicine, and ordinary common sense do seem to satisfy the restrictions, however,
and are targets for the learning and reasoning ideas ofthe theory:
• Actor-object representation.
• Importance-dominated matching.
• Analogy-based reasoning using constraint transfer.
• If-then rules learned by solving problems.
• If-then rules improved by modifications based on near misses.
• If-then rules augmented by unless parts.
• Blocking censors that create fences around rules using prima facie evidence.
--- PAGE 73 ---
WINSTON 59
RELATED WORK
3.9
This work builds on the MACBETH system (Winston, 1979, 1981, 1984),
whichconcentratedonanalogyandruleacquisition. Minsky'sviewsoncensorsalso
had a major influence (Minsky, 1980).
Toalesserextent,theideaoflearningbynearmissisinvolved(Winston, 1970).
However, in this newer work, not only is there a different purpose, there is a much
greaterdegree ofparticipationbythelearning system inthelearningprocess, because
many precedents, rules, and censors have to be accepted or retrieved and made to
worktogether, notjustasinglemodelandnearmiss. Consequently, onthespectrum
ranging from learning by being told to learning by discovery, this newer work lies
furthertoward the learning-by-discovery end.
Theaugmentedif-thenruleisaspecialcaseoftheannotatedif-thenruleintroduced by Goldstein and Grimson in a paper on flight simulation (1977). They suggested that if-then rules should exhibit certain unlesslike conditions (which they
called caveats) as well as rationales, plans, and control information. The work of
BrownandVanLehn (1980)onexplainingsubtractionbugsisamorerecentprecedent
forusing censors to block rules, although theircensors (which they call critics) are
triggeredby what a rule does, ratherthan by unless conditions.
Theideathatcensorsshouldworkonlywiththefactsinhandisavariantonthe
themeofreasoningusinglimitedresources, anideathatisdiscussedwidely, particularly in the expert systems literature.
Itwasobservedinconversationthatthedefinitionofbachelorreallyshouldsay
something about a man's being expectedto be married (John Mallery, private communication) leadingtheauthortotry handlingthebachelorproblemwiththeunless
framework. Another colleague pointed out that the prima facie conjecture does not
make sense unless all reliable, potentially relevant forward chaining is done first
(Boris Katz, private communication). Changing the designator if-plausible, used in
a previous version ofthis chapter, to unless was also suggested by a colleague
(Jonathan H. Connell, private communication).
And ofcourse, analogy was first studied in Artificial Intelligence by Evans
(1963); the matcher in his geometric analogy program ranked matches according to
built-in, implicit importance criteria. Matches viewing two figures as rotations of
oneanother, forexample, were regardedas strongerthan matches viewingthe same
two figures as reflections. Evans was able to use fixed, built-in importance criteria
because he worked only with the world ofsimple figures.
ACKNOWLEDGMENTS
This paper was improved by comments from Robert C. Berwick, J. Michael
Brady, Boris Katz, Michael Lowry, and Karen A. Prendergast.
--- PAGE 74 ---
60 CHAPTER 3: LEARNING BYAUGMENTING RULES
The research was done atthe Artificial Intelligence Laboratory ofthe Massachusetts Institute ofTechnology. Support forthe Laboratory's artificial intelligence
researchisprovidedinpartbytheAdvancedResearchProjectsAgencyoftheDepartment ofDefense underOffice ofNaval Research Contract N00014-80-C-0505.
References
Berwick,R., "LocalityPrinciplesandtheAcquisitionofSyntacticKnowledge,"Ph.D.diss.,Department
ofElectrical EngineeringandComputerScience, MIT, 1982.
Brown,J.S.,andVanLehn,K.,"RepairTheory:AGenerativeTheoryofBugsinProceduralSkills,"CognitiveScience, Vol. 4, No. 4,pp. 379-426, 1980.
Davis, R., "ApplicationsofMetaLevelKnowledgetotheConstruction, Maintenance, andUseofLarge
KnowledgeBases," Ph.D. diss., StanfordUniversity, 1979. (PublishedinKnowledge-BasedSystemsinArtificialIntelligence, R. Davisand D Lenat(Eds.), McGraw-Hill, NewYork, 1980.
Evans, T. G., "A Heuristic Program to Solve Geometric Analogy Problems," in SemanticInformation
Processing, M. Minsky, (Ed.), MITPress,Cambridge, 1968. (BasedonPh.D. diss., Department
ofElectrical Engineering, MIT, 1963.)
Goldstein,I.P.,andGrimson,E., "AnnotatedProductionSystems:AModelforSkillAcquisition,"ProceedingsoftheFifthIJCAI, Cambridge, Mass., pp. 311-17, 1977.
Katz,B., "AThree-StepProcedureforLanguageGeneration,"AIMemoNo. 599, MIT, December 1980.
Katz,B., andWinston,PH.,"ParsingandGeneratingEnglishUsingCommutativeTransformations,"AI
MemoNo.677,MIT,May 1982.Seealso"ATwo-WayNaturalLanguageInterface,"inIntegrated
InteractiveComputingSystems,P.DeganoandE.Sandewall,(Eds.),North-Holland,Amsterdam.
1982.
Lehnert,W., "PlotUnitsandNarrativeSummarization," CognitiveScience, Vol. 5, No. 4. pp. 293-331.
1981.
Michalski, R. S. andWinston, P. H., "VariablePrecisionLogic," AIMemoNo. 857, MIT, 1985.
Minsky, M., "Jokesandthe LogicoftheCognitive Unconscious," AI MemoNo. 603. MIT, November
1980.
Draftonsocietyofmindtheoryofthinking.
Rieger,C, "OnOrganizationofKnowledgeforProblemSolvingandLanguageComprehension"ArtificialIntelligence, Vol. 7, No. 2, 1978.
Winograd,T., "TowardsaProcedural UnderstandingofSemantics" AI MemoAIM-292. StanfordUniversity, November 1976. (AlsoappearsasReportNo. STAN-CS-76-580, DepartmentofComputer
Science, Stanford University, 1976.)
Winston, P. H., "LearningStructural Descriptions from Examples" Ph.D. diss., MIT. 1970. ^ Published
inashortenedversion in ThePsychologyofComputerVision, P. H. Winston(Ed.), McGraw-Hill.
New York. 1975.)
--- PAGE 75 ---
WINSTON 61
-, "LearningbyCreatingandJustifyingTransferFrames,"ArtificialIntelligence, Vol. 10, No. 2,
pp. 147-72, 1978.
, "Learning and Reasoning by Analogy," Communications ofthe ACM, Vol. 23, No. 12,
pp. 689-703, December 1980. (Available with details as "Learning and Reasoning by Analogy:
TheDetails." AIMemoNo. 520, MIT, April 1979.)
"LearningNewPrinciplesfromPrecedentsandExercises,"ArtificialIntelligence,Vol. 19,No.3,
pp.321-50, 1982. (Availablewithdetailsas"LearningNewPrinciplesfromPrecedentsandExercises: TheDetails," AIMemoNo. 632, MIT, May 1981.)
, "Learningby AugmentingRulesandAccumulatingCensors: TheDetails." AIMemoNo. 678,
MIT, May 1982.
,ArtificialIntelligence, 2ded, Addison-Wesley, Reading, Mass., 1984.
"ImprovingLearnedConstraintsUsingSymbolicCorrelation," forthcoming.
Winston,PH.:Binford,T.O.; Katz,B., andLowry,M.R.
"LearningPhysicalDescriptionsfromFunctional Definitions, Examples, and Precedents," Proceedings ofAAAI-83, Washington, D.C.,
pp. 433-39, 1983.
Woods, W. A., "ProceduralSemanticsasaTheoryofMeaning," BoltBeranekandNewmanReportNo.
4627, March 1981. (Alsopublishedin ComputationalAspectsofLinguisticStructures, A. Joshi,
I. Sag, andB. Webber(Eds.), CambridgeUniversity Press, Cambridge, 1982.)
--- PAGE 76 ---
--- PAGE 77 ---
LEARNING TO PREDICT SEQUENCES
Thomas G. Dietterich
Oregon State University
Ryszard S. Michalski*
MassachusettsInstitute ofTechnology
Abstract
Thischapterconsiderstheproblemofdiscoveringarulecharacterizingagiven
sequence of events (objects) and able to predict a plausible continuation of the
sequence. This prediction is nondeterministic because the rule doesn't necessarily
tellexactlywhicheventsmustappearnextinthesequencebutratherdeterminesaset
ofplausible nextevents. It is assumed thatthe individual events in the sequence are
characterizedbyasetofattributesandthatthenexteventdependssolelyonthevalues
oftheattributesforthepreviouseventsinthesequence. Theattributesareeitherinitiallygivenorderivedfromtheinitialonesthroughachainofinferences. Threebasic
rulemodelsareemployedtoguidethesearchforasequence-generatingrule: decomposition, periodic, anddisjunctivenormalform(DNF). The searchprocessinvolves
simultaneously transforming the initial sequences to derived sequences and instantiatinggeneralrulemodelstofindthebestmatchbetweentheinstantiatedmodeland
thederivedsequence. AprogramcalledSPARC/E isdescribedthatimplementsmost
ofthe methodology as applied to discovering sequence-generating rules in the card
game Eleusis. This game, which attempts to model the process of scientific discovery, is used as a source ofexamples illustrating the performance ofSPARC/E.
*OnleaveofabsencefromtheUniversityofIllinoisatUrbana-Champaign.
--- PAGE 78 ---
64 CHAPTER 4: LEARNINGTO PREDICTSEQUENCES
INTRODUCTION
4.1
Inductive learning-that is, learning by generalizing specific facts or
observations-is a fundamental strategy by which we acquire knowledge about the
world. Computer models ofinductive learning have been studied from the AI perspective for several years now, and one result ofthis research has been the identificationofseveraldifferentkindsofinductivelearning. Atleastthreedifferenttypes
have been studied: (1) instance-to-class induction, (2)part-to-whole induction, and
(3) conceptualclustering.
Instance-to-class inductionhasreceivedthe mostattention. Here, thelearning
systemispresentedwithindependentinstancesrepresentingsomeclass, andthetask
isto induceageneraldescriptionoftheclass. The instances canbe specific physical
objects, actions, processes, images, and so on. The learned class description (also
called the conceptdescription) can be used to classify new instances whose correct
class is not known.
An example ofthis type oflearning problem is determining diagnostic rules
from a set ofdiagnosed cases ofdiseases. For example, Michalski and Chilausky
(1980)describealearningprogram, AQ11, thatispresentedwithasetofindependent
training instances, each ofwhich is an example ofa soybean plant with a given disease. The AQ11 program then induces a general description of that disease. This
descriptioncanbeappliedtodiagnosetheoccurrenceofthisdiseaseinothersoybean
plants. Fromseveralhundredsuchtraininginstancescoveringnineteendifferentsoybean diseases, AQ11 has inferred a set of nineteen diagnostic rules. Several other
researchers have investigated instance-to-class learning problems (e.g., Winston,
1970; Buchanan and Mitchell, 1978; Mitchell, Utgoff, and Banerji, 1983). Reviews
of various methods for such instance-to-class induction appear in Michalski,
Carbonell, and Mitchell (1983) and Dietterich et al. (1982).
Thesecondtypeofinductivelearning-part-to-whole induction-hasreceived
lessattention. Part-to-wholeinductioninvolvesconstructingadescriptionofawhole
objectbyobservingonlyselectedpartsofit, forexample, hypothesizingthedescription ofa whole scene, given a set offragments ofthe scene. An important part-towhole inductionproblem isdiscoveringadescriptionofa sequence ofobjects where
the "part"consistsofthefirstkelementsofthesequenceandthe "whole" isthetotal
sequence.
Thistypeofpart-to-wholeinductionproblemhasbeenstudiedinthepastunder
the name of sequence extrapolation or letter-sequence prediction. Simon and
Kotovsky(SimonandKotovsky, 1963; Simon, 1972; KotovskyandSimon, 1973). for
example, study problems inwhichaprogram(oraperson) isgiven partial sequences
such as
ABXBCWCDV...
and asked to predict the next few letters in the sequence. Their program does this b\
--- PAGE 79 ---
DIETTERICH AND MICHALSKI 65
first finding a sequence-generating rule and then applying that rule to predict the
continuation ofthe sequence. In this case, the rule might state thatthe sequence is a
periodically repeating subsequence ofthree letters in which the first two letters are
successors ofthe letter appearing in the previous period and the third letter is the
predecessorofthe corresponding letter in the previousperiod. Related workonthis
typeoflearninghasbeendonebySolomonoff(1964), Hedrick(1976),andHofstadter
(1983, 1985).
Thethirdtypeoflearningproblem-conceptual clustering-has receivedvery
littleattention. Wemention ithereonly forcompleteness. Clusteringproblemsarise
when several objects (or situations) are presented to a learner and the learner must
inventclasses into whichthe objects canbe usefully grouped. An exampleofsucha
problem is learning sound systems in spoken language. The humanear is capable of
distinguishing among a wide variety ofspoken sounds. However, any given human
language groups all ofthese sounds into a relatively small number of equivalence
classes(roughlyfifty). Allsoundswithinagivenclassareregardedasbeingidentical
forpurposesofcommunication. Recently, Michalski(1980)andMichalskiandStepp
(1983) developed a method and a computer program, CLUSTER, for conjunctive
conceptualclustering that can solve such learning problems.
Thischapterpresentsfurtherresearchonthesecondtypeofinductivelearning
problem, thatis, part-to-wholeinduction. We areparticularly interestedinsequence
prediction problems that are much more complex than letter-series prediction.
Letter-series prediction is a very simple problem for two reasons. First, in letter
series, each object in the sequence has only one attribute-its name. Second, the
desired sequence prediction rule is deterministic, because it is assumed thatthere is
only one legal continuation ofthe sequence. This chapterpresents amethod fordiscoveringsequencepredictionrulesincasesinwhichtheobjectsmaybedescribedby
many relevant attributes and the sequence prediction rule is nondeterministic. This
typeoflearningproblemiscalledanondeterministicpredictionproblem, oranNDP
problem.
NDP
In an problem, the learner is presented with a finite sequence ofevents.
Each event is characterized by the values ofa number ofdiscrete-valued attributes.
The goal is to find a sequence-generating rule that, given the first k events, states
the values ofthe attributes1 that must be true ofevent k + 1. Since the sequencegenerating rulemayonly statevaluesforsomeoftheattributes, therulemaynotnecessarily predictauniqueeventk + 1. This is whatmakesthe rule nondeterministic.
Becauseonlyapartialdescriptionoftheoriginal sequence is soughtandbecausethe
description may involve new attributes not present in the initial set, a very large
'Itisassumedthattheruleisexpressedintermsofattributesthatareeitherobservableattributesofobjects
presentinthesequenceuptothemomentwhenanewobjectisgeneratedorattributesthatcanbederived
fromsuchobservableattributesbysomeknown inferencerules.
--- PAGE 80 ---
66 CHAPTER 4: LEARNINGTO PREDICTSEQUENCES
numberofhypothesesmayneedtobeexamined. Thismakesthisproblemmuchmore
difficult than the previously studied letter-series extrapolation problems.
The card game Eleusis (Abbott, 1977; Gardner, 1977) represents a nondeterministicsequencepredictionproblem. We will useexamplesfromthisgametoillustrate the proposed general methodology fordiscovering rules forevent sequences.
4.1.1 Eleusis: An Exemplary Nondeterministic Prediction Problem
An interesting NDP problem arises in the card game Eleusis, invented by
RobertAbbott (1977; Gardner, 1977). Eleusis is an inductivegame in which players
attempt to discover a generating rule (known only to the dealer) for a sequence of
cards. This "secret rule" is invented and recorded by the dealer before the game.
Eachplayer, inhisorherturn, addsonecardtothesequence, andthedealerindicates
whetherthecardisacorrect(orincorrect)extensionofthesequence(i.e., satisfiesor
does not satisfy the secret sequence-generating rule). Players who play incorrectly
arepenalizedbyhavingadditionalcardsaddedtotheirhands. Thegoalofeachplayer
istogetridofall thecards inhis orherhand, which is only possible ifcorrectcards
areplayed. Thecardsplayedduringthegamearedisplayedinthe formofalayoutin
which the correctcards form the "main line" and incorrectcards form "side lines"
branching down from the main line at the card that they follow. Figure 4-1 shows a
typical Eleusis layout for the sequence-generating rule "Play alternating red and
blackcards."Inthisgame,the3ofheartswasplayedfirst, followedbythe9ofspades
andthejackofdiamonds. Allofthesewerecorrect. Followingthejack, the5 ofdiamonds was played. Itappears ona sideline below thejack, because it was not acorrectextensionofthesequence. (Atthispoint, ablackcardisrequired.)The4ofclubs
was then correctly played, and so on.
Below are several examples ofsequence-generating rules for Eleusis:
• Ifthe lastcardwasaspade, playaheart; ifthelastcardwasaheart, playadiamond; ifthelastcardwasadiamond, playaclub; andifthelastcardwasaclub,
play a spade.
• Play a card one point higheror one point lower than the last card.
• Ifthelastcardwasblack, playacardhigherthanorequaltothatcard; ifthelast
card was red, play lower orequal.
• Play alternating even and odd cards.
Mainline: 3H 9S JJDD 44CC JJDD 22CC 1100DD 2C 5H
Sidelines: 5D AH AS 8H
8H 10S 7H
QD 10H
Figure4-1: A sample Eleusis layout.
--- PAGE 81 ---
DIETTERICH AND MICHALSKI 67
• Playstringsofcardssuchthateachstringcontainscardsallinthesamesuitand
has an odd number ofcards in it.
There are fourimportantpointstonote aboutthisgame. First, observethatan
Eleusisruletypicallyallowsanyofseveralcardstobeplayedlegallyaftereachcard.
Hence, Eleusis provides an instance ofthe nondeterministic prediction problem.
Second, noticethatthe rulesemploydescriptorsortermsthatdonotappearin
theinputsequence. Theinputdataprovidesonlythesuitandrank(value)ofeachcard
and its position in the sequence. The sequence-generating rules, however, may
includesuchtermsaseven, odd, black, red, andstringsofcardssuchthateachstring
containscardsallinthesamesuitandhasanoddnumberofcardsin
it.
Thelearning
programmustbridgethisgapbetweenthetermsappearingintheinputsequenceand
thetermsneededforexpressingtherules. Tobridgethisgap, onehastosolvewhatis
called the description space transformationproblem.
The thirdpoint is that several different logical forms are employed to express
the rules. Some rules take the form ofa set ofif-then rules: "Ifthe last card was a
spade, play aheart. ..." Otherrulesare statedas simpledisjunctions: "Play acard
one pointhigheroronepointlowerthanthelastcard." And still others describe the
layout as aperiodically repeating sequence: "Play alternating even and odd cards."
Thelearningprogrammusthavethecapacitytocreatedescriptionsthatcapturethese
different logical forms. The authors' approach to the solution is to divide different
sequence-generatingrulesintoafewgeneralclassesaccordingtothelogical formof
the rules. Each class is represented by an abstract model, or logical schema, which
canbeparameterizedandtheninstantiatedtoyieldaparticularsequence-generating
rule. Forthis reason, this method is called amultiple model learning method.
Finally, itshouldbenotedthatthespaceofpossibleEleusisrulesisverylarge.
Indeed, thereisinprinciplenolimittothenumberofsecretrules. However, tomake
the game interesting to human players, Eleusis has a point-scoring scheme that
encourages the dealer to choose only fairly simple sequence-generating rules. The
program SPARC/Edescribedhere is capable ofrepresenting morethan 10137 rules.2
2Thisestimateisbasedoncomputingthespaceofallsyntacticallylegal VL 22conjunctivestatements(see
section4.3)containingthefollowingsetofdescriptors(eachdescriptorisfollowedbythenumberofelementsinitsvaluesetandthenumberofpossibleselectorsthatcanbeformedusingthoseelements): suit
(4,9), rank(13,91), color(2,3), facedness(2,3), parity (2,3), primeness(2,3), rankmod3 (3,7), dsuitOI (4,9), d-suit02 (4,9), d-rank01 (25,300), d-rank02 (25,300), s-rank01 (25,300), s-rank02
(25,300), d-color01 (2,3), d-color02 (2,3), d-facedness01 (2,3), d-facedness02 (2,3), d-parity01
(2,3), D-PARITY02 (2,3), D-PRIMENESS01 (2,3), D-PRIMENESS02 (2,3), D-RANKMOD3-01 (3,7), Drankmod3-02(3,7).Thesuitandrankmod3descriptorsarecyclicallyordered,andtherankdescriptors
are interval descriptors. All othersare nominal. InadecompositionrulewithalookbackofL = 2, the
firstsevendescriptorsappearthreetimes-onceforeachcard. Hence,thetotalnumberofpossibleconjunctsis(9*91*3*3*3*3*7)3*(9*300*300*3*3*3*3*7)2 = 2.11221*1034 .Iftherearefourconjunctsina
rule, thenweobtain [2.11221*1034 ] 4 - 1.99*10137 .
--- PAGE 82 ---
68 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
Now thattheseremarkshavebeenmade, thethreemainproblemsaddressedin
this chapterare pointed out:
• Transforming the original description space to aid induction
• Applying multiple rule models to discover sequence-generating rules
• Developing a strategy and overall system architecture for learning with multiple models
Inthe nexttwo subsections, the firsttwoproblems are described along with a
review of how each has been solved in existing systems. Discussion ofthe third
problem, the overall system architecture, is postponeduntil section4.4.
4.1.2 Transforming the Description Space
The problem of transforming the initial problem description arises in any
domain in whichthegivendata (e.g., thetraining instances inconceptlearning) are
observationsormeasurementsthatdonotincludetheinformationdirectlyrelevantto
thetaskathand. Forexample, incharacterrecognition, theinputtypicallyconsistsof
a matrix of light intensities representing a character, but the relevant information
includes position-invariantproperties ofletters such as the presence ofa line on the
left or right ofa character, occurrence ofline endings, closed contours, and so on
(e.g., Karpinski and Michalski, 1966). These position-invariant properties can be
made explicitby applying description spacetransformations to the raw data.
Anexampleofalearningprogramthatperformsdescriptionspacetransformation is INTSUM, which is part of the Meta-DENDRAL system (Buchanan and
Mitchell, 1978). INTSUM is presented with raw training instances in the form of
chemical structures (graphs) and associated mass spectra, represented as fragment
massesandtheirintensities. Foreachfragmentinthemassspectrum, INTSUMmust
determinethebondsthatcouldhavebrokentoproducethatfragment. Asimplemass
spectrometer simulator is used to develop these hypothesized bond breaks. Each of
theresultingtransformedtraining instanceshastheformofachemical structureand
asetofbondsthatbrokewhenthatstructurewasplacedinthemassspectrometer. Itis
this information that is provided to the remaining parts of the Meta-DENDRAL
system (programs RULEGEN and RULEMOD).
Incharacterrecognitionprogramsand in Meta-DENDRAL, thedatatransformationsarefixed inadvance. Inmany learningprograms, however, thepropertransformationsare not know a priori. In suchcases, a learning system needs to select or
invent appropriate description space transformations.
The type ofdescription space transformation performed by a program is a
useful criterion forcharacterizing learning methods. The simplest learning methods
(e.g., linear regression) determine only the coefficients for an a priori determined,
fixed set of variables arranged in a predetermined expression. More sophisticated
are learning algorithms, such as the A t( algorithm (Michalski. 1%9. 1972) and the
--- PAGE 83 ---
DIETTERICH AND MICHALSKI 69
candidate elimination algorithm (Mitchell, 1978, 1983), that are able to determine
which attributes are relevant and how they should be combined. Another level of
sophistication is obtained when a learning program applies a set ofpredetermined
transformationstothedatapriortoinductivegeneralization(BuchananandMitchell,
1978; Soloway, 1978). These programs augment the basic inductive algorithms by
applyingasetofpredeterminedtransformationstothedatapriortoinductivegeneralization. Thenextstepofdifficulty is representedbylearningalgorithmsthatselect
descriptionspacetransformationsundertheguidanceofspecialheuristics. Very few
researchers have addressed this problem (e.g., Lenat, 1983; Michalski, 1983). The
most sophisticated algorithms currently envisioned-but not yet developed-would
becapableofdiscovering newdatatransformations. Figure4-2 showsthis spectrum
ofinductive learning problems.
TheSPARC/Emethodpresentedinthischapterfallsundercategory4offigure
4-2,asitselectstransformationsunderheuristicguidance. Theprogramhasavailable
four general classes of description space transformations (see section 4.2) from
which itselectstheappropriateonestoapply undertheguidanceofdomain-specific
heuristics.
4.1.3 Learning with Multiple Models
The second major problem that arises in the Eleusis prediction problem
involvesusingmultipledescriptionmodelsintheprocessofinductivelearning. This
problemhasnotreceivedmuchattentioninpreviousAIresearch. Almostallexisting
learning systems employ only a single model for determining the space ofpossible
output descriptions (hypotheses). Many systems, for example, use conjunctive
models to represent concepts; that is, they assume that the learned concept will be
expressedasaconjunctionofpredicates. Byconstrainingthesearchtoconsideronly
conjunctive descriptions, they greatly simplify the learning problem.
A more general approach, employed by Michalski (1969, 1972), constrains
descriptionstobeindisjunctivenormalformwiththefewestconjunctivestatements.
The induction algorithm first finds one conjunctive statement, then another, and so
on until all ofthetraining instancesarecovered. Meta-DENDRALemploys afairly
elaborate simulatorofthe operation ofthe mass spectrometertoguide its search for
conjunctive cleavage rules (Buchanan and Mitchell, 1978). In general, current
learning systems use a single model, and very few authors have made their models
explicit.
Determinecoefficients
2. Selectrelevantvariablesandcombinethem
3. Applypredeterminedtransformations
4. Selecttransformationsunderheuristicguidance
5. Discovernewtransformations
Figure4-2: Spectrumoflearningproblemsinorderofincreasingdifficulty.
--- PAGE 84 ---
70 CHAPTER 4: LEARNINGTO PREDICTSEQUENCES
One researcher who has employed multiple models is Persson (1966). He
applied four different models to the problem of extrapolating number and letter
sequences. Briefly, these models were the following:
• A model that computes the coefficients and the degree of a polynomial by
applying Newton's forward-difference formula (the degree can be arbitrarily
large).
• Anextendedmodel thatdiscoversexponential rules ofthe formABC whereA
isapolynomialofdegree4orless, andBand Carepolynomialsofdegree 1 or
less (i.e., Band C are ofthe formax + b).
• Asimpleperiodic model forperiodsoflength2 (i.e., intertwined sequences).
• A generalizationoftheKotovsky and Simon model forThurstone letter series
thatcan discover simple periodic and segmented sequence-generating laws.
Thesemodelsareappliedbytheprograminaratherunusuallearningsituation
inwhichtheprogramisgivenasequenceofsequenceextrapolationproblems. Thus,
in addition to attempting to solve each individual sequence extrapolation problem,
Persson'sprogramtriestopredictthekindofsequencepredictionproblemthatitwill
next receive.
Persson's work shows the value ofemploying multiple description models to
search for sequence-generating rules. The major limitation of Persson's approach,
however, isthatitisspecifictonumber-andletter-sequenceprediction. Hismethods
cannotsolvethemoregeneralpredictionproblemsdescribedinthischapterinwhich
eventshavemultipleattributes(bothnumericalandnonnumerical)andthesequences
are characterized by nondeterministic, logic-based sequence prediction rules.
Onecanconceiveaspectrumoffivemodel-basedlearningmethods(seefigure
4-3). The simplest approach is to use a single model. This has been the common
approach inAI thus far. The next step isto provide a learning program with a set of
models from which it wouldchoose the mostappropriate ones. This is the approach
usedby Persson. Thethirdlevelofsophisticationwouldbetohavetheprogramgenerateapredetermined setofmodelsby applying agiven setofdatatransformations.
Thismethodcouldbeimprovedfurtherbyhavingtheprogramdecide which models
to generate on the basis of special heuristics. Finally, an even more sophisticated
program would be able to invent new models and apply them to guide the learning
process.
1. Single model
2. Selection fromafewmodels
3. Predeterminedgenerationofmodels
4. Heuristically guidedgenerationo(models
5. Discoveryofnew models
Figure4-3: Spectrumoi model based methods inordero\~increasingdifficult).
--- PAGE 85 ---
DIETTERICH AND MICHALSKI 71
Theapproachdescribedinthischaptersearchesapredeterminedspaceofpossible models in adepth-first fashion and hence falls under category 3 offigure4-3.
The maintheoreticalcontributionsofthis research includethedevelopmentoftechniques for (1) selecting description space transformations, (2) applying multiple
description models, and (3) matching instantiated models to the transformed
sequences using abidirectional search.
4.1.4 Overview of Solution
This sectiongives an overview ofthe approach to solving NDPproblems presentedhere. Asdescribedabove, thelearningprogram isgivenan input sequenceof
events. Itisassumedthatasequenceofeventsisgivenandthatthetaskistofindanondeterministic sequence-generating rule characterizing the input sequence and able to
predict its plausible continuation. In the proposed solution, the learning program is
suppliedwithvariousoperatorsforsequencetransformationandwithspecificationsof
different rule models. (The implemented method employs four sequence transformation operators andthree rule models.)
The sequence transformation operators are repeatedly applied to the input
sequencetoyieldderivedsequencesinwhichadditionalfactsaboutthesequenceare
madeexplicit. Thisisabottom-upprocessofelaboratingandreformulatingthedata.
Simultaneously, throughatop-downprocess, thegeneralrulemodelsarespecialized
by fillinginvariousparametersandformulastoobtainspecific sequence-generating
rules. Lastly, the learning program applies three model-fitting algorithms (one for
each model) to fit the partially refined model to the transformed data. Thus, the
learning programconductsbothabottom-upelaborationofthedataandatop-down
specialization ofthe rule models until one ofthe model-fitting algorithms can be
applied to find a match between the elaborated sequence and some specialized rule
model. One or more resulting specialized rules are output as candidate sequencegenerating rules.
Figure4-4illustratesthisprocessschematically. Thetop-downmodelspecialization process occurs in parallel with the bottom-up data transformation process,
thus constituting a kind ofbidirectional search.
Model instantiation, asusedinthischapter, isanextensionofthe well-known
AI technique of schema instantiation. Schema instantiation has been applied, for
example, by SchankandAbelson (1975)tointerpretnaturallanguage, by Englemore
and Terry (1979) to interpret X-ray diffraction data in protein chemistry, and by
Friedland (1979) to plan genetics experiments. Model instantiation differs from
schemainstantiationinthecomplexityoftheinstantiationprocess. Model instantiationinvolvesnotonlyfillinginpredeterminedslotsorsubstitutingconstantsforvariables but also synthesizing a logical formula ofan assumed type. For example, in
ordertoinstantiateeachofthethreemodelsdescribedbelow, theprogram mustsynthesizeaconjunctionofpredicatesoradisjunctionofsuchconjunctionsthatsatisfies
--- PAGE 86 ---
72 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
ClassesofModels
Generate
Specific
Model
Specific
1r Model
FitModel i
toData .
Derived
1\ Sequence
Transform
Original
Description
OriginalSequence
Figure4-4: Schematicdescriptionoftherulediscoveryprocess.
certain constraints. Model instantiation methods share with schema instantiation
methodstheadvantagethattheyareefficientandalsoeffectivewithnoisyanduncertain data. The constraints provided by the models (or schemata) drastically reduce
the size ofthe space that the program must search.
Theprincipal disadvantageofmodelandschemainstantiation methods isthey
require that substantial amounts ofdomain knowledge be built into the program. A
ring architecture is employed inthe design ofthe learning program, as described in
section4.4, tokeepthisdomain knowledgeexplicitandeasily modified. This architecturefacilitatestheapplicationofthesystemtoavarietyofproblemsbysimplifying
the process ofchanging the domain-specific parts ofthe program.
The remaining sections ofthis chapterdiscuss the following: (1) the methods
used for representing and transforming the initial training instances; (2) the techniquesforrepresentingthemodelsandsequence-generatingrules; and(3)thedetails
ofthe program SPARC/E, which implements most ofthe described methodology.
The model-fitting algorithms are presented and the program is illustrated by a few
selected examples ofits operation when applied to the inductive card game Eleusis.
4.2 TRANSFORMING THE ORIGINAL DESCRIPTION SPACE
Now that the problemtobe solved (the NDPproblem) has been defined and its
solution sketched, thedetailsofthat solutioncan be presented. First, the language is
--- PAGE 87 ---
DIETTERICH AND MICHALSKI 73
presented fordescribing the original sequence. Then, the transformation operators
are described forchangingthis initial representation intoa form more amenable for
discovering sequence-generating rules.
4.2.1 Representing the Initial Sequence
A sequence ofobjects is represented as an indexed list:3
<qu 92, • • • ,qk>
Eachobjectq
isdescribedbyasetofattributes (alsocalleddescriptors)fu f
2, . . . ,
/„, which can be viewed as functions mapping objects into attribute values. To state
that attribute/ ofobject qj has value r, we write
Ui(qj) r]
This expression is called aselector. Forexample, if/ is colorand ris red, then the
selector
[color(^) = red]
states thatthe colorofthejth object in the sequence is red.
Eachattributeisonlypermittedtotakeonvaluesfromafinitevalueset, called
thedomain D(fy, ofthatattribute. Thisconstraintispartofthe background knowledgethathastobegiventotheprogram. Forexample, inadeckofcards, thedomain
of the suit attribute is {clubs, diamonds, hearts, spades}. Additional knowledge
aboutthedomainsetcanberepresented. Inparticular,thedomainsetmaybelinearly
ordered, cyclically ordered (i.e., have a circular, wraparound ordering), or tree
ordered. We willseebelowhowthesedomainorderingsareappliedtotheproblemof
representing cards in an Eleusis game.
Acompleteinitialdescriptionofasingleobjectqjf calledanevent, isanexpressiongivingthevaluesforalltheattributesapplicableto%. Thisisusuallywrittenasa
conjunction ofselectors:
UMj) = r^mqj) = r 2] . . . [/„($) = rn]
Itcan also be represented as a vector ofattribute values:
(n, r2, . . . ,r n)
Thisvectornotationsuggeststhateachobjectdescriptioncanbeviewedasapointin
the eventspaceE:
E = D(/i) x D(/ 2) X ... X D(f n)
whereD(/-)isthedomainofattribute/. Thiseventspacecontainsallpossibleevents.
3AsummaryofthenotationalconventionsusedinthischapterappearsintheAppendixtothischapter.
--- PAGE 88 ---
74 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
Acompletedescriptionoftheinitial
sequence isalistofconjunctionsofselectors (or alternatively, a list ofattribute vectors)-one conjunction for each object in
the sequence. Each sequence description can thus be viewed as a trajectory in the
eventspaceE.Thespaceofallpossiblesequencesisthesetofallpossibletrajectories
ofeventsinE. Itisimportanttonote, however, thatbecauseofthediscretenessofthe
space, these trajectories are not continuous; that is, two adjacent events in the
sequence may notbe "close" in the event space.
4.2.2 Transformation Operators
As mentioned in section 4.1, it is often necessary to transform the initial
sequence into a derived sequence in order to facilitate the discovery of sequencegeneratingrules. Suchadatatransformationcanbeviewedasamapping 77rom one
setofsequences S, containing objects Q, describedby attributes F, to another set of
derivedsequencesS', containing derivedobjects Q'and described by derivedattributes F'.
T ...:<S,Q,F>^<S',Q',F'>
p, rs
wherep, r, s ... are parameters ofthe transformation that control its application.
Eachtransformationmaybeappliediteratively; thatis, theoutputofonetransformation can be the input to a subsequent transformation. The authors have found four
basic transformations to be especially useful for discovering sequence-generating
rules: (1)addingderivedattributes, (2)segmenting, (3)splittingintophases, and(4)
blocking. Each ofthese will now be described in turn.
4.2.2.1 Adding Derived Attributes
ThesimplesttransformationdoesnotchangethesetofsequencesSorthesetof
objectsQbutonlythesetofattributes F Forexample, inEleusis, theinitialset Fcontains only two attributes: the rank and suit ofa card. These can be augmented by
deriving such attributes as color (red or black), facedness (faced or nonfaced),
parity (odd or even), and primeness (prime or not prime in rank). Although the
adding-derived-attributes transformation has no parameters, in cases where many
such attributes could be derived the program must use some heuristics to decide
which attributes should be generated and added to the derived sequence.
4.2.2.2 Segmenting
The segmenting transformation derives a new sequence that is made up of a
new set ofobjects Q' , which are described by a new set ofattributes F'. The new
sequence is produced from the original sequence by dividing the original sequence
into nonoverlapping segments. Each segment becomes a derived object in the new
sequence. The only parameter ofthis transformation is the segmentation condition
--- PAGE 89 ---
DIETTERICH AND MICHALSKI 75
that specifies how the original sequence is tobe divided into segments. Threetypes
of segmentation conditions are distinguished: (1) those that use properties of the
originalobjectstodeterminewherethesequenceshouldbebroken, (2)thosethatuse
properties of the original objects to determine where the sequence should not be
broken, and (3) those that use properties ofderivedobjects to determine where the
original sequence shouldbe broken.
For example, suppose the original sequence consists of physical objects
described by attributes such as weight, color, and height. An example ofeach
type ofsegmentation condition follows:
1. Break when [weight^, _i) > 10][weight(<?,) < 10].
According to this condition, the original sequence is to be broken (between
<7,_| andq t)atthepointwheretheweightofanobjectchangesfromabove 10to
under 10.
2. Don'tbreakas long as [color(^/_i) = colorC^)][weight^,) > 10]
This condition states that the original sequence will not be broken (between
q - andq) ifthecolorstaysthe same andthe weightremainsabove 10. Itwill
{ \ t
be broken at any point where eitherone ofthese conditions fails to hold.
3. Break sothat [length(^') = 2].
This condition states that derived objects (#,-') should be subsequences of
length 2 from the original sequence (i.e., pairs ofadjacent objects from the
original sequence).
ThechoiceofattributesF'fordescribingthenewlyderivedobjects Q depends
onthesegmentationconditionusedtosegmentthesequence. Forexample, ifthesegmentationconditionis [length^
) = 2], attributesofinterestmightincludethesum
ofthevaluesofthetwooriginalobjects,themaximumvalue,theminimumvalue,
andsoon. The lengthofthesegmentwouldnotbeofinterest, sincebydefinitionit
isaconstant. However, ifthesegmentationconditionis [color(g,_,) = color(^,_
j)],
the length ofthe segment could be an interesting attribute and should be derived.
Also, thecolorsharedbyallofthecards inthe segmentmightbeofinterest. Inthe
implementation described here, the user provides an a priori knowledge base that
specifies which attributes should be derived. Every user-specified attribute is
derived unless the program can prove from the segmentation condition that the
attributewouldnothaveawell-definedvalueforeachsegmentinthesequenceorelse
would be trivially constant for all segments.
Often,asegmentationconditionleadstothecreationofincompletesegmentsat
thebeginningandendoftheoriginal sequence. Theseboundarycasescancreatedifficulties during model instantiation, so they are ignored during rule discovery but
checked during rule evaluation.
--- PAGE 90 ---
76 CHAPTER 4: LEARNINGTO PREDICTSEQUENCES
4.2.2.3 Splitting
ThesplittingtransformationsplitsasinglesequenceintoasequenceofPseparatesubsequencescalledphases: <ph\,ph 2, . . . >. Sequence/?/*,startswiththeobject
q (theobjectofthe ithposition intheoriginal sequence) andcontinues withobjects
taken from succeeding positions at distance P apart in the original sequence. The
objects inphase sequencephiare referredtoas <ph iX ph i2 ph i3 . . . >. Hence, after
splitting, derivedobjectphjjis identicaltooriginalobjectqi+p*(j-i). Pistheparameter of the splitting transformation and denotes the number of phases (the period
length). Figure4-5 shows the splitting operation withP = 3.
The objects within each phase retain the linear ordering that they had in the
original sequence. Thephasesthemselvescanbeconsideredtobecyclicallyordered
sothat/?/*! precedesph 2, whichprecedesph 3, andsoon, untilph P, whichisfollowed
byph again. Consider, forexample, the following sequence:
<1 8 2 9 3 10 4 11>
The splitting transformation with P = 2 would produce the sequence <ph, ph 2>
where
phi = <1 2 3 4>
ph = <8 9 10 11>
Sincethesplittingtransformationsimplybreakstheoriginalsequenceofobjectsinto
subsequences ofthe same objects, no new descriptors are defined. The descriptors
usedtocharacterizeobjectsineachofthephasesarethesameasthoseusedtocharacterize the objects in the original sequence.
The splitting transformation can be applied to break one sequence prediction
problem into several subproblems-one for each phase. This enables the system to
discoverperiodic rules.
4.2.2.4 Blocking
The blocking transformation converts the original sequence into a new
sequence made upofa new setofobjectsB'and a new set ofattributes F'. The new
sequence is created by breaking the original sequence into overlapping segments
Original sequence: <</, q2 qy </4 <7.s <fr <7? ft </»>
Derivedsequence: <ph
ph 2ph^~>, where
ph\\ <ph\,\ ph\i ph\.\>
phi. <p/ii| ph:: pft&a)
/>/*,: </;/»,, ph y: ph, -, N
(whereph tJ = qk as indicatedby vertical alignment, e.g..ph u: « </4.)
Figure4-5: SplittingtransformationwithP = 3.
--- PAGE 91 ---
DIETTERICH AND MICHALSKI 77
calledblocks. Eachobjectb { inthenewsequencedescribesablockofL + 1 consecutiveobjectsfromtheoriginalsequence, startingatobjectq (calledthehead)andproceedingbackwardstoobjectq -L(whereL isthelookbackparameteroftheblocking
transformation). Figure 4-6 shows the blocking operation forL = 2 (block length
of3).
Severalattributesarederivedtodescribeeachblock. Foreachattributeaapplicabletotheobjectsintheoriginalsequence,theattributesaO, a1, alaredefined
. . . ,
thatareapplicabletotheobjectsinthederivedsequence. a0(£,)hasthesamevalueas
A(qd; Al(bi)hasthesamevalueasa(<7,_,); andsoonuntilal(Dj), whichhasthesame
value as a(^
_l). In other words, the original attributes are retained in the new
sequence, buttheyarerenamedsothattheyapplytowholeblocksratherthantoindividual objects in the original sequence. The numerical suffix on the new names
encodes the relative position ofthe original object q in blockby
Forexample, supposethe original sequence ofobjects is <q x q2 q?, g4 q$> with
attributes rank and suit, where
[RANKfo) = 2][SUIT(0,) = H]
[rank(? 2) = 4][suit(4 2) = S]
[RANK(# = 6][SUIT(0 = C]
3) 3)
[RANK(g = 8][SUIT(# = D]
4) 4)
[RANKfeO = 10][SUITES) = H]
SupposeweapplytheblockingtransformationtothissequencewithL = 2toobtain
the derived sequence ofblocks <b 3 b A b 5>. Then the descriptors rankO, rankI,
rank2, suitO, suitI, and suit2 will be derived with the values
b [rank2(6 = 2][suit2(6 = H] &
3: 3) 3)
[RANKlfo) = 4][SUITl(fc 3) = S] &
[rank0(6 = 6][suit0(£ = C]
3) 3)
b 4: [rank2(£ 4) = 4][suit2(& 4) = S] &
[rank1(& = 6][sunT(b = C] &
4) 4)
[rank0(£ = 8][suit0(£ = D]
4) 4)
O D r e i r g i i v n e a d l s s e e q q u u e e n n c c e e : : <q } q2 <b q^ 3 q b * 4 b <? 5 5 b qe 6 b q , i V<78>
whereb,arederivedobjectsdefinedasfollows:
bi <q\ qi qi>
b* <qi q^ q4>
b 5 <qi q* qs>
be <q4 qs q*>
b 7 <<?5 qt qi>
b» <<76 qi £g>
Theuriderlinedobjectsaretheheadobjectsofeachblock.
Figure4-6: TheblockingtransformationwithlookbackparameterL = 2.
--- PAGE 92 ---
78 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
b [rank2(£ = 6][suit2(6 = C] &
5: 5) 5)
[rank\(b 5) = 8][suit1(65) = D] &
[rankO(/? = 10][suitO(Z? = H]
5) 5)
Thistransformation leadstoahighly redundant representationofthe information in the original sequence. Forexample, the information about suit and rank of
theoriginalobjectg3isrepeatedassuitOandrankOofblock/?3, suitI andrankI of
blockb
andsuit2andrank2ofblockb
However, thisderivedsequenceofblocks
facilitates the representation ofthe relationships between objects in the original
sequence. Many sequence prediction rules involve such relationships.
To represent relationships between objects, additional descriptors called sum
anddifferencedescriptorsaredefined. Inthecaseoftheabovesequence,thedescriptorss-rankOI, s-rank02, d-rank01, d-rank02, d-suit01, andd-suit02arecreated.
The value of s-rankOI(^) is the sum of rank0(7?,) and rank1(^ ). The value of
d-rankOKT?/) is the difference between rankO(/?,) and rank 1(£,). Thus, in addition
to the selectors shown above, the following selectors would also be derived for the
new sequence:
b 3: [s-rank01(Z? 3) = 10][s-rank02(^ 3) = 8] &
[d-rank01(6 3) = 2][d-rank02(Z? 3) = 4] &
[d-suit01(£ 3) = 1][d-suit02(Z? 3) = 2]
b 4: [s-rank01(6 4) = 14][s-rank02(£ 4) = 12] &
[d-rank01(6 4) = 2][d-rank02(& 4) = 4] &
[d-suit01(£ 4) = 1][d-suit02(£ 4) = 2]
b 5: [s-rank01(& 5) = 18][s-rank02(^ 5) = 16] &
[d-rank01(6 5) = 2][d-rank02(£ 5) = 4] &
= =
[d-suitOI(^s) 1][d-suit02(£ 5) 2]
Usingthis representation, itis relativelyeasytodiscoverthat [d-rank01(/?,) = 2] is
true forall blocks b
Ordinarily, sumanddifferenceattributesonlymakesenseforattributessuchas
rankwhosedomainsetsarelinearlyordered. Wehaveextendedthedefinitionofdifference tocoverunordered and cyclically ordered domain sets as well. Foran unorderedattributesuchascolor, whosedomainsetis {red, black}
d-color01 takeson
the value ifthe colorO(£,) = coLORl(fr,) and 1 otherwise. For attributes with
cyclically ordered domain sets, such as suit with values {clubs, diamonds, hearts.
spades}, d-suit01 is equal to the number of steps in the forward direction that
are required to get from suit1(£,) to suiT0(fr,). If suit1(/?,) = diamonds and
SUITOR) = Clubs, D-SUITUl(b,) = 3.
The sum and difference attributes make the ordering o\~the original sequence
explicit intheattributesthatdescribeeachblock. Consequently, it isnolongerneces-
--- PAGE 93 ---
DIETTERICH AND MICHALSKI 79
sary to represent the ordering between blocks. Hence, the model-fitting algorithms
discussedbelowtreatthederived sequence (ofblocks) asan unorderedsetofevents.
One difficulty with the above notation is that the numerical suffixes are not
very easy to read, especially when they are combined with sum or difference prefixes. Hence, an alternative representation has been developed that is more comprehensible. Inthis notation, selectorsthatrefertoblocks, such as [sunT(fr,) = H],
are written as selectors that refer to objects in the original sequence, such as
[suit(^/_!) = H]. Similarly, selectors such as [d-rankOI(^) = 3] are written as
[rank(<7,) = rank(^,_i) + 3]. This notation makes the meaning ofthe selectors
clear without the blocks b being explicitly mentioned.
For purposes ofimplementation, however, the first notation (which refers to
blocks explicitly) is better because it enables the program to treat all sequencesincluding derived sequences-uniformly. In contrast to this, the second notation
works only whenoneblockingtransformationatmost is applied. Multipleblocking
transformations cannot be captured without the blocks being explicitly mentioned.
Since it is rare that more than one blocking transformation is needed, and since the
second notation is more understandable, it will be used forthe rest ofthis chapter.
4.3 REPRESENTING SEQUENCE-GENERATING RULES AND
MODELS
A sequence-generating rule is a function g that assigns to each sequence of
objects <qu q2, . . . , qk> a nonempty set ofadmissiblenextobjects Q k+\\
g(<qu q2, . • • ,qk>) = Q k+]
Q k+1 isthe setofall objectsthatcouldappearasthe nextobjectinthe sequence. For
example, in the rule "Play a card whose rank is one higherthanthe previous card,"
thevalueofthefunctiongwhenappliedtoasequencewhoselastcardisthe4C isthe
set ofcards {5C, 5D, 5H, 5S}. This is written
g«. . .4C» = Q k+] = {5C, 5D, 5H, 5S}
Each set Q k+l maycontainonly oneevent, orit maycontainalarge setofpossibleevents. Ifforallk, thesequence <quq2, . . . , qk> ismappedbygintoasingleton
set, thenthe ruleisadeterministicrule; otherwise, itisanondeterministicrule. This
chapter addresses the problem of discovering a nondeterministic sequencegeneratingrule,g, giventhesequence <qu q2, . . . , qk> whereq { arecharacterizedby
a finite set ofdiscrete attributes.
--- PAGE 94 ---
80 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
The sequence <q\, q2, . . . , g*> can be viewed as the set ofassertions
qi g(<tfi>)
^ E #(<#,, . . . , ^_i>)
(Recallthatthevalueofg(s), wheresisasequence, isase?ofpossible nextobjects.)
These assertions are positive instances ofthe desired sequence-generating rule.
In Eleusis, negative instances are provided by the cards on the sidelines-that
is, thecardsindicatedbythedealerasbeingincorrectcontinuationsofthe sequence.
A sideline card q* played aftercard q-± provides a negative instance ofthe form:
qf $ g(<q\,qi, qi>)
Thegoal istofindanexpression forgthatis consistentwiththese training instances
and satisfies some preference criterion. (An expression g is consistent with the
training instances ifit characterizes all ofthe positive instances and excludes all of
the negative instances.)
The preference criterion in this methodology, and generally in learning systems,attemptstoevaluateacandidateruleintermsofitsgenerality,predictivepower,
simplicity, and so on. These semantic properties are difficult to compute, however.
Instead, virtually all learning systems employ syntactic criteria that correspond in
somewaytothese semanticcriteria. Syntacticcriteria-suchasthenumberofselectors in aconjunction and the number ofconjuncts in a disjunction-will only correspond to the semantic criteria ifthe representational framework is well chosen (see
McCarthy, 1958). As noted in the introduction, most previous AI research on
learning hasemployeda single representational frameworkormodel fordescribing
the rules or concepts to be learned. In Eleusis, a single framework is insufficient.
Instead, three basic models have been developed that were found to be useful: the
DNF model, thedecomposition model, and theperiodicmodel. When these models
areemployed, syntactic criteriacanbe usedtoapproximate semantic criteriaduring
evaluation.
A model is a structure that specifies a general syntactic form for a class of
descriptions (in our case, sequence-generating rules). A model consists of model
parametersandasetofconstraintsthatthemodelplacesontheformsofdescriptions.
The processofspecifyingthe values forthe parametersofa model iscalledparameterizing the model. The process offilling in the form ofthe parameterized model is
called instantiating the model. A fully parameterized and fully instantiated model
forms a sequence-generating rule. Models can be instantiated using the original
sequence or, more typically, using a sequence derived by the application o\ some of
the data transformations discussed in the previous section.
--- PAGE 95 ---
DIETTERICH AND MICHALSKI 81
Allthree modelsusethevariable-valuedlogic calculus VL224forrepresenting
sequence-generatingrules. VL22isanextensionofthepredicatecalculusthatusesthe
selector as its simplest kind of formula. The VL 22 selector is substantially more
expressive than the simple selector presented above in section 4.2.1. Recall that the
simple selectorhas the form
IMqj) r]
whereas the VL 22 selectorhas the form
[fi(x\,x2, . . . ,x n) = r x V r2 V • • • V rj
IntheVL22 selector, attributes^cantakeanynumberofarguments (xux 2, . . . , x n).
Furthermore, the attributes/cantake on any one ofasetofvalues {ru r2,
. . .
rm).
The V denotestheinternaldisjunctionoperator, thatis, thedisjunctionofvaluesof
the same attribute. Thus, the selector
[rank(4,) = 9 V 10 V J V Q V K]
indicatesthattherankofobjectq
canbeeither9, 10,J, Q, orK. Inthiscase,thesame
selector couldbe expressed alternatively as
[RANKfe) 9],
since the domain ofthe rank attribute is known tobe linearly ordered with a maximum value of K (king). To aid comprehensibility, VL22 provides the operators <
> , < , > , and =£, in addition tothebasic = operator.
Examples oftypical selectors include:
• [rank(^) ^ RANK(#/_i)]
(paraphrase: the rank ofq
is different fromthe rank ofg,_j)
• [surrfa) = suit(^_,) + 1]
(paraphrase: the suit increases by one from q x - \ to q {)
• [RANK(g,) + RANK((7,_ 2) > 10]
(paraphrase: the sum ofthe ranks ofq t and ^ ; _ 2 is greaterthan 10)
Now thatthebasicnotationofVL22hasbeenintroduced, eachofthethreerule
models is presented in turn.
4.3.1 The DNF Model
TheDNFmodelsupportsthebroadclassofrulesthatcanbeexpressedasauniversally quantified VL^ statementindisjunctivenormal form. The DNF model has
oneparameter, thedegreeoflookbackL. Anexampleofa DNF rule (withL = 1) is
4VL22denotesversion2ofthevariable-valuedlogiccalculussystemVL2.
--- PAGE 96 ---
82 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
V/([color(^) = COLOR(^-j)] V [Rank(<?,) = RANK(<7,_,)])
whichcanbeparaphrasedas "Everyobject(qi) inthesequencehasthesamecoloror
the same rank (orboth) as the preceding object (#;_j)."
DNF
In general, a rule is acollection ofconjuncts (C,) ofthe form
ViXdfe) V C 2(</,) V ... V C k(qd)
Theuniversalquantificationoveriindicatesthatthisdescriptionistrueforallobjects
qi in the sequence.
Anadditionalconstraintspecifiedinthe DNFmodelisthatthenumberofconjuncts k shouldbe close tothe minimum that produces adescription consistent with
the data.
4.3.2 The Decomposition Model
Thedecompositionmodelconstrainsthedescriptiontobeasetofimplications
ofthe form
u =>*.
U =>tf
Lm =» R„
where the => sign indicates logical implication.
Themodel statesthattheleft-and right-hand sides, L y andR jt mustall be VL 22
conjunctions. Theleft-handsidesmustbemutuallyexclusiveandexhaustive-thatis.
the following two statements are true:
L, V U V ... V L m and
Vj,k (j * k) => - (Lj & L k)
Thefirststatementsaysthatatleastoneoftheleft-handsidesL isalwayssatisfied. Thesecondstatementsaysthatif/andkaredifferent, thenLjandL
cannotboth
be satisfied simultaneously.
A decomposition rule describes the next object in the sequence in terms of
characteristics ofthe previous objects in the sequence. Forexample, the rule
V/(([color(^,_,) = black] => [parityO?,) = odd]) &
([colorO?,-!) = red] => [parity(^) = even]))
isadecomposition rule that saysthat ifthe lastcard was black, the next card must be
odd and ifthe last card was red, the next card must be even.
The decomposition model has a lookback parameter L that indicates how far
backinthesequencethesequence-generatingrulemust "look" inordertopredictthe
nextobject inthe sequence. Theabove rule hasa lookback parametero( 1. because it
examines </,_ (the previous object in the sequence).
--- PAGE 97 ---
DIETTERICH AND MICHALSKI 83
4.3.3 The Periodic Model
This model consists of rules that describe objects in the sequence as having
attribute valuesthat repeatperiodically. Forexample, the rule "Play alternating red
and black cards" is a periodic rule. The periodic model has two parameters: the
periodlengthP, andthelookbackL. TheperiodlengthparameterPgivesthenumber
ofphases in the periodic rule. A periodic rule can be viewed as applying a splitting
transformation to split the original sequence into P separate sequences. Each separatephasesequencehasasimpledescription. ThelookbackparameterLtellshowfar
back, within a phase sequence, a periodic rule "looks" in order to predict the attributes ofthe next object in that phase. The periodic model imposes the additional
constraint (orpreference) thatthedifferentphasesbedisjoint (i.e., any givencard is
only playable within one phase).
A periodic ruleisrepresentedasanorderedP-tupleofVL22conjunctions. The
jth conjunction describes the7th phase sequence. The rule
<[color(/?/*m ) = red], [rank(/?/z 2,/) ^ RANK(/?/& 2>/-i)]>
isaperiodicrulewithP = 2andL =
whichsaysthatthesequenceis madeoftwo
(interleaved)phases. Eachcardinthefirstphaseisred;eachcardinthesecondphase
has a rank at least as high as the preceding card in that phase. Hence, one sequence
that satisfies this rule is <2H 3C 10H 5S AD 6S 6H 6C>.
4.3.4 Derived Models
Thethreebasicmodelscanbecombinedtodescribemorecomplexrules. Basic
models can bejoined by conjunction, disjunction, and negation. For example, the
rule "Play alternating red and black cards such that the cards are in nondecreasing
order" is a conjunction ofthe periodic rule
<[color(/?/*u ) = red], [coLOR(ph 2j) = black])
and the DNF rule
[RANK(<?,) RANK(tf,_,)].
4.3.5 Model Equivalences and the Heuristic Value of Models
The reader may have noticed that the decomposition and periodic models
appear to be special cases of the DNF model. In particular, assuming that the
--- PAGE 98 ---
84 CHAPTER 4: LEARNINGTO PREDICTSEQUENCES
left-handsideclausesinadecompositionrulearemutuallyexclusiveandexhaustive,
the decomposition rule
U =>/?!
L => R
2 2
L m =£> R m
DNF
can be written as the rule
[L, & *,] V \U & RA V • • • V \Un & R m]
Similarly, ifthephases (C,) ofaperiodic rulearemutuallyexclusiveandexhaustive,
then the periodic rule
<Cj, C Q>
2, • . . ,
can be reexpressed as adecomposition rule ofthe form
C, => C
c => c
2 3
C ^ Q
This transformation from the periodic model into the decomposition model
does not work when the phases ofthe periodic rule overlap. Consider, forexample,
the following periodic rule in which the phases are neither mutually exclusive nor
exhaustive:
<[coLOR(ph {i) = red],[RANK(/?/* 2,/) = even])
(paraphrase: play alternating red and even cards)
Becausethephasesoverlap, thetransformation intoadecomposition rule produces a slightly different rule:
[coLOR(g,_,) = red] => [parity(<7,) = even] &
[parity(<7,_,) = even] => [color(^) = red]
To see how the two rules differ, considerthe sequence ofcards
<3D2D4C
. . . >
Thissequencesatisfiesthesecond rule (thefirst if-thenclausecanboappliedtwice),
but not the first rule (since the 4C is not red).
--- PAGE 99 ---
DIETTERICH AND MICHALSKI 85
Evenwhentheconstraintsofmutualexclusionandexhaustionareviolated,
itis
alwayspossibletodevelopsomeequivalent DNF ruleforanyperiodicordecomposition rule. This is so because one can always provide additional descriptors that capturetheparticularrelationship. Theresulting DNF rulesarenotalwaysassuccinctor
comprehensible, however, asthe sameruleexpressedusing oneoftheothermodels.
Considerthis same periodic rule. Suppose that a new descriptor, called position, is defined whose value for each object q { is the position / ofthe object in the
sequence. With this descriptor, the above rule can be encoded as
[position^,) = odd] => [color(#,) = red] &
[position^,) = even] => [parity(<?,) = even]
Since any sequence-generating rule can be expressed in the DNF model, it is
reasonable to ask why multiple models should be used at all. The answer is that the
primaryvalueofmultiplemodelsisthattheyprovideheuristicguidancetothesearch
forplausible rules. Hence, though the DNF model is capable ofrepresenting all of
theserules, itisnothelpfulfordiscoveringthem. Inshort, itisepistemologicallyadequate but not heuristically adequate (see McCarthy and Hayes, 1969; McCarthy,
1977). Eachmodeldirectstheattentionofthelearning systemtoasmall subspaceof
thespaceofallpossible DNFVL22rules. Thenextsectionshowshowtheconstraints
associated with each model are incorporated into special model-fitting induction
algorithms.
4.4 ARCHITECTURE AND ALGORITHMS
Section 4.3 described the three basic processes involved in discovering
sequence-generatingrules: (1)transformationoftheoriginalsequenceintoaderived
sequence, (2)selectionofanappropriatemodel(anddeterminationofitsparameters)
for the given sequence, and (3) fitting (instantiation) ofthe models to the derived
sequence. Sections4.2 and4.3 presentedthe fourdatatransformations andthethree
models. This section covers the third step offitting specialized models tothe transformedsequence. Themodel-fittingprocessismosteasilyunderstoodinthecontext
ofthe program architecture, so this section also discusses the system's architecture.
4.4.1 Overview of the System
The processes in the system (see figure 4-7) are structured into four
components-the three basic ones mentioned above plus an evaluation component.
The processes oftransforming the initial sequence and of selecting and parameterizing a model are performed in parallel. Then model-fitting algorithms use the
transformed sequence to instantiate the parameterized model to obtain a candidate
sequence-generating rule. Thesecandidate rules arethen evaluatedtodeterminethe
final set ofrules.
--- PAGE 100 ---
CHAPTER 4: LEARNING TO PREDICTSEQUENCES
Transformationofthe Selectionofamodel
original sequenceinto anddeterminationof
aderivedsequence itsparameters
Instantiationoftheparameterized
modeltofitthederivedsequence
Generationofcandidaterules
Evaluationofcandidaterules
and
Selectionofthebesthypothesis
Figure4-7: Basicprocessesinrulediscovery.
The reasonforperformingdatatransformationandmodel selection inparallel
is that these two processes are interdependent. For example, ifa periodic model is
selected (with period length P), then a splitting transformation (with number of
phasesP) needstobeappliedtothesequence. Thesetwoprocessescanbeviewedas
simultaneous, cooperative searches oftwo spaces: the space ofpossible data transformations and the space ofpossible parameterized models.
4.4.2 Overview of the Concentric Ring Architecture
Inorderforthelearningsystemtobeeasilymodifiedtohandleentireclassesof
NDP problems, the system is structured as a set ofconcentric knowledge rings (see
figure4-8). A knowledge ring is a set ofroutines that perform a particular function
using only knowledge appropriate to that function. The procedures within a given
ringmay invokeotherproceduresinthatringorinringsthatareinsidethegiven ring.
Under these constraints, the concentric ring structure forms a hierarchically organized system.
Ideally, the rings should be organized sothat the outermost ring uses the most
problem-specificknowledgeandperformsthemostproblem-specificoperationsand
the innermost ring uses the most general knowledge and performs the most general
tasks. Such an architecture improves the program's generality because it can be
applied to increasingly different NDP problems by the removal and replacement o\'
the outer rings. Ifthe program is to be applied to radically different learning problems, all but the innermost ring may need to be replaced.
--- PAGE 101 ---
DIETTERICH AND MICHALSKI 87
Figure4-8: Theknowledgeringarchitecture.
Theringarchitectureisusedhereasfollows. Theoutermostringsperformuser
interface functions and convert the initial sequence from whatever domain-specific
notation is being used intoa sequence ofVL22 events. The innermost ring performs
themodel-fittingfunctions. Itexpectsthedatatobeproperlytransformedsothatthe
datahavethesameformasthemodelstowhichtheyaretobefitted. Theintervening
rings conduct the simultaneous processes of developing a properly parameterized
model and transforming the input sequence into an appropriate derived sequence.
The intervening rings alsoevaluatethe rulesdiscoveredbytheinnermost ring
usingthe knowledge available in each ring.
4.4.3 The System SPARC/E
SPARC (Sequential PAttern Recognition) is a general program designed to
solve a variety ofnondeterministic prediction problems using the ring architecture.
So far, only a more specific version of the program, called SPARC/E, has been
implemented. SPARC/E is tailored specifically to the problem ofrule discovery in
the game Eleusis. It is made up offive rings, as shown in figure 4-8.
This section describes the functions of each ring in SPARC/E. The Eleusis
layoutshowninfigure4-9willbeusedtoillustratetheseringfunctions. Recallthatin
an Eleusis layout, the main line shows the correctly played sequence ofcards (positive examples). The side lines, which branch outbelowthe main line, containcards
Main line: 3H 9S 4C JD 2C 10D 8H 7H 2C
Sidelines: JD AH AS 10H
5D 8H 10S
Figure4-9: SampleEleusislayout.
--- PAGE 102 ---
88 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
thatdonotsatisfytherule-thatis, incorrectcontinuationsofthesequence (negative
examples).
4.4.3.1 Ring 5: User Interface
Ring5,theoutermostring, providesauserinterfacetotheprogram. Itexecutes
user'scommandsforplayingthecardgameEleusis,aswellascommandsforcontrolling the search, datatransformation, generalization, and evaluation functions ofthe
program. OnecommandinRing5 isthe INDUCEcommandthatinstructsSPARC/E
to look for plausible NDP rules that describe the current sequence. When the
INDUCEcommand isgiven, Ring5callsRing4tobegintherulediscoveryprocess.
Ring 5 provides Ring 4 with an initial sequence of VL 22 events in which the only
attributes are suit and rank.
4.4.3.2 Ring 4: Adding Derived Attributes
Ring 4 applies the adding-derived-attributes transformation to the initial
sequenceofcards. Thisinvolvescreatingderivedattributesthatmakeexplicitcertain
commonly known characteristics of playing cards that are likely to be used in an
Eleusis rule: color, parity, faced versus nonfacedcards, and so on. The user of
theprogramprovidesadefinitionforeachdescriptorthatistobederived. Figure4-10
shows the layout from figure4-9 after ithas been processed by Ring 4. The plusses
VL 22event Positiveor
negative
[RANK(card,) = 3][suiT(card,) = H] &
[PARiTY(cardi) = odd][coLOR(card|) = red] &
[pRiME(cardi) = N][FACED(card|) = Y] +
[ [ [p P R R A A i R N M i K E T ( ( Y c c ( a a c r r a d d r 2 d 2 ) ) 2) = = = 9 N ] o ] [ d [ s d F u ] A i [ C T( c E c D o a ( L r c O d a R 2 r ( ) d ca 2 = ) rd = S 2 ] ) N & = ] black] & +
[ j [ p P R R A A i R N M i K E T ( ( Y c c ( a a c r r a d d r i d ) 3) 3) = = = J Y ] o ] [ [ d s F d u A ] i C [ T E ( c D c o a ( L r c O d a R 3 r ( ) d c . = a O rd D =3 ] ) Y & = ] red] &
[ [ [ P p R A R A R i N i M K T E ( ( Y c c ( a a c r r a d d r 3 d 3 ) ) 3) = = = 5 N ] o ] [ d [ s d F u ] A i [ C T c ( E c o D a L ( r O c d R a 3 r ( ) d ca = 3) rd = D 3 ] ) Y = & ] red] &
P R
A A
R N
i K
T (
Y c
( a
c r
a d
r 3 d
3) =
= 4
] e
[ v
s e
u n
n ]
( [
c c
a o
r L
d O
3 R
) (
= a
r C
d ] 3)
= black] &
|RANK(card 4) = J](suiT(card4) = D] &
(PARiTY(card4) = odd][coLOR(card 4) = red] &
|pRiME(card 4) = Y]|FACED(card 4) = Y] +
etc.
Figure4-10: Derived layoul alter Ring4 processing ofthe layout in figure4 i>
--- PAGE 103 ---
DIETTERICH AND MICHALSKI 89
and minuses along the right-hand side ofthe figure indicate whether the event is a
positive example or a negative example of the sequence-generating rule. These
derived events are passed to Ring 3 for furtherprocessing.
4.4.3.3 Ring 3: Segmenting the Layout
Ring3 isthefirstEleusis-independentring. Itappliesthesegmentingtransformation to the sequence supplied by Ring 4. In the present implementation, the end
points of each segment are determined by applying a segmentation predicate
P(card,_
card,)toallpairsofadjacenteventsinthesequence. WhenthepredicateP
evaluatestoFALSE,thesequenceisbrokenbetweencard,-
andcard,toformtheend
ofa segment. Typical segmentation predicates used are the following:
[RANK(card,) RANK(card,_i)]
[RANK(card () =
RANK(card,_!) + 1]
[coLOR(card,) coLOR(card,_i)]
[suiT(card ;)
suiT(card,-_i)]
[PARiTY(cardy) PARiTY(card,_i)]
Other techniques forperforming segmentation, such as providing a predicate
that becomes TRUE at a segment boundary (see section 4.2.2.2), are not implemented in SPARC/E.
Ring 3 searchesthe spaceofpossible segmentations usingtwosearch-pruning
heuristics. After each attempt to segment the sequence, it counts the number of
derivedobjects(segments)kinthederivedsequence. If&islessthan3,thesegmentationisdiscardedsincetherearetoofewderivedobjectstouseforgeneralization. IfA:
ismorethanhalfofthenumberofobjectsintheoriginal sequence, thesegmentation
is also discarded because in this case many segments contain only one original
object. Segmentedsequencesthatsurvivethesetwopruningheuristicsarepassedon
to Ring 2 for furtherprocessing.
One segmentation that Ring 3 always performs is the "null" segmentationthatis, italwayspassestheunsegmented sequencedirectlytotheinnerrings. Figure
4-11 showsasamplelayoutandthe resultingderivedlayoutaftersegmentationusing
the segmentation condition [suiT(card,) = suiT(card/_!)]. The derived objects
(segments) are denoted by variables segment,. The negative event
[suiT(segment 2) = D][coLOR(segment 2) = red][LENGTH(segment 2) = 3] is
obtainedfromthesegment<5D 2D 4D>
whichendsina"side-line" card. Notice
thatthevery lastcard inthe sequence, theking ofspades, is notincludedinany segment. Thisisbecausethekingisthefirstcardofanew segment, anditis impossible
to know how long that segment will be until it is completed. Once a sequencegenerating rule is found by the inner rings, Ring 3 will check to make sure that the
king ofspades is consistent with the rule.
--- PAGE 104 ---
90 CHAPTER 4: LEARNINGTO PREDICTSEQUENCES
Thelayout:
3H 5D 2D 7C AC 9C JH 6H 8H QH KS
5S 4D AH
Thederivedsequence:
Descriptionof Positiveor
derivedobject negative
[suiT(segmenti) = H][coLOR(segment,) = red] &
[LENGTH(segmenti) =1] +
[suiT(segment 2) = D][coLOR(segment 2) = red] &
[LENGTH(segment 2) = 2] +
[ [ s L u E i N T G ( T s H e ( g s m e e g nt m 2 e ) nt = 2) D = ][c 3] oLOR(segment 2) = red] & -
[ [ s L u E i N T G ( T s H e ( g s m e e g nt m 3 e ) nt = 3) C = ][c 3] oLOR(segment 3) = black] & +
[suiT(segment4) = H][coLOR(segment 4) = red] &
[LENGTH(segment =4] +
Figure4-11: Samplelayoutandsegmentedsequenceundersegmentationcondition
[suiT(card]) = suiT(card,_i)].
SPARC/E derives the descriptors color, suit, and length to describe each
derivedobject. Thechoiceofwhichdescriptorstoderive involvesthree steps. First,
length is derived whenever the segmentation transformation is applied. Second,
any descriptorthat is tested inthe segmentation predicate (in this case, suit) is also
derived. Third, any descriptor is derived whose value can be proved to be the same
forallcardsineachsegment. Inthiscase, color isderivedbecause, ifsuit isaconstant, then color is also a constant. Using this segmentation, SPARC can use the
DNF model to discoverthatthe segmented sequence can be described as
[LENGTH(segment/) = LENGTHCsegment/-]) + 1]
That is, the length of each segment of constant suit (in the main line)
increases by 1.
4.4.3.4 Ring 2: Parameterizing the Models
Ring2 searchesthespaceofparameterizationsofthethreebasic models. Each
model isconsidered inturn. Foreach model. Ring2 developsa setofderivedevents
based on each allowed value ofthe lookback parameter L and the number ofphases
parameterP. The usercancontrol which models should be inspected and what ranee
ofvalues forLand Pshould be investigated. Bydefault, the program will inspect the
--- PAGE 105 ---
DIETTERICH AND MICHALSKI 91
decompositionmodelwithL = 0, 1, or2 andtheperiodic model withP = 1 or2or
L = or 1. The DNF model is not inspected under the default settings for the
program.
Specifically, Ring 2 performs the actions listed below depending on which
model is being parameterized:
Thedecompositionmodel. ForthedecompositionmodelwithlookbackparameterL,
Ring2appliestheblockingtransformationtobreakthesequencereceivedfromRing
3 intoblocksoflengthL. Afterblocking, alloftheattributesthatdescribedtheoriginal objects are converted intoattributesthatdescribethe wholeblock (asdiscussed
insection4.2.2.4above). Furthermore, sumanddifferencedescriptorsarederivedto
represent the relationships between adjacent objects in the original sequence. The
resulting derived events can be viewed as very specific if-then clauses of the following form:
Given an initial sequence ofobjects <qu q2, . . . , qm>, let us look at block b l
which describes the subsequence <q t -L, • • , <fr-i, qi>- Let Fj, j = 0, 1, . . . , L,
denotethe setofselectorsdescribing object<?/_, renamed sothatthey apply toblock
bj. For example, F, could be selectors [sunT(Z?,) = H][rank1(6/) = 3]-selectors
thatoriginally referredtoobjectg,_,. Letd(Fj, F k) denoteall ofthedifference selectors obtainedby "subtracting" eventF k from eventF jf and let s(Fj, F k) denote all of
the summation selectors obtained by "summing" events Fj and F k. For example,
d(F , F,) could include the selectors [d-suit01(&,) = 2][d-rank01(£,) = - 3]
obtained from "subtracting" F, fromF
With these definitions, the derived events for the decomposition model have
the following form:
F, & . . . & F L => F & d(F , F,) & . . . & d(F , F L) &
s(F , F.) & ... & s(F , F L)
Suppose, forexample, thatthe initial sequence ofcards is
<2H 4D 6S 8C>
with only the suit and rankdescriptorsbeingemployed. Then supposethatRing2
appliestheblockingtransformationwithalookbackof2. Figure4-12 showsthetwo
derivedeventsthatwillbeproducedby Ring2 (thecorrespondingnotationis shown
tothe right ofeach group ofselectors).
Thesederivedeventsnolongerneedtobeordered, sincetheorderinginformationis madeexplicitwithintheevents. Theseeventshavetheformofveryspecificifthen clauses. This facilitates the model-fitting process in Ring 1.
The DNF model. For the DNF model with lookback parameter L, the sequence
derived in Ring 3 is blocked in avery similarmanner, exceptthat only the selectors
--- PAGE 106 ---
92 CHAPTER 4: LEARNING TO PREDICTSEQ
Derivedevent Abbreviation
[RANKl(^_,) = 4][SUITl(^_,) = D] F,
[rank2(<7,_ 2) = 2][suit2(4,_ 2) = H] F 2
-^.
[rank0(4,) = 6][suit0(^,) = S] Fo
[d-rank01(#,-,#,•_|) = 2][d-suit(<7,,^,_.) == 2] <*(F,F,)
[ [ d s - - r r a a n n k k 0 0 2 1 ( ( ^ ^ „ - < , 7 ^ , _ _ i) 2) = = 1 4 0 ] ] [d-suit(<7,,^,_2) '= 1] d(F ,F 2)
[s-rank02(^,^_ 2) - 8] s(F,¥2)
[rank1(<7,_,) = 6][suit1(^,_i) = S] Ft
[rank2(<7,_ 2) = 4][suit2(^,_ 2) = D] F 2
[rankO(^) = 8][suitO(<7,) = C] Fo
[d-rank01(9/,9/_i) = 2][d-suit(<7„<7,_1) == 1] </(F ,F,)
[ [ d s - - r r a a n n k k 0 O 2 I ( ^ ^ , , , , ^ ^ , , - _ ,) 2) = == 1 4 4 ] ] [d-suit(^,,^,_2) ' -3] < j * ( ( F F , , F F , 2 ) )
[s-rank02(<7
,4,_2) = 12] s(F,F2)
Figure4-12: Sampleeventsshowingwhichsumanddifferencedescriptorsarederived.
describing q { are retained inthe description ofblockb {. The derived events have the
following form:
F & d(F , F.) & ... & d(F , F L) & s(F 0i F,) & . . . s(F , F L)
These events are very specific conjuncts that are passed to the A q algorithm
(Michalski, 1969, 1972) in Ring 1, where they are generalized to form a DNF
description.
Theperiodic model For the periodic model with period length P and lookback L,
Ring 2 performs a splitting transformation followed by a blocking transformation.
First, the sequence obtained from Ring 3 is split into P separate sequences. Then
each separate sequence is blocked into blocks oflength L + 1. The derived events
havethe same form as the events derived forthe DNF model. Note that because the
blocking occurs afterthe splitting, the lookback takes place only within a phase.
ToprovideanexampleofthefunctionofRing2, figure4-13 showssomeevents
from figure 4-10 after they have been transformed in preparation for fitting to a
decomposition model with L = 1.
4.4.3.5 Ring 1: The Basic Model-fitting Algorithm
Ring 1 consists ofthree separate model-fitting algorithms: the Aq algorithm.
the decomposition algorithm, and the periodic algorithm.
--- PAGE 107 ---
DIETTERICH AND MICHALSKI 93
Derivedevent Positiveor
negative
[ [p R a A r N i K t l y f 1 o (l ) b-.) = = 3] o [ d s d u ] rr [ l c (f o c l 2 o ) r = 1(^ H 2 ] ) = red]
[PRIME1(^2) = Y][FACED\{b2) = N]
[rank0(6 2) = 9][suit0(£ 2) = S][parityO(£ 2) = odd]
[ [ c f o a l c o e r d O O ( ( 2 ^ > 2 2 ) ) = = N b ] la [ c d k - ] r [ a p n r k i 0 m 1 e ( O £ (£ 2) 2)= = + N 6 ] ]
[D-SUITO10 = +l][D-PARITY01(fc = N]
[d-color01( 2 6 ) 2) = Y][d-prime01(^ 2 2 ) )= Y]
[d-faced01(£ 2) = Y][s-rank01(6 2) = 12]
[ [
p R
a A
r N
i K
ty 1
1 ^
( 3
^ )
3) =
= 9
] o
[ d S
d U
] I
T [
c 1
( o
^ l
3 o )
1(^ S
3 ] )
black]
[rankO(£ 3) = J][suitO(& 3) = D][parityO(& 3) = odd]
[colorO(£ 3) = red][PRiMEO(/?3) = Y]
[facedO(& = Y][d-rankO10 = +2]
[d-suitOI 3 ^ ) ) = +2][d-parity0 3) 1(£ = N]
[d-color01(& = Y][d-primeO10 = Y]
[d-facedO10 3 3 ) ) = Y][s-rankO10 3) 3)= 20]
Figure4-13: Someeventsoffigure4-10transformedfordecompositionL = 1.
The A q algorithm (Michalski, 1969, 1972) is applied to fit the DNF model to
thedata. A qattemptstofindthe DNF descriptionwiththefewestnumberofconjunctivetermsthatcoversallofthepositiveexamplesandnoneofthenegativeexamples.
The algorithm operates as follows: First, a positive example, called the seed, is
chosen, andthesetofmaximallygeneralconjunctiveexpressionsconsistentwiththis
seed and all ofthe negative examples is computed. This set is called astar, and it is
equivalent to the G-set in Mitchell's (1978) version space approach (ifthe G-set is
computedwiththe seedpositiveexampleandall ofthenegativeexamples). Oneelementfromthis starischosentobeaconjunctintheoutput DNF description, andall
positiveexamplescoveredbyitareremovedfromfurtherconsideration. Ifanypositive examples remain, the process is repeated; some positive example that was not
covered by any member of any preceding star is selected as a new seed. In this
manner, a DNF description with few conjunctive terms is found. If the stars are
computed without any pruning, then Aqcanprovide atight bound on the number of
conjuncts that would appear in the shortest DNF description (i.e., with the fewest
conjunctive terms).
Thedecompositionalgorithmisan iterativealgorithmthatseekstofitthedata
toadecomposition model. Thekeytaskofthedecompositionalgorithmistoidentify
afewattributes, calleddecompositionattributes, fromwhichthedecomposition rule
--- PAGE 108 ---
94 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
can be developed. A decomposition attribute is an attribute that appears on the lefthand sideofan if-thenclauseofadecomposition rule. Forexample, the decomposition rule
[coLOR(card,_i) = black] => [PARiTY(card,) = odd] &
[coLOR(card/_!) = red] => [PARiTY(card,) = even]
decomposes on color. Hence, color is the single decomposition attribute.
The algorithm uses a generate-and-test approach ofthe following form:
decomposition-attributes := {} The empty set
while rule is not consistent do
begin
generate a trial decomposition
(based on positive evidence only)
for each possible decomposition attribute
test these trial decompositions against
the data
select the best decomposition attribute and
add it to the set decomposition-attributes
end
Theprocessofgeneratingatrialdecompositiontakesplaceintwosteps. First,
aVL22conjunction is formed foreachpossiblevalueofthedecomposition attribute.
All positive events that have the same value ofthe decomposition attribute on their
left-hand sides are merged together to form a single conjunction of selectors. This
VL22 conjunction forms the right-hand side ofa single clause in the decomposition
rule. Withinthisconjunction, aselectoriscreatedforeachattributebytheformation
ofthe internal disjunction ofthe values inthe corresponding selectors inthe events.
Forexample, usingall oftheeventsderived inRing2 forthe sample layout in figure
4-9, the decomposition algorithm generates the trial decomposition shown in figure
4-14 forthe PARiTY(card
_1) attribute.
Since there areonly twovalues (oddandeven) forthedecomposition attribute
inthe sequenceshown in figure4-9, twoconjunctionsare formed. The firstconjunction is obtained by merging all ofthe positiveevents forwhich [PARiTY(card,_ 1) =
odd]. There are four such events. The first selector in that conjunction.
[RANK(card,) = 9 V 4 V 2], isobtainedby formingthe internal disjunctionofthe
values ofRANK(card ;) in each ofthe fourevents.
--- PAGE 109 ---
DIETTERICH AND MICHALSKI 95
[PARiTYCcard,-]) = odd]
[RANK(card,) = 2 V 4 V 9]
[suiT(card,) = S V C][PARiTY(card,) = even V odd]
lack][PRiME(card () = Y V N]
[D-RANK(card„card,_i) = -7 V -5 V +6]
[D-suiT(cardMcard,_,) = 1 V 2 V 3]
[D-PARiTY(card„card,_i) = Y V N]
[D-coLOR(card„card,_i) = Y V N]
[D-PRiME(card,,card,_,) = Y V N]
[D-FACED(card„card,_i) = Y V N]
[s-RANK(card„card,_,) = 12 V 13 V 9] &
[PARiTY(card,_i) = even]
[ [s R u A i N T K ( ( ca c r a d r , d ) () = = H 7 V V D 8 ][ V PARi 1 T 0 Y( V car J d ] ,) = even V odd]
[coLOR(card,) = red][PRiME(card,) = Y V N]
[FACED(card,) = Y V N = ] -lV -2V7V8]
[D-RANK(card„card_,)
[D-suiT(card,,card,_ i i) = V 1]
[D-PRiMECcard/^ard,-!) = Y V N]
[D-FACED(cardMcard,_,) = Y V N]
[s-RANKCcard^card,-,) = 12 V 15 V 18]
Figure4-14: TrialdecompositiononthePARiTY(card,_,)attribute.
Thesecondstepinformingatrialdecompositionistogeneralizeeachclausein
thetrial rule. Thegeneralizationisaccomplishedbyapplyingrulesofgeneralization
to extend internal disjunctions and drop selectors. (See Michalski, 1983, for a
description ofvarious rules ofgeneralization.) Corresponding attributes in the differentclausesofthedecompositionrulearecompared,andselectorswhosevaluesets
overlap are dropped. When these rules of generalization are applied to the trial
decomposition of, for example, parity, the following generalized trial decomposition is obtained:
[PARiTYCcard,-!) = odd] ~ [suiT(card,) = C V S][coLOR(card,) = black] &
[PARiTY(card/_,) = even] - [suiT(card,) = H V D][coLOR(card/) = red]
This is avery promising trial decomposition. However, ithas been developed
using only positive evidence-without considering the possibility that it may cover
somenegativeevents. Hence,thetrialdecompositionmustbetestedagainstthenegative events to determine whether or not it is consistent. It turns out that the generalized trial decomposition shown above is indeed consistent with the negative
evidence.
--- PAGE 110 ---
96 CHAPTER 4: LEARNINGTO PREDICTSEQUENCES
After a trial decomposition has been developed for each possible decomposition attribute, the best decomposition attribute is selected according to a heuristic
attribute-quality functional. The attribute-quality functional tests such things as the
numberofnegativeeventscoveredbythetrialdecomposition, thenumberofclauses
withnonnullright-handsides, andthecomplexityofthetrialdecomposition(defined
as the numberofselectors thatcannotbe written with a single operatorand a single
value). The chosen trial decomposition forms acandidate sequence prediction rule.
Ifthecandidateruleisnotconsistentwiththedata(i.e., stillcoverssomenegativeexamples),thenthedecompositionalgorithmmustberepeatedtoselectasecond
attributetoaddtotheleft-handsidesoftheif-thenclauses. Thishastheeffectofsplitting each ofthe if-then clauses into several more if-then clauses. Forexample, if we
first decomposed on PARiTY(card,_ 1) and then on FACED(card,_,), we would obtain
four if-then clauses ofthe form:
[PARiTY(card,_!) = odd][FACED(card,_i) = N] =>
. . .
[PARiTY(card,_i) = odd][FACED(card,_!) = Y] =£>
. . .
[PARiTY(card,_!) = even][FACED(card/_!) = N] =>
. . .
[PARiTY^ard/.O = evenllFACEDCcard/-!) = Y] =>
. . .
The periodic algorithm is similar to the decomposition algorithm. For each
phaseoftheperiod, ittakesallofthepositiveeventsinthatphaseandcombinesthem
toformasingleconjunctbyformingtheinternaldisjunctionofallofthevalue setsof
corresponding selectors. Next, rules ofgeneralization are appliedtoextend internal
disjunctionsanddropselectors. Finally, correspondingattributes indifferentphases
arecompared, andselectorswhosevaluesetsoverlaparedropped ifthiscanbedone
without covering any negative examples.
4.4.3.6 Evaluating the NDP Rules
Once Ring 1 has instantiated the parameterized models to produce a set of
rules, the rules are passed back through the concentric rings ofthe program. Each
ring evaluates the rules according to plausibility criteriabased on knowledge available in that ring. Ring 2, for example, applies knowledge of the fact that valid
sequencescanbecontinuedindefinitely. Itcheckstoseethattherulepredictsthatthe
sequencecouldbesocontinued. Ring3, whichappliesthesegmentationtransformation, applies its knowledge about the tail end ofthe unsegmented sequence to make
sure it is consistent with the rule. (Recall that the segmentation transformation is
sometimes unable to segment the last few events in the sequence.) Ring 4 tests the
rule using the plausibility criteria for Eleusis. These criteria are as follows:
1. Preferrules with intermediatedegreeofcomplexity. In Eleusis. Occam's razor
does notalwaysapply. The dealer is unlikely tochoose a rule that is extremely
simple, because it wouldbetooeasytodiscover. Verycomplex rules will not be
--- PAGE 111 ---
DIETTERICH AND MICHALSKI 97
discoveredby anyone, and sincethe rules ofthe game discourage such anoutcome, the dealer is not likely to choose such complex rules either.
2. Preferruleswithanintermediatedegreeofnondeterminism. Rules withalow
degree ofnondeterminism lead to many incorrect plays, thus rendering them
easy to discover. Rules that are very nondeterministic generally lead to few
incorrect plays and are therefore difficultto discover.
Rules that do not satisfy these heuristic criteria are discarded. The remaining
rules are returned to Ring 5 where they are printed forthe user.
EXAMPLES OF PROGRAM EXECUTION
4.5
Inthissection, someexampleEleusisgamesarepresented, alongwiththecorresponding sequence-generating rules that were discovered by SPARC/E. Each of
thesegameswasanactualgameplayedamongpeople, andtherulesarepresentedas
they were displayed by SPARC/E (with minortypesetting changes).
The raw sequences presented to SPARC/E had only two attributes: suit and
rank. SPARC/E was given definitions ofthe following derivable attributes:
• color (red forhearts and diamonds; black for clubs and spades)
• face (true ifthe card is a faced, picture card, false otherwise)
• prime (true ifthe card has aprime rank, false otherwise)
• mod2 (the parity value ofthe card, ifthe card is even, 1 otherwise)
• mod3 (the rank ofthe card modulo 3)
• lenmod2 (when SPARC/E segments the main sequence into derived subsequences, itcomputes the length ofeach ofthe subsequences modulo 2)
Threeexamplesoftheprogramexecutionarepresented. Hereare somepoints
tonoticeinreadingtheexamples: First,eachruleisassumedtobeuniversallyquantified over all events in the sequence. This quantification is not explicitly printed.
Second, when the value set of a selector includes a set of adjacent values-for
example, [RANK(card,) = 3 V 4 V 5]-this is printed as [RANK(CARDI) -
3 . . 5] . The computation times given are for an implementation in Pascal on the
CDC CYBER
175.
4.5.1 Example 1
Inthis example, we showtheprogramdiscovering a segmented rule. The program was presented with the following layout:
Main line: AH 7C 6C 9S 10H 7H 10D JC AD
Side lines: KD 5S QD
--- PAGE 112 ---
98 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
continued: 4H 8D 7C 9S IOC KS 2C 10S JS
3S 9H QH
6H AD
The program only discovered one rule for this layout, precisely the rule that the
dealerhad in mind (1.2 seconds required):
RULE 1: LOOKBACK: NPHASES: 1 PERIODIC MODEL
SEGMENTATION CONDITION = [COLOR CARDI)=COLOR(CARDI-l)]
PERIOD< [LENM0D2(PH1SEGMENTI)=1] >
The rule states that one must play strings ofcards with the same color. The
strings must always have odd lengths. The segmentation condition states that a segmentisastringofcardsallofthesamecolor. CARDI referstothe/thcardintheoriginal sequence. SPARC/E discovered this rule as a degenerate periodic rule with a
periodlengthPof1. Hence, PH1SEGMENTI referstothe7thsegmentinphase 1 (the
onlyphase)ofthederivedsequence. Actually, therulethatthedealerhadinmindhas
one additional constraint: a queen must notbe played adjacent to ajack orking.
4.5.2 Example 2
The second example requires theprogramtodiscovera fairly simple periodic
rule. Here is the layout:
Mainline: JC 4D QH 3S QD 9H QC 7H QD
Sidelines: KC 5S 4S 10D
Main line continued: 9D QC 3H KH 4C KD 6C JD 8D
Main linecontinued: JH 7C JD 7H JH 6H KD
The program discovered three equivalent versions ofthe rule, which can be
paraphrasedas "Playalternatingfacedandnonfacedcards." Herearethe rules (0.49
seconds required):
RULE 1: LOOKBACK: 1 NPHASES: DECOMPOSITION MODEL
[FACE(CARDI-1)=FALSE] =>
[RANK(CARDI)>JACK]
[RANK(CARDI) >RANK(CARDI- l)
[ FACE ( CARDI)=TRUE] &
--- PAGE 113 ---
, >
DIETTERICH AND MICHALSKI 99
[FACE(CARDI-l)=TRUE] =>
[RANK(CARDI)=3 . . 9]
[RANK(CARDI) <RANK(CARDI- l)
[FACE(CARDI) =FALSE]
RULE 2: LOOKBACK: 1 NPHASES: 1 PERIODIC MODEL
PERI0D<[RANK(PH1CARDI) >3]
[RANK(PHICARDI) *RANK(PH1CARDI- 1)]
[FACE(PHICARDI) *FACE(PH1CARDI- 1)]
RULE 3: LOOKBACK: 1 NPHASES: 2 PERIODIC MODEL
PERIOD<[RANK
PH1CARDI) >JACK]
[RANK(PHICARDI) > -RANK(PH1CARDI- l)+20]
[FACE
PH1CARDI)=TRUE]
[RANK(PH2CARDI)=3 . . 9]
[RANK(PH2CARDI)=-RANK(PH2CARDI-l)+5 14]
. .
[FACE(PH2CARDI)=FALSE]>
Rule 1 isadecompositionrulewithalookbackof1. Rule2expressestheruleas
adegenerate periodic rulewithasinglephase. Rule 3 expressesthe rule inthe "natural" way as a periodic rule oflength 2.
Notice that, although the program has the gist ofthe rule, it has discovered a
number of redundant conditions. For example, in rule 1, the program did not use
knowledge of the fact that [RANK(card/)>jack] implies [FACE(card ;) = true],
and therefore it did not remove the former selector. Similarly, because ofthe interaction ofthe two conditions in rule 1, [RANK(card,)>RANK(card / _ 1)] is completely
redundant.
4.5.3 Example 3
The third example shows the upper limits ofthe program's abilities. During
this game, only one ofthe humanplayersevengotclosetoguessing the rule, yetthe
--- PAGE 114 ---
] > ,
100 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
programdiscoversagoodapproximationoftheruleusingonlyaportionofthelayout
that was available to the human players. Here is the layout:
Main line: 4H 5D 8C JS 2C 5S AC 5S 10H
Side lines: 7C 6S KC AH 6C AS
JH 7H 3H KD
4C 2C QS
10S 7S
8H 6D
AD 6H
2D 4C
The program produced the following rules after 6.5 seconds:
RULE 1: LOOKBACK: 1 NPHASES: DNF MODEL
[RANK(CARDI)<5][SUIT(CARDI)=SUIT(CARDI-1)+1] V
[RANK(CARDI) > 5] [SUIT(CARDI)=SUIT(CARDI- 1)+3]
RULE 2: LOOKBACK: 1 NPHASES: 1 PERIODIC MODEL
PERI0D<[RANK(PHICARDI) =RANK(PH1CARDI- 1) - 9]
[RANK(PHICARDI) = -RANK
PH1CARDI-1)+4,5,7,11,13,17]
[SUIT(PH1CARDI)=SUIT(PH1CARDI-1)+1,2,3]>
RULE 3: LOOKBACK: 1 NPHASES: 2 PERIODIC MODEL
PERIOD<[RANK(PH1CARDI)=ACE,2,8,10]
[RANK(PHICARDI) = -RANK(PH1CARDI- 1)+1,8,9,10]
[RANK(PH2CARDI)=5 JACK] [SUIT(PH2CARDI) =SPADES]
. .
[RANK(PH2CARDl)=RANK(PH2CARDI-l)+ -0 . . 6]
[RANK(PH2CARDI)= -RANK(PH2CARDI-l)+8 14]
. .
[SUIT(PH2CARDl)=SUIT(PH2CARDI-l)+0 . . 2]
[COLOR(PH2CARDI)=BLACK] [PRIME(PH2CARDI) =PTRUE]
[PRIME(PH2CARDI) =PRIME(PH2CARDI- l)
[M0D2(PH2CARDI) = 1] [M0D2(PH2CARDI- l) =M0D2(PH2CARDI-l)+0]
[M0D2(PH2CARDI) = -M0D2(PH2CARDI-l)+0] [M0D3(PH2CARDI) = 2]
[M0D3(PH2CARDl)=M0D3(PH2CARDI-l)+0]
[M0D3(PH2CARDI) = -M0D3(PH2CARDI-l)+l]
The rule that the dealer had in mind was the following:
[suiT(card,) = suiT(card,_!) + 3]
[RANK(card,)>RANK(card,_i)] V
--- PAGE 115 ---
DIETTERICH AND MICHALSKI 101
[surr(card,) = surrCcardf-i) + 1]
[RANK(card,)
<RANK(card/_
j)]
Thereisastrongsymmetryinthisrule: theplayersmayeitherplayalowercard
inthenext "higher" suit(recallthatthesuitsarecyclicallyordered) orahighercard
in the next "lower" suit. The program discovered a slightly simpler version ofthe
rule (rule 1) that happened to be consistent with the training instances. Note that
adding 3 to the suit has the effectofcomputing the next lower suit.
Theothertworulesdiscoveredbytheprogramareverypoor. Theyaretypical
ofthekinds ofrulesthattheprogramdiscovers whenthemodeldoesnotfitthedata
very well. Both rules are filled with irrelevant descriptors and values. The current
program has very little ability to assess how well a model fits the data. These rules
should not be printed by the program, because they are highly implausible.
SUMMARY
4.6
Amethodology has beenpresented fordiscovering sequence-generating rules
foraclassofnondeterministicpredictionproblems. Themainideasbehindthismethodology are (1) the use ofdescription space transformations ofthe initial data and
(2) the use ofdifferent rule models to guide the search forthe candidate rules. Four
different description space transformations (adding attributes, blocking, splitting
intophases, and segmenting) andthree models (DNF, periodic, anddecomposition)
have been presented.
The main part ofthe methodology has been implemented in the program
SPARC/E and applied to the rule discovery problem in the card game Eleusis. The
experiments with the programdemonstratedthat it can discover most ofthe Eleusis
secret rules played in ordinary human games and that it often outperforms human
players.
Themethodologyisquitegeneralandcanbeappliedtoothernondeterministic
predictionproblems in whichthe objects inthe initial sequence are describableby a
set of finite-valued attributes. The main strengths of the method are that it can
(1) solvelearningproblemsinwhichtheinitialtraininginstancesrequiresubstantial
description space transformation and (2) search very large spaces ofpossible rules
using a set ofrule models forguidance.
Many aspectsofthismethodology remaintobeinvestigated. Wehavenotconsidered NDPproblems in which (1) thetraining instances are noisy, (2) thetraining
instances have internal structure so that an attribute vector representation ofthese
instancesisnotadequate, and(3)thesequence-generatingrulesarepermittedtohave
exceptions. Application ofthis methodology to real-world problems will probably
also require the development of additional sequence transformations and rule
models. Also, more heuristics need to be developed that can be used to guide the
application oftransformations and models.
--- PAGE 116 ---
102 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
Despiteitsabilitytooutperformhumanplayers, thecurrentimplementationof
SPARC/E has several shortcomings. The program presently conducts a nearly
exhaustive depth-first search of the possible models and transformations. Much
could be gained by having the program conduct a best-first, heuristically guided
search instead. The present implementation does not include the ability to evaluate
the plausibility of the rules it discovers. It is also not able to simplify rules by
removingredundantselectors,norisitabletoestimatethedegreeofnondeterminism
oftherule. Thelasttwocanbeimplementedwithouttoomuchdifficultybytheinclusionofinferenceroutinesthatmakemorecompleteuseofthebackgroundknowledge
already availabletotheprogram. Finally, an importantweaknessofthecurrentprogram is its inability to form composite models.
In addition to these specific problems, there are some more general problems
thatfurtherresearchintheareaofsequence-generatingrulesshouldaddress. First, in
somereal-worldproblems, theremaynotbeone, butseveralexamplesequencesthat
aregovernedbythe same sequence-generating rule. Suchproblemsoccur, inparticular, indescribingtheprocessesofdiseasedevelopmentinmedicineandagriculture.
A specific problem of this type that has been partially investigated involves predicting the time course ofcutworm infestation in a cornfield and estimating the
potentialdamagetothecrop(seeDavis, 1981 Bairn, 1983; andBoulanger, 1983). In
this problem, several sequences ofobservations are available-one for each field
and there is a needtodevelopageneral sequence-generating rulethatpredicts all of
these sequences.
A second general problem for further research is handling processes in which
timeisacontinuousvariable. Inparticular, thereistheproblemofprogramsthatperform qualitative modeling of such processes and qualitative evaluation of trends
based on example event sequences. AI research has so far given little attention to
these tasks.
ACKNOWLEDGMENTS
Earlier versions of this chapter appeared in the Proceedings ofthe InternationalMachineLearning Workshop (1983) and subsequently in theArtificialIntelligenceJournal, Vol. 24, No. 2, pp. 187-231, 1985. The authors wish tothankthe referees of the Workshop and the Journal for their helpful suggestions and to
North-Holland Publishing Company, Amsterdam, for permission to print this versionhere. ThanksalsogotoDannyBerlinforbringingseveralerrorstoourattention.
Theauthorsaregrateful to Robert Abbott for inventingthe game Eleusis. which was
the original motivation for this research and the source ofexamples for SPARC/E.
They also thank Donald Michie forchallenging them to develop such a program.
A part ofthe work was done while the second author worked at the Artificial
Intelligence Laboratory at the Massachusetts Institute of Technology. Support for
MIT's Artificial Intelligence research is provided in part by the Advanced Research
--- PAGE 117 ---
DIETTERICH AND MICHALSKI 103
Projects Agency under Office ofNaval Research Contract No. N00014-80-C-0505.
The authors gratefully acknowledge the partial support of the National Science
Foundation under Grant DCR-84-06801 and the Office of Naval Research under
Grant No. N00014-82-K-0186.
APPENDIX: NOTATIONAL CONVENTIONS
The following notationalconventionsareemployed inthischapter. Ingeneral, lowercase lettersdenote
objectsinsomesequence(q,ph,b)orindexvariables(i,j,k)orthelengthsofsequences(m,n).Uppercase
lettersdenotesetsofobjects,attributes,andsoon(Q,F,S),aswellasparametersofmodelsandtransformations(L, P). Smallcapitalsdenoteattributes(color, rank).
<> Anglebracketsdenotesequencesofobjects, e.g., <2 4 6 8>, aswell
asperiodicrules,e.g., <[color(/?/z1() = red],[coLOR(/?/z2.,) = black]>
The/thobjectinaninputsequence
q,' The/thobjectinaderivedsequence
q~ Anobjectthatconstitutesanincorrectextensionofthesequenceafter
object<7,_,
b, The/thblockinasequencederivedbytheblockingtransformation
ph, The/thphasederivedbythesplittingtransformation
phjj They'thobjectinthe/thphaseafterasplittingtransformation
n Thenumberofdescriptors
E Thespaceofpossibleevents
F Thestartingsetofattributesforatransformation
5 Thestartingsetofsequencesforatransformation
Q Thestartingsetofobjectsforatransformation
F' Thesetofderivedattributesfromatransformation
S' Thesetofderivedsequencesfromatransformation
Q' Thesetofderivedobjectsfromatransformation
Fj Thesetofselectorsdescribingobjectq,-jinblockb,
g Thesequence-generatingfunctionthatmapsasequenceintoasetof
objectsQk+1 thatcanappearascontinuationsofthesequence
Qk+ Thesetofobjectsthatcanappearascontinuationsofthesequence
<<?,, q2, qk>
--- PAGE 118 ---
104 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
P Thenumberofphasesparameterofthesplittingtransformationandthe
periodicmodel
L Thelookbackparameteroftheblockingtransformationandallthreemodels
[fi(qj) = rk] Asimpleselector, whichassertsthatfeature/ofobjectq } hasthevaluerk
[f<( a j) = r \ V r2 V r3]} Aselectorcontaininganinternaldisjunction. Itassertsthat/canhavethe
valuer, orr2 orr3
dprefix Thedprefixonanattributenameindicatesthatitisadifferenceattribute.
Hence, d-rank((7„^,_i) isequaltorank(<7,) - rank(^,_,)
sprefix Thesprefixonanattributenameindicatesthatitisasummationattribute.
Hences-rank(^„^,_i) isequaltorankO?,) + rank(#,_i)
d{F„Fj) Thesetofdifferenceselectorsobtainedby "subtracting" selectorsFfrom
s{F„Fj) Thesetofsummationselectorsobtainedby "adding" selectorsF,andFJ
="> Logicalimplication
References
Abbott, R., "The New Eleusis," 1977. (Available fromtheauthor, Box 1175, General PostOffice, New
York, N.Y. 10116.)
Bairn, P. W., "AutomatedAcquisitionofDecisionRules: ProblemsofAttributeConstructionandSelection," M.S. thesis, DepartmentofComputerScience, UniversityofIllinois, Urbana. 1983.
Boulanger, A. G., "The Expert System PLANT/CD: A Case Study in Applying the General Purpose
InferenceSystemADVISEtoPredictingBlackCutwormDamageinCorn," M.S. thesis. DepartmentofComputerScience, UniversityofIllinois, Urbana, 1983.
Buchanan, B. G., and Mitchell, T. M., "Model-directed Learning of Production Rules," in PatterndirectedInference Systems, D. A. Waterman and F. Hayes-Roth (Eds.), Academic Press, New
York, 1978.
Cohen, P. R., and Feigenbaum, E. A., TheHandbookofArtificialIntelligence, Vol. 3. Kaufmann. Los
Altos, Calif., 1982.
Davis, J., "CONVART: A Program forConstructive InductiononTime-Dependent Data." M.S. thesis.
DepartmentofComputerScience, UniversityofIllinois, Urbana, 1981.
Dietterich,T.G.,London,R.,Clarkson,K.,andDromey,G., "LearningandInductiveInference."'inTlie
HandbookofArtificialIntelligence, Vol. 3,P.R.CohenandE. A. Feigenbaum(Eds). Kaufmann.
LosAltos, Calif., 1982.
Dietterich,T. G.,andMichalski, R. S.. "InductiveLearningofStructural Descriptions: EvaluationCriteria and Comparative Review ofSelected Methods.*' Artificial Intelligence. Vol. \b. No ; .
pp. 257-94. 1981.
--- PAGE 119 ---
DIETTERICH AND MICHALSKI 105
Englemore,R.,andTerry,A., "StructureandFunctionoftheCrysalisSystem,"ProceedingsoftheSeventhIJCAI, Tokyo,pp. 250-56, 1979.
Friedland,P.E., "Knowledge-BasedExperimentDesigninMolecularGenetics,"ReportNo.HPP-79-29,
DepartmentofComputerScience, StanfordUniversity, 1979.
Gardner, M., "OnPlayingtheNewEleusis, TheGameThatSimulatestheSearchforTruth," Scientific
American, No. 237, pp. 18-25,October 1977.
Hedrick,C.L., "LearningProductionSystemsfromExamples,"ArtificialIntelligence,Vol.7,No. 1,pp.
21-49, 1976.
Hofstadter, D. R., "TheArchitectureofJUMBO," ProceedingsoftheInternationalMachineLearning
Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois atUrbana-Champaign,
pp. 161-70,June22-24, 1983.
, "AnalogiesandRolesinHumanandMachineThinking," 1985, inpress.
Karpinski, J., and Michalski, R. S., "A SystemThatLearnstoRecognize Hand-written Alphanumeric
Characters,"ProceInstituteAutomatyki, PolishAcademyofSciences, No. 35, 1966.
Kotovsky, K., andSimon, H. A., "Empirical TestsofaTheoryofHuman Acquisition ofConcepts for
SequentialPatterns," CognitivePsychology, No. 4, pp. 399-424, 1973.
Langley,P.W., "DescriptiveDiscoveryProcesses:ExperimentsinBaconianScience,"ReportNo.CMUCS-80-121, DepartmentofComputerScience, Carnegie-MellonUniversity, 1980.
Lenat, B., "The Role ofHeuristics in Learning by Discovery: Three Case Studies," in Machine
Learning: An ArtificialIntelligenceApproach, R. S. Michalski, J. G. Carbonell, and T M.
Mitchell (Eds.),Tioga, PaloAlto, Calif., 1983.
McCarthy,J., "ProgramswithCommonSense," inProceedingsoftheSymposiumontheMechanization
ofThoughtProcesses, NationalPhysicalLaboratory, pp. 77-84, 1958.
, "Epistemological Problems ofArtificial Intelligence," Proceedings ofthe Fifth IJCAI, Cambridge, Mass., pp. 1038-44, 1977.
McCarthy,J., andHayes, P., "SomeEpistemologicalProblemsfromtheStandpointofArtificialIntelligence,"inMachineIntelligence4, B. MeltzerandD. Michie(Eds.),EdinburghUniversityPress,
Edinburgh, 1969.
Michalski, R. S., "OntheQuasi-MinimalSolutionoftheGeneralCoveringProblem." inFifthInternationalSymposiumonInformationProcessing, FCIP69, Yugoslavia, Vol. A3, 2-12, 1969.
, "A Variable-valued Logic System as Appliedto Picture Description," in GraphicLanguages:
ProceedingsoftheIFIP Working Conferenceon GraphicLanguages, P. NakeandA. Rosenfeld
(Eds.), Vancouver, B. C, pp. 20-47, 1972.
, "Knowledge Acquisition Through Conceptual Clustering: A Theoretical Framework and an
AlgorithmforPartitioningDataintoConjunctiveConcepts,"JournalofPolicyAnalysisandInformationSystems, Vol.4, No. 3, pp. 219-44, September 1980.
, "A Theory and Methodology ofInductive Learning," ArtificialIntelligence, Vol. 20, 111-61,
1983.
Michalski, R. S., Carbonell,J. G., andMitchell, T M. (Eds.), MachineLearning:AnArtificialIntelligenceApproach, Tioga, PaloAlto, Calif., 1983.
--- PAGE 120 ---
106 CHAPTER 4: LEARNING TO PREDICTSEQUENCES
Michalski, R. S., and Chilausky, R. L., "Learning by Being Told and Learning from Examples: An
ExperimentalComparisonoftheTwoMethodsofKnowledgeAcquisitionintheContextofDevelopinganExpertSystemforSoybeanDiseaseDiagnosis,"PolicyAnalysisandInformationSystems,
Vol. 4, No. 2,June 1980.
Michalski, R. S., and Stepp, R. E., "Learning fromObservation: Conceptual Clustering," in Machine
Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
Mitchell, T. M., "Version Spaces: An Approach to Concept Learning," Report No. STAN-CS-78-711.
DepartmentofComputerScience, StanfordUniversity, 1978.
Mitchell, T. M., Utgoff, P. E., and Banerji, R. B., "Learning by Experimentation: Acquiring and
RefiningProblem-SolvingHeuristics,"inMachineLearning:AnArtificialIntelligenceApproach,
R. S. Michalski,J. G. Carbonell, andT. M. Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
Persson, S., "Some Sequence Extrapolating Programs: A Study ofRepresentation and Modeling in
Inquiring Systems," Report No. CS50, Department ofComputer Science. Stanford University,
1966.
Samuel, A. L., "Some Studies in Machine Learning Using the Game ofCheckers," in Computersand
Thought, E. A. FeigenbaumandJ. Feldman(Eds.), McGraw-Hill, NewYork, 1963.
, "Some Studies in Machine Learning Using the Game ofCheckers II-Recent Progress" IBM
JournalofResearchandDevelopment, Vol. 11, No. 6, pp. 601-17, 1967.
Schank,R.,andAbelson,R., "Scripts,Plans,andKnowledge,"ProceedingsoftheFourthIJCAI,Tbilisi.
Georgia, USSR, pp. 151-57, 1975.
Simon, H. A., "ComplexityandtheRepresentationofPatternedSequencesofSymbols," Psychological
Review, Vol. 79, 369-82, 1972.
Simon, H. A.,andKotovsky, K., "HumanAcquisitionofConceptsforSequentialPatterns,"PsychologicalReview, Vol. 70, pp. 534-46, 1963.
Solomonoff,R.S., "AFormalTheoryofInductiveInference,"InformationandControl,Vol.7,pp. 1-22.
224-54, 1964.
Soloway, E. M., "Learning = Interpretation -I- Generalization: A Case Study in Knowledge-Directed
Learning,"ReportNo.COINSTR-78-13, DepartmentofComputerandInformationScience.UniversityofMassachusetts, Amherst, 1978.
Winston, P. H., "LearningStructural Descriptions fromExamples." ReportNo. AI-TR 231. MIT. 1970.
--- PAGE 121 ---
SHIFT OF BIAS FOR INDUCTIVE
CONCEPT LEARNING
Paul E. Utgoff
UniversityofMassachusetts atAmherst
Abstract
Programs that learn inductive generalizations from examples are guided not
onlybythetraining instancesthatthey observebutalsobybiasthatdetermineshow
generalizationsaretobeformed, giventheobservedtraininginstances. Thischapter
examinestheessentialroleofbiasininductiveconceptlearning, arguesthatonebias
canbebetterthananother, andpresentsaprogram, STABB, thatshiftsitsbiasduring
the course oflearning fromexamples. The main objective is to show thatthe search
for a good bias, part of the learning task, can be performed mechanically. Byproductsoftheresearchareamethodologyforshiftingbiasandthetwoalgorithmsof
STABB.
INTRODUCTION
5.1
Except forthe presented examples and counterexamples ofthe concept being
learned, all factorsthat influencehypothesis selectionconstitutebias. These factors
include the following:
1. The language in which hypotheses are described
2. The space ofhypotheses that the program can consider
3. The procedures thatdefine in what order hypotheses are tobe considered
4. Theacceptancecriteriathatdefinewhetherasearchproceduremaystopwitha
given hypothesis or should continue searching forabetterchoice
--- PAGE 122 ---
108 CHAPTER 5: SHIFTOF BIAS
5.1.1 Concept Learning as Searching a Hypothesis Space
A conceptisaclassificationrulethatpartitionsadomainofinstancesintothose
instances that satisfy the rule and those that do not. For example, the concept ofa
prime number is apartitionofthesetofnumbers intothosethatareprime andthose
thatarenot. Theclassificationruleis, "xisprimeifandonlyifxisanintegerandxis
evenlydivisibleonlybyxand 1." A classification rulecan serveasasetdescription.
Hereafter, theterms classification rule, conceptdescription, andsetdescription are
used synonymously.
Viewing concepts as sets provides a useful tool. For example, the concept
"numerically less than" can be described as the set ofordered pairs that satisfy the
"numericallylessthan" relation. Theconcept "threadinganeedle" canbedescribed
as the set ofevent sequences that result in a threaded needle.
Intheparadigmoflearningfromexamples, alearningprogramisshownexamplesandcounterexamplesofthetargetconcept. Thetargetconceptistheoneconcept
that correctly classifies all the instances in the domain. Examples ofthe target concept are referred to aspositive instances. Similarly, counterexamples of the target
concept are referred to as negative instances. Ifa classification rule is true for all
observed(presentedbyatrainer)positiveinstancesandisfalseforallobservednegative instances, then the concept that corresponds to the classification rule is consistentwiththeobserveddata. Theobjectiveoftheconceptlearneristoinferaclassification rule that describes the targetconcept.
Foradomain ofinstances I there are 2111 distinct subsets overI. The domain I
maybeinfinitelylarge, inwhichcasethesetof2|!|distinctconcepts isalso infinitely
large. ThesetofallsubsetsoveradomainIisreferredtoasthesetofalldistinctconceptsoverI. Iftwodescriptionsrefertothesamesubsetofinstances,thenthetworeferenced concepts are not distinct. Two concept descriptions that each describe the
same concept are synonyms.
The problemofinferringaconceptbasedonobservationofpositive and negativetraininginstancesiseasilyviewedasoneofsearchingaspaceofconceptsforone
that best classifies the instances in the domain (Mitchell, 1982). As illustrated in
figure 5-1, a concept A classifiesthe instances betterthan a concept B ifand only if
the concept A classifies more instances correctly thandoes B. An instance is classified correctlyifand only ifthe classification isconsistent with the target concept T.
Note that this definition is based solely on the number of correctly classified
instances. Ifit were the case that some misclassifications were worse than othersguilty versus innocent-then an alternate definition ofbetterwould be needed.
Ideally, the concept learner would immediately select the target concept as its
hypothesis, but the learner does not know which concept is the target concept.
Becausetheconceptlearnercannotknowthetargetconceptaheadoftime, thelearner
cannot select a hypothesis based on closeness offit with the target concept.
--- PAGE 123 ---
. .
UTGOFF 109
Figure5-1: ConceptAbetterthanconceptB.
Nevertheless, aconcept-learning program requires someclassification metric
sothatitwillhaveabiasthatallowsittoevaluatewhetheronehypothesisistobepreferredtoanother. Forexample, consistencywiththetraininginstancesisonefeature
ofa hypothesis that is customarily included in a classification metric; a hypothesis
thatmisclassifiessomeofthetraininginstancescannotbethetargetconcept, assuming
correct classification by the trainer. A classification metric that includes the consistency feature guides the concept learner to prefer any consistent hypothesis to any
inconsistenthypothesis. Additionalfeaturesofhypothesesortheirdescriptionsarenecessary todefine furtherhow to choose from among consistent hypotheses.
5.1.2 Role of Bias in Concept Learning
A program that learns concepts from examples is guided toward an inductive
generalizationnotonlybythetraininginstancesbutalsobybiasthatdetermineshow
tochoosefromamongthehypotheses. Withoutthebias, theprogramhasnobasisfor
selecting one hypothesis in favor of another. By selecting a hypothesis, a concept
learnermakesaninductiveleapbecausetheconceptlearnerguessesthatthehypothesis correctly classifies all the instances in the domain; that is, the concept learner
assumesthatthecurrenthypothesisisthetargetconcept. Hadthebiasbeendifferent,
the learner would have selected a different hypothesis and as a result would have
made adifferent inductive leap.
Considertwo features ofbias:
1. A strong bias is one that focuses the concept learner on a relatively small
number ofhypotheses. Conversely, a weakbias is one that allows the concept
learnerto consider a relatively large number ofhypotheses.
2 A correctbiasisonethatallowstheconceptlearnertoselectthetargetconcept
Conversely, an incorrectbias is one that does not allow the concept learnerto
select the target concept.
A concept learner's task is simplest when bias is strongest and correct because the
concept learner immediately selects the only choice available-the target concept.
--- PAGE 124 ---
110 CHAPTER 5: SHIFTOF BIAS
A concept learner's task is mostdifficult when bias is weakestbut incorrectbecause
the concept learner cannot select the target concept and has no other guidance
regarding hypothesis selection.
Figure5-2 illustratesthatinductioninthelearning-from-examplesparadigmis
afunction oftwoarguments, thetraining instances, andthebias forhypothesispreference. Given any particular sequence of training instances, however, induction
becomesafunctionofoneargument, thebias. Thischapteraddressestheproblemof
selecting a bias, not the problem ofselecting a training sequence.
Consideran example ofalearning scenariothat showsthe role ofbias. Person
A istolearnaconceptfromexamplespresentedby personB. PersonBonly presents
correctly classified positive and negative instances of the concept. It is the job of
person A to inferthe target conceptthat person B is endeavoring toteach. Person B
presents the first instance:
(3.4) isapositive instance
Person A may perhaps form a hypothesis ofthe concept such as "an ordered pair of
numbers." Person B then presents the second instance:
(6.5) isa negative instance
Person A may perhaps revise his or her hypothesis ofthe concept to be "an ordered
pairofnumberswherethe firstis numerically lessthanthe second." Ifperson A has
notmadearandomchoice fromamongthe setofdistinctplausiblehypotheses, then,
bydefinition, heorshehasbeenguidedbybiastochoosethe "lessthan" hypothesis.
Consider some alternative consistent hypotheses that A could have chosen:
1. An ordered pair ofnumbers in which the first is an odd integer
2. An ordered pair ofnumbers in which the second is an even integer
3. Anorderedpairofnumbers inwhichthe first isanodd integerandthe second
is an even integer
Program
Author
Bias
Search Hypothesis
Knowledge
Program
Training
Instances
Trainer
Figure5-2: Roleofbias in inductivegeneralization
--- PAGE 125 ---
UTGOFF 111
Anorderedpairofnumbersinwhichthefirstisanoddintegerorthesecondis
an even integer
5. An ordered pairofnumbers the binary sum ofwhich has a 1 in the4's place
6. Anorderedpairofnumbersthedecimal sumofwhichhasa inthe 10 'splace
7. A pair ofnumbers the sum ofwhich is 7
8. An ordered pair ofnumbers in which the second is 1 more thanthe first
9. An ordered pair ofnumbers in which the first is not 1 more than the second
Noteintheaboveexamplethatthehypothesesmakeuseofotherconceptssuch
as order, odd integer, even integer, logical and, logical or, logical not, addition,
binary notation, decimal notation, digit's place, and the "one more than" relation.
These are terms that are commonly associated with numbers and logical combinationsoffeatures. Otherhypothesesareconsistentthataredifficulttostatesuccinctly,
for example, some apparently arbitrary set ofpairs of numbers. When people are
involved, human factors come into play. For example, one assumes that there is a
vocabularyofconceptsthatthepeopleshare. Furthermore,personAmaytrytoguess
what kind ofconcept person B wouldtry to teach.
5.1.3 Kinds of Bias
Several kinds ofbias canbe characterized at an abstract level.
1. There can be a total preference ordering over the hypotheses. In the above
example involving pairs ofordered numbers, there are ten consistent hypotheses. An example ofa total preference ordering would be to have the concept
learnerpreferthosetenconcepts,togetherwithothers, insomespecifiedorder.
There are other methods for specifying a preference ordering. For example,
onecanpreferonehypothesistoanotherbasedonthesyntaxofthedescription;
forexample, onecanpreferahypothesisthatdoesnotuselogicalortoonethat
does.
2. There can be &partialpreference ordering. One can selectthe mostpreferred
hypothesiswhenthereisexactlyone, orchooserandomly fromthesetofmost
preferred, or not choose at all and simply keep track of the set of most
preferred.
3. It is possible to restrict the space of hypotheses through which the concept
learner conducts its search. Ifa space is used in which not all hypotheses are
represented, the concept learner is unable to select hypotheses that are not in
the space. Use ofa restricted hypothesis space is a very practical method for
focusing the concept learneron a set ofpreferred hypotheses.
4. There can be a combination of kinds of bias. For example, one can use a
restricted spaceofhypothesesandspecify apreferencecriterionforthosethat
are in the space.
--- PAGE 126 ---
112 CHAPTER 5: SHIFTOF BIAS
5.1.4 Origin of Bias
The research in inductive concept-learning programs to date has contributed
severalgoodtechniquesforusingagivenbiastoguidethewaytochoosingahypothesis. These programs do not, however, include directed methods by which the programitselfshiftsthebias. Michalskihasrecognizedtheneedtovarythebiasandhas
providedamechanismthrough whichtheuserofhisprogramcan specifythebiasto
be used foragiven run (Michalski, 1983). Theprogram author spends considerable
timeandeffortsearchingforabiasthatwillallowtheprogramtoperformsatisfactorily. Astheauthorrepeatedlyshiftsthebiasbyhandandreteststhelearningprogram,
a search for a better bias is taking place. Human authors use both their experience
with previous bias failures and their knowledge as domain experts to guide their
search for abetterbias. Bias A is betterthan bias B ifthe hypotheses selected when
bias A is used are better classifiers than those selected when bias B is used. This
abilitytosearchforabetterbiasisamajorskillthatresearchersapply. Asresearchers
work on getting good performances from learning programs, they do part of the
learning task themselves by improving the bias employed by the concept-learning
program. An inductive concept-learning program should be able to conduct its own
searchforanappropriatebias. Untilprogramshavesuchcapability, thesearch foran
improvedbiaswillcontinuetobedonebyhand. Thelackofsuchtechniqueswasthe
motivation forthe research reported here.
An inductive concept-learning program that includes search for a better bias
learns how to learn. By changing one's bias, one changes not only what is to be
induced inthecurrentlearningtaskbutalso whatistobe induced in future learning
tasks. An open question is whether there are elements ofbias that are useful across
many learning domains. For example, is a bias toward comparative simplicity of
hypothesis descriptions universally useful? No such universally useful bias has yet
beenpositivelyidentified. Theorientationinthischapteristowardtheabilitytoshift
bias. Shiftofbias isalessdifficultproblem because itispossibletoobservecasesof
manual search forand shift to a betterbias. An important experiment (taking many
years) would be to observe many cases ofconcept-learning programs that shift bias
andthentrytogeneralizeonthepropertiesofthebiasesthatwere selected. This isa
possible approach toward identifying characteristics of bias-for example,
simplicity-that occuracross many domains.
5.1.5 Another View of Bias
One kindofbiasmentionedabove insection5.1.3 isuseofa restricted hypothesis space. A standard method for defining a restricted hypothesis space is to use a
description language in which not all concepts are describable. Ifa concept is not
describable in the description language, then it docs not exist as a hypothesis in the
hypothesis space. For example, a restricted description language might include a
description fortheconcept ofaprime numberandvetexclude any description forthe
--- PAGE 127 ---
UTGOFF 113
conceptofanoddinteger. Insuchacase, theconceptlearnercouldselectthehypothesis "prime number" and could not select the hypothesis "odd integer."
The description language defines the restricted hypothesis space and is therefore a contributor to bias. Ifone changes the description language, then one is also
changing the bias. Thus, in the case of bias coming from a restricted hypothesis
space, searching for a better bias is equivalent to searching for a better description
language.
5.2 A FRAMEWORK FOR SHIFTING BIAS
This section lays outathree-step framework forbuilding aprocedurethat can
shiftthebiasusedbythelearningprogram. Anapproachtoshiftofbias isdependent
onboththekindofbiasthatthelearningprogramusesandtheformalisminwhichthe
bias is encoded.
5.2.1 Encoding Bias
The remainder of this chapter focuses on bias that comes from using a
restricted hypothesis space. The restricted hypothesis space is defined by a concept
descriptionlanguageinwhichnotallconceptsovertheinstancedomainaredescribable. A hypothesis space thatdoes not include all distincthypotheses is incomplete.
There are two major reasons for choosing to study bias that comes from an incomplete hypothesis space:
1. An incomplete hypothesis space can be representedby a description language
inwhichnotalldistinctconceptsaredescribable. Theproblemofsearchingfor
a better bias becomes one of searching for a better description language. A
descriptionlanguageiseasilyrepresentedasadatastructure, somethingthatis
fareasierto manipulatethan acontrol structure.
2. Therealreadyexistsaconcept-learningalgorithm, Mitchell'scandidateeliminationalgorithm(Mitchell, 1977, 1978),thatencodesbiasasarestrictedhypothesis
space. This permits the immediate study ofmethods for shifting bias.
Forbiasthatcomesfromarestricteddescriptionlanguage, therearetwomajor
contributors to the bias:
Biascomesfromtheformalisminwhichthedescriptionlanguageisexpressed.
Aformalismisaformal systeminwhichalanguageisexpressed. Forexample,
a formal grammar ofstring substitution rules is a formalism for specifying a
formal language. Predicatecalculusisaformalismforexpressingwell-formed
formulas that represent theories. A theory can be used as a description ofall
that is true or false according to the theory.
2. Biascomesfromthelanguageasexpressedinagivenformalism. Alanguageis
asetofsentencesordescriptions. Foraformallanguage, thesetofsentencesin
--- PAGE 128 ---
114 CHAPTER 5: SHIFTOF BIAS
the language is the set of sentences that can be produced by applying some
sequence of legal string substitutions that are specifically permitted by the
grammar. Ifa sentence in the alphabet ofsymbols cannot be produced by followingrules inthegrammar, thenthe sentence isnotinthelanguage. Thusthe
definition of the language defines the restrictions-that is, bias-on what is
describable within the formalism.
Thischapterexaminesbiasthatcomes fromusing a restrictedlanguage withinaformalism; it does not examine bias that comes from a given formalism. Methods are
developed forchanging the languagethat is available totheconceptlearnerwithin a
given formalism but not for changing the formalism in which that language is
expressed. Changeofformalismand understandingofbiasesimpliedby agiven formalism are open areas for further study. Section 5.3.3 shows an example ofbias
coming from the formalism.
A biascanbetoostrong ortooweak. As mentioned in section 5.1.2, theextent
towhich adescription language is restricted determinesthe strength ofthe bias. Ifa
biasisstrongandcorrect,thentheconcept-learningtaskisrelativelyeasybecausethe
concept learnerwillbeguidedto selectionofthetargetconcept. Ifthe bias is strong
and incorrect, then the concept learner may eliminate all hypotheses from the
restricted space. If a bias is weak, the concept learner may do little better than
randomselectionofahypothesis. Givensomefixednumberoftraininginstancesand
some fixed amount oftime, a bias that is too weak may prevent the concept learner
from eliminating enough hypotheses from the space to be able to isolate a single
hypothesis, such as the target concept. Correctness ofbias determines whether the
concept learner is able to select the target concept as a hypothesis. Strength ofbias
determineshowmanyhypotheseshavebeeneliminatedbeforeanytraininginstances
are observed. With no bias, there is no induction.
Thischapterfocusesontheproblemofshiftingfromastrongbiastoonethat is
weaker. Theproblemofmovingintheotherdirection, fromaweakbiastoonethat is
stronger, is briefly considered in section 5.3.2.7. The choice ofdesigning a method
for weakening a strong bias is based on the view that learning is itselfa process of
ruling out some hypotheses in favor ofothers that are to be retained, for example.
Mitchell's candidate elimination algorithm. Ifa bias is too weak, then the concept
learnercan, in theory, continue to observe training instances until the space ofconsistenthypotheses issufficientlypruned. Ontheotherhand, ifabias is incorrectand
toostrongandthespaceofconsistenthypotheses isemptied, theconceptlearnerwill
notbenefit from furthertraining. The problemofan incorrectandoverly strong bias
is serious-just as in racism and other forms of prejudice. Program authors who
design a restricted hypothesis spacecharacteristicallyerrtowarddefiningtoo strong
a bias because they include in the hypothesis space only those concepts that they
know about and suspect may be useful in the domain. Ifthe bias is incorrect, then a
--- PAGE 129 ---
UTGOFF 115
concept-learningprogramshouldbeabletoweakenitsbiasinacontrolledmannerin
an attempt to make the bias correct yet not too weak.
5.2.2 When to Shift to a Weaker Bias
Before shiftingtoaweakerbias, one mustdeterminethata shift is warranted.
A sufficient condition is the following:
//the hypothesis space does not include a concept that is consistent with the
observedtraininginstancesandthetraininginstanceshavebeencorrectlyclassifiedbythetrainer, then thebiasistoostrong.
Ingeneralashifttoaweakerbiascanbemadewhenevertheconceptlearneris
no longer willing to accept any ofthe hypotheses from which it must choose. For
example, ifaconcept-learningalgorithmallowssomenumberofapparentlymisclassified instances in a hypothesis, then a shift to a new bias can be made when the
numberisexceeded. Ifthesameinstanceisclassifiedaspositiveononeoccasionand
as negative on another, then the two training instances can be discarded.
5.2.3 How to Shift to a Weaker Bias: A Three-Step Framework
The objective ofshifting to a weaker bias is to enable the concept learner to
selectahypothesisthatisconsistentwiththetraininginstancesandthatisascloseto
the targetconcept as possible. Because the concept learnercannot know which conceptisthetargetconcept, itisnecessarytouseheuristicmethodsfordecidingexactly
how to weaken the bias.
The framework for shifting to a weaker bias studied here consists ofthree
steps:
1. Recommend (via heuristics) new concept descriptions to be added to the concept description language.
2 Translaterecommendationsintonewconceptdescriptionsthatareexpressedin
the formalism ofthe concept description language.
3. Assimilateany newconceptsintotherestrictedspaceofhypotheses sothatthe
organization ofthe hypothesis space is maintained.
Step 1 determines abetterbiastowhichto shift. Steps2 and 3 arethe mechanics for
carryingouttheshift. Theresultingnewconceptdescriptionlanguageisasupersetof
the formerdescription language, and therefore it provides a strictly weakerbias.
5.2.3.1 Recommending a Better Bias
Thetaskofstep 1 isto formulate recommendations fornew descriptionstobe
added to the space ofhypotheses. The recommendations are then given as output to
step 2. How does one identify a betterbias to which to move?
--- PAGE 130 ---
116 CHAPTER 5: SHIFTOF BIAS
In the framework, the concept learner shifts to a new bias only when the
existinghypothesisspacenolongercontainsanyconsistenthypotheses. Thefactthat
the existing description language contains no consistent descriptions shows that the
existingbias is incorrect. Newdescriptivecapability is addedtothedescription languagetoalterthebiassothatthenewspaceofhypothesescontainsatleastoneconsistent hypothesis.
Figure5-3showstwopartitionsofthespaceofalldistincthypotheses. Thefirst
partitionconsistsoftwosubsets: thosedescriptionsthatare inthe restricted spaceof
hypotheses to be searched by the concept learnerandthosethat are not. The second
partition also consists oftwo subsets: those descriptions that are consistent with the
training instances and those that are not. As concept learning takes place, the new
knowledgeoftheclassificationofthetraining instanceallowstheconceptlearnerto
identify inconsistent hypotheses. Thus, the set ofhypotheses consistent with the
training instances shrinks during concept learning. When the set of consistent
hypotheses within the restricted space-that is, the intersection-becomes empty,
then a shifttoa weakerbias is required. To weaken the bias, it is necessary to move
the partition that definesthe restrictedhypothesis space sothatthere is again a nonempty set ofconsistent hypotheses within the now-less-restricted space ofhypotheses. Thus the set ofdescribable hypotheses grows during bias weakening.
Although the goal is to add consistent hypotheses to the restricted space, they
should not be chosen at random. Recall from section 5.1.2 that the selection of a
hypothesis by the concept learner constitutes an inductive leap because the hypothesis specifiestheclassificationoftheunobserved instances. Ifconsistenthypotheses
were to be added to the hypothesis space without regard to how they classify unobserved instances, then there wouldbe noconfidence in an inductive leapthat results
from selecting such a consistent hypothesis.
Theproblemfortheconceptlearneristomovetoanewbiasthatwillcausethe
concept learner to make satisfactory inductive leaps based on the observed data.
Because the concept learner cannot already know which concept is the target concept, the concept learner must make an intelligent guess based on heuristics for
shifting to a weaker bias.
Theproblemofselectingthe initial bias iscustomarily solved by the researcher.
On the basis ofthe experience ofthis author and several colleagues, it can be stated
AH Distinct Hypotheses
Describable / ^ A-^ Consistent
Figure5-3: Partitionsofthe spaceofalldistinct hypotheses.
--- PAGE 131 ---
UTGOFF 117
thatthe firstchoice forthe bias will be incorrect. Inthe absence ofamechanism for
correctly selectinganinitial bias, theproblemofshiftingtoabetterbiaswill remain
important. Human researchers can do abetterjob ofchoosing a bias on subsequent
trials because they have learned something from the previous failures. Thus the
problemofshiftingtoanewbiasdiffersfromtheproblemofinitially selectingabias
because one shifts to a different bias with the added knowledge ofthose biases that
have failed in the past.
Consideranexample heuristic thatcorresponds to Vere's technique (1980) for
constructing counterfactual descriptions:
Ifthe description language does not contain a consistent description, then
construct a new consistent description that is a counterfactual ofexisting
descriptions.
The heuristic relies on existing descriptions in the restricted hypothesis space, the
training instances, andthe set-difference operator " - " todeterminethe classifications ofunobserved instances. Forexample, one coulddefineanew concept "negative number" as "a numberthat is not positive."
5.2.3.2 Translating Recommendations into New Concept
Descriptions
Thesecondstepoftheframeworkistotranslatetherecommendationsintonew
descriptions within the formalism ofthe concept description language. The recommendations are received as input from step 1. The new descriptions are given as
output to step 3.
Therecommendationsspecifynewdescriptivecapabilitytoaddtothedescriptionlanguage. Step 1 usesasecondlanguage, arecommendationlanguage, tospecify
newconceptsthatshouldbecomepartoftheconceptdescriptionlanguage. Arecommendation is not itselfpart ofthe description language because a recommendation
correspondstodescriptivecapabilitythatisnotyetintheexistingconceptdescription
language. A necessary step for acting on a recommendation is the identification of
corresponding conceptsthatare describable inthe formalism ofthedescription language. Theapproachusedintheframeworkistotranslateeachrecommendationinto
one or more descriptions expressed in the formalism ofthe description language.
Again consider the counterfactual example. Suppose there is a recommendationtoaddanew concept "negative number," definedas "anumberthat is notpositive." The recommendation may be ofthe form "add a new description N such that
N = R - P," whereR isthesetofrealnumbersandPisthesetofpositivenumbers.
Thedescriptionlanguagemaynotcontainthesetdifferenceoperator"-
If"-"
not in the description language, then one or more translation steps could be used to
remove it fromthe "P - P" partofthe specification. One stepcouldbetotranslate
"P - P" to "P fl ~ P." Asecondstepcouldbetoevaluate " ~P" withafunction
thatcomputesacomplementarydescriptionwithintheformalismofthelanguage. A
third step could be to evaluate the intersection of "P" and the result ofevaluating
--- PAGE 132 ---
118 CHAPTER 5: SHIFTOF BIAS
"~P." By performing such translation steps, onecancompute descriptions that are
expressed in the formalism of the description language. Translation of unusable
knowledge into usable knowledge via equivalence-maintaining transformations has
been pursued by Mostow (1983) and by Keller (1983)
5.2.3.3 Assimilating New Concepts into the Hypothesis Space
The third and final step of the framework is to assimilate new descriptions
expressed inthe formalismofthedescriptionlanguage intothe space ofhypotheses.
The new descriptions are received as input from step 2.
Assimilation is essential because new descriptions must be made available to
theconcept-learning program whilethe integrity oftheexisting concept description
language is maintained. Forany new conceptto become part ofthe description language, some changeto the description language is necessary. In the framework, the
concept learner assimilates a new concept by adding it to the restricted space of
hypotheses that is searched by the concept learner.
There are two majorproblems:
Howdoesoneassimilateanewdescription?Themechanicsdependontheformalismofthedescriptionlanguage. Forexample, forapartially orderedspace
ofdescriptions, assimilating anew descriptioncorrespondstodefining one or
morenewsubsetlinksbetweenthenewdescriptionandexisting, moregeneral
descriptions. That is, a new descriptionx is assimilated as a specialization of
existing descriptionyby assertingx C y.
2. Whereinthedescriptionlanguagedoesoneassimilateanewdescription? This
too depends on the organization ofthe hypothesis space. For example, for a
partially ordered space ofdescriptions, deciding which sets are immediately
moregeneralthananewdescriptionisanontrivialproblem. Becauseaconcept
tobeassimilated is notyet inthedescription language, one needs tobe able to
evaluatesubsetrelationshipsbasednotonsubsetlinksandmatchingbutinstead
on the definition ofthe new concept.
As an example, again consider the problem ofassimilating a new description
that originated from the construction ofa counterfactual description. Assume that
the space ofhypotheses is organized by a partial order on the subset relation. How
wouldonegoaboutassimilatinganewconcept N definedasthe setofnegative numbers?IfthedefinitionofjVwasderivedthroughtranslationbyevaluating/? - P, then
it is known that R - P C R. N could then be assimilated by asserting NCR.
5.3 STABB: A Program That Shifts Bias
STABB (Shift To A Better Bias) is a program consisting o\' two procedures
designed according to the framework described in section 5.2. One procedure is
called least disjunction, the other, constraint back-propagation. STABB was
--- PAGE 133 ---
UTGOFF 119
incorporated into the existing LEX program (Mitchell, 1983), thereby giving LEX
the ability to shift its bias. (STABB is presented in greaterdetail in Utgoff, 1984.)
5.3.1 Overview of LEX
LEX isaconcept-learningprogramthatcreatesandrefinesheuristicsthatsuggestwhetheragivenoperatorwouldbeappliedtoagivenproblemstateinaforwardsearchproblemsolver. Associatedwitheachoperatorisaheuristicthatrepresentsthe
concept "setofstatestowhichthisoperatorshouldbeapplied." Theproblemforthe
concept learner is to determine this set correctly for each operator.
LEX learns heuristics in the domain ofintegral calculus. The program is initiallygivenasetofproblem-solvingoperators. Eachoperatorhasadomainofstates
towhichtheoperatorcanbeapplied, arewrite rule, andarangeofstatesthatcanbe
produced by application ofthe operator. For each operator, the domain ofapplicabilitydescribesstatestowhichtheoperatorcanbeapplied. Incontrast, foreachheuristic, the domain ofapplicability describes states to which the associated operator
shouldbe applied.
As showninfigure5-4, LEX isasystemthatgeneratesproblems, solvesproblems, criticizes solutions, andlearnsheuristicsthatguidetheproblemsolvertosolutions of future problems with fewer wasted search steps. The generalizer is the
modulethatdoestheactualconceptlearningbyeliminatinginconsistenthypotheses.
STABB was added to LEX by modifying the generalizer sothat, when no currently
availablehypothesisisconsistentwiththetraininginstances, thegeneralizerinvokes
STABB to shift LEX to a new bias.
Problem
Generator
Practice Partially
Problem Learned
Heuristics
Problem
Generalizer
Solver
Traceof
Problem- Training
Solving Instances
Behavior
Critic
Figure5-4: Top-level flowofcontrol in LEX.
--- PAGE 134 ---
120 CHAPTER 5: SHIFTOF BIAS
Theproblemsolversolvesaproblembyforwardsearchuntilastateisproduced
thatcontainsnointegral. Thecriticthencriticizesthesolutiontreethatwasproduced
by the forward search. Foreach operator application along the minimum cost solutionpath, thecriticlabelstheoperatorapplicationapositiveinstanceshowingastate
to which the given operator should be applied. For each operator application that
leadsawayfromtheminimumcostsolutionpath, thecriticlabelstheoperatorapplication anegative instance showing a stateto whichthe given operator should notbe
applied. The critic then passes the set oftraining instances to the generalizer so the
generalizercan update the heuristics being learned forthe operators. Foreach heuristic, the generalizeruses Mitchell's candidate elimination algorithm to maintain a
versionspaceofallcandidateversionsoftheheuristicthathavenotbeenrefuted. Ifa
version spacebecomes empty, then all hypotheses in the restricted space ofhypotheses are inconsistent. That is the criterion used by STABB for determining when to
move to a differentbias. At thatpoint, LEX calls STABB.
5.3.1.1 Description Language
Although LEX learns a heuristic for each operator, it has a single concept
descriptionlanguage. Intheonelanguagethereisneedtobeabletodescribeanyheuristic that is tobe learned.
LEX's description language uses the formalism of a context-free grammar.
Customarily, grammars are used to specify a language of terminal sentences. As
such, the set ofall terminal strings defines the instance language, or the set ofall
describable instances. To define the concept description language, LEX uses a
grammar, butthelanguageconsistsofallsententialforms, nonterminalorterminal.
A sentential formthatcontainsonlyterminal symbolsisadescriptionofaninstance
inthedomainandsimultaneouslyadescriptionofthe singleton setthatcontainsthat
instance. A sententialformthatcontainsanonterminalsymbolisadescriptionofaset
ofinstances in the domain. Forexample, in the LEX grammarthe nonterminal trig
describestheset {sin, cos,tan, esc, sec, cot} , wheretrigisanonterminalandeachof
sin, cos, tan, esc, sec, andcotisaterminal. ThecompleteLEXgrammar is shownin
figure 5-5.
There are three points to note regarding LEX's grammar:
1. LEX uses a less familiar mathematical notation for function combinations. If
f(x) = g(x) OP h{x), whereOP is somecombiningoperatorsuchas "+ " then
onecanalsorefertothefunctionusingcombinedname (g OP h). Forexample.
forsin(x) + cos(jc), thecombinednamewouldbe(sin -I- cos)or, using LEX's
grammar, (+ sincos). This wasdone forsimplicity sothat the argument variable, in this casex, appears only once in the expression, whateverthe expression may be.
2. There are three descriptions-afr, r, and knmz- for which recognition predicates are defined. A recognition predicate is an algorithmic recognizer for the
--- PAGE 135 ---
UTGOFF 121
(opix ix)affr ax pol trans
/l\
(ffraffr) (mon(+- pol mon) abstrig eln
(frafr)
(opaxax exp
sin cos tan esc sec cot
/\c (• c id) (" idk) id (• c (" idk))
(opec) cprim
/\\
enz enm k
/xx\
/enmzXknAz Aknm
r - l knmx
afr recognizedas {x atom(jt) A ~ numberp(*) A ~ ingrammarp(jc)}
r recognizedas {x | (numberp(x) A ~ integerp(jc)) V ~ eq(jc,'e)}
knmz recognizedas {x \ integerp(jt) /\ ~ (x E {0,-1})}
Figure5-5: LEXgrammarforintegralcalculus.
elementsofagivenset. Thepredicate istrue ifandonly iftheargumentisrecognizedasbelongingtothegivenset. LEXusesselectedrecognitionpredicates
for efficiency to take advantage of parsing already done by the LISP interpreter. Thus the vocabulary ofsymbols consists ofLISP atoms and lists, not
single characters. For all x such that recognition predicate (x), there is effectively a grammar rule "recognition predicate => x." In particular, the following rules apply:
a. Somethingisanafrifandonly ifitisanonnumericatomthatisnotused
explicitly elsewhere in the grammar.
b. Something is an r ifand only ifit is a noninteger numberorthe atom e
(Euler's constant).
--- PAGE 136 ---
122 CHAPTER 5: SHIFTOF BIAS
c. Something is a knmz ifand only ifit is an integer that is neither nor
-1.
3. Any nonnumeric symbol mayhavetrailingdigitsappendedtoit. Forexample,
the symbol sincanalsobegivenas sinl. Thedigits are notpartofthe symbol.
Instead,thedigitssimplymake itpossibletoreferenceaparticularsymbolinan
expression, forexample ( + sinl (*cossin2)). LEXcanobserveorignoresuch
digits, depending on how it needs to use the expression. For example, when
matching descriptions, LEX can ignore the digits. On the other hand, when
rewriting a state, LEX can use the digits to insure that it operates on the
intended symbols.
5.3.1.2 Matching Two Descriptions
Theabilitytodeterminewhetheronedescriptionismoregeneralthanorequal
toanotheriscentraltomanyconcept-learningalgorithms. Atabasiclevel istheclassificationtaskofdeterminingwhetheraninstance, adescriptionofasingletonset, is
included in a given concept. For many learning algorithms there is a need to determinewhetheroneconcept,notnecessarilyasingletonset, isincludedinanother. The
candidateeliminationalgorithmthat LEXusestomaintaintheversionspaceforeach
heuristic specifically requires a matching predicate that tests the "more general
than" relation.
ForLEX, adescriptionaismoregeneralthanorequaltoadescriptionbifand
only ifagrammatically derivesboraderivesasentential formwithin b. Ifdescription a derives b, then a matches onto b. Ifdescription a derives a sentential form
properly within b, then a matches into b. For matching, LEX uses a function
match(a, b, flags) that returns a list ofderivation trees showing how a matches b.
The flags argument is not pertinent to the discussion here. Ifa is not more general
than b, then the match function returns a null list. Forexample, cos is more general
than ( + sin cos) because cos grammatically derives (in zero steps) the cos within
( + sincos). Asasecondexample, trigismoregeneralthan(-I- sincos) intwoways.
First, trig derives sin. Second, trig derives cos.
5.3.1.3 Operator Language
Each operatorconsists ofthe following set ofitems:
LHS A concept description forthe domain ofthe operator
RHS A concept description for the range ofthe operator
FORWARD A LISPexpressionthat, whenevaluated, computesthe values
for all atoms in RHS that are not used in LHS
--- PAGE 137 ---
UTGOFF 123
BACKWARD ALISPexpressionthat, whenevaluated, computesthevalues
for all atoms in LHS that are not used in RHS
COMMENT A description in English ofwhatthe operatordoes
Thedomainandrangeofeachoperatoraredescribableintheconceptdescriptionlanguage. Theoperatormaybeappliedintheforwardorbackwarddirection. Thisdoes
not mean that every operatorhas an inverse. What itdoes mean isthat forthe setof
statesinthedomainforonedirectionoftheoperator,themechanicsexistfordefining
how to compute the setofstates in the range forthat direction ofthe operator.
Some operatorapplications requirecalculationofnewvalues. Anexample ofone
thatdoesnotrequirecalculationiscommutation, (*frl fr2) => (*fr2frl),becauseall
symbolsintherangeareboundinthedomain. Ingeneral,extracalculationisnecessary
onlywhensymbolsareusedintherangethatarenotboundinthedomain. Notethatthe
abilitytoprovideeachoperatorwithaprocedureforcalculatingvaluestobeusedinthe
range gives the operatorlanguage the full powerofthe LISP interpreter.
5.3.2 Least Disjunction: A Goal-Free Method
Leastdisjunction is oneofthetwoproceduresusedby STABB. Theprocedure
isdesignedaccordingtotheframeworkpresentedinsection5.2. Theprocedureuses
theobservedpositiveandnegativetraininginstancesandtheexistingdescriptionlanguage as inputs. The method does not make use ofthe learning goal; that is, itdoes
notusethefactthatthepurposeofthelearningistofindadomainofapplicabilityfor
which the operator should be applied. The method only considers the observed
instances as syntactic entities. As such the method isgoalfree.
The procedure shifts bias by adding a new concept description to the concept
description language. The constructed description is equivalent to a least-specific
disjunctive description that is consistent with the training instances.
5.3.2.1 Recommending a Better Bias
To recommend abetterbias, the procedure employs the following heuristic:
Ifaconsistentdescriptionisneeded,thenconstructaleast-specificdisjunctionof
existingdescriptions.
The motivation forbuilding a new description from existing descriptions is thatthe
existing descriptions probably already describe concepts that are useful in the
domain. The reason for using a least-specific (most-general) form is that the mostspecificformisexactlythesetofpositiveinstances, andthecorrespondinginference
is therefore that all unobserved instances are negative instances. Defining a new
descriptionasadisjunctionofexistingdescriptionsallowstheclassificationinformation in the existing descriptions to be incorporated into the new description.
The leastdisjunction procedurecalculates a specification ofanew conceptby
searchingforaleast-specificdisjunctionthatisconsistentwiththetraininginstances.
--- PAGE 138 ---
124 CHAPTER 5: SHIFTOF BIAS
A leastdisjunctionisadisjunctionofminimallyspecificdescriptionsinthelanguage
suchthateachdisjunctcoversasmanypositiveinstancesaspossiblewithoutcovering
any negative instances, and every positive instance is covered by at least one ofthe
disjuncts. A least disjunction is computed in four steps:
1. Create an initial disjunctive description that is the set of positive training
instances. Thesetofpositivetraininginstancesisthemost-specificdisjunctive
description, and it is always consistent with the training instances, by definition. Thisfactguaranteesthattheleastdisjunctionprocedurecanalwaysfinda
new consistent description.
2. Searchforallgeneralizationsofthedisjunctsthatproducedescriptionsthatare
moregeneralthansomeofthepositivetraining instancesbut notmoregeneral
thananyofthenegativeinstances. Thisgenerationstepisdoneinastraightforwardmannerbyefficientgenerationofcombinationsoftwoormoredisjuncts,
coupled with pruning ofpaths that produce inconsistent generalizations.
3. Fromtheresultinglistofgeneralizations (conceptdescriptions), eliminateany
conceptdescriptionthatis more specific orequalto some otherdescription in
thelist. Thepurposeofthisstepistoremoveanydescriptionthatisnotneeded
inthe disjunctive description being constructed.
4. Removethoseembeddingexpressionsofeachdisjunctthatare identical forall
the disjuncts. This step is included because all the disjuncts may share some
singlecommoncontext. Ifthedisjunctivedescription underconstruction is to
be as general as possible, any such common context must be discarded. The
removal ofsuch common context embodies a secondary heuristic:
Ifaconceptisusefulinonecontext, thenit mayalsobeusefulinanother
context.
For example, assume that the previous three steps produced a disjunctive
description \x • sin(;c) • dx \J \x cos(*) dx. Then, inthisstep, theprocedurewould removethecommoncontext \x • ... (x) • dxfromeachdisjunct,
leaving sin V cos. The mechanism for doing this step is as follows:
Alignthedisjunctsaccordingtohowthedomainoftheoperatormatched
each disjunctive term.
b. Foreach disjunct, delete all embedding expressions ofthe disjunct that
occur identically in allthe disjuncts.
5.3.2.2 Translating Recommendations into New Concept
Descriptions
Thesecondstepoftheleastdisjunction procedure istotranslatethedisjunctive
description, recommended by the first step, into a description using the formalism
of the LEX description language. Specifically, it is necessary to create a new
--- PAGE 139 ---
UTGOFF 125
description that does not use the disjunction operator V required for describing a
least disjunction. The procedure does this simply by creating a new symbol for the
vocabulary and then defining the new symbol as more general than each ofthe disjuncts in the disjunctive description. In the LEX grammar, this step corresponds to
creating a new symbol NS and then foreach disjunctd; adding a new grammar rule
NS => dj to the grammar that defines the concept description language. For
example, to translate the new description sin V cos, the procedure could create a
new symbol, say, N16S, and then add two new grammar rules N16S => sin and
N16S => cos. As a result oftranslation, the new concept is describable without
explicit disjunctionbecause the disjunction is implied by the grammar rules.
5.3.2.3 Assimilating New Concepts into the Hypothesis Space
The third step ofthe least disjunction procedure is to assimilate new descriptions, created by the second step, into the LEX description language. During the
translation step, a new description NS was created by defining grammar rules from
NS to other descriptions in the concept description language. The new description
NS is not yet part ofthe description language, however, because NS itselfis not yet
derivable in the grammar.
TheleastdisjunctionprocedureassimilatesanewdescriptionNS intwosteps:
Defineasetofdescriptions mg thatconsistsofthemostspecificdescriptionsin
the description language that are more general than all the disjuncts used to
definethe newconceptNS. Each such description includes oneormore negativeinstances, butthatisall right; theobjectiveistosplicethenewdescription
into the description language. For example, in the case ofsin V cos, the set
mgis {trig}.
2. Foreachdescriptionmg addanewgrammarrule mgj = NStothegrammar,
For example, as shown in figure 5-6 when mg is {trig} assimilation is completed by adding the grammar rule trig => N16S.
sin cos tan esc sec cot m cos tan esc sec cot
Figure5-6: Biasbeforeandaftershift.
--- PAGE 140 ---
126 CHAPTER 5: SHIFTOF BIAS
5.3.2.4 Requirements
Three conditions must be met in order to guarantee that the least disjunction
method can construct and assimilate a new concept description:
1. Everyconceptcontainingexactly oneinstancemustbe intheconceptdescription language. This insures that it is always possible to construct a consistent
disjunctive description.
2. The concept containing every instance must be in the description language.
Thisinsuresthattherewillalwaysbeadescriptionthatismoregeneralthanany
constructed disjunctive description.
3. There must be a method for asserting the "subset" predicate for two descriptions. This is necessary so that assimilation of a new description can be
effected.
5.3.2.5 Experiment
Mathematicalexpressionsareshownbelow instandardLeibniznotation. LEX
wasgiventhreeproblems: \x • sin(jc) • dx, \x • cos(x) • dx, and Jjc tan(x) dx.
Forthefirstproblem, \x • sin(x) • dx, theproblemsolverfoundthe solution shown
in figure 5-7. The generalizer then refined the heuristic for op12, integration-byparts, by using jjc sin(jc) • dx as a positive instance. The resulting version space
was as follows:
G: {\fix) • g(x) dx; u = /(*); dv = g(x) dx}
S: {jjc • sin(*) • dx; u = x; dv = sin(jc) • dx}
A version space is an efficient representation ofall descriptions in the description
language that are consistent with the training instances. All that needs to be stored
explicitly is the set G ofmost-general consistent descriptions and the set S ofmostspecific consistentdescriptions. All otherconsistent descriptions are stored implicitly between the boundaries.
\ x • sin(x) • dx; u=x; dv=sin(x) dx
opl2: ju • dv => u v- jv • du
-x cos(x)- j -cos(x) • dx
op3: Jc • f(x) • dx => c • jf(x) dx
-x • cos(x)+jcos(x) • dx
oplO: jcos(x) • dx => sin(x)
-x cos(x)+sin(x)
Figure5-7: Solutionfor Jji • sin(jr) • dxusing integration-b) pans
--- PAGE 141 ---
UTGOFF 127
LEXsolvedthe secondproblem, \x • cos(jc) • dx, alsoby startingwithop12.
LEX's generalizer updated the version space forop12 to the following:
G: « {J/ * to • g(x) • dx; u = fix); dv = g(x) • dx]
S: trig(x) dx; u = x; dv = trig(x) dx}
When LEX tackled the third problem, \x • tan(jc) • dx, it tried to use the
integration-by-partsapproachasitdidforthefirsttwoproblems, butitdidnotwork.
Asaresult,thecriticidentifieduseofop12onthestate jjc tan(x) • dxasanegative
instance. Thebiasintheexisting languageprohibitedthegeneralizerfromfindinga
consistent description that could include sin and cos but exclude tan. Accordingly,
STABB was calledto shift LEX's bias.
The control strategy ofSTABB is first to try the constraint back-propagation
proceduredescribedbelowinsection5.3.3. Thatproceduredoesnotalwaysproduce
arecommendation. Ifconstraintback-propagationdoesnotrecommendashift, then
STABB tries the least disjunction procedure, which can always recommend a new
consistent description. In this experiment, the constraint back-propagation procedure did not produce a recommendation for a shift, so STABB used the least disjunction procedure.
The leastdisjunction procedure tookthe following steps:
1. Recommend that a description equivalent to sin V cos be added to the
description language.
2. Translate the recommendation that sin V cos be describable into a new
description N16S defined as being more general than sin and cos.
3. Assimilate the new concept N16S as more specific than trig.
Figure 5-6 shows the relevant portion of LEX's description language before
andafterthe shifttotheweakerbias. Followingthe shift, LEX reinitializedtheversion space for op12 and then reprocessed the training instances. After the training
instances were reprocessed, the version space forop12 was as follows:
G: {\f(x) • N16S(jc) • dx;u = f(x); dv = N16S(jc) dx}
S: {Jjc • N16S(jc) • dx; u = x; dv = N16S(jc) • dx}
5.3.2.6 Language Shift and Version Spaces
In general, it is necessary to adjust the version space boundary sets after the
shifttoanewbias. Recallthatversionspaceboundarysetsareacompactrepresentation of the set of all consistent hypotheses in a given description language. If the
description language is enriched by the addition of new descriptions, the version
spaceisnotguaranteedtorepresentthesetofallconsistenthypothesesinthenewlanguage. Itisonlyguaranteedtorepresentthesetofallconsistenthypothesesintheold
language. There may be consistent descriptions in the enriched language that are
incorrectlyexcluded fromtheversion spacethatwascalculated with theunenriched
--- PAGE 142 ---
128 CHAPTER 5: SHIFTOF BIAS
language. For this reason it is necessary to compute a new version space in the
enriched language. An obvious method, the one used by STABB, is simply to start
over and reprocess all the training instances.
An important ramification ofshiftingtheconceptdescription language is that
the version spaces for the other heuristics, computed in the old language, may be
affectedaswell. Eventhoughtheversionspacesfortheotherheuristicsarenotempty
(i.e., there are consistent descriptions), shift ofthe language removes the guarantee
that the version spaces contain all consistent descriptions describable in the language. Thus it is necessary to recompute the version spaces forallthe heuristics.
5.3.2.7 Obsolete Descriptions: Strengthening Bias
One problem that occurs with the least disjunction procedure is creation of
descriptions that later become obsolete. For example, in subsequent activity a new
description N17S, defined as more general than N16S and exp, was constructed and
addedtotheconceptdescriptionlanguage. ThecreationofN17Stodescribetheheuristic forop12 rendered N16S unnecessary. That is, the originaljustification forthe
creation of N16S- to enable one to describe the heuristic of op12-was no longer
present. Thisraisestheproblemofdiscardingobsoletedescriptions,aformofstrengthening bias. How does one identify a stronger bias? Three possible approaches are as
follows:
Keeptrackofwhyadescriptionwascreated. Forexample, onecouldassociate
with N16S the fact that it was created so that the heuristic for op12 would be
describable. If the justification for the existence of a description becomes
invalid, in this case that N16S ceases to be used in the description ofthe heuristicforopl2, onecouldthenremovethedescription(assumingnootherjustification exists-e.g., the description is not in use in describing some other
heuristic). Thus, whenN17S iscreatedand N16S is nolongerusedtodescribe
the heuristic forop12, the existence ofN16S would no longer bejustified.
2. As suggested by Banerji (Ranan Banerji, personal communication, 1984).
associate with each description the number of places in which the concept
learnercurrently uses it. A program could automatically discard descriptions
that fall below some specified usage.
3. Conduct a search for candidate descriptions in the description language that
canberemovedfromthelanguagewithoutcausinganyversionspacecomputed
in such a language to be empty. This is uninteresting for any single version
spacebecauseanycandidatedescriptioncanbeeliminatedaseasily asanother.
Formultipleversionspacesbasedonasingledescriptionlanguage, however, as
inthecaseofLEX, theproblem is interesting. Onewouldattempt to search for
a further restricted description language that would result in the overall
smallest nonempty version spaces for all the heuristics.
--- PAGE 143 ---
UTGOFF 129
5.3.2.8 Choosing Among Syntactic Methods
Inthe experiment above, acounterfactual method could have recommendeda
new concept "trig except tan." The least disjunction method recommended a new
concept "sin or cos." There is no apparent advantage to either one. Ifthe concept
learner had some idea or clue about the syntactic properties ofthe target concept,
then there might possibly be a criterion for preferring one method to another, but
such a criterion is extremely weak.
Descriptionsthatarenotbasedonwhyaninstanceiseitherpositiveornegative
do not capture the essence of a concept. A disjunctive description that is a list of
disjoined positive instances describes only that they are positive, not why they are.
Similarly, a counterfactual description that is a list ofexcepted negative instances
describes only that they are excepted, not why they are.
Asanexample, considertheproblemofmakingtheinductiveleapthatpositive
instance {2,4,6,8} and negative instances {1,3,5,7} are indicative ofthe set ofeven
integers. Assumethatthereisadescriptionforthesetofintegersbutnosubclassesof
integers. Consider four possible descriptions:
1. Syntactic via disjunction: 2 V 4 V 6 V 8
& ~ 1)&~3)&~5)&~7
2. Syntactic via counterfactual: (((integers
3. Analytic via division: {x remainder^,2) = 0}
4. Analytic viabinary: {x logicaland(jc,1) 0}
Syntactic methods are useful because they can lead to descriptions ofuseful
sets. A preferable approach, however, is to attempt to draw on information that can
drive an analytic method, forexample, constraintback-propagation.
5.3.3 Constraint Back-Propagation: A Goal-Sensitive Method
Constraintback-propagation (Utgoff, 1982) istheotherofthetwoprocedures
usedby STABB. Theprocedureisdesignedaccordingtotheframeworkdescribedin
section 5.2. Unlike the least disjunction procedure, constraint back-propagation
takesadvantageofthefactthattheconceptstobelearneddescribewhenandwhennot
toapplygivenoperators. Inadditiontothetraininginstancesandthedescriptionlanguage, the procedure also uses a set of backward problem-solving operators, a
description ofthe set ofsolved problems, and the solution sequence from which the
traininginstanceswereextractedtohelpdetermineashifttoanewbias. Assuch, the
method is goalsensitive.
5.3.3.1 Recommending a Better Bias
When LEX cannot find a consistent description to describe the domain ofa
heuristic, insteadofexamining individualtraining instances, theprocedureanalyzes
--- PAGE 144 ---
130 CHAPTER 5: SHIFTOF BIAS
a solution sequence for which the operator application was a positive instance to
determine how to adjust LEX's bias. Each constructed description describes a constrained set that is needed to describe the constrained domain of the operator
sequence. By deducing the domain ofan operator sequence such that application of
theoperatorsequencetoastate inthedomainwill produceasolution, theprocedure
can identify useful new concepts to add to the description language.
To recommend abetterbias, the procedure employs the following heuristic:
Ifanoperatorsequenceleadstoasolution,thencreateanewconceptdescription
thatdescribesthedomain oftheoperatorsequence.
This heuristic is chosen because ofthe a priori knowledge that the recommended
domain ofapplicability foragiven operatoris the union ofthe domains ofall useful
operator sequences that start with a given operator. To describe the union, one can
start by being able to describe identifiable subsets in that union.
Tocomputeaspecificationofthedomainofanoperatorsequencethatproduces
a state in agiven range, inthis casethe setofsolvedproblems, one applies a deduction procedure known as constraintpropagation orgoalregression. Propagation of
constraints through anoperatorsequence is awell-known technique; what is new in
STABB istheuseofconstraintpropagationasaheuristicmethodforidentifyingnew
concepts that should be describable in the description language. The STRIPS program (Fikes, Hart, and Nilsson, 1972) computes preconditions for macro-operators
by reasoning about constraints. Waldinger's program for achieving simultaneous
goals(Waldinger, 1976)usesgoalregressiontodeducearestricteddomainofanoperatorsuchthatapplicationoftheoperatortoastateintherestricteddomain isguaranteed to produce a state in an intended restricted range. Stallman and Sussman's EL
(StallmanandSussman, 1977)computesvaluesatvariouspointsinelectricalcircuits
byreasoningfromknownvalues, aprocedureanalogoustosolvingasystemofsimulMOLGEN
taneousequations. Stefik's (Stefik, 1980)plansexperiments in molecular
genetics. Whenever constraints become known or specialized during the planning
process, actions that are inconsistent with the constraints are eliminated. The
CRITTER program (Kelley and Steinberg, 1982) uses constraint propagation to
reason about digital circuits. From a statement ofoutput specifications and definitions ofcomponents in a circuit, the program deduces input specifications of one
componentandpropagatesthembackwardthroughthe restofthe components in the
circuit.
Constraint back-propagation is a procedure for deducing the domain of an
operatorsequenceormacro-operatorthatproducessomeconstrainedrangeofstates.
The constraint back-propagation function, cbp, used by STABB is as follows:
cbp(seq) =
if length(seq)==0
then
solved
--- PAGE 145 ---
UTGOFF 131
else
abop(head(seq) inter(dom(head(seq) ,cbp(tail(seq)
, ) ))
with abbreviations:
cbp constraint-back-propagation
seq operator-sequence
abop apply-backward-operator
head first-element-of-list
inter set-intersection
dom domain-description-of-operator
solved set-of-all-solved-states
tail list-with-first-element-removed
Ifan operator sequence opseq is oflength 1, it means that the operator produced a
solvedproblem. Byintersectingthesetofallsolvedproblems, describedbyaxinthe
LEX grammar, with the range ofthe forward operator, the procedure calculates the
subsetoftherangecontaining solvedproblems. As shown in figure5-8, byapplying
the backward version ofthe operator to the subset, the procedure calculates a constraineddomainoftheforwardoperatorsuchthatapplicationoftheforwardoperator
to any state in the constrained domain produces a solved problem. If an operator
sequence is oflength greater than 1, it means that the tail ofthe operator sequence
producedasolvedproblem. Byapplyingconstraintback-propagationtothetailofthe
sequence, the procedure calculates a constrained domain for the tail. That constraineddomaindescribesthesetofstatesforwhichthetailofthesequenceleadstoa
solvedproblem. As such, theconstraineddomain forthetail ofthe sequencecanbe
intersectedwiththerangeoftheoperatorattheheadofthelist, andthebackwardversionoftheoperatorattheheadofthelistcanbeappliedtocalculatetheconstrained
domain ofthe entire sequence.
The constraint back-propagation procedure uses backward operators, as
opposed to strict inverse operators. An inverse operator by definition undoes the
effectofthecorrespondingoperator. Anoperatorthatisaone-oneorone-manymappinghasafunctionalinverse. Anoperatorthatisamany-oneormany-manymapping
does nothaveafunctional inverse, becausethere is notauniquevalue inthedomain
Figure5-8: Constraintback-propagation.
--- PAGE 146 ---
132 CHAPTER 5: SHIFTOF BIAS
thatmapstotherange. Abackwardoperatordiffersfromaninverseoperatorbecause
mapsastateintherangetothesetofstatesinthedomain. Thusbackwardoperators
map sets ofstates to other sets ofstates.
Each LEX operator includes two LISP expressions for computing values as
part ofthe operator application. One ofthose expressions, FORWARD, is straightforward because the forward operators are written to map a single problem state in
the domain ofthe operatorto a single problem state in the range. The otherexpression, BACKWARD, is ofa different nature because it can map a set ofstates in its
domain (the range ofthe forward operator) to a set ofstates in its range. Each LISP
expressionforhowtoapplytheoperatorinthebackwarddirectionmustencodehow
that is to be done. Considerthree cases:
1. Ifno LISPevaluation is needed, as with commutation, (+ frl fr2) <£> ( + fr2
frl), then there will be no need to shift bias, because any description in the
domainofthebackwardoperatorisimmediatelydescribableintherangeofthe
backward operator.
2. If some LISP evaluation is required but the result of applying the backward
operatoris asingle state, thenthe state isdescribable inthe range ofthe backward operator.
3. If some LISP evaluation is required and the result ofapplying the backward
operator is a set of states, then the operator calls a function named lookup-or-make. Thelook-up-or-makefunctionconstructsarecognitionpredicate, based on the LISPevaluation that is to be applied to the argument ofthe
recognition predicate and the constrained set in which the result must be
includediftheargumentistobeconsideredrecognized. Ifarecognitionpredicate based on these two arguments already exists, then the corresponding
symbol inthedescriptionlanguage is returned. Ifsuch a recognitionpredicate
does notexist, itiscreatedandassociatedwithanew symbol, assimilated into
thedescription language (described below), and returned.
Consideranexample. There isanoperatorop99c => 2 (c/2), where c is any
real number. Theprocedure isgiventhetaskofpropagating2 • kbackwardthrough
op99, where k isthe setofintegers. The intersectionof2 • (c/2) and 2 • kevaluates
to 2 • k. If2 • k is passed through p99backward , the constrained domain ofthe forwardoperatormustbethe setofnumbers inwhicheach isequivalentto 2 multiplied
by some integer, that is, the even numbers. The set ofeven integers is. ofcourse, a
propersubsetofthesetofreal numbers. This illustration iscontinued inthe next two
sections.
Althoughimplementationofeachofthethreestepsofthe frameworkofsection
5.2 isdescribed separately below, the three steps are actually done foreach operator
in turn, as the constraints back-propagate through the solution sequence. That is.
therecanbeashiftofbiasviarecommendation, translation, andassimilation at each
step in the operator sequence.
--- PAGE 147 ---
UTGOFF 133
5.3.3.2 Translating Recommendations into New Concept
Descriptions
Step2 ofthe constraintback-propagation procedure is to translate the recommendations produced by step 1 into descriptions using the formalism ofthe LEX
description language. Thecbpfunction manipulatesexpressionsconsisting notonly
ofdescriptions in LEX's description language but also ofcalls to the intersection
function and to backward operator definitions. To translate such recommendations
into LEX descriptions, theprocedure must removeall referencesto intersection and
backward operators. Such references are translated by evaluation. Thus the procedure depends on being able to evaluate the intersection oftwo descriptions and on
being able to apply a backward operator. Certain backward operators were difficult
toimplementforLEX, forexample, thebackwardoperatorforintegration-by-parts.
Anytimethatanintersectioncannotbecalculatedorabackwardoperatorcannotbe
applied, the procedure fails in the translation step.
New descriptions are created during the translation process when necessary.
Forexample, applying p99backwardto2 • krequires creationofadescription forthe
set ofeven integers. Ifthe description ofa constrained domain is computed by an
operationfoundonlyinanoperator, forexample, multiplication, thenthetranslation
procedureconstructsanewdescriptionintheformofarecognitionpredicate. Ingeneral, for some set w computed as w = ybackward^^ the corresponding recognition
predicate is {x
match(y,f(x))}. Thepredicatedefines thoseelementstowhich one
can apply the operation and produce a result that satisfies the constraint. For
example, foradescriptionofthesetinwhicheachelementisequivalentto2 • k, for
somek,thecorrespondingrecognitionpredicateis {x match('k,[x/2])}. Theprocedurecreatesanewdescriptionby generating anew symbol NS forthe vocabulary of
thelanguageandthenassociatingthedefinitionofthenewrecognitionpredicatewith
the new symbol by adding a grammar rule ofthe form NS => new-recognitionpredicate.
Creation ofa new definition for a recognition predicate does not necessarily
mean that a new description will be incorporated into the concept description language. Whenanewdefinitioniscreated,thetranslationalgorithmfirstsearchesfora
concept in the language with the same definition. Here, same means "syntactically
equal"; testing functional equivalence is a classical nontrivial problem. However,
becausetheproceduregenerates recognitionpredicates inonly oneway, any attempt
to create a second identical definition will be thwarted by this test.
Inadditiontoapplicationofbackwardoperators, thereistheproblemofevaluating intersection of descriptions. Intersection is evaluated with the following
algorithm:
Intersection^,y) =
If Match(x,y) then y,
else
--- PAGE 148 ---
134 CHAPTER 5: SHIFTOF BIAS
If Match(y,x) then x,
else Most-General (lntersection(Next-More-Specific(x) ,y) U
Intersection^,Next-More-Specific(y)
Most-General is afunctionthatreturnsthe mostgeneral descriptions from its argument,andNext-More-Specificisafunctionthatreturnsthosedescriptionsinthelanguage that are immediately more specific than the argument. When Intersection
returns more than one description, then the intersection is the disjunction ofthose
descriptions and the disjunction is not describable in the description language. For
example, Intersection(sin,(+ trig ( + fn fr))) returns (4- trig ( + fn sin)) V
( + sin ( + fn fr)) due to the multiple ways in which sin can specialize (+ trig
(+ fn fr)) attrig or fr. Disjunction is not a problem, however, because the routines
within the least disjunction procedure (section 5.3.2) can be called to translate and
assimilate adisjunctive description.
5.3.3.3 Assimilating New Concepts into the Hypothesis Space
Step 3 of the constraint back-propagation procedure is to assimilate new
descriptions, created by step 2, into the LEX description language. To assimilate a
new description NS, the procedure must add the new description NS to the concept
descriptionlanguage. ThoseconceptsthataremorespecificthanNSarewelldefined
by the recognition predicate created during translation in step 2. The concept NS is
not yet part ofthe language, however, because NS is not itselfyet derivable in the
grammar.
The constraint back-propagation procedure assimilates a new description NS
by addinganewgrammarruleofthe formd => NS. Thedescriptiond is the unconstrained description that was used in the domain ofthe corresponding operator. For
example, forop99, the operator rewrites c as 2 • [c/2]. When a new description for
evenintegers, say N22S, isassimilated, thenewgrammarrulewouldbec => N22S.
Using a description in an operator's domain as the generalization ofthe new
description has two drawbacks, as follows:
1. Althoughthemethoddoesnotincorrectlyassertanysubsetrelation, itisnevertheless weakbecauseitdoes notassertall correct subset relations. Forexample,
itis not incorrecttoassertthatthe setofeven integers isa specializationofthe
setofreal numbers, buttheassertion is weakbecause it does not tell the whole
story. Itwould be much strongertoassert that the set ofeven integers is a specialization ofthe set ofintegers.
2. Withthisassimilationmethod, theextenttowhichthedomainofanoperatoris
large (general) is theextent to which the properpointofassimilation is poorly
specified. A program author will want to write operators in as general a form
as possible to maximize the number of legal transformations available to
the problem solver. The task ofassimilating a new concept description into a
--- PAGE 149 ---
UTGOFF 135
partially ordered hierarchy ofdescriptions is done properly by making a new
description immediately morespecificthanthemostspecificexistingdescriptions possible.
5.3.3.4 Requirements
Four conditions must be met in order to guarantee that the constraint backpropagation method can construct and assimilate a new concept description:
1. A description ofthe set of solved states is needed so that the procedure can
intersecttherangeofanoperatorwiththesetofsolvedstatestocomputetheset
ofsolved states that can be produced by the operator.
2. A functiontocomputethe intersectionoftwodescriptions is necessary sothat
the range ofan operator can be constrained as necessary. The case of intersectingtherangeofanoperatorwiththe setofsolvedproblemsisnecessary so
thatapplicationofthebackwardoperatortotheconstrainedrangewillleadtoa
constraineddomainthatdescribesthesetofstates "solvedbyapplicationopj."
3. For each operator, a definition ofhow to apply the operator in the backward
direction is needed sothataconstrained domaincan be calculated by application ofthe operator.
4. Forany operatorthatincludes arithmetic computation, orotherkinds ofcomputationnotmodeledinthegrammar, itisnecessarythatsuchcomputationsbe
capable ofbeing incorporated in aconcept description.
5.3.3.5 Experiment
LEX found a solution for jcos7(x) dx similar1 to that shown in figure 5-9.
LEX wasthengiven jcos6 (X) • dxforwhichthe same solution failed and Jcos5 (jc) •
dx for which the same solution worked. The description language did not contain a
description that both included jcos7 (jc) • dx and jcos5(jc) dx and excluded
Jcos6 (jc) • dx. LEX called STABB to shift the bias so that the language could
describe the heuristic forop51. STABB triedtheconstraintback-propagation procedure, andtheback-propagationshouldhaveproceededasshowninfigure5-10. However, one stepwastoodifficultforthe intersection algorithm, discussedbelow. Otherwise, the method worked.
STABB correctly handled the lasttwo steps ofthe back-propagation sequence
(firsttwostepsofthesolutionsequence). In j (cos2 (jc)) k ) • cos(jc) • dx, theexponent
k denotes that the exponent must be an integer. This constraint propagated earlier
'LEXdoesnothaveamultiply-polynomialoperator, sotheactual solutionsequencewasbizarre.
--- PAGE 150 ---
136 CHAPTER 5: SHIFTOF BIAS
Jcos7 (x) • dx
op51: fc => f[c-1] ' f
cos6(x) • cos(x) • dx
op50: fc => (f2
j(cos2(x))3 • cos(x) • dx
op52: cos2 => 1-sin2
j(l-sin2(x))3 • cos(x) • dx
jfc(g(x)) • g'(x) • dx => jfc(u) • du;
iopu4=g3(:x)
j(l-u2 ) 3 • du, u=sin(x)
multiply polynomial
Figure5-9: Solutionfor Jcos7(jc) • dx.
jcosodd(x)
T p51backuard
jcoseven(x) • cos(x) • dx
Op50backward
j(cos2(x))k • cos(x) • dx
T p52backward
jpolyk(f(x)) • f*(x) • dx
T p43backward
jpolyk(u) • du, u=f(x)
T factor polynomial
Figure5-10: Back-propagationfor Jcos7(jc) • dx.
from the fact that the solution sequence works only ifthe polynomial can be multiplied out to remove the exponent. Back-propagation of the product of the set of
integers k and 2 from the cos2 through the exponent in Op hackward50 caused a new
description tobecreated forthe set ofeven integers, that is, the set ofelements such
thateachelementistheproductofsomeintegerand2. Priortothisshiftofbias, LEX
could not describe the set ofeven integers. Back-propagation ofthe set ofeven integers through the exponent in p backw;,rd51 caused a new description to be created for
--- PAGE 151 ---
UTGOFF 137
the set ofodd integers, that is, the set ofelements in which each is the difference of
someeven integerand 1. Priortothis shiftofbias, LEXcouldnotdescribethe setof
odd integers.
Thesetwoshiftsbenefit LEX notonlybyenabling ittodescribethe heuristics
forop51 andop50butalsobyallowingittoconsideranddescribeheuristicsforfuture
learning problemsas well. Forexample, there isasimilarsolutionmethod forproblems ofthe form sinodd(x) dx.
The definition constructed for "even integer" was {x match('k, [jc/2])}.
Becauseop50isdefinedtooperateonanyreal-valuedexponent, thenewconceptwas
assimilatedasaspecializationofreal. Thenewconceptshouldhavebeenassimilated
asaspecializationofinteger. Insection5.3.3.6below, amethod forcorrectassimilation is discussed.
Thethird-from-laststepintheback-propagationsequencefailed. Thedescription that would have propagated through p43backward is jpolyk(/(x)) • f'{x) • dx.
Notethattheexpressionuses/(jt)aswellas/'(x), indicatingthat/'(*) issensitiveto
the/(jt) ofthe samecontext. LEXcanrepresentasimilardescription jpolyk(/(jt)) •
g(x) • dx, butitis incapableofdescribingtheconstraintthatthederivativeoff(x)be
equal tog(x) dx. Thatis notalargeproblem for p43backward ifu is an instance ofa
function, because in such cases the derivative can simply be computed. Ifu correspondstosomelargersetoffunctions, thenthemissingconstraint-thatderivativeof
f{x) is equal to g(x) • dx-cannot be tolerated. When the next propagation step
through p52backward isstarted, thelackoftheconstraintisfatal. Intheexperiment, it
would be necessary to intersect sin(jc) with \po\yk(f(x))-f'(x) • dx. There is an
intersection, (polyk(sin(;c)) • cos(jc) • dx. The intersection algorithm computes
trivially that/ Pi sin is sin, but notice that/' must then be cos. LEX's description
language (and the intersection function) cannot representthis kind ofcontext sensitivity. This exposes the fact that the formalism, a context-free grammar, is biased
against descriptions that include context sensitivity ofthis kind. This suggests that
mechanisms for shift ofbias must address choice and change of formalism. The
abilitytoidentifyexactlythoseelementsofbiasthatcomefromagivenformalismis
an importantopen research question. Onepossible solution is touse abias-free formalism, but that is probably less preferable to using a formalism with a desirable
bias. Thus shift ofbias may also entail shift offormalism, a little-studied problem.
There are two points to note regarding op50:
1. Op50 is not mathematically correct when the value offunction/is negative.
Squaringthe valueofthegivenfunctionwillchangeanegativevaluetoapositive value, and the principle nth root ofa positive real is positive. However,
LEX does not have knowledge regarding such limitations on the law ofexponents. The experiment shows that STABB was able to define a useful concept
"even integer" by deducing a class ofproblems for which the sequence does
work. Applicationofop50whentheexponent is odd is mathematically illegal
--- PAGE 152 ---
138 CHAPTER 5: SHIFTOF BIAS
when/(jc) has a negative value. LEX applies op50 illegally in such cases and
simplycannotprogresswhen, lateron inthesolutionsequence, thereisapolynomial raised to a noninteger power. Thus STABB learned about "even
integer" from an operator sequence that reached a conclusion only when an
exponentwaseven. An improvementto LEXwouldbetohave itdetectmathematical inconsistency inanoperator. Forexample, if LEXcoulditselfidentify
themathematicalproblemwiththelawofexponents foranegativebaseandan
evenexponent, it wouldhaveasecondmethodforlearningabouteven integers
as well as a method fordetecting mathematical inconsistency in op50.
2. Op50hadtobe added specifically sothattheproblemcouldbe solved. This is
duetothe factthat LEX is aforward-searchproblem solver. The entire reason
that LEX should rewrite cosc (jc) as (cos2 (jc)) c/2 , mathematical correctness notwithstanding, is to allow op52 (cos2 => 1 - sin2 ) to be applied. The actual
chainofreasoningusedbyahuman isprobablysomeformofmeans-endsanalysis. Forexample, the reasoning steps could be as follows:
a. To obtain a simple polynomial, worktoward using the change ofvariable operator (op43).
b. Tosetuptheintegrandforchangeofvariable, worktowardintroducing
sin.
c. Tointroducesin , worktowardintroducingacos2sothatop52cantransform it into 1 - sin2 .
d. To create acos2 use the law ofexponents.
LEX cannot do means-ends analysis, however, so op50 was necessary.
5.3.3.6 Knowledge-Based Assimilation
Thedescriptionfor "eveninteger" couldhavebeenassimilatedcorrectly inthe
experiment in the following way: an alternative definition of "even integer" is
[v (3jc G k y = 2 • x)}. This definition can be generated mechanically as
| |
easilyasthatgeneratedbySTABB. Fromtheconstraintback-propagation
it is
known
thatxand2areintegers. Ifit werealsoknownthattheintegersareclosedundermultiplication, then a procedure could easily prove that every y is also an integer. The
critical piece of knowledge for the proof is that the integers are closed under
multiplication.
Forsuchanassimilationmethodtowork, thelearningprogramhas firsttoformulatethe rightsubproblem, andthen ithastosolve it. Fortheeven integerexample,
the rightsubproblem istodeterminewhetherthe setofk • k ismorespecific thanor
equal to the set ofintegers.
To formulate good subproblems for the assimilation task, LEX can search for
subproblems by generatingexpressions within a bounded space. The specific bound
forthe even integerexample is the expression 2 • k. The general bound is the set of
--- PAGE 153 ---
UTGOFF 139
realnumbersc. 2Theprogramcouldexhaustivelyconsidergeneralizationsofthespecific bound that are not more general than the general bound. For the even integers
example, using LEX's description langauge, some of those generalizations are
[k • k], [2 • r], [k r], and [r • r]. Foreach expression so generated, the learning
programcoulduseanyknowledge ithasaboutthesetofobjectsimpliedbythegenerated expression. Ifthe program has knowledge about a given set, it could query an
expert. It would be a pleasant surprise ifa learning program asked such intelligent
questions as, "Are the integers closed under multiplication?" or, in mathematical
terms, "Istheresultof[k • k] alwaysk?" Thisapproachidentifiesknowledgeworth
having, an important problem for knowledge acquisition programs.
When the expert provides additional knowledge, the learning program could
save the piece ofknowledge forpotential application in the future. One method for
encoding such acquired knowledge is to embed it in the concept description language. Forexample, one could insertthe set k • kas a specialization ofk.
5.3.3.7 Bias in Formalism of Description Language
In section 5.3.3.5 the intersection algorithm was shown to be inadequate
because itcould not handle certain context-sensitive constraints. In the firstexperiment there was a need to intersect the description sin(;c) with a description that
included/(x)andg(x), whereg(x) wastobeconstrainedtobethederivativeoff(x).
Theintersectionofsin(x)and/(x)wascomputedeasilyassin(jc),butasaresultofthe
inability to specify thederivativeconstraintfortheg(x), there was no knowledge in
theintersectionalgorithmthatg(x)dependedonthe/(x). Tofinishthecomputation,
the intersectionalgorithmwouldhavehadtocalculateg(x)bycomputingthederivative, in this case, ofsin(x) as cos(x).
Thecorrespondencebetween/(X)andg(x)couldnotbepracticallyrepresented
in LEX's grammar. In addition, intersection failedbecause it would have hadtouse
the correspondence for its calculation. The LEX formalism is biased against such
descriptions. Shift ofbias can include shift of formalism as well. More work is
needed on understanding the bias offormalism.
5.3.3.8 Interaction of Operator Language and Description Language
The grammar for LEX's concept description language uses string rewrite
rules. Theoperatorsusestringrewritesandarithmeticrewrites. Thus, throughcomposition of symbolic and arithmetic transformations, it is possible to construct
2Notethatalthoughthedescriptionlanguagedoesnotcontaintheexplicitaxiomthat[c • c]alwaysevaluatestoarealresult,theoperatorforevaluatingmultiplicationdoes. Itwouldbeasignificantimprovement
foraprogramsuchasLEXtobeabletousesuchknowledgealreadyencodedintheoperators.
--- PAGE 154 ---
140 CHAPTER 5: SHIFTOF BIAS
descriptionsthatarenotfoundinthe LEXgrammar. Thisisthereasonthatconstraint
back-propagation can be used to deduce new descriptions. The problem-solving
experience shows which operator sequences should be followed. Back-propagation
ofconstraints over such a sequence can yield useful compositions ofsymbolic and
arithmetic transformations. Thebias-shiftingproceduretranslatesany conceptsthat
are defined through an arithmetic composition into recognition predicates that are
added to the concept description language.
The operator language and the concept description language were written for
different purposes. The experiment has shown an area in which they interact.
Grammar rewrite rules andoperator rewrite rules both define how one description
can be transformed into another. Further consideration should be given to whether
the present distinction between these two classes oftransformation rules is worthwhile. It maybeusefultoprovideasingleformalismthatpermitsstring rewritesand
arithmetic rewrites.
In addition to the problem ofmixing the two classes ofrewrite rules for constraint propagation, having a separate operator language and description language
allows other inconsistencies. For example, because the interaction ofthe operator
language and the concept description language was not originally well understood,
certain liberties were taken when the operators were written. For example, the
change ofvariable operator is
^ =
\g(f(x)) ' f'{x)dx \g(u)du, u f(x)
One may wonderhow the domain ofan operator can describe g(f(x)) • /'(*) when
theconceptdescriptionlanguagecannot. Thisispossibleonlybecausethedomainof
theoperatorisdescribedas \g(f(x)) • h(x)dxandthecodeintheoperatorexplicitly
tests whether derivative(/(jc)) = h(x). Ifthe relation holds, the operator performs
thechangeofvariable. Iftherelationdoesnothold,theoperatorrewritestheproblem
state to include the symbol fail, which is not in LEX's grammar. At that point, the
problem solver will never be able to proceed to a solution along that branch ofthe
searchtreebecause fail does notappearinthedomainofany operator. Several other
operatorsusethisfailconvention. Thisiscounterproductive, however, because LEX
thenfutilelytriestolearnaheuristicwithouttheabilitytodescribethedomainofthe
heuristic in the formalism ofthe concept description language. It is a poor idea to
permit descriptive capability in the operator language that cannot be described
within the formalism ofthe concept description language.
5.3.3.9 A Method for Computing a Strong and Correct Bias
Ifthe ability to do constraint back-propagation exists, it appears possible to
compute an initial bias that is strong and correct for problems that can be solved
withinn steps. Thatis, an initial biasiscomputedpriortoobservationoWuivtraining
instances. Consider n = 0. The set of states that is solved in steps is the set o\
solvedstates. Considern = 1. Intersectionofthe rangeofanoperator\\ttfa the set of
--- PAGE 155 ---
UTGOFF 141
solvedstatesandapplicationofthebackward operatoryieldthe setofstatesthatcan
besolvedwithexactlyn = 1 operatorapplication. Any newdescriptionsneededfor
theconstraintpropagationstepareaddedtothelanguageduringthepropagationstep.
Consider n > 1. The constraint back-propagation technique would deduce the
domainsoftheoperatorsthatleadtoastatethatis solvable inn - 1 steps. Thus itis
possibletodeducethedomainsofall operatorsequencesoflengthlessthanorequal
to n that lead to a solution. Descriptions other than those created during constraint
back-propagation are unnecessary because the only descriptions ever needed are
those ofdomains ofoperator sequences.
Themajorbenefitofthisapproachisthatthedescriptionlanguagewillbecome
sufficiently enriched that the domain ofa heuristic will be describable in the language(foranyoperatorapplicationinvolvedinasolutionsequenceofnotmorethann
steps). Adrawbackoftheapproachisthatonly goodoperatorsequencesjustifyheuristics, somanyofthededuceddescriptionswillneverbeneededforaheuristic. Nevertheless, the approach does not introduce descriptions that are not needed for
describing the domain ofsome operator sequence. In practical applications, the
length ofthe longest operator sequence will be sufficiently small, forexample, less
thantwenty steps, to makethis approach computationally acceptable. For LEX, the
longest operator sequence observed is approximately fifteen steps. Although there
are potentially |set of operators!" sequences of length n, there will be significant
pruningwheneverthedomainofoneoperatordoesnotintersecttherangeofanother.
Research on finding an initial bias that is strong and correct is an important unexplored area.
Thistechniquecancomputeabiasthat is too weakor, equivalently, adescription language that is too rich. There are two reasons forthis, as follows:
Asmentionedabove, thelanguagewillbeabletodescribeconceptsneededfor
operator sequences that a program like LEX would learn through experience
notto use.
2. Useful operator sequences may have the same beginning subsequence and yet
diverge atsomepoint. As shown in figure5-11, iftwo sequences diverge, then
there is no need to distinguish between the two trajectories while they still
share a common path. For example, there is no need to describe individually
both the domain ofoperator sequence OpA-OpB andthe domain ofoperator
sequence OpA-OpC. It is sufficientto be able todescribe the domain ofoperator sequence OpA-(OpB V OpC).
Despite the factthat it computes a bias that is weakerthan necessary for describing
the heuristics that LEX will ultimately learn, the approach is promising because it
computesonlydomainsofapplicabilitythat LEXcaneverconsider,givensomesetof
operators. LEX has no need to describe any other set.
--- PAGE 156 ---
142 CHAPTER 5: SHIFTOF BIAS
OpA
Figure5-11: Divergingoperatorsequences.
5.3.3.10 Familiar Uses of Constraint Back-Propagation
Constraintpropagation isafamiliarprocess, but it is notalways recognizedas
such. Banerji (1980) presents a form ofconstraint back-propagation for tic-tac-toe.
There he shows deduction ofthe concept "fork."
Consider an example from the domain ofchess, as illustrated in figures 5-12
through 5-15. It is white's move. In move 1, white elects to guard its unprotected
knight] with its bishop (figure 5-13). In move 2, black's bishop captures white's
knigh^ (figure5-14). Inmove 3, white'sbishopcapturesblack'sbishop(figure5-15).
White has removed its guard for its knight! Black's rook can now safely capture
Figure5-12: Anexample fromthedomainofchess
--- PAGE 157 ---
UTGOFF 143
£i <&
Figure5-13: Firstmove.
JL £>
Figure5-14: Secondmove.
--- PAGE 158 ---
144 CHAPTER 5: SHIFTOF BIAS
Figure5-15: Thirdmove.
white'sknight. Whitewantstounderstand whatledtoafreecapture forits opponent
in orderto avoid giving away a free capture in future play.
Consider a reasoning process based on constraint back-propagation. Assume
that white has the (conjunctive) concept ofa "free capture":
freecapture (x,y,pl,p2)
it is x's turn
x has a piece at pi
x's piece at pi can move to p2
y has a piece at p2
y does not have a piece that can move to p2
White reasons backward through move 3 to see that when it made the capture, the
pieceit movedhadbeenguardinganotherwhitepiece. Theconceptofa "freecapture
thatgivesawayafreecapture" or, withashortername, "indirecttrade." isdeduced:
indirecttrade(x,y,pl,p2,p3,p4)
freecapture(x,y,pl,p2)
y has a piece at p3
x has a piece at p4
y's piece at p3 can move to p4
x's piece at pi can move to p4
--- PAGE 159 ---
UTGOFF 145
if x's piece at pi were at p2,
then it could not then move to p4
White reasons backward through move 2 to see that when black made its capture,
black produced aposition that was an indirecttrade. The concept ofa "capture that
leaves an indirect trade" is deduced:
captureleavinganindirecttrade (x
pi,p2
it is x's turn
x has a piece at p3
x's piece at p3 can move to p4
y has a piece at p4
y has a piece at pi
y's piece at pi can move to p4
y has a piece at p5
y's piece at pi can move to p5
if y's piece at pi were at p5,
then it could not then move to p<4
x has a piece at p2
x's piece at p2 can move to p5
Whitereasonsbackwardthroughmove
toseethatremovingblack'sfreecaptureby
guarding a threatened piece with a guard that is already guarding some other piece
will lead to a capture leaving an indirect trade forblack.
A player that deduces these concepts and tests possible successor board positions with respecttotheseconceptsbecomes abetterchessplayer. One endeavorsto
beabletopredictconsequencesofmovesbasedonrecognitionofpatternsratherthan
generating and evaluating successor board positions. The main lesson from this
example is that constraint back-propagation can serve as a fundamental mechanism
fordeducing useful classes that it is worthwhile to describe.
CONCLUSIONS
5.4
Theprimaryobjectiveoftheresearchreportedinthischapterwastoshowthat
the search foragoodbiasforinductiveconceptlearningcanbeperformedmechanically. Aby-productwastheprogramSTABBanditstwoprocedures, leastdisjunction
andconstraintback-propagation. More workis neededon the fundamental problem
ofshiftofbias. Three researchdirectionsthatemerge (seebelow) shouldbepursued
further.
--- PAGE 160 ---
146 CHAPTER 5: SHIFTOF BIAS
Assimilation Tasks Generating KnowledgeAcquisition Problems. There is evidence
thatassimilationtaskscanpointtonewknowledgethatit wouldbeuseful toacquire.
Forexample, the procedure described in section 5.3.3.6 forproving subset relationships for recognition predicates, created by constraint back-propagation, requires
knowledge that the concept learner may or may not have. The procedure can create
conditional statements ofthe form "ifxtheny c zV Ifthe concept learneralready
knowsthatx istrue, then the subset relationy c z is proved. Ifthe concept learner
doesnotknowx, then it mustascertainx. Itwouldbea step forwardto see LEX ask
questionssuchas, "Aretheintegersclosedundermultiplication?" whileworkingon
an assimilation task.
Calculation ofan Initial Bias. There is evidence that a strong bias can be deduced
when concept learning involves learning constrained domains for problem-solving
operators,aswithLEX. Amechanicalprocedureforcalculatinganinitialbiaswould
constitute an advance in machine learning. In section 5.3.3.9 a procedure was suggested thatexhaustively applies constraintback-propagationtoall operatorsequences
thatleadtoaproblemsolutionwithinanoperatorsequenceofsomespecifiedlength.
Solution paths will be pruned whenever the constrained domain of an operator
sequence becomes empty.
Goal-Free versus Goal-Sensitive Methods. There is evidence that goal-sensitive
methods are stronger than goal-free methods. The principal advantage of goalsensitivemethods isthatthey attempttouseallavailable information instead ofonly
selected portions. The two procedures ofSTABB- least disjunction and constraint
back-propagation-contrast sharply in this regard. The least disjunction procedure
examines only the training instances and the current description language when it
searches for a better bias. In contrast, the constraint back-propagation procedure
takesadvantageoftheadditional knowledgethatthetraining instancesarepartofan
operator sequence that LEX's critic identified as good. Because it uses that knowledge ofthe learning context, the constraint back-propagation procedure is sensitive
tothelearninggoalathand. Asaresult, theconstraintback-propagationprocedure is
able to deduce constrained domains of applicability, the original objective of the
learning process. Thus learning programs may be able to depend less on empirical
induction via observation oftraining instances and more on analytic deduction via
observationofsolutionsequences. Whateverthe learningcontext-whether learning
thattakesplace inaproblem-solvingdomainorlearningthat simply improvesclassificationskills-aprocedureforshiftingbiasshouldmakeuseofthebest information
available.
--- PAGE 161 ---
UTGOFF 147
ACKNOWLEDGMENTS
The work reported here comes from the author's Ph.D. research conducted in
the Computer Science Department at Rutgers University. This work was supported
by National Science Foundation Grant No. GMCS80-08889, National Institute of
Health Grant No. RR-64309, Rutgers University Laboratory for Computer Science
Research, and Siemens Research and Technology Laboratories.
For their helpful input on this and related work, the author thanks Tom
Mitchell, Ran Banerji, Saul Amarel, N. S. Sridharan, Bob Smith, Rich Keller,
Donna Nagel, Pat Schooley, Tom Dietterich, Jaime Carbonell, Ryszard Michalski,
SmadarKedar-Cabelli, and Tony Bonner.
References
Banerji.R.B.,ArtificialIntelligence:ATheoreticalApproach,ElsevierNorth-Holland,NewYork, 1980.
Fikes,RE.,Hart,P.E.,andNilsson,N.J.,"LearningandExecutingGeneralizedRobotPlans,"Artificial
Intelligence, Vol. 3, pp. 251-88, 1972.
Keller, R. M., "Learning by Re-Expressing Concepts for Efficient Recognition," Proceedings ofthe
AAAI-83, Washington, D. C,pp. 182-86, 1983.
Kelly,V.E.,andSteinberg,L.I., "TheCritterSystem:AnalyzingDigitalCircuitsbyPropagatingBehaviorsandSpecifications,"ProceedingsoftheAAAI-82 , Pittsburgh, Pa., pp. 284-89, 1982.
Michalski, R. S., "ATheory and Methodology ofInductive Learning," ArtificialIntelligence, Vol. 20,
No. 2, pp. 118-61, 1983.
Mitchell, T M., "VersionSpaces:ACandidateEliminationApproachtoRuleLearning,"Proceedingsof
theFifthIJCAI, Cambridge, Mass., pp. 305-10, 1977.
, "Version Spaces: An Approach toConcept Learning," Ph.D. diss., Stanford University, 1978.
(Also available as Report No. STAN-CS-78-711, HPP-79-2, Department of Computer Science,
StanfordUniversity, 1978.)
, "GeneralizationasSearch,"ArtificialIntelligence, Vol. 18, No. 2, pp. 203-26, 1982.
Mitchell, T M., Utgoff, P. E., and Banerji, R. B., "Learning by Experimentation: Acquiring and
RefiningProblem-SolvingHeuristics,"inMachineLearning:AnArtificialIntelligenceApproach,
R. S. Michalski,J. G. Carbonell, andT M. Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
Mostow, D. J., "Machine Transformation ofAdvice into a Heuristic Search Procedure," in Machine
Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.),Tioga, PaloAlto, Calif., 1983.
Stallman,R. M.,andSussman,G.J., "ForwardReasoningandDependency-DirectedBacktrackingina
SystemforComputer-AidedCircuitAnalysis,"ArtificialIntelligence, Vol. 9, No. 2, pp. 135-96,
1977.
--- PAGE 162 ---
148 CHAPTER 5: SHIFTOF BIAS
Stefik, M. J., "Planning with Constraints," Ph.D. diss., Stanford University, 1980. (Also available as
ReportNo. STAN-CS-80-784, DepartmentofComputerScience, StanfordUniversity, 1980.)
Utgoff, P. E., "ShiftofBiasforInductiveConceptLearning," Ph.D. diss., RutgersUniversity, 1984.
Utgoff, P. E., and Mitchell, T. M., "Acquisition ofAppropriate Bias for Inductive Concept Learning."
ProceedingsofAAAI-82, Pittsburgh, Pa., pp. 414-17, 1982.
Vere, S. A., "Multilevel Counterfactuals forGeneralizationsofRelational Concepts and Productions,"
ArtificialIntelligence, Vol. 14, No. 2, pp. 138-64, 1980.
Waldinger, R., "Achieving Several Goals Simultaneously," inMachineIntelligence, E. W. Elcock, and
D Michie(Eds.),Wiley, NewYork, 1976.
--- PAGE 163 ---
THE EFFECT OF NOISE ON CONCEPT
LEARNING
J. Ross Quinlan
NewSouth Wales InstituteofTechnology
Abstract
Concept-learning systems are intendedtodiscovergeneralclassification rules
fromtheexaminationofagiven setofexamplesdescribedintermsofacollectionof
properties. This chapter looks at the effect ofnoise in these descriptions ofthe discovery ofclassification rules andontheiraccuracy. A modified form ofan existing
rule-building algorithm that can tolerate noisy descriptions is presented. After discussingtheresultsofthreesetsofexperiments, theauthormakesseveralconjectures
aboutthe way the classificationtask should be formulated when noise is expected.
INTRODUCTION
6.1
Michalski (1983) definesinductiveinferenceastheprocessofgoingfromspecificobservationalknowledgeaboutsomeobjectsanda(possiblynull) initial inductive hypothesis to an inductive assertion that "strongly" or "weakly" implies or
accounts for the observations. One subdomain of inductive inference is concept
learningfrom examples, inwhichthe specific knowledgeconsists ofasetofobjects
belonging to known classes. The inductive assertion is expressedas a classification
rule for assigning any object, seen or unseen, to a class. The strong implication
requirement is satisfied ifthe rule correctly classifies all known objects.
These objects are known only through theirdescriptions in terms ofa collectionofproperties, whichmightincludemeasurements, yes-no indicators, andqualitativeassessments. Forexample,thedescriptionofamanmightincludehisweight(a
measurement),whetherornotheishealthy(aBooleanvaluebasedonjudgment),and
the colorofhis eyes (a subjective division into categories).
--- PAGE 164 ---
150 CHAPTER 6: THE EFFECTOF NOISEON CONCEPT LEARNING
In real-world classification tasks, the description ofan object will often contain errors. Some sources of these errors are faulty measurement, ill-defined
threshholds (e.g.
when isaperson "tall"?), andsubjectiveinterpretationofamultitudeofinputs (e.g., whatcriteriaareusedwhendescribing aperson as "athletic"?).
Regardless ofthe source, however, theseerrors canbe expected to affectthe formation and use ofclassification rules in two ways.
First, inductiveinferencesystemsmustemploy someformofgeneralizationto
anticipate unseenobjects. This isusuallyaccomplishedby identifying subsets inthe
given setofobjectsthat sharecommonproperties. Errors in the description ofthese
objectswilltendtoconfuseanygeneralizationmechanismsofthistype. Suppose, for
example, that the subset of red-haired people is classified differently from others;
this information may be difficult to discover ifmany red-haired people have been
erroneously described as "fair" or "brunette" and vice versa. Secondly, problems
arise independently when the rule formed from the given objects is used to classify
another object. The classification rule is couched in terms ofthe description ofan
object, and ifthis description contains errors, the result given by the classification
rule forthe object in question might well be incorrect also.
Thispaperinvestigatestheeffectofdescriptionerrorsonboththeformationof
classification rules andtheiruse inclassifyingobjects. The formalismisarelatively
simple one in which objects are described in terms ofa fixed list ofproperties, or
attributes, eachwithits ownsmall, unorderedcollectionofdiscreteattributevalues.
A classificationruletakestheformofadecisiontreethatexaminesthevaluesofsome
attributes ofan object in orderto assign it to a class.
Since this work involves observing the effects ofdescription errors, a way to
control the error must be included. This has been handled here by introducing
varyinglevelsofnoiseintothecorrectdescriptionofanobject. Specifically, suppose
anoiselevel ofn percenthasbeenappliedtosomeattributeA. Wheneveranobject's
descriptionis nowgenerated, thetruevalueofA forthatobjectwillbereplacedwith
n percent probability by a randomly chosen value. This process models description
errorsastheoccasional, nonrepeatable substitutionofapossibly incorrect value for
the true attribute value. Notice that this is not the same as replacing the value ofan
attributewithadifferentvalue. Inthecaseofabinaryattribute, thelatterwouldgive
a situation in which a noise level of 100 percent in some attribute A would simply
invert the value ofA with no loss ofinformation.
The effect ofdescription errors is always to degrade the performance of the
classification rules. In general, the decision tree produced when the given set of
objects contains noise will not be the same as that produced in the noise-free case.
When the tree is used to classify an object, it could then give a wrong result either
becauseitisthewrongtreeorbecausethedescriptionoftheobject beingclassified is
incorrect. Thenoise modelallowstheerrorrate indescriptionstobe varied at will to
showtheeffectontheperformanceoftherule, measured intermsoftheproportiono\
occasions on which it gives the correct classification ofan object.
--- PAGE 165 ---
QUINLAN 151
This chapter first describes an existing top-down algorithm for constructing
classificationrulesintheformofdecisiontreesand showshow ithasbeenmodified
to deal with erroneous descriptions of this type. Three experiments are then discussed in which the level ofnoise applied to descriptions is varied and thedegradationofperformanceoftheclassificationruleisobserved. Theresultsoftheseexperiments suggest several hypotheses relevantto conceptlearning with noisy data.
6.2 ALGORITHM FOR DEVELOPING CLASSIFICATION RULES
Thebasicaimofaconcept-learningsystemistoconstructaruleforclassifying
objects from knowledge ofa training set ofobjects whose classes are given. In the
formalismusedhereallobjectsaredescribedbyafixedcollectionofattributes, each
withits ownsetofdiscretevalues. Eachobjectbelongstooneoftwoclasses, denoted
and 1, respectively, althoughtheextensionto morethantwo classes is straightforward. A ruleisexpressedasadecisiontree:eachinteriornodeconsistsofatestofan
attributewithonesubtreeforeverypossiblevalueofthatattribute, andeachleafhas
an assigned class signaling the appropriate outcome oftheclassification rule.
Ifthere are no two objects that have the same value foreach attribute and yet
belongtodifferentclasses, theattributes areadequate fortheclassificationtask. (If
this property does not hold, there are pairs ofobjects belonging to different classes
that cannot be distinguished on the basis ofthe information provided by the attributes.) Previous work (Quinlan, 1983) reported on a system called ID3 that was
developedforthiscase. ID3constructsadecisiontreeusingatop-down, divide-andconquer approach: select an attribute, divide thetraining set into subsets characterizedbythepossiblevaluesoftheattribute,andfollowthesameprocedurerecursively
with each subset until no subset contains objects from both classes. These singleclass subsets correspond then to leaves ofthe decision tree and can be labeled with
thatclass.
Themethoddependsforitspracticalityonmakingagoodchoiceofattributeto
testateachstage. ID3takesaninformation-theoreticapproachasfollows: Imaginea
setSofobjectsandlet/?betheproportionofthemthatbelongstoclass 1. A decision
treeformedfromSishypothesizedthatcorrectlyclassifiesall objects inS. Anarbitraryobjectpassedtothisdecisiontreewillbeassignedbyittoclass orclass 1, and
if5isinsomesense "typical" oftheobjectsthatmightbepassedtotherule, itseems
reasonabletoexpectthattheproportionofsuchobjectsassignedbythedecisionrule
to class 1 would also bep. From another perspective, the decision tree can be
regardedas amessagegeneratorthat says "class 1" with probabilitypand "class 0"
with probability 1 - p. The information content ofsuch a message is given by
M(S) = -p\og p - (l-p)\og (l-p).
2 2
This M(S) is a measure ofthe information that must be supplied by the hypothetical
decision tree derived from Sor, alternatively, serves as apredictorofits complexity.
--- PAGE 166 ---
152 CHAPTER 6: THE EFFECTOF NOISE ON CONCEPT LEARNING
Forexample, let,4beanattributewithnpossiblevaluesA u Ai, A* Ifthe
. . ,
decisiontree were tohaveas its rootatestonthevalue ofA, Swouldbe divided into
subsets Sj, S 2, . . . , S n sothatS,containedall objects fromSwith valueA l ofA. The
expected information requirement in this case can be expressed as
B(S,A) = sum,(W, x M(Sj))
where the weight W- t is the proportion ofthe objects in subset 5,-. The information
gained by testing attributeA is then the difference
M(S) - B(S,A),
which, asmightbeexpected, isalwaysgreaterthanorequaltozero. Astherootofthe
decision tree for set 5, ID3 chooses the attribute A that maximizes this information
gain or, equivalently, that minimizes the expected residual information B(S,A).
6.2.1 Modifications to Cope with Noise
The presence ofdescriptionerrors complicates this situation somewhat. Ifthe
descriptionsoftheobjectcontainnoise, it maybethattheattributesarenolongeradequate for the classification task. For example, iftwo objects belonging to different
classes differ only in one attribute and that attribute is affected by noise, the perceived descriptions ofthe objects may be identical. The result may be one or more
training(sub)setscontainingobjectswithidenticaldescriptionsbutdifferentclasses,
inwhichcasenoamountoftestingattributeswillproduce single-classsubsets. What
ought to be done in a situation ofthis kind?
Let us supposethat wedecidetoassociateclass cwith this leaf. There are two
valuesforcthatsuggestthemselves. Wecouldgeneralizeournotionofclasstoacontinuous value lying between and 1; a class of0.7, say, would be interpreted as
meaning, "with probability0.7, objectscorrespondingtothis leafbelong toclass 1."
Each object at the leafwould then have an associated classification error given by
ifthe object is really class 0: c 0;
ifthe object is really class 1: 1 c.
So, ifthesubsetofobjectsattheleafcontainedn fromclass andn fromclass 1, the
obvious choice for c would be
C = W,/(/lo + /!|).
It is straightforward to verify that thischoice forc minimizes the sum ofthe squares
oftheclassificationerrorsofallobjectsattheleaf. Thealternativechoice forcwould
betoadopt a voting model, assigning all objects to the class that has the most representatives at the leaf. In this case
if//,, is greater than //,: c = 0;
otherwise: c 1.
--- PAGE 167 ---
QUINLAN 153
It canbe shown that this choice forc minimizes the sum ofthe classification errors
ratherthan the sum oftheir squares. Both choices forthe class cto associate with a
leafhave been tried; for later reference they are called theprobability and majority
methods, respectively.
In building a decision tree, one may electto stop testing an object's attributes
when it appears that there are no further tests relevant to its class. The generalized
procedure for building a decision tree T(S) for the set S ofobjects can be stated as
follows:
• Ifallobjectsin5belongtothesameclass, T(S)isaleaflabeledwiththatclass.
• IfM(S) = B(S,A) forall attributesA, testing furtherattributes seemsunhelpful
to determining the class ofthe objects. The decision tree T(S) is again a leaf
whose associated class is determined by either the probability or majority
method mentioned above.
• Otherwise, find an attributeA that minimizesthe residual informationB(S,A)
and set T(S) to the decision tree
J{S T(S T(S ns„)
X) 2) 3)
where T(Si) is the decision tree for the subset 5", ofSconsisting ofthe objects
that have the rth value ofthe selected attributeA.
Although itlooksreasonable,thisapproachyieldsnonsensicallylargedecision
trees. ForexampleletSbeatrainingsetcontainingobjectsofbothclasses, andletus
supposethatnofurthertestingofattributesisrelevanttoS. LetAbearandombinaryvaluedattribute. Unlesstheproportionofclass 1 objectswitheachvalueofattribute
A is exactlythe same as theproportion ofclass 1 objects in all ofS(a mostunlikely
event, statistically speaking), B(S,A) will be less than M(S), and it will appearthat
testing attributeA is worthwhile.
TheobviousexpedientofrequiringthatB(S,A)belessthanM(S) - hforsome
thresholdhdoescircumventtheproblem,buttrialsfoundthatvaluesofhsufficiently
large to preventtesting irrelevant attributes significantly degradedthe performance
ofthe procedure when the attributes were indeed adequate.
Onesolutiontothisproblemcomesfromthechi-squaretestforstochasticindependence. Consider again a set Sofobjects and an attributeA with m values. Ifthe
--- PAGE 168 ---
154 CHAPTER 6: THE EFFECTOF NOISEON CONCEPT LEARNING
values ofA forthese objects arejust noise, the values would beexpected tobe unrelatedtotheobjects'classes,andthusthevaluesofattributeAandoftheclasswouldbe
expectedtobeindependent. Theconverseisnottrue, butifthereisreasontosuspect
that class and attribute are independent, one explanation could be thatA is noise. If
oneallowsthistacitassumptionthat independence indicates noise, the development
proceeds as follows:
LetN[i,c] denotethenumberofobjectsinSwiththei thvalueofattributeAthat
belongs to class c. Ifan object's class is independent ofits value ofattributeA, the
numberN'[i,c] that we would expect to find is givenby
N'[i,c] = N[i,*] x N[*,c]/\S\
where * denotes summation over a parameter; for example, N[i,*] is
N[i,0] + N[i,l]. Then the statistic
sum,sum {(N[i,c] - N'[i,c])2IN'[i,c]}
m -
is approximately chi-square with 1 degrees offreedom. Ifthis valueexceeds a
tabulated value, the hypothesisthatan object's class andthe value ofA are independentoverS(i.e., thaiA isuselessforclassifyingS)canberejectedwithknownconfidence (Hogg and Craig, 1970, 313ff.).
This is incorporated into the tree-building procedure by requiring that no
attributeA willbeconsideredfortestingunlessonecanreject, withahighdegreeof
confidence, the hypothesis that the value ofA is independent ofthe class over the
givensetS. Oneminordifficultyisthatthechi-squaretestisunreliableforverysmall
valuesoftheexpectationsN', sothecommonpracticeofusingthetestonly whenall
values ofN' are at least4 has been followed. Forthese experiments, the chi-square
test was used with a confidence level of99 percent; its use resulted in a dramatic
reduction in the size ofthetree when noiselevels were high, with nodegradation in
the noise-free case.
6.3 CLASSIFICATION TASKS
Since this study was intended to be an empirical investigation with practical
applications, it was important to use real (as opposed to invented) classification
tasks;artificialoneswouldalmostinevitablycontainbuilt-inassumptionsthatmight
influence the results. Forthis reason, the data used in the experiments described in
the following sections weretaken without modification frompreviousworkonclassifying board positions in a small chess end game. Two domains were used. In the
first, the universe consists of551 objects described in terms ofthirty-nine binaryvalued attributes. About 74 percent ofthe objects belong toclass 1, and the smallest
known correct decision tree for the task contains 175 nodes. (As this indicates, the
classification task is indeedadifficult one.) One oftheattributes is redundant in the
sense that a correct rule can be constructed without reference to it. The second
--- PAGE 169 ---
QUINLAN 155
domainexhibits amore skewed class distribution-only about 15 percentofthe428
objects belong to class 1. There are twenty-three binary-valued attributes, none of
them redundant, and a much simpler correct decision tree ofeighty-five nodes is
known. The two classification tasks will be referred to as task 551 and task 428,
respectively.
6.4 FIRST EXPERIMENT
The first experiment investigated the effect ofnoise on building and using a
decisiontree. Ineachtrial, the wholecollectionofobjectswascorruptedtoapredetermined level and used to form a decision tree. A different corrupted copy ofall
objects was thengenerated, andeach objectwasclassifiedby thedecisiontree. The
results obtained were compared with the known classes ofthe objects, and a mean
errorwascomputedasthe sumoftheabsolutedifferencebetweenthe real andcomputedclassesexpressedas apercentage ofthetotal numberofobjects. Each experimentwasrepeatedtwentytimeswithdifferentcorruptionsofthetrainingsetandtest
cases to give a more reliable average, together with an indication ofits variance.
Theformoftheexperimentwasasfollows: Eachattributeinturnwasindividually corrupted with noiselevels 5, 10, 15, 20, 30, 40, ... , 100percent, giving inall
twelve average errors showing the effect ofnoise in that attribute alone. The same
noiselevelswerethenappliedtoallattributessimultaneously. Finally,theclassinformationinthetraining setwascorruptedtothesamelevels. Thesetrialsthusgavean
indicationofmisclassificationerrorundertwelvelevelsofnoiseforeachattributein
isolation, forall attributestogether, andfortheclass informationinthetraining set.
6.4.1 Task 551
Thesameprocedurewasfollowedwithbothclassificationtasks,butthefirstof
them will be focused on here. As would be expected, the effects ofthe noise vary
markedly from attribute to attribute. Figure 6-1 shows the degradation of performanceasnoiseisaddedtoattribute2 (themostsensitive), attribute32 (theleastsensitiveand,asithappens,theredundantattribute),andtheaverageoverthethirty-nine
attributes. This agrees with our intuition that, in classification problems, some
attributesaremoreimportantthanothers. Infact, theimportanceofanattributecan
bedefinedwithoutreferencetonoiseastheunavoidableaverageclassificationerror
that results ifthe attribute in question is deleted altogether from the data, thus preventingitsuseinthedecisiontree. Thisquantitycanbemeasuredforanytrainingset
andisnottiedtoaparticularconcept-learningapproach. Ifweremoveattribute2, for
example, 146 pairs of objects give rise to identical descriptions in terms of the
remainingattributes. Ofthesepairs, eightarebothobjectsofclass0, ninety-fourare
both objects ofclass 1, and forty-fourpairs contain one object from each class.
--- PAGE 170 ---
156 CHAPTER 6: THE EFFECTOF NOISE ON CONCEPT LEARNING
Attribute2
Attribute32
40 60
Noise(%)
Figure6-1: Classificationerrorasafunctionofnoiseforattributes2,32, andtheaverageforallattributes(firstexperiment, task551).
It is only these last pairs that cause a problem because, given the (remaining)
informationavailable, noclassification schemecouldpossiblydifferentiatebetween
them. The importance ofattribute 2 can thenbe calculated as the total error arising
from deleting that attribute; this figure will be the same whether the probability or
majority method is used. Expressed as a percentage ofthe number ofobjects, this
figure is therefore 44/551, or 8 percent. Similarly, the importance ofthe redundant
attribute 32 is zero, and the average ofall attributes is 1.0 percent. The ranking of
attributes by importance agrees well with their ranking by error with 100 percent
noise. Noticethat, infigure6-1, theerrorwith 100percentnoise forattributes2, 32,
and the average is very close to their respective importances plus 1.8 percent.
Forthecasesabovewhereoneattributeatatime wasaffectedby noise, results
usingthetwoleaf-labelingmethods(probabilityandmajority)areindistinguishable.
This can be explained by observing that, even when a single attribute is completely
corrupted, it can cause only pairs ofobjects to share a common description. In this
situation, both methods give the same expected error. When noise corrupts class
information in the training set or several attributes, however, more than two objects
can share the same description, and differences between the methods become
apparent.
Theeffectofnoiseappliedtoclassinformationwillbeconsideredfirst. Ifclass
is treated as a sort ofattribute, its importance would necessarily be 50 percent tor
everytwo-classclassificationtask. Thereason forthis isthat, iftheclass information
--- PAGE 171 ---
QUINLAN 157
in the training set is replaced by a random variable, the resulting classification rule
would also be random and so would be expected to be correct halfthe time. Figure
6-2 shows experimental agreement with this result, but it also demonstrates clearly
thatthedegradationtakesadifferentformfromthatappearinginthepreviousfigure.
Thecurveobtainedusingtheprobabilityleaf-labelingmethodisinitiallylinear, then
bulges slightly before returning to near the predicted 50 percent at very high noise
levels. The corresponding curve for the majority method is similar at low noise
levels,butitfallswellbelowtheotheratintermediatevaluesofnoisebeforerejoining
it when class information is completely random.
Figure 6-3 shows the effect ofnoise applied simultaneously to all attributes.
Notice that the average error does not tend to 50 percent. It might seem that, ifthe
descriptionofallobjectswererandom, wewouldgetthesameresultasthatproduced
by destroying class information. But in this case the training set still contains class
frequency information that will be reflected in the resulting decision tree. If the
entiretraining setweretobecomeasingleleaf, theprobability methodwouldassign
this leafa class c whose value is 410/551, or0.74, giving an expected error of
0.74 x 0.26 + 0.26 x 0.74,
which comes to 38 percent. The majority method would assign c the value 1, since
there are more class 1 objects than class objects, giving an expected error of
0.74 x + 0.26 x 1,
r/V
Probability
Method /
g 3° S^-
A/ +/^
Majority
Method
\ i i i i
20 40 60 80 100
Noise(%)
Figure6-2: Classificationerrorfrom noi^ •rormat;< (firstexperiment, t
--- PAGE 172 ---
158 CHAPTER 6: THE EFFECTOF NOISEON CONCEPT LEARNING
Noise(%)
Figure 6-3: Classification error from noise in attributes simultaneously (first experiment.
task551).
or26 percent. The measured average errors were very close tothese values. Notice
thatthetwomethodsproduce similar results initially, butthe majority method actually leads to adecline in average errorathigher noise levels.
After an initial increase, the size ofthe decision tree decreases as noise is
increased, averaging only two nodes whenthedescriptions are completely random.
This demonstrates the utility ofthe chi-square test mentioned earlier, as without it,
the average number ofnodes for decision trees obtained from random descriptions
exceeded 500 nodes, even though they gave similarerror rates.
6.4.2 Task 428
The same procedure was followed with the second classification task. The
results, summarized in figures 6-4, 6-5, and 6-6, agree with those reported above.
Adding noise to single attributes produces saturation curves similar to those
shown in figure 6-1. The most important attribute for this task is attribute 1, with
value 5.1 percent, and figure 6-4 shows the increase in classification error as noise
increasesforthisattributeandfortheaveragesingleattribute. Asbefore, it makesno
difference in this case whether the probability or majority method is used to label
leaves ofthe decision tree.
Errorintroducedbynoiseinthetrainingset'sclassinformationagainapproaches
50percent. Thistime, though, therearemoremarkedbulgesaboveandbelowthecentral lineartrend (figure6-5)duetothe probability and majority methods, respectively.
--- PAGE 173 ---
QUINLAN 159
20 40 60 100
Noise(%)
Figure6-4: Classificationerrorfromnoiseinattribute 1andaverageforallattributes(firstexperiment,
task428).
,-s 30
20 40 60 80 100
Noise(%)
Figure6-5: Classificationerrorfromnoiseinclassinformation(firstexperiment, task428).
--- PAGE 174 ---
160 CHAPTER 6: THE EFFECTOF NOISEON CONCEPT LEARNING
Figure6-6: Classificationerrorfromnoiseinallattributes(firstexperiment, task428).
Small-scaletrialsona synthetic tasksuggestthatthesebulges are linkedtothedifferencebetweentherealclassdistributionandthosesubstitutedbynoise, withlargerdiscrepancies causing largerbulges.
The curves when all attributes are corrupted simultaneously (figure 6-6)
closely match those of figure 6-3. For this task, the expected error at 100 percent
noiseusingtheprobability methodis25 percent, andthatforthe majority method is
15 percent, as confirmed by the observed values.
SECOND EXPERIMENT
6.5
Thefirstexperimenttreatednoiseasauniformcommoditythatappliedbothto
theconstruction andtothe useofclassification rules. Since noise was present in the
training set, the decision tree itselfcontained errors, and further errors were introducedasaconsequenceoffaultyattributevalueswhenthetreewasusedtoclassifyan
object. The second experiment was designed to separate the effects of noise on
forming aclassification rule and onusingthat rule. A correctdecisiontree was first
constructedusingallobjectswithnoaddednoise. (Sincethetraining setwas a noisefree one and the attributes were adequate, there were no leaves containing objects
from both classes. Consequently, both the probability and the majority methods o\
labeling leaves give the same decision tree.) This correct tree was then used to classify objects corrupted in a manner similartothe manner in which it was done in the
previous experiment.
--- PAGE 175 ---
QUINLAN 161
As before, each trial was repeatedtwenty times. The sametwelve noise levels
were applied to each attribute individually and then to all attributes together. The
measure ofdegradation was again the average classification error over all objects.
We will focus for the moment on task 551. Once more, the importance ofan
attribute was a good predictor of its sensitivity to noise, with the most important
(attribute 2) being most affected. When single attributes were corrupted by noise,
the degradation in classification performance now increased linearly. When all
attributes were corrupted simultaneously, though, a different saturation curve was
generated.
The solid curves of figure 6-7 show the consequences of applying noise to
attribute2 andtheaveragesingleattribute. Thebrokencurvesarethecorresponding
results fromthe firstexperiment in which noise was also appliedtothe training set.
Figure6-8 shows similarcurves forthe trials in which all attributes werecorrupted
simultaneously. Very surprisingly, the degradation in classification performance
whennoiseisexcludedfromthetrainingsetbecomesconsiderablyhigherinallthree
cases. The effect is not entirely uniform, though, and the maximum degradation
experienced with some less important single attributes is somewhat lower than
before.
Thesameseriesoftrialswas repeatedwithtask428. The results inall respects
mirrored those above.
Thisexperimentdemonstratesthatnoisehasadifferenteffectwhentheunderlyingclassification rule is correctthan when the rule itselfwas formed from anoisy
20 40 60 80 100
Noise(%)
Figure6-7: Classificationerrorusingcorrectdecisiontree(secondexperiment, task551).
--- PAGE 176 ---
162 CHAPTER 6: THE EFFECTOF NOISEON CONCEPT LEARNING
40 60
Noise(%)
Figure 6-8: Classification error using correct decision tree, all attributes corrupted together (second
experiment, task551).
training set. Theformofsingle-attributedegradationisdifferent(linearversus saturation), andeventhoughtheclassificationrule iscorrect, higherlevels ofnoiseproduce a significantly higheraverage misclassification rate than previously.
6.6 THIRD EXPERIMENT
Thefirsttwoexperimentsbothassumedthatthewholeuniverseofobjectswas
availableforconstructionofaclassificationrule. Thisthirdexperimentwasintended
tocheckthatthesameresultsappliedinthemoreusualcaseinwhichonlyasubsetof
the objects wasused as atraining set. The resulting decisiontree was tested on both
knownobjects (those in thetraining set) andunseen objects that were not taken into
account when the tree was formed.
Aswiththesecondexperiment, similarresultswereobtained forbothtask551
and task428, soonly the formerwill bediscussed here. Each trial was structured as
follows: A subset containing one-fifth ofthe 551 objects was selected randomly (a
differentsubseteachtime)andcorruptedby the imposition ofnoise. A decisiontree
wasformedusingthecorruptedobjectsasatrainingset. A newcorruptedcopyofall
551 objects was then generated and each object was tested using the decision tree.
Eachtrial wasagainrepeatedtwentytimes. Thecorruptionsused mimickedthoseof
thefirstexperiment: 5, 10, ... , 100percentnoiseappliedtoeachattribute inturn, all
attributes simultaneously, and to the class information in the training set.
--- PAGE 177 ---
QUINLAN 163
Even when the training set is noise-free, the fact that it is notcomplete means
thatthedecisiontreegeneratedfrom itwillnotcorrectlyclassifyallobjects. There is
thusabaseerrorbeforeanyeffectsofnoise, foundtobe 13.2 + 0.8percent(or, toput
it another way, a classification rule formed from about 20 percent ofthis universe
correctly classified about 87 percent ofthe universe).
There was aconsiderably highervariance on the results forthese experiments
and correspondingly poorer curve fits. The effect ofnoise on attribute 2 again produced the highest degradation, with apeakvalue of21 percent. The effects ofnoise
onattribute 2, all attributes together, andtheclass information appear in figure 6-9.
Generally speaking, the resultswithlow noiselevelsare displacedbythebaseerror,
but the curves are similar in form to those obtained from the first experiment. The
errordue toclass noise still approaches 50percent, andthe errordue to noise in all
attributes together again saturates toward 38 percent or 26 percent, depending on
whetherthe probability or majority leaf-labeling method is used.
6.7 FINDINGS
The resultsoftheexperiments, especiallythefirstandsecond, suggestseveral
policy-levelconjecturesconcerningtheuseofinductiveinferencemachineryinnoisy
environments. Theseareconjecturesbecausetheirapplicabilityacrossclassification
problems and across rule-inducing systems has notyet been established.
Class >f
y^ AH
s* jjS Attributes
30 /
- •
20 % * ,
Attribute2
r i*""*"
10 BaseError
1 1 l i i
20 40 60 80 100
Noise(%)
Figure6-9: Classificationerrorfromincompletetrainingsets(thirdexperiment, task551).
--- PAGE 178 ---
164 CHAPTER 6: THE EFFECTOF NOISEON CONCEPT LEARNING
• Itisimportanttoeliminatenoiseaffectingtheclass membershipoftheobjects
in the training set.
The importance ofthe class information is 50 percent, as discussed underthe
firstexperiment. This figure is much higherthan the importance ofany attribute in
either classification task studied here, where the maximum value was 8 percent for
attribute2oftask551. Thefirstexperimentconfirmedthatnoiseintheclassinformation degrades performance linearly for lower noise levels, and in particular the
average classification error should then be halfthe noise rate even when all objects
are included inthetraining set. Clearly, high levels ofnoise intheclass information
ofthe training set would generate unacceptable classification rules anyway.
• It is not worthwhile expending effort to eliminate noise from the attribute
valuesofobjectsinthetrainingsetifthereisgoingtobeasignificantamountof
noise when the induced classification rule is used in practice.
This observation comes from figures 6-7 and 6-8. Ifthe noise in the average
attributeisgreaterthanabout45percent,theperformanceofthecorrectdecisiontree
(formed fromanoise-freetraining set) fallsbelowthatofthetreeobtained whenthe
samelevelofnoisewasalsopresent inthetraining set. The sameholdstrue whenall
attributes are affected together by noise. It would seem that the tree-forming algorithmtendstoavoidusing noisy attributes, butthis feature is nullified ifthe training
setisnoisefreeandsogivesnoindicationofthenoisetobeexpectedinpractice. Even
at the lower levels ofnoise, the performance ofthe correct tree is only marginally
superiortothatofthetreeformedfromthetrainingsetwhoseattributesweresubject
to the same level ofnoise experienced during the testing phase.
• We are better offdispensing altogether with noisy, less important attributes.
Consideragain figure 6-1 forthe firstclassification task. Since attribute 32 is
redundant, its importance is zero; that is, ifthis attribute is excluded altogether, the
decision tree will still perform correctly. On the other hand, including the attribute
witheven slightlevelsofnoiseallowstheattributetobeused inthedecisiontreeand
leads to classification errors when it is used. Again, the average importance ofthe
attributes is 1 percent, sodispensing with this "average" attribute will cause only 1
percentclassificationerrors. Thefigureshowsthat fewerclassificationerrors would
be produced ifthis average attribute wereexcluded when its noise level exceeded 15
percent. Finally, performance wouldevenbe improved ifattribute 2 with its highest
importance (8 percent) wereexcluded ifthe noise level in this attribute exceeded 46
percent. Inshort, thelesstheimportanceofanattribute, themore readily it shouldbe
discarded altogether when it is corrupted by even low Levels ofnoise.
--- PAGE 179 ---
QUINLAN 165
• The payoffin noise reduction increases with the importance ofthe attribute.
Thisstatementisnotasobviousasit mayappear. Alltheexperimentshaveconfirmed a strong relation between the degradation in performance as an attribute is
corrupted and the importance of that attribute. Remember that importance of an
attribute is not a subjective term but a value that can be computed over any given
training set, noisy orotherwise. Therefore, ifadditional resources were availableto
minimize errors in describing objects, the importance model would indicate the
attribute to which the extra effort could most profitably be applied.
• The majority method ofassigning classes to leaves is preferable to the probability method.
Any concept-learning system operating in a noisy environment must face the
problem ofwhat to do when apparently indistinguishable objects in its training set
belongtodifferentclasses. Any such systemmustdecidetoassignasubcollectionof
indistinguishable objectsto someclass, andthispaperhas exploredtwo ofthe more
obvious waysto makethatassignment. The majority method, as notedearlier, leads
to the minimum number ofexpected errors on retrial when the objects at a leafare
assigned to a single class. In some situations, such as when a single attribute is
affectedby noise, the majority andprobability methodsgive identical results. However, figures 6-2, 6-3, 6-5, and 6-6 demonstrate that the majority method also gives
lowererror rates when the decisiontree is used to classify new objects (ordifferent
corruptionsofoldobjects), andthediscrepancybetweenthemethodscanbesubstantial when noise rates are high.
CONCLUSION
6.8
The workreportedhere falls intothreeparts. First, atree-building procedure
has been developed that can cope with noisy data. Second, the procedure has been
implemented(inthelanguage C)andextensivetrialscarriedoutontworealclassificationtaskswithcontrolledadditionofnoise. Finally,asaconsequenceoftheexperimental resultsobtained inthesetrials, conjectureshavebeenadvancedabouttheuse
ofnoisy attributes and the expected benefits ofreducing noise.
Thisstudyhasbeenalmostentirelyempirical. Thenextstepisobviouslytotry
tounderstand, atamoreabstractlevel, someoftheobservedphenomena, suchasthe
different degradation curve shapes for noise in attribute and class information. It
wouldalsobe interesting to find outwhethersimilar results are obtained with alternative systems forconstructing classification rules.
--- PAGE 180 ---
166 CHAPTER 6: THE EFFECTOF NOISEON CONCEPT LEARNING
References
Hogg, R. V., andCraig, A. T., IntroductiontoMathematicalStatistics, 3ded., Macmillan, New York,
1970.
Michalski,R.S., "ATheoryandMethodologyofInductiveLearning,"inMachineLearning:AnArtificial
IntelligenceApproach, R. S. Michalski,J. G. Carbonell, andT. M. Mitchell (Eds.), Tioga, Palo
Alto, Calif., 1983.
Quinlan, J. R., "Learning Efficient Classification Procedures and Their Application to Chess End
Games," inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski, J. G. Carbonell,andT. M. Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
--- PAGE 181 ---
LEARNING CONCEPTS BY ASKING
QUESTIONS
Claude Sammut
UniversityofNewSouth Wales
Ranan B. Banerji
SaintJosephs University
Abstract
Two important issues in machine learning are explored: (1) the role that
memory plays in acquiring new concepts and (2) theextentto which the learnercan
take an active part in acquiring these concepts. This chapterdescribes MARVIN, a
program that uses previously learned concepts to learn new concepts. The program
formshypothesesabouttheconceptbeinglearnedandteststhehypothesesbyasking
the trainerquestions.
LearningbeginswhenthetrainershowsMARVIN anexampleoftheconceptto
belearned. Theprogramdetermineswhichobjectsintheexamplebelongtoconcepts
storedinthememory. A descriptionofthenewconceptisformedby usingthe information obtained from the memory to generalize the description of the training
example. The generalized description is tested when the program constructs new
examples and shows them tothe trainer, asking ifthey belong tothe target concept.
INTRODUCTION
7.1
Frompersonalexperienceweknowthatit ishardtolearnnewconceptsunless
we already understand quite a lot about the subject we are studying. For example,
whenmathematicsistaughtinschool, theteacherbeginswithasimpleproblemlike,
Whatarenumbers?Later, morecomplicatedconcepts, suchasadditionandthenmultiplication, arepresented. Usuallymanyyearsofpatientaccumulationofknowledge
--- PAGE 182 ---
168 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
in a given field are required before one can fully understand the most complex conceptsinthatfield. Tobeabletolearninstages, thestudentmusthaveamemoryheor
shecanusetointegratealltheknowledgeacquired. Asnewconceptsareaddedtothe
memory, the student's vocabulary broadens, enabling him or her to describe still
more complex concepts.
Aswellasusingmemory, ahumanstudentalsoasksquestionstolearn. Oftena
teacher will show the student an example toexplain a new concept. However, a few
examplesarenotalwaysenoughtodefinetheconceptcompletely. Itiseasytogeneralizefromasmallnumberofexamplesandcomeupwithanincorrectideaoftheconceptthattheteacherwantsthestudenttolearn. Questionsoftenhelptoidentifythose
parts ofthe concept that have been misunderstood.
MARVIN isaprogramthatcanlearncomplexconceptsbyusingits memoryof
simplerconceptstohelpdescribethenewerconcepts. MARVIN alsoasksthetrainer
questions to check that its description ofthe concept is correct. A concept is informally defined as the description ofa set ofobjects in some universe. An object is
calledapositiveexampleofaconceptiftheobjectisinthe setdefinedbytheconcept
description. A negative example is an object not contained in the set.
MARVIN begins learning a new concept when the trainer shows it an object
that is a positive example of the concept to be learned (the target concept). The
description ofthe example represents a concept that contains only one object, the
example itself. Thetargetconcept is ageneralization ofthe example because it containsthatobjectandperhapsmanyotherobjects. OneconceptP isageneralizationof
anotherconcept Q if Qdescribes a subsetofP. Alternatively, we say that Q is aspecializationofP. MARVIN'Staskistodiscoverthedescriptionofthetarget. Itwilldo
thisby searchingforageneralizationofthe initialexamplethatcontainsallthepositive examples ofthe target and none ofthe negative examples.
Suppose P is a generalization of the initial example. Also suppose that P
describes a subset of the target. That is, P only contains objects that are positive
examples of the target. We say that P is a consistent generalization of the initial
example. If Q isageneralizationthatcontainsanobjectnotinthetarget, thenwewill
say that Q is an inconsistentgeneralization ofthe target. A generalization ofthe initial example is called a trialconcept.
MARVIN'ssearchforthetargetconceptisaspecific-to-generalsearch, thatis.
theprogrambeginswiththe initialexampleandcreatesanewtrialconceptby generalizing the example slightly. Ifthe generalization is consistent, then MARVIN createsanewtrialthatisageneralizationofthepreviousone. Tofindout it ageneralization is inconsistent, theprogramconstructsobjectsthatarecontained inthe trial and
shows them to the trainer. Ifthe trainer answers that one ofthe objects is not contained in the target, then the generalization is inconsistent. When this occurs,
MARVIN tries tocreate a new trial that is more specific than the previous one; that
is, ittriestocreateaconceptthatexcludesthe negativeexamplesofthe target. Ifspecialization fails to produce aconsistent trial, then a different generalization is tried.
--- PAGE 183 ---
SAMMUTANDBANERJI 169
MARVIN will continue to make as many generalizations as it can as long as it can
maintainconsistency. When ithasrunoutofgeneralizations, theconceptdescription
is stored in the program's memory.
So far we have not specified how MARVIN describes concepts. The concept
descriptionlanguageisasubsetoffirst-orderpredicatelogicverysimilartoPROLOG.
MARVIN constructsexample objectsby taking aconceptdescriptionandexecuting
itas ifit were aprogram. For MARVIN, learning aconcept is equivalentto synthesizing a logic program.
The program consists offour majorcomponents:
• A memory containing the descriptions ofconcepts that have been learned or
provided by the trainer as background knowledge
• A pattern matcher that determines ifobjects shown to the program belong to
concepts stored in the memory
• A simple theorem prover that, given a concept description, generates objects
that satisfy the conditions in the description
• A searchstrategythatdirectstheoperationoftheothercomponents inorderto
findadescriptionfortheconceptthatthetraineristryingtoteachtheprogram
The work on MARVIN has grown out ofearlierefforts by Banerji (1964) and
Cohen (1978), whoboth stressedthe importanceofalearning system'sbeingableto
extend its powerto describe concepts through learning. In the following section the
representation language usedby MARVIN is described.
7.2 REPRESENTING CONCEPTS IN FmSTORDER LOGIC
A concept is represented by a set ofHorn clauses. These are expressions in
first-order predicate calculus having the form
P(X) - Q{X) & R(X) & S(X).
That is, anobject X belongstotheconceptP ifthepredicates QandRandSaretrue.
The clause will be called a definition ofthe conceptP.
Asanexample, letusdescribetheconcepttee. Anobject X isateeifitconsists
oftwoobjects. OneobjectA may haveany shapeandliesontopofanotherobjectB,
which is a brick and which is standing up.
tee(X)
A is_part_of X&
B is_part_of X&
A is_onB &
any_shapeG4)
lyingM)
--- PAGE 184 ---
170 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
brick(B)
standing(fi).
Wecanthinkofthisclauseasdescribingthesetofallobjects X
thatsatisfytheconditions on the right-hand side ofthe arrow; any_shape is defined as
any_shape(X) brick(X).
anyjshape(X) <- wedge(X).
That is, the shape ofan object in this world may be abrick or a wedge.
MARVIN'slong-termmemory isadatabaseofsuchclauses. Theprogramalso
has ashort-term memory containing the set offacts that describe individual objects
presentedby thetraineras examples. Byfactwemeanaunitclause, that is, aclause
with no right-hand side. Thus an instancex ofthe concept tee would appear in the
short-term memory as
a is_part_ofx.
b is_part_ofx.
b is_on a.
brick(a).
wedge(b).
standing(a).
lying(6).
This describes a wedge lying on top ofabrick.
Originally generalization and specialization were defined in terms ofsets of
objects. These definitions help one to understand what those operations mean, but
they do not give one an effective way ofperforming generalizations or specializations. ToconstructadescriptionofaconceptP thatis more general than Q, the programmusttransformtheexpressioninthedescriptionlanguagethatrepresentsQinto
anotherexpressionthatrepresentsP;thatis, generalizationsandspecializationsmust
be defined as operations that manipulate the description language.
7.3 GENERALIZATIONS
& &
Suppose x is an object defined by the following predicates: Q\(x) R\{x)
S\(x). Ifthere is a concept represented by
P(X) - Q,(X)&R (X)&S (X).
P(X) - Q (X)&R] (X)&S ] (X).
2 2 2
thenx isa positive example ofthe concept P. Note that the conjunction o(predicates
describingx matches theconjunction on the right-hand side ofthe first clause. Now
supposexisanobjectappearing in some visual scene shown to MARVIN. Since v is
an example ofP, the program can try to generalize the description of the scene by
--- PAGE 185 ---
SAMMUTAND BANERJI 171
asking, "IfI replacexwith some otherexample ofP, has the scene been changed in
any essential way?" In the last example ofthe previous section, b was a wedge and
also an instance ofany_shape. The description ofxcan be generalized by replacing
wedge(b) by any_shape(b) Wedge(fr) matches the right-hand side ofone clause of
any_shape.
Whenreplacementoperationsareperformed, clausesarethoughtofas rewrite
rules. Rewriterulesaremostcommonlyusedtodefinethegrammarofalanguage. In
fact,thesetofclausesstoredin MARVIN'smemorydefinesalanguage. Sentencesin
the language describe objectsbelongingtoconcepts known to MARVIN. A distinction should be maintained between the concept description language (i.e., Horn
clauses in first-orderlogic) andthe languagethatdescribes the setofall objects recognizablebytheprogram. Thelatterisasubsetofthedescriptionlanguage. Shapiro
(1981) gives a more rigorous discussion of the relationship between these two
languages.
Replacement-such as changing wedge(b) to any_shape{b)-is the fundamental operation used ingeneralization. Whentherearealarge numberofconcepts
stored inthelong-termmemory, many such replacementsarepossible. Thedescriptionofone instanceofaconceptmaybetransformed intomanydifferentgeneralizations. Themainproblemforthelearningprogramistosearchefficientlythroughthe
space ofall such generalizations. Before a formal definition of generalization is
given, let us consider anotherexample.
Suppose we wish to describe a column of bricks as a brick standing on the
ground or a brick standing on another column. This is a recursive description consisting oftwo clauses:
column(X)
brick(X)
standing(X)
Xis.onK
ground(y).
column(X)
brick(X)
standing(X)
Xis_onF&
column(F).
It is implicit that variables such as Kare existentially quantified. Suppose thatthese
clauses as well as others form the set ofclauses in the long-term memory $ - M h
Also assume that there is a setoffacts C in short-term memory that consists ofthe
following unit clauses:
brick(a). (1)
standing(a). (2)
--- PAGE 186 ---
172 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
a is_on b. (3)
brick(Z?). (4)
standing(fr). (5)
b is_on c. (6)
ground(c). (7)
Ifb is substituted forthe variable and cforthe variable Y, then predicates (4), (5),
(6), and (7) match the right-hand side ofthe firstclause in thedefinition ofcolumn
that is, b is an example ofthe concept column. Thesubstitution forthis match is the
set ofpairs {X/a, Y/B} C can be elaboratedby the addition ofthe predicate column(b) to form C\
brick(tf). (1)
standing(fl). (2)
a is_on b. (3)
brick(fc). (4)
standing^). (5)
b is_on c. (6)
ground(c). (7)
column(^). (8)
Now predicates(1), (2), (3), and(8)matchtheright-handsideofthesecondclauseof
columnwiththesubstitution {X/a, Y/b]
Thusanewset C
canbeconstructedbythe
addition ofcolumn(a) to C
Theprocesscalledelaboration simplyentails findingoutwhich objects in the
short-term memory are examples ofconcepts that MARVIN knows. This requires
that the program match object descriptions with the right-hand sides ofclauses and
then add the left-hand side to the short-term memory.
The pattern-matching operations used in theelaboration will now be defined.
It is convenient to think ofconjunctions ofpredicates as being equivalent to sets of
predicates. Inwhatfollows, setswillalmostalwayscontainpredicatesorclauses, not
objects.
Definition 1: Givena setofclauses
~M<
S - M
k k
andasetofpredicatesC, wesaythat C isanelaborationofC ifthereisanM,and
a substitution asuch that
oM C CandC = C U o{S,}.
Theset Crepresentsthesetoffactsthatarepresentedtotheprogramby thetraineras
a description ofan instance ofthe concept to be learned. The program expands of
--- PAGE 187 ---
SAMMUTAND BANERJI 173
elaborates this definition by finding those clauses stored in memory whose righthand sides match a subset ofC. The left-hand sides ofthe clauses are added to C to
form The pattern matcher uses a simple unificationprocedure (Robinson, 1965)
toconstructsubstitutionsforthevariablesintheclause. Theeffectofthiselaboration
istoaugmentthedescriptionoftheexampleusingthe knowledgethat MARVIN has
acquired previously.
Whenpredicate (8) wasaddedto C, itenabled MARVIN tofindmorematches
in memory. In this way, sets of predicates can be elaborated repeatedly, giving a
sequence ofnew sets derived from C
Definition2:Givenaset C
wedefineasequenceC
. . .
C„suchthatC,+
isan
elaborationof Qand C„ cannotbeelaborated further. We write
AlLElaborations(Co) = C
representingthe setofall predicatesderived from C
The informationobtainedthroughelaborationisusedtoconstructhypotheses,
ortrials, fortheconceptbeinglearned. Theinitialtrial T isalwaysequivalentto C
The program may generalize 7]to a new trial T i+\ by replacing predicates in T t that
match the right-hand side ofa clause with the predicate on the left.
Againusing the columnexample, webegin with T equivalentto C The first
trial T maybeobtainedby replacingthepredicates(4), (5), (6), and(7)withcolumn
(b), giving:
brick(fl). (1)
standing(a). (2)
a is_on b. (3)
column(£>). (8)
ClearlythisisageneralizationofT
sincebmaynowbeacolumnofanyheight. The
followingdefinition showsexactlyhowwearriveatareplacementoperationsuchas
the one above.
D cl e a f u i s n e it i i n o m n e 3 m : o I r f y T t S is - at N ri T al su c c o h nc t e h p a t tw a i n t d hthe is su a bs s t u i b t s u e t t io o n f o T f t, a, an M d = ift a h M e ' re t e h xi e s n t w s e a
define a replacementoperationthatwill createanewtrial T i+1 suchthat
T i+i = Tj - a{S}.
T i+, iscalledageneralization of7|. Ifthere is morethanoneclause inthedefinition
ofthe concept S, the 7}+1 is more general than 7}, since S describes a larger set of
objects than does.
Inthe example, T, can be generalized further. All ofthe remaining predicates
matchthesecondclauseinthedescriptionofcolumn. Theycanallbereplacedbythe
singlepredicatecolumn(a). Thus, asequenceofmoregeneraltrialscanbeproduced.
Definition4: IfT , . . . T kisasequenceoftrialssuchthat T t generalizesto T i+\,
then we say T satisfies T k.
--- PAGE 188 ---
174 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
Notice that making generalizations is exactly the same as recognizing that an
objectbelongs to a concept. Although the program may construct many generalizations, only those that are less general than or the same as the target are ofinterest.
Once a generalization has been created, we must be able to test if it is consistent
or not.
Definition5: Trial T isconsistentwiththetarget T ifanyobjectthatsatisfies T
also satisfies
The next section describes what is done when an inconsistent trial is
constructed.
7.4 SPECIALIZATIONS
generalization results in the replacementofasetofpredicates in atrial by a
single, more general predicate. The replacement operation throws away some specificinformationcontainedintheset (i.e., thosepredicatesthatmatchedtherighthand side ofthe clause) in favor ofthe more general statement S (i.e., the left-hand
side ofthe clause). Ifthe new trial is consistent, then the information lost was not
important. However, ifthegeneralizationisinconsistent, thentoomuchinformation
was lost. To make T i+x more specific, MARVIN reexamines the predicates in M to
determine which ones contain essential information.
Supposewewishtoteach MARVINwhatanarchis. Assumethat, amongother
things, MARVIN has already learned the concepts any_shape and same_shape,
shown below.
any_shape(X) brick(X). (1)
any_shape(X) wedge(X).
- &
same_shape(X,Y) brick(X) brick(y). (2)
same_shape(X,y) - wedge(X) & wedge(K).
Thefollowingsetofpredicatesdescribestheexampleofanarchshownbythetrainer:
a is_part_ofx.
b is_part_ofx.
c is_part_ofx.
a is_on b.
a is_on c.
b left.ofc.
b does_not_touch c.
lying(a).
wedge(a).
standing(b).
brick(fc). (3)
--- PAGE 189 ---
SAMMUTAND BANERJI 175
standing(c).
brick(c). (4)
Thisformstheinitialtrial T Thefinaldefinitionofarch shouldincludethespecification that the two columns b and cmay have any shape as long as they are both the
same. Now let us begin generating trial descriptions of arch by performing
replacements.
1. Since b is a brick, it is an instance ofany_shape; thus one possible trial T
replaces brick(b) above with anyjshape(b), using clause (1).
2. Thisgeneralizationwouldallowbtobeawedgewhilecremainsabrick. Note
thatthegeneralizationisnottotallyincorrectsincebmaybeawedge; however,
additional information mustbeaddedtoqualifythegeneralization. This additional information is obtained by searching for another replacement that
involves at least one ofthe predicates removed from the original trial.
3. Predicates(3)and(4) matchtheright-handsideofclause (2), andpredicate(3)
alsomatchestheright-handsideofclause(1). Anewtrial T
maybeformedby
adding some_shape{b, c) to T\, This specialization creates a consistent trial.
Note that since clause (2) completely subsumes the clause (1) predicate, any_
shape(b) canbe ignored inthe newtrial. Ingeneral, this would notbethe case. Specialization can now be defined as follows:
Definition6: Suppose Tisatrial concept. Let
P = T- M 1 U a'{5'}
beageneralizationof7obtainedby replacingasubsetofpredicates,thatis, ]
with areferencetothe conceptS'. Let
T2 = T - M 2 U a2{S2
beanothergeneralizationofT, suchthat
M 1 H M 2 * 0;
then T] U T2 ismorespecificthaneither Tx or T2
The purpose ofthe specialization operation defined above is to conjoin two
generalizations ofT; that is, specialization will force the programto search forconjunctions ofgeneralizations. Without specialization, MARVIN could only discover
MARVIN
generalizations that consist of single replacements. will usually attempt
one generalization-say, and ifit finds the generalization to be inconsistent it
will look for T2 to make the trial more specific.
7.5 CONSTRUCTING EXAMPLES TO TEST HYPOTHESES
To find out ifa trial T i+] is consistent or not, MARVIN shows the trainer an
instanceof7]+-. Iftheprogramcancreateanobjectthatdoesnotbelongtothetarget
butdoesbelongto 7}+ {, then T i+\ isinconsistent. However, sincetheprogramhasnot
--- PAGE 190 ---
176 CHAPTER 7: LEARNING CONCEPTS BY ASKING QUESTIONS
yetlearnedthedescriptionofthetargetconcept, howcanweguaranteethatanobject
not in the target is shown to the trainer when the trial is inconsistent?
The set All_Elaborations(r ) contains all predicates that can be inferred from
Ifthetargetconceptcanbelearnedatall, then itsdescriptionmustbe asubsetof
All_Elaborations(r ). Any objectthatfailsto satisfy any ofthepredicates inthis set
cannotbelongtothetarget. Totestthetrial T i+U MARVIN constructs an objectthat
doesnotsatisfyanyofthepredicatesinAll_Elaborations(r
withtheexceptionthat
T i+] andanythingthatitimpliesmustbesatisfied. Thisguaranteesthat, ifatallpossible, anobjectwillbeconstructedsothatitbelongstothetrialconceptbutnottothe
target. Ifthe trial is consistent, then the object mustbelong to the target.
Definition7:LetT i+, beageneralizationof7}. Anyobjectthatsatisfies T l +, but
negates eachelementof
All_Elaborations(r,) All_Elaborations(7}+l)
iscalledacrucialobject.If7}isconsistentwiththetargetandT i+1 isnot,thenno
crucial object satisfiesthetarget.
Recall that in the example in section 7.3 the description T ofa column consisting oftwo blocks was generalized to the description T of a block resting on
anothercolumn. By Definition7, anobjectthatcanbeusedtotestthegeneralization
may be constructed by the following procedure:
1. Find all elaborations of T :
brick(tf). (1)
standing(a). (2)
a is_on b. (3)
brick(^). (4)
standing^). (5)
b is_on c. (6)
ground(c). (7)
column(^). (8)
column(a). (9)
2. Find the set ofall predicates implied by 7,, that is, all elaborations of T,
brick(a). (1)
standing(a). (2)
a is_on b. (3)
column(^). (8)
column(tf). (9)
3. Find AlLElaborations (7i) All_Elaborations(r,):
brick(fc). (4)
standing(b). (5)
--- PAGE 191 ---
SAMMUTAND BANERJI 177
b is_on c. (6)
ground(c). (7)
4. Construct an objectthat satisfies T but does not satisfy any ofthe fourpredicates(4), (5), (6), or(7). Theresultingobjectwillconsistofabrickontopofat
leasttwomorebricks. Negationofpredicates(4), (5), (6), and(7)preventsthe
bottom ofthe column from being a single brick standing on the ground.
Methods forgeneralizingand specializingconceptdescriptionhave now been
developed, as well as a way oftesting whether those descriptions are consistent or
not. Inthe following section, alloftheseelements willbebroughttogethertocreate
the complete MARVIN.
AN OVERVIEW OF THE LEARNING ALGORITHM
7.6
Let us now summarize MARVIN's learning algorithm:
To learn a new clause:
1. Receive an example ofthe concept fromthetrainer.
2. Find the list of all replacement operations obtainable from the primary
predicates.
3. Use these operations to generalize the initial trial.
4. Store the resulting concept in memory.
To find the list ofreplacementoperations:
1. Let C initially be the set ofall facts describing the training example.
2. Find the set/ofall concepts implied by C.
3. Append /to C
4. Repeatthisprocedureuntilnomoreimpliedconceptscanbefound. Theresult
is AlLElaborations(C). The list ofreplacement operations consists ofall the
clauses used to find the implied concepts in AlLElaborations(P) (see Definitions 1 and 2).
To generalize atrial T§:
1. Choose the firstconcept in the list ofreplacement operations.
2. Perform the replacementto obtain T i+\ (see Definition 3).
3. Try to qualify T i+].
4. If7}+i cannot be qualified, abandon this replacement.
5. Repeat this procedure with the next replacement operation in the list.
--- PAGE 192 ---
178 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
To qualify a trial, T i+l:
1. Construct a crucial object that satisfies T i+, but does not satisfy the set
All_Elaborations(7]) All_Elaborations(71+i)
(see Definition 7).
2. Asktrainer ifobject satisfies target. Ifit does, then T i+, has been qualified.
3. Ifitdoes not, then specialize T i+\ to a new trial 7]+24. Try to qualify T i+2.
To specialize a trial T i+X:
- M
1. Search list of replacement operations for a clause S such that
M M
fl M'+ ' =£ 0; /+ ' comes from the replacement that produced 7J+i.
2. Perform replacement on 7}+i to obtain the new trial T i+2 (see Definition 6).
To construct an example from a trial T i+l:
1. To construct an example from P& Q: ConstructP, Construct Q.
2. To construct an example from an atomic predicate P when there is a set of
clauses {P Z?,} in memory:
a. Select aB such that
# D (All_Elaborations(7;) - All_Elaborations(7;+ 1)) =
b. Construct an example using the selectedB h
3. To construct an example from an atomic predicate P when there is no clause
P - B, add P to the set ofpredicates representing the example.
7.7 A TYPICAL LEARNING TASK
In this section the steps involved in learning the description ofan arch will be
discussed. This time the description will be slightly more involved than the version
discussedin section7.4. MARVIN will learnthatanarchconsistsofanobjectofany
shapelyingontopoftwocolumnsofequal height. Thecolumnsareadjacenttoeach
other, but they must not touch.
Initiallytheprogram's memory isempty. Sinceconcepts stored inthe memory
are essential to performing generalizations, any object shown to MARVIN that it
cannotrecognizeissimplyrememberedwithoutanyattemptatgeneralizations. Ifthe
trainerstatesthatxisabrickandxisanexampleoftheconceptany_shape, MARVIN
will rememberthis fact.
When MARVIN beginsoperation, itoftenstartsby learningbasic conceptsby
rote; any_shape and anyjorientation will be learned in this way.
any_shape(X) brick(X).
any_shape(X) wedge(X).
--- PAGE 193 ---
SAMMUTAND BANERJI 179
any_orientation(X) standing(X).
any_orientation(X) lying(X).
Theseconceptsmaybewrittenouttofileattheendofonelearningsessionand
reloaded at a latertime so that they need not be relearned.
Thedescriptionofarchwillcontainreferencestotwoquitecomplexconcepts,
namely, column and samejieight. A concept like arch cannot be learned unless the
otherconceptstowhich itrefersarealready inthememory. Inananalogytodescribing
somethinginEnglish, onecannotadequatelydescribeatableunlessoneknowsabout
words like leg andflat. After teaching MARVIN about shape and orientation, the
trainerpresents examples ofcolumns.
Thedetailsofthelearningsessionwillnotbediscussedhereuntiltheprogram
starts learning about arches. It is importantto note that the description ofcolumn is
recursive, that is, itrefersto itself. Becauseofthis, thetrainermusttakecare in presentingexamplesofcolumns. Thefirstexamplemustteach MARVIN aboutthenonrecursive disjunct ofthe description. That is the first clause shown below. Having
learnedthis, theprogram isthenabletorecognizetherecursivenatureofmorecomplex examples.
column(X)
brick(X)
standing^)
Xis_onF&
ground(Y).
column(X)
brick(X)
standing(X)
Xis_onr&
column(y).
Inthislearningsession, columnswillconsistonlyofbricks. Thesameconsideration
for recursive concepts applies to learning samejieight. The heights oftwo columns
may be compared by scanning down both columns to see ifthe ground is reached at
the same time.
same_height(X,y)
ground(X)
ground(K).
same_height(Xl, XI)
brick(Xl)&
standing(Xl)&
brick(X2)
standing(X2)
--- PAGE 194 ---
180 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
XI is_on Y\ &
X2 is_on Y2 &
same_height(n, Y2).
To complete MARVIN'S background knowledge so that it can learn about
arches, the trainer must also teach it the following concepts:
Xadjacenuo Y- X left.of Y
Xadjacenuo Y- Yleft_ofX.
Xmay_touch Y - X touches Y
Xmay_touch Y- X
does_not_touch Y.
Xmay_be_on Y - X is_on Y
Xmay_be_on Y- X is_not_on Y
Assumingthatall the aboveconcepts are now present in MARVIN's memory,
the trainer can begin to teach it what an arch is. The program is presented with the
description ofan instance ofarch, shown below (see figure 7-1).
A part_ofX
Bpart_ofX.
Cpart_ofX.
A is_on B.
A is_on C.
B is_onD.
is_on E.
ground(D).
ground(£).
BlefurfC.
Bdoes_not_touch
Iying(i4). Figure7-1: Anexampleoftheconceptarch.
wedge(/l).
standing(fi).
brick(fi).
standing(C).
brick(C).
Having seenanexampleoftheconcept, thepattern matcherdetermines which
objects in the example belong to concepts in the memory. This recognition process
resultsinthegenerationofallpossiblereplacements. Theseareshownbelowwiththe
predicates to be replaced on the right-hand side of the arrow and the predicate
replacing them on the left.
any_shape(fl) brick(fl). (1)
any_shape(C) brick(C). (2)
any_shape(/l) wedge(/4). (3)
--- PAGE 195 ---
SAMMUTANDBANERJI
any_orientation(5) standing(5). (4)
any_orientation(C) standing(C). (5)
any_orientation04) lying(^). (6)
column(£) - brick(fl) & standing^) & B is_on D & ground(D). (7)
column(C) - brick(C) & standing(C) & Cis_on E& ground(£). (8)
same_height(D, E) - ground(D) & ground(£). (9)
same_height(5, C) (10)
brick(£)
standing^)
brick (C)
standing(C)
B is_on D &
C £&
is_on
same_height(A E).
Badjacent.to C - B left.of C (11)
# may_touch C - Z?does_not_touch C (12)
/4 may_be_on5 - A is_on5. (13)
A may_be_on C - A is_on C. (14)
Z?may_be_on D - 5 is_onD. (15)
Cmay_be_on £ - Cis_on
£. (16)
Theprogramtakeseach replacementinturnandattemptstocreate aconsistenttrial.
The first replacement attempts to generalize the shape ofB. This results in the following trial:
A part_ofX.
flpart_ofX.
Cpart_ofX.
A is_onB.
A is_on C.
B is_on D.
Cis_on
ground(D).
ground(ZT).
left_ofC.
£does_not_touch
Iying04).
wedge(^).
standing(#).
standing(C).
brick(C).
anyjshape(fi).
--- PAGE 196 ---
182 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
Totestthistrial, theprogramasksifthefollowingfigureisanexampleofanarch(see
figure 7-2):
A part_ofX.
B part_ofX.
Cpart_ofX.
A is_on B.
A is_on C.
B is_on D.
Cis_on
ground(D)
ground(£).
left_ofC.
Bdoes_not_touch C
lying04).
Figure7-2: Anotherexampleoftheconceptarch.
wedge
(.4).
standing(#).
standing(C).
brick(C).
wedge(#).
Anewobjecthasbeencreated in whichthe shapeofBhasbeenchangedtoawedge.
The trainer responds no, indicating that this is an inconsistent trial.
Since the trial is inconsistent, anothertrial that is more specific must be constructed. This is done by finding another replacement that involves the predicate
brick(B). The program selects replacement (7), resulting in the following new trial:
A part_ofX.
£part_ofX
Cpart_ofX.
A is_on B.
A is_on C.
B is_on D.
is_on E.
ground(D)
ground(£).
B C
left_of
Bdoes_not_touch
lying(/4).
wedge(/l).
standing(C).
brick(C).
column(B).
--- PAGE 197 ---
SAMMUTANDBANERJI 183
This generates the following training example (see figure 7-3):
A part.ofX.
£part_ofX.
Cpart.ofX
A is_onB.
A is_on C.
is_on E.
ground(£).
fllefCofC.
Bdoes_not_touch
lying(^).
wedge^).
standing(C).
brick(C).
brick(£).
standing(5).
B is_on_1. Figure7-3: Acounterexampleoftheconceptarch.
brick(_l).
standing(_l).
_1 is_on _2.
ground(_2).
Since replacement (7) generalizes B to any column, MARVIN constructs a new
example inwhichB isacolumnbutnotthe sameoneasbefore. B is nowatwo-brick
column. _1 and_2arenamesgeneratedbytheprogramtostandfornewobjects. Since
theleftandrightcolumnsofthearchmustbethesameheight,thetrainerrespondsno
again. Thus the trial mustbe specialized even further. Since the right-hand sides of
replacements (7) and(10)haveanonempty intersection, replacement(10) is selected
to make the trial more specific. This new trial is as follows:
A part_ofX.
£part_ofX
Cpart_ofX
A is_onB.
A is_on C.
B C
left_of
Bdoes_not_touch
Iying04).
wedge(^).
column(fi).
same_height(£,C).
--- PAGE 198 ---
184 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
The program now constructs a new example in which both columns are two bricks
high. Thetrainer indicatesthatthistrial isconsistent, so MARVIN cannow resume
generalizing.
TheremainingquestionsaskedbyMARVIN willnotbeshownindetail. However, the program will go on totest the shape ofA, which can be generalized to any
shape. However,Acannotassumeanyorientation; it mustbelyingdown. MARVIN
willdiscoverthat B isadjacenttoCbutmustnottouch C. Finallyitwillconcludethat
the description ofan arch is as follows:
A part_ofX.
#part_ofX
Cpart_ofX.
A is_on B.
A is_on C.
column(Z?)
same_height(Z?, C).
column(C).
any_shape(/l).
lying(^).
Badjacent_to
Bdoes_not_touch
In all, eight questions are asked before MARVIN is finished. However, it
shouldbenotedthatingeneralthenumberofquestionsaskeddependsonthenumber
ofconcepts in memory that match parts ofthe example. As the size ofthe memory
increases, so will the numberofquestions.
7.8 CONCEPTS THAT MARVIN HAS LEARNED
Inthissectionsomeoftheconceptsthat MARVIN hasbeenabletolearnwillbe
described.
Sinceconceptsare representedasHornclausesinfirst-orderlogic, theycanbe
viewed as logic programs. Thus MARVIN is able to perform not only as a learning
system,butalsoasanautomaticprogrammingsystem. Theclassofprogramsthatcan
besynthesizedislimitedbytheway inwhichvariablesarecreated. Whenthetrainer
presents an example to the program, each object in the example is given a name.
When the description is generalized and turned into a concept, the object names
become variables. MARVIN has no ability to invent existentially quantified
variables other than those derived from the example, nor can it deal with universal
quantification.
--- PAGE 199 ---
SAMMUTANDBANERJI 185
Some ofthe concepts MARVIN has learned are as follows:
• Listmanipulation. A list can be represented by a recursive concept similarto
column. A column is an object with a top that is a brick and a bottom that is
anothercolumn. Similarly, alistisanobjectwithaheadofsomespecifiedtype
andatailthatisanotherlist. Thetrainercanteach MARVIN toappendlistsby
showing it examples consisting ofinput/output pairs for the concept append.
Using a PROLOG-like notation, the trainer might present an example like
append([ ], [7], [7]). Anotherexample mightappend([]], [2], [7, 2]). Ifthese
two examples were shown in that order, MARVIN would have enough informationtosynthesizebothclausesoftherecursiveconcept. Once it knowshow
to append lists, MARVIN can then learn to reverse them, again by seeing
examples ofinput/output pairs.
• Arithmeticon numbers representedasstringsofbinarydigits. A string ofbits
can be represented as a list ofobjects that can be either or 1. MARVIN can
learntocompare numbers by learning theconcept less. As examples, theprogram would be shown pairs ofnumbers in which one was less than the other.
Input/output pairs can also be presented to the program in order to teach it
about addition and other arithmetic operations.
• Sorting. Oncetheprogramknowshowtodoarithmeticandmanipulatelists, it
canlearnhowtosortlistsofnumbers. Itcanlearnasimpleinsertionsort. However, because ofthe program's inability to invent new existentially quantified
variables, learning more efficient sorting algorithms, such as quicksort, is
beyond its capabilities.
• Grammar rules. MARVIN is capable of learning to recognize sentences in
context-freegrammars. Forexample,theprogramhasbeentaughttorecognize
avery limitedsubsetofEnglish. A stringofwordscanberepresentedasalist.
Concepts such as verb, noun, and so on, are taught first, so that the program
canidentifypartsofspeech. Afterthat, MARVINcanlearntorecognizenoun
phrases and verb phrases and so onuntil the representation ofacomplete sentence has been acquired. One ofthe interesting problems encountered in the
course ofteaching such grammatical concepts to MARVIN is that concepts
suchasnounphraseandverbphrasecanrefertoeachother. Sincetheconcepts
are disjunctive, the concepts can be taught in steps. One concept is partially
learned, thenpartofthe second islearned, andthenthedescriptionofthefirst
conceptcan be completed. Sammut discusses this problem more fully (1981).
• A more difficult language recognitionproblem that MARVIN has learned to
solveisthatposedbyHayes-RothandMcDermott(1978). Thetaskwastolearn
the rules to transform a sentence in the active form, such as "The little man
--- PAGE 200 ---
186 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
sangalovely song," tothepassiveform-"Alovely song was sung by the little
man."
• Geometricalconceptsasdescribedby Michalski (1980) havealsobeenlearned.
MARVIN has proved to be capable of learning many concepts in a variety of
domains. In the next section areas ofpossible improvement will be discussed.
7.9 DETECTING ERRORS IN CONCEPT DESCRIPTIONS
Ithasbeenseenthatthetrainerhastheresponsibilityforteaching MARVIN all
thenecessary background knowledgerequiredtodescribeany newconceptthatisto
be learned. This means that the trainer mustcarefully choose the training examples
andtheorderinwhichtheyarepresented. Toseewhathappenswhenconceptsarenot
taught in the best order, let us return to the description ofcolumns.
We now wish to make the definition slightly more general by allowing cylindersaswellasbrickstomakeupacolumn, butnotwedges. Supposethatanyshapein
ourblocks world is defined as follows:
any_shape(X) brick(X).
any_shape(X) cylinder(X).
any_shape(X) wedge(X). (Rl)
If MARVIN is shown an example ofa column without first being taught to distinguish wedgesfrombricksandcylinders, itwillconstructthewrongdescriptionfora
column. Rememberthattogeneralizeadescription, theprogramreplacespredicates
thatmatchtheright-handsideofaclausewiththecorrespondingleft-handside. Ifthe
exampleofthecolumncontainedbricks,then MARVINwouldrecognizethebrickas
belongingtoanyjshapeandattempttogeneralize. Totestthegeneralization,
would
constructanotherinstanceoftheconcept. Inthiscase, theprogramcouldconstructa
column withcylinders. Thiswouldbe an instanceofthetargetconcept, eventhough
the description it has created is too general.
If MARVIN had first learned a concept such asflat_top defined as
flat_top(X) brick(X).
flaUop(X) cylinder(X).
thenthecorrectdescriptionofacolumncouldbe learned. Theproblemhere is. How
can we determine that a concept description has been learned incorrectly?
Let us suppose that we are carrying on an extended dialogue with MARVIN,
andatsomepointweseethattheprogramhasstatedthatanobject isateeeventhough
it is not. MARVIN's description oftee must be incorrect. However, supposing that
teeis nowdefinedasabricklyingonacolumn, isthebugthedescriptionoitee itself,
or is it in one ofthe concepts that it refers to, such as column? We can "debug" the
concept by using a method similar to Shapiro's backtrace (1981). To illustrate this
--- PAGE 201 ---
SAMMUTANDBANERJI 187
method, letuscontinue withtheteesandcolumns. Assumethattheconceptdescription any_shape, above, has been learned, as well as the following:
column(X) (R2)
ground(Y).
column(X) (R3)
any_shape(X)
standing^)&
Xis_on_r&
column(F).
tee{T) (R4)
Xis_part_of
Kis_part_of
brick(X)
lying(X)
Xis_onr&
column(y).
Thefirstclauseinthisversionofcolumnhasbeensimplifiedtomaketheexplanation
easier.
Nowsupposethatthefollowingobject,X, hasbeenincorrectlyrecognizedasa
tee (see figure 7-4):
A part_ofX. (1)
Bpart_ofX
(3)
A is_on B. (4)
Bis_on
C. (5)
brick(^). (6)
lying(/4). (7)
wedge(B). (8)
standing(fi). (9) Figure7-4: Anexampleoftheconcepttee.
ground(C). (10)
We willdebugtheconceptdescriptionsbytracingthroughthesteps MARVIN
took to recognize this object incorrectly as a tee. Let us first list those steps.
1. Predicate (8) matches (Rl), the third clause of any_shape. Performing a
replacement leaves us with the following:
/lpart_ofX. (1)
flpart_ofX (3)
A is_on B. (4)
B is_on C. (5)
brick(/l). (6)
--- PAGE 202 ---
188 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
lyingC4). (7)
standing(B). (9)
ground(C). (10)
any_shape(fl). (11)
2. Predicate (10) matches (R2), the first clause of column. Replacing predicate
(10) withthecorrespondingpredicate intheleft-hand sideofthatclauseleaves
the following:
ApsLVt_ofX.
(1)
£part_ofX.
(3)
A is_on B. (4)
B is_on C. (5)
brick(/l). (6)
lying04). (7)
standing^). (9)
any_shape(5). (11)
column(C). (12)
3. Predicates(5), (9), (11), and(12)match(R3),thesecondclauseofcolumn. The
replacement leaves the following:
Apart_ofX.
(1)
Bpart.ofX
(3)
A is_on B. (4)
brickG4). (6)
lying^). (7)
column(£). (13)
4. All ofthe remaining predicates match (R4), the description oftee, leaving
tee(X). (14)
Thus, the object has been incorrectly recognized as a tee.
When the trainer has told MARVIN that this derivation is incorrect, the program retraces its steps, asking the trainer to confirm that each replacement should
have taken place. The following questions correspond to each ofthe steps above.
MARVIN
1. checks the first replacement for correctness by asking the trainer.
"Is B an example ofany_shape?" Ifthe answer is yes, then the program continues. In this case, a wedge is an example ofany_shape.
2. Checking step2, MARVIN asks, "Is Can example ofa column?" Again, the
answer is yes.
3. Finally, checking the third replacement, MARVIN asks, 'is B an example o\
column?" This time, the trainer answers no. Thus the offending clause. (R3).
has been identified.
--- PAGE 203 ---
SAMMUTAND BANERJI 189
Note that the description ofany_shape is not incorrect. It is the definition of
column that is too general. The next step in debugging the concept is to identify the
predicate within the clause that is incorrect.
Since the object B was incorrectly classified as a column, MARVIN asks the
trainertochangethedescriptionofBsothatit becomesacolumn. Thetrainershould
maketheleastnumberofchangespossible. Assumethatthedescriptionischangedto
abrickstandingontheground. Now theprogrammayconsiderwhythenewobject X
is a column andB is not. The two descriptions are:
wedge(fl). brick(X).
standing^). standing(X).
B is_on C. X is_on Y.
ground(C). ground(y).
Thepropertiesthatthetwoobjectshaveincommoncanbeignored, leavingonlythe
shape. The replacement (Rl) caused wedge(B) to be generalized to any_shape{B).
Sincethis replacementresultedinamisclassification, itshouldnotbeusedinclause
(R3). MARVINcannow tellthetrainerthatithaslocatedthebugandaskthetrainer
toteachitaboutcolumnsagain, thistimenotusingany_shape. Morespecifically, the
program can ask, "Is there a distinction between brick(B) and wedge(B) that you
haven't explained yet?"
7.10 CONCLUSION
When we characterize learning as a search process, we make the following
assumptions:
1. There is a language that is used to describe concepts.
2. state in the search space is a collection ofdescriptions written in this language. Given a set ofexamples, a goal state is a set ofdescriptions such that
eachpositive example satisfies some description andno negative example satisfies any description.
3. There is a set ofgeneralization rules fortransforming states into new states.
4. A strategy for using the set of examples as a guide to choosing the useful
sequence oftransformations that leads one to the goal state.
Theproblemoflearning istosearchthroughthe space withasequenceofgeneralizations until a path to the goal is found. The search space is determined by the
language used to describe concepts. Learning systems such as LEX (Utgoff and
Mitchell, 1982) and MIS (Shapiro, 1981) have languages thatare fixed atthe startof
the learning task. System INDUCE (Michalski, 1983) can automatically construct
newterms inthelanguage, butthe rulesforconstructingthem mustbegiven. Inprogramssuchas MARVIN(Sammut, 1981) anditspredecessor, CONFUCIUS(Cohen,
1978), learning systems have been »ti fed in \hich the description language
--- PAGE 204 ---
190 CHAPTER 7: LEARNING CONCEPTS BYASKING QUESTIONS
changesdependingonthestate, thatis, onthesetofdescriptionslearned. Putanother
way, in previous workthetransformations operated on single sentences in the state,
butintheauthors' worktherulestakethewholestateintoconsideration. Thismakes
for a significant difference in the efficiency ofthe descriptions learned and, under
many circumstances, in the efficiency ofthe search process itself.
MARVIN is able easily to extend its language because it uses sets of Horn
clauses as descriptions, just as MIS does. The program is designed to emulate
teacher/student learning. Unlike LEX, it is not initially given a hierarchy ofconcepts. Therelationshipsamongconceptsarebuiltupasthetrainerpresentsnewones.
As inhumanteacher/studentinteractions, itisexpectedthat simpleconcepts will be
taughtbeforecomplexones. Forexample, additionistaughtasaconceptbeforemultiplication, sincethedescriptionofmultiplicationmayuseaddition. Thecollectionof
concepts as a whole forms a model ofthe world in which the program exists.
MARVINdoesnotpassivelyacceptdatafromthetrainer. It "performsexperiments" to test its hypotheses by constructing its own training examples. This providesthetrainerwith feedbackindicatinghowwell MARVIN has "understood" the
concept. Withimprovementsthatwillallowittodetectandcorrectmisconceptions,a
MARVIN
learning system like could find applications inthe interactive acquisition
ofknowledgeforexpertsystems. However, itsmaincontributionsthus farhavebeen
in demonstrating aprogram whose language grows in descriptive power as it learns
new concepts and in being able to use those concepts as procedures to perform
actions, that is, to build objects.
ACKNOWLEDGMENTS
ThispaperwaswrittenwhilethefirstauthorwaswiththeDepartmentofComputer Science, University of Illinois at Urbana-Champaign. We thank R. S.
Michalski for his help during the stay at Illinois. We also thank Ian Hayes for his
helpful comments on adraft ofthis paper.
References
Banerji, R. B., "A LanguagefortheDescriptionofConcepts," GeneralSystems9, 1964
Cohen,B.L., "ATheoryofStructuralConceptFormationandPatternRecognition,"Ph.D.diss.. DepartmentofComputerScience, UniversityofNewSouthWales, Sydney, Australia, 1978.
Hayes-Roth, P., and McDermott, J., "An Interference MatchingTechnique for Inducing Abstractions."
CommunicationsoftheACM, Vol. 21, pp. 401-11, May 1978.
Michalski, R. S., "Pattern RecognitionasRule-Guided Inference." IEEE Transactionson PatternAnalysisandMachineIntelligence. Vol. 2, No. 4. pp. 349-61.July 1980.
--- PAGE 205 ---
SAMMUTANDBANERJI 191
-, "ATheoryandMethodologyofInductiveLearning," inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski, J. G. Carbonell, andT. M. Mitchell (Eds.), Tioga, PaloAlto,
Calif., 1983.
Robinson, J. A., "A MachineOriented Logic Basedon the Resolution Principle,"JournaloftheACM,
Vol. 12, No. 1, pp. 23-41,January 1965.
Sammut,C.A., "LearningConceptsbyPerformingExperiments,"Ph.D.diss.,DepartmentofComputer
Science, UniversityofNewSouthWales, Sydney, Australia, 1981.
Shapiro,E. Y., "InductiveInferenceofTheoriesfromFacts,"TechnicalReportNo. 192,YaleUniversity,
1981.
Utgoff, P. E., and Mitchell, T. M., "Acquisition ofAppropriate Bias for Inductive Concept Learning,"
ProceedingsofAAAI-82, Pittsburgh, Pa., pp. 414-17, August 1982.
--- PAGE 206 ---
--- PAGE 207 ---
CONCEPT LEARNING A
RICH INPUT DOMAIN:
Generalization-Based Memory
Michael Lebowitz
Columbia University
Abstract
Automatic concept learning from large amounts of complex input data is an
important anddifficultprocess. This chapterdiscusses howtheuse ofapermanent,
generalization-basedmemorycanserveasanimportanttoolindevelopingprograms
that learn in rich input domains. The use ofGeneralization-Based Memory (GBM)
allowsprogramstodeterminewhatconceptstolearnaswellasdefinitionsoftheconcepts. The chapter presents two programs under development at Columbia that
employGBM, UNIMEM, andRESEARCHER, andexplainshowtheyperformconcept evaluation and generalization ofcomplex structural descriptions.
INTRODUCTION
8.1
Automaticconceptlearningintheformofgeneralizationhasbeen showntobe
useful in interpreting and organizing large amounts ofinformation about a domain
(Lebowitz, 1980, 1983a; Schank, 1982). Itisalsoaninterestingtaskin its ownright.
Recently this author has been concerned with the development ofnew methods of
concept formation thatemploy apermanent memory ofpreviously determinedconcepts along with the examples that led to their creation. These methods involve the
determinationofwhatconceptsto learn as well asthedefinitionsoftheconcepts. In
particular, the focushasbeenontheproblemsofconcept formation fromastreamof
inputthatiscomplexinanyofseveraldifferentways. Thischapterdetailsthisclassof
--- PAGE 208 ---
194 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
problems, presents the basic learning technique used, known as GeneralizationBased Memory (GBM), and indicates solutions to some of the specific problems
involved.
Much ofthe concept-learning research that has been done in artificial intelligence has consisted either of supplying programs with examples, and possibly
counterexamples, ofspecifiedconceptsandhavingtheseprogramsdeterminedefinitionsoftheconcepts (e.g., Winston, 1972; Mitchell 1982; Dietterichand Michalski,
1983);orofusinglargelyanalytictechniquestoclassifyinput(e.g., Michalski, 1980;
Langley, 1981). In "real-world" settings, the crucial concepts to be learned-those
that best help explain and organize information about a domain-are not presupplied; rather, it is necessary to determine these concepts from a stream ofvery
complexinputdata. Consequently, thisresearchconcentratesnotjustonmethodsfor
comparingexamplesbutalsoonmethodsfordeterminingwhatexamplestocompare,
which largely determines the concepts to create.
Taking examples from various programs he has worked on, the author looks
here at how intelligent systems could extract generalizations to the same extent as
humanlearnersfromcomplex inputstreams suchasthefollowing: "Statesthathave
largeschoolexpenditureshavehighpercapitaincomes" (frominformationaboutthe
statesoftheUnitedStates); "Alargeclassofdiskdrivesusesflexible(floppy)disks"
(from patent abstracts about disk drives); or "Terrorist attacks in Northern Ireland
are frequently carried outby the IRA" (from news stories aboutterrorism).
Generalization-Based Memory was developed for IPP, a computer program
that read, remembered, and generalized from news stories (Lebowitz, 1980, 1983a,
1983b), basedonintuitionsabouthowcomplexepisodesmightbe stored inmemory
bypeopleinamanneranalogoustoSchank's MOPs (Schank, 1980, 1982; Riesbeck,
1981) and Kolodner's E-MOPs (Kolodner, 1984). The author believes it is advantageoustousethe sametechniques inmoretraditional concept-learningenvironments
and for intelligent information systems that make use ofcomplex streams ofinput.
The presentation here of the problems of concept learning from complex input
focuses on two intelligent information systems being developed at Columbia,
UNIMEM and RESEARCHER, both ofwhich use GBM.
UNIMEM
is a program that can accept a large quantity ofrelatively unstructured facts about a domain, use generalization techniques to determine important
concepts, and then use these concepts to organize the information in a fashion that
allows further generalization and intelligent question answering. For example, if
informationaboutthestatesintheUnitedStatesisgiventosuchaprogram(adomain
used inprototypetesting), theprogrammightdeterminethatNewEnglandstates, or
UNIMEM
states with large education budgets are useful concepts. is being used to
study problems that can arise when the individual items used for learning are not
highly structured but consist simply ofa set ofdescriptive features.
The problems in forming concepts from complex input data involved in research with UNIMEM include the following: determining the impact o( domain-
--- PAGE 209 ---
LEBOWITZ 195
dependent knowledge on concept learning; categorizing numeric input information
so that generalization is possible; evaluating and refining concepts from further
examples; using concepts that very slightly contradict new input items-those like
Winston's "near misses" (1972) but that are not preidentified as such; dealing with
concepts that change over time; and answering questions based on GeneralizationBased Memory. In this chapter, the basic techniques for using GBM and for evaluUNIMEM
ating concepts in the context of are presented.
In contrast with UNIMEM, RESEARCHER (Lebowitz 1983c, 1983d) deals
withhighlystructuredphysicaldescriptionsofdevices. RESEARCHER readspatent
abstracts in natural language form andthen remembers andgeneralizes information
fromthesetexts, automaticallycreatingappropriateobjectclasses. Completeunderstanding (andgeneralization) ofpatentabstracts requires many kinds ofanalysis. To
datethefocushasbeenonthecomplexphysicaldescriptionsoftheobjectsdescribed
(e.g., partx is on top ofparty), as opposed to, forexample, functional characteristics. Inthischapter RESEARCHER isusedasacontextinwhichtodiscusstheproblems ofcomparing complex, highly structured representations.
Figure 8-1 shows some typical concepts generalized by each of the
Generalization-Based Memory programs mentioned here. The IPP and UNIMEM
generalizations were actually madeby the programs (althoughthe English was generatedby hand), andthe RESEARCHERexamples aretargetconcepts that can currently be learned from simplified input.
Theremainderofthischapterdescribeshowthisresearchrelatestootherwork
in concept formation and presents an overview of the concept-learning methods
employed in this research, concentrating on the use ofGeneralization-Based Memory. Finally, concept evaluation and generalization of complex structural descriptions, typical problems in this research, are described.
IPPConcepts:
BombingsinElSalvadorcausedamagebutdonotoftenhurtanyone.
UrbanterroristsinItalyfrequentlyusesilencer-equippedpistols.
UNIMEMConcepts:
Stateclass-Highurbanpercentage, lowminoritypercentage, moderateincome, lowtaxes,
manufacturingimportant(RI, NJ,TX, MI, FLA,OH)
Stateclass-Highvalueoffarmland, fairlyhighpopulation, manufacturing, agriculture, tourism
important(NC, ARK, TENN, MINN, WISC, VA, MO)
RESEARCHERConcepts:
Floppydiskdrive
Doubledensitydiskdrive
Fullyencloseddiskdrive
Figure8-1: Examplesof GBM concepts.
--- PAGE 210 ---
196 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
8.2 COMPLEX INPUT DOMAINS
The intelligent information systems being developed by the author basically
engageinwhatMichalskicallsmultipleconceptlearningfromobservation (descriptive generalization) (1983). These programs are given large numbers of examples
with noprespecification ofthe concepts to generalize; they acquire sets ofconcepts
bydeciding what instancestocompareandhow suchexamplesare similar. Theconceptstheyderiveareoftenoverlapping; thatis, manyconceptscandescribethe same
example.
ThetasksoftheseprogramsalsoinvolveaspectsofMichalski'sconceptacquisition. In addition to determining the properties of instances in the classes they
create, they fitobjectstothoseclasses. Thereareelements intheseprogramsbothof
observingpatterns indataandofdevelopingdiscriminantdescriptionsoftheclasses
thereby derived.
This research is characterized by several other properties that are somewhat
novel (particularly in combination) forworking systems but thatare, in the author's
opinion, crucial to the development of useful, dynamic information systems. The
first parameter characterizing this work is that it involves "pragmatic"generalizations; thatis, theconceptsdescribewhatisusually, butnotnecessarilyalways, true.
Thismeans, crucially, thatmethodsthatinvalidategeneralizedconceptsonthebasis
ofa single counterexample are not acceptable. In the same vein, it is not a requirementthateveryconceptthatcouldlegitimatelybegeneralizedbefound. Theclassof
pragmatic generalizations provides more powerand flexibility in representing what
it is possible to learn about a rich domain.
The pragmatic nature ofthese generalizations is in sharp contrast with most
otherlearning methods. Althoughtherehasbeenworkdealingwithnoisy inputdata
(e.g.
Quinlan, 1983, andtosomeextentMitchell, 1982), ithasalwaysbeenassumed
thatthe generalizations themselves perfectly described the world, even ifthey were
perhaps obscured inthe inputdata. The needtodeal withpragmatic generalizations
strongly affects all aspects ofthe author's work.
Secondly,thelearningdiscussedhereisincremental. Whensystemsarecontinuallyreceivinginput, itisnotpossibleforthemtowaitforallexamplestobeavailable
for inspectionbeforethey createconcepts. Inthis work it is requiredthat afterevery
example is processed the systems have made the best possible generalizations based
on the input that has been processed. Although it is possible to imagine the incrementalapplicationofmanyothermethods, mostotherlearningresearchhasassumed
that all the input is available to the learning process at once and that the process is
rerun from scratch if new information is added. A notable exception is the work of
Winston (1972), in which a concept is incrementally developed (although only a
single concept at a time is learned from specially selected inputs).
Finally, it is expected that these systems will ultimately deal with large numbers ofexamples. It is the ability to deal with many examples and mam concepts
--- PAGE 211 ---
LEBOWITZ 197
simultaneouslythatgiveshuman learnersthepowerthatintelligentinformation systemsshouldhave. Nomethodthatrequirescomparisonofanewinstancewithall, or
a large portion ofall, previous examples will be acceptable, for computational reasons. Evencomparison withgeneralizedconcepts mustbe done inaprincipledway.
Furthermore, these systems must deal with whatever examples they are given, not
with input that is specially prepared (as by a teacher). In addition, there are sometimescases inwhichthe individual itemstobegeneralizedarethemselvescomplex,
RESEARCHER.
as in
Although there has been learning research that involves large numbers of
examples(e.g.
Quinlan, 1979), muchofithasbeenstatisticallyoriented(seeCohen
andFeigenbaum, 1982), andlittleofithasdealtwithpragmaticgeneralizations(with
the exception ofSchank, 1982, and related research). The fact that all concepts are
notguaranteedtobelogicallycorrectturnsouttohaveamajoreffectonthelearning
process.
The author believes that methods fordealing with the type ofinput described
here will be necessary in developing systems that take full advantage ofthe large
quantitiesofcomplexinformationavailable. Oneareathathasnotbeenaddressedbut
thatwillbeimportantintheauthor'sfuturework istheuseofexplanation-basedgeneralizationofthe sortdiscussedinDeJong (1983), Mitchell (1983), Mostow (1983),
and Riesbeck (1983).
8.3 GENERALIZATION-BASED MEMORY
This section provides an overview ofthe techniques used to form concepts as
partofmaintainingaGeneralization-BasedMemory. Forclarity, thewaytheprocess
works in UNIMEM is described, but the main techniques are identical in IPP and
RESEARCHER.
ThebasicideaofGeneralization-BasedMemory isthatageneralizationsystem
beginstocreateahierarchyofconceptsthatdescribeasituationfromasmallnumber
ofexamples, andthenitrecordsinmemoryspecificitems, boththoseexamplesfrom
whichtheconceptsaregeneralizedandothers, interms ofthegeneralizedconcepts.
More specific generalizations are recorded along with specific examples under the
more general cases. GBM involves identifying and defining multiple concepts, as
opposed to maintaining a single model ofaconcept.
In orderto standardize terminology, the objects stored in memory are used to
build generalizations-that is, the input examples-are referred to as instances. In
UNIMEM UNIMEM
these are descriptions ofobjects in a domain. In an instance
is described in terms of a set offeatures (essentially property/value pairs). As
will be seen, RESEARCHER uses more complex descriptions of instances. The
combinations of generalizations, themselves sets of features, and the events and
--- PAGE 212 ---
198 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
subgeneralizations they organize are called GEN-NODEs. ] GEN-NODEs form the
basis ofGBM. The structure ofa typical GEN-NODE is shown in figure 8-2. The
manner in which GEN-NODEs are combined to form a concept hierarchy is illustrated in figure 8-3.
Generalization-BasedMemorybasicallyconsistsofoneormorehierarchiesof
GEN-NODEs thatdescribe concepts ofincreasing specificity.2 As shown in figures
8-2 and 8-3, instances and sub-GEN-NODEs are stored under each GEN-NODE
usingdiscriminationnetworks (D-NETs) (Charniaketal., 1980). (Notethatagiven
GEN-NODEcanorganizebothinstancesandmorespecificGEN-NODEs.)D-NETs
provideanefficientwaytoretrieveanyobjectstoredwithagivensetofindices. Inthe
GBM model, every feature ofan instance or sub-GEN-NODE is initially usedas an
index, resultinginshallow, bushyD-NETsthatallowretrievalofanobjectgivenany
one ofits features. The resulting plethora ofindices is pruned by ceasing to use as
indices features thatpertain to a large numberofobjects in a given D-NET.
Theuseofahierarchy ofGEN-NODEs with D-NETs as amethodofmemory
organizationallowsefficientstorageofinformation, since information inageneralizationdoes nothavetoberepeatedforeachinstancethatitdescribes. Inaddition, it
allows relevantgeneralizationsandinstances-andonlyrelevantgeneralizationsand
instances-to be found efficiently in memory during processing, allowing further
GBM
generalizations. Thispropertyof islargely independentofthe specific knowledge representation being used.
Theuseofconcepthierarchies to organize information about concepts intelligently and efficiently is not a new one. Semantic networks (Quillian, 1978), frame
systems (Minsky, 1975), and MOPs (Schank, 1980, 1982), among many other formalisms, all includethisproperty.
Aprimary
featureofthe representation language
GEN-NODE
descriptivefeatures
sub-GEN instance
D-NET D-NET
insItaInc I esunder
GEN-NODE
this
MoreI
ific
GEN-NODEs
Figure8-2: GEN-NODEstructure.
'GEN-NOTEswerecalledS-MOPsinIPP, sincetheyareinsomesensespecializedversionsofSchank's
MemoryOrganizationPackets(Schank, 1982).
technically,throughmethodsnotdescribedinthispaper,thesetofGEN-NODEsmayformnotatreebut
adirectedacyclicgraph.
--- PAGE 213 ---
LEBOWITZ
GEN-NODE
sub-GEN discriminationnet
GEN-NlODE GEINNODE GEN-NODE
instancediscriminationnets
instances
instances instances
GEN-NODE
instances
Figure8-3: SchematicstructureofGBM.
KRL (Bobrow and Winograd, 1977) is its ability to allow inheritance to be implementedeasily. WassermanandLebowitz (1983) showhowframe-basedschemescan
be applied to physical object descriptions. What is new here is the dynamically
changingnatureoftheconcepthierarchyanditsusetoguidethedevelopmentoffurtherconcepts. Onlyalimitedamountofworkhasbeendoneonautomaticallygeneralizing concept hierarchies, including Hayes (1977), Michalski and Stepp (1983),
andSammutandBanerji (1983), andthisworkhasnotdealtwithpragmaticgeneralizations orparticularly large numbers ofexamples.
Theprocessofmaintaining GBM is arelatively simple one, once the memory
organization method hasbeendefined. As each new instance is processed, the most
specific GEN-NODE that describes it is found. This is done easily and efficiently,
usingthediscriminationnetsthatindexthe GEN-NODEs inmemory, startingwitha
very general node that covers the whole range of instances in the domain. Then,
before the instance is actually indexed underthat GEN-NODE, a check is made for
instances already storedtherethathaveadditional features incommon with the new
instance; thesecanbefoundusingtheinstanceD-NET. Ifthereareenough suchfeatures(oneofmanyadjustableparametersof GBM 3 ),anewconceptisgeneralizedand
the contributing instances are indexed there. Otherwise, the new instance is simply
stored underthe existing GEN-NODE.4
TwofurtherimportantfeaturescharacterizeGBM. Sinceconceptsaregeneralized on the basis ofonly a few instances, they must be evaluated to eliminate overgeneralization (including the elimination of whole concepts). This is discussed in
section 8.4. The second feature is the use ofpredictability. Space does not permit a
discussionofpredictabilityhere (see Lebowitz, 1983a), butthebasic idea isthatthe
^Futureresearchmaylookathowtheparametersof GBM couldbeadjustedautomatically.
4The process is actually abit more complex, since a given instance can be stored in multiple spots in
memoryfortwodifferentreasons.Eitherinstancecanbeclassifiedinitiallyinseveraldifferentways,each
ofwhich would indicateaplacetostoreit, orseveral different "mostspecific" GEN-NODEs mightbe
found,eachofwhichwouldleadtotheprocessingdescribed.
--- PAGE 214 ---
200 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
presenceofsomeofthefeaturesofaconceptinaninstanceindicatestherelevanceof
the concept and that these features canbe identified quite easily using GBM.
GBM
Further details of the algorithm used to maintain are shown in figures 8-4, 8-5, and 8-6. Figure 8-4 shows thatthe additionofa new instanceto GBM
consists of finding the GEN-NODE (or GEN-NODEs) that best describes the
instance(updatingfeatureconfidencefactorsasinsection8.4), followedbyindexing
thenewinstance(whichincludesacheckfornewgeneralizations). Figure 8-5 shows
the process that searches for the GEN-NODE that best describes the new instance
(essentially a depth-first search heuristically guidedby features ofthe new instance
thathavenotbeenexplained)
andfigure 8-6showshowthe new instanceis actually
added to memory, possibly causing new concepts tobe generalized.
GBM
The use of as described in this section can successfully satisfy the
domain characteristics described in section 8.2, as follows:
1. Allconceptsgeneralizedin GBM are "pragmatic." Noconceptisremovedbya
single counterexample; instead, the process described in the next section is
used to evaluate all concepts. The generalization process is also pragmatic
because it can sometimes miss concepts that could be found by comparing
instancesthatwerestoredinwidelydifferentpartsofmemory, butthisseemsa
reasonable trade-offto avoid combinatoric numbers ofcomparisons.
2. GBM is inherently incremental. As each instance is added to GBM, the best
possible concepts that can be generalized so farare made.
3. GBM is ideal forlearning from largenumbers ofexamples. The use ofahierarchy ofconcepts that organize specific instances allows only instances that
mightleadto generalizationstobecomparedtoeach other. Relevant concepts
are easily found. It is also an efficient way to store the concepts.
GBM
The details ofupdating are further illustrated with an example in section 8.5, following a discussion ofconcept evaluation.
Receivenew inputinstance(aslistoffeatures).
Search GBM formostspecificGEN-NODEthatdescribes
instancebycallingSEARCH (root-node, inputfeatures)
(figure8-5).
Add new instancetoGBM, generalizing ifnecessary
(figure8-6).
GBM
Figure8-4: updatealgorithm.
--- PAGE 215 ---
LEBOWITZ 201
SEARCH(GEN-NODE, unexplained-features)
IncreaseconfidenceinanyfeaturesofGEN-NODE
intheunexplainedfeaturelist(note
-GEN-NODE
isguaranteedtobea "potentiallyrelevant" node
bythewaythealgorithmisstructured).
Arethereanyfeaturesin GEN-NODE yes Decrementtheconfidenceof
contradictedintheunexplained thosefeaturesandreturn
featurelist? NIL. (Iftheconfidenceofafeatureislowenough,deleteit.)
Foreachsub-GEN-NODE, sx, ofGEN-NODEwithat
leastonefeatureintheunexplainedlist(determined
byusingthesub-GEN-NODEdiscriminationnetwork),
call
SEARCH
(sx,
[unexplained-features-GEN-NODE
features]).
DoesSEARCHreturnanon-NILsetofnodesforanysx?
yes
Returntheunionofthoselists. ReturnGEN-NODE
Figure8-5: Searching GBM formostspecificGEN-NODE.
8.4 CONCEPT EVALUATION
As mentioned in the previous section, the concept-learning process described
hereinherentlyleadstoovergeneralization, particularlyinadomaininwhichthereis
a large amount of information about each instance. For this reason, each concept
learnedisevaluatedovertime. ForeachgeneralizationmadebyUNIMEM, anevaluationprocesscontinuallylooksforlaterinstancesforwhichthegeneralizationmight
be relevant. This occurs as a normal part ofthe memory search process, since the
generalizationstobe evaluated are exactly thosethat mightbeusedto storethe new
UNIMEM
instances. checks whethera relevantgeneralization isconfirmed orcontradicted by each new instance.
A new instance found by UNIMEM is considered to contradict an applicable
conceptifitpossessesapredictive feature indicatingthattheconceptis relevantand
another feature with the same property as the concept (such as the region ofa state)
--- PAGE 216 ---
202 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
UPDATE(GEN-NODE, new-instance)
Defineunexplained-featuresasthefeaturesofthenew instance
thatarenotpartofGEN-NODE(oritsparentnodes). The information
canberetainedfromSEARCH.
Collectthesetofall instancescurrently storedunderGEN-NODE
thathaveatleastoneofnew-instance'sunexplainedfeatures. (This
canbedoneusingGEN-NODE's instancediscriminationnetwork.)
Doanyoftheseinstancesshareenoughelse incommon
withthenew instancetowarrantanewgeneralization?
yes
Indexthenew instanceinGEN-NODE's
instancediscriminationnetwork,
usingalltheunexplained-features
as indices. Return.
Foreachsuchinstance, createanewGEN-NODE
withtheunexplainedfeaturessharedbythenew instance
andtheinstanceoftheGEN-NODE.
IndexthenewGEN-NODE intheGEN-NODE'ssub-GEN-NODE
discriminationnet, usingeachofitsfeaturesasan index.
2. Indexboth instancesunderthenewGEN-NODE, asabove.
De-indextheoldinstancefromtheoriginalGEN-NODE's
instancediscriminationnetwork.
Return.
Figure8-6: UpdatingGBM.
but with a different value (such as Midwest instead ofEast). Intuitively, when this
condition occurs, confidence in the concept should be reduced.
GBM
Early versions of confidence for generalizations in simply involved
addingorsubtractingpointsfromanumericconfidence level foreach GEN-NODE,
resulting in a property much like the confidence inconclusionsdiscussed in Collins
(1978) or the confidence in rule application used in some expert systems, such as
MYCIN (Shortliffe, 1978). In a domain rich in information, this technique will not
suffice, sincetherewillalmostalwaysbeextraneousinformation ineachgeneralized
--- PAGE 217 ---
LEBOWITZ 203
conceptasthe resultofinevitablecoincidencesthatwillcauseconfidence intheconcept to be undermined.
Ideally, when a generalization is disconfirmed the "bad" (overly specific)
partsshouldbethrownawayandthe "good" partskept. Theproblemthenreducesto
identifyingthecomponentsofageneralizationthatareoverlyspecific sotheycanbe
deleted, leaving intact a valid generalization. Furthermore, for this process to be
useful, it mustbedoneataminimumcost, occurringasanaturalpartofthememory
update process, one would hope, and requiring only a small amount ofextra record
keeping. The task is somewhat similar to that for which pattern recognition techniques areused (see CohenandFeigenbaum, 1982, foranAIperspectiveonpattern
recognition), but itdeals withconcrete, ifpragmatic, conceptdefinitions ratherthan
statistical representations.
The solution devised for UNIMEM is straightforward. Instead of keeping a
singleconfidencelevelaspartofeachGEN-NODE, UNIMEM tracksthenumberof
timeseach featureofaconcept is confirmedorcontradicted. Ineffect, aconfidence
level is maintained foreach feature ofeach concept ratherthan a single value foran
entire concept.
Specifically, a counter is maintained for each feature ofeach generalization,
andthesecountersareincrementedordecrementedastheirfeaturesareconfirmedor
contradicted, respectively, in asituation in whichaconcept isdeemed relevant. The
counter modification occurs as UNIMEM determines which GEN-NODEs best
describe a new instance, as explained in section 8.3. Ifa counter passes a negative
threshold(anotheradjustableparameter), thenthefeaturecanbeeliminatedfromthe
generalization, since ithasbeenwrong much moreoftenthanright. Iftoomany features of a generalization have been eliminated, the entire generalization is eliminated. Detailsofthisprocessandanexampleofitsapplicationinthedomainoffootball plays can be found in Lebowitz (1982).
When this schemewasaddedto UNIMEM, it provedquiteeffective inculling
extraneousfeaturesfromgeneralizations; ittotallydisconfirmedonlythoseconcepts
thatwerecompletelytheresultofcoincidence. Inseveraltestdomainsthisprocedure
producedgeneralizedconceptsthatmadeexcellentintuitivesense. A simpleexample
fromthedomain involvinginformationaboutstates inthe United States ispresented
here. Use ofthis domain is fully explained in the detailed example in section 8.5.5
GNDl:
INCOME RANGE INC3:4
TAXES RANGE TAX2
SCH00L-EXP RANGE SCH3:3
MINORITY-PCT RANGE MINI:
Organizing: IOWA, KANSAS, MICHIGAN, MONTANA NEBRASKA, PENNSYLVANIA, TEXAS
Figure8-7: FinalUNIMEN generalization.
5Adifferentrunoftheprogram isused fortheexamplehere.
--- PAGE 218 ---
204 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
Figure 8-7 illustrates aconcept (GND1) generalized by UNIMEM. This conceptdescribes states with moderately highpercapita income, ratherlow taxes, high
school expenditures, and fairly low minority population (the last is actually a broad
category that covers most states). This concept can be used to describe the seven
states listed.
Figure 8-8 shows how this concept was initially generalized from Iowa and
Nebraska. Notice that these states are similar in a number ofadditional ways-for
UNIMEM
example, they are both farm states-so initially generalized an overgeneral, andnotwidelyapplicable, concept. Thesefeatures, whichareextraneousin
thesensethattheyinhibitwiderapplicationoftheconcept, wereultimatelyremoved
by the evaluation process described in this section, leaving a much more useful
concept.
8.5 A UNIMEM EXAMPLE
AsafurtherillustrationofhowGBM
ismaintained, includingtheformationof
new concepts, an example is presented here that is taken from an actual run of
UNIMEM. In this run the program was provided with a numberoffacts abouteach
GBM
stateintheUnitedStates. Figure8-9showsasmallportionof afterinformation
fromforty-twostates (notincludingOregon) hadbeenaddedtomemory. (The states
were presentedto UNIMEM in random order.6
Each GEN-NODE infigure 8-9 is shown intermsofa setoffeatures. Forfeatures derived from numeric data, the third column ofeach feature (the value) indicates acategory derived fromthe numeric valueby a method described in Lebowitz
(1985). Forexample, the fourth feature of GEN-NODE GND1, taxes, has the value
TAX2:5, indicatingthatthetaxrateforthestatesdescribedbythis GEN-NODE falls
GND1:
CRIME-RATE RANGE CRI3:5
STATE-DEBT RANGE DEB2
INCOME RANGE INC3:4
TAXES RANGE TAX2
MIGRATION-NET RANGE MIG1:9
SCH00L-EXP RANGE SCH3:3
STATE REGION FARM
MINORITY-PCT RANGE MINI:
Organizing: IOWA, NEBRASKA
Figure8-8: Initialgeneralization.
6SinceUNIMEMhascertainsubjectiveaspects,inthesenseusedbyAbelson (1973)andCarbonell( NS1 ).
theconcepts formed in GBM varydependingontheorderinstancesarc added. However, theeffect docs
notseemtobestrong,andtheconceptevaluationprocessdescribedinthenextsectiontendstoleadtosun
ilar, though not necessarily identical, conceptsarisingovertime.
--- PAGE 219 ---
LEBOWITZ 205
GNDO
[ARIZONA MASSACHUSETTS NEWMEXICO SOUTHDAKOTA WESTVIRGINIA]
GND1
INDUSTRY TYPE MANUFACTURING (20)
INDUSTRY TYPE TOURISM (-2)
INDUSTRY TYPE AGRICULTURE (16)
TAXES RANGE TAX2 (14)
MINORITY RANGE MINI: (32)
STATE SIZE SIZ4:6 (deleted)
STATE REGION MT (deleted)
INDUSTRY TYPE MINING (deleted)
INDUSTRY TYPE ELECTRONICS (deleted)
GND5
INCOME RANGE INC3:4 (4)
INDUSTRY TYPE MINING (1)
SCHOOL-EXF RANGE SCH3•3 (0)
STATE SIZE SIZ4 6 (deleted)
URBAN-PCT RANGE URB6 6 (deleted)
[UTAH]
GND7
CRIME-RATE RANGE CRI5 5 (-D
STATE-DEBT RANGE DEB3 7 (1)
INDUSTRY TYPE GOVERNMENT (-1)
STATE SIZE SIZ4 6 (0)
URBAN-PCT RANGE URB6 6 (0)
[ COLORADO NEVADA]
GND13
STATE-DEBT RANGE DEB5 7 (0)
FARM-VAL RANGE FAR5 6 (0)
STATE SIZE SIZ4 6 (0)
URBAN-PCT RANGE URB6 6 (0)
[MICHIGAN MINNESOTA]
Figure8-9: A sectionofUNIMEMGBM withoutOregon.
inthe secondoffivecategories; that is, it is ratherlow. The numeric value following
UNIMEM
each featureindicates 'scurrentconfidenceinthatfeature(asdescribedin
theprevious section). These values startatzero. Thethreshold foreliminating afeaturewas - 3 forthisrun. Thefeaturesfollowedby "deleted" arenolongerpartofthe
generalizations butwereoriginally includedandthendeletedby theconceptevaluation algorithm. Listed under each GEN-NODE are the instances (states) indexed
there.
Thesectionof GBMshowninfigure 8-9includesfiveGEN-NODEs. Thetoplevelnode, GNDO, hasnofeaturesandhencedescribesallinstances. ItservestoorgaGBM
nize the hierarchy for states and index any instances not yet described by any
generalization. GND1 describes states with fairly low taxes, low minority population, andindustriesthatincludemanufacturing, tourism, andagriculture. Additional
--- PAGE 220 ---
206 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
featurespresentwhen it was created (fromIdahoand Colorado, as ithappens), have
been deletedto make the GEN-NODE more widely applicable.
GND1 organizesseveral sub-GEN-NODEs, oneofwhich, GND5, isshownin
figure 8-9. This node describes middle-income mining states with large school
expenditures. UtahisindexedunderGND5. ThisGEN-NODE inturnorganizestwo
yet-more-specific GEN-NODEs, GND7 and GND13. GND7 describes mid-sized
states with relatively highcrime rates, moderate statedebt, governmentas a significant industry, and a high proportion ofurban population. Colorado and Nevada are
indexed under it. 7 GND13 describes mid-sized states with high-valued farm property, fairly high state debt, and a high proportion of urban population. It indexes
Michigan and Minnesota. Notice how for the states at the bottom ofthe hierarchy,
such as Colorado, Nevada, Michigan, and Minnesota, none ofthe information in
GEN-NODEs GND1, GND5, andGND7 orGND13 will havetobe repeatedforthe
specific instance, since it can be inherited.
GBM
With containingtheinformationinfigure8-9, informationaboutOregon
was next added to memory. Figure 8-10 shows the first phase of this addition
*(run-state 'ore£'on)
Features: OREGON (STATE)
STATE REGION WS
POPULATION RANGE P0P5:7
URBAN-PCT RANGE URB5:6
MINORITY RANGE MIN1:2
MIGRATION-NET RANGE MIG8:9
STATE SIZE SIZ4:6
SCH00L-EXP RANGE SCH3:3
CRIME-RATE RANGE CRI4:5
STATE-DEBT RANGE DEB5:7
MILITARY-MONEY RANGE MIL4:9
INCOME RANGE INC3:4
FARM-VAL RANGE FAR4
TAXES RANGE TAX2
INDUSTRY TYPE MANUFACTURING
TYPE FORESTRY
TYPE TOURISM
TYPE FOOD-PROCESSING
TYPE AGRICULTURE
Best existing S-MOP(s)
GND5 potential remindings: UTAH
<and others>
Figure8-10: UNIMEM findingaGEN-NODEthatdescribesOregon.
7Notethatalthoughthetotalnumberofurbanresidentsinthesestatesisprobablysmall,theproportionof
suchresidents ishigh.
--- PAGE 221 ---
LEBOWITZ 207
procedure. Shown are the features given to describe Oregon. Also shown are the
results ofthe search phase, in which UNIMEM determined that GND5 (as well as
GEN-NODEs in other parts ofGBM) best described the new instance. GND5 was
selectedbecauseitcontainedatleastonefeatureofOregon(two, infact-incomeand
school expenditures), none of its features is contradicted by Oregon, and neither
GND7 nor GND13 is appropriate (GND7 conflicts in state debt and urban percentage, and GND13 conflicts in farmland value and urban percentage).
Having decided that GND5 is the GEN-NODE that currently best describes
Oregon, UNIMEM proceeds to update GBM by attempting to index Oregon under
that node. The results ofthis process are shown in figure 8-11. During the indexing
process, UNIMEM noticesthatUtah, whichisalreadyindexedunderGND5,hasthe
identical values forstate size, crime rate, and region ofthe country as does Oregon.
ThusanewGEN-NODE, GND50,canbecreatedwiththesefeatures. (Italsoinherits
all the features ofGEN-NODEs GND1 and GND5.)
Figure 8-12 shows how GBM has been changed by the addition of Oregon.
GND50, thenewGEN-NODE, hasbeenaddedunderGND5. OregonandUtahhave
both been indexed there. Note also how the confidences of features supported by
Oregonhavebeenincrementedandthosecontradictedhavebeendecremented, using
thealgorithmdescribedintheprevious section. Forexample, in GND13 confidence
instatedebtandstatesizehasincreasedandconfidenceinfarmvalueandurbanpercentage has gone down.
RESEARCHER
8.6
As mentioned earlier in this chapter, RESEARCHER (Lebowitz, 1983c,
1983d) is aprogram that reads patentabstracts and adds information from themtoa
Creating more specific STATE (GND50) than GND5 from events UTAH OREGON
with features:
STATE REGION WS
STATE SIZE SIZ4:6
CRIME-RATE RANGE CRI4:5
SCH00L-EXP RANGE SCH3:3
INCOME RANGE INC3:4
INDUSTRY TYPE MINING
MINORITY RANGE MIN1:2
TAXES RANGE TAX2
INDUSTRY TYPE MANUFACTURING
TYPE TOURISM
TYPE AGRICULTURE
<processing for other GEN-NODEs that describe Oregon
Figure8-11: UNIMEMaddingOregontoGBM.
--- PAGE 222 ---
25 7
208 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
GNDO
[ARIZONA MASSACHUSETTS NEWMEXICO SOUTHDAKOTA WESTVIRGINIA]
GND1
INDUSTRY TYPE MANUFACTURING (21)
INDUSTRY TYPE TOURISN (-D
INDUSTRY TYPE AGRICULTURE (17)
TAXES RANGE TAX2 : (15)
MINORITY RANGE MINI: (33)
STATE SIZE SIZ4:6 (deleted)
STATE REGION MT (deleted)
INDUSTRY TYPE MINING (deleted)
INDUSTRY TYPE ELECTRONICS (deleted)
GND5
INCOME RANGE INC3:4 (5)
INDUSTRY TYPE MINING (0)
SCHOOL-EXI RANGE SCH3:3 (1)
STATE SIZE SIZ4:6 (deleted)
URBAN-PCT RANGE URB6:6 (deleted)
GND7
CRIME-RATE RANGE CRI5:5 (-2)
STATE-DEBT RANGE DEB3 : (0)
INDUSTRY TYPE GOVERNMENT (-2)
STATE SIZE SIZ4:6 (1)
URBAN-PCT RANGE URB6:6 (-D
[COLORADO NEVADA]
GND13
STATE-DEBT RANGE DEB5:7 (1)
FARM-VAL RANGE FAR5:6 (-D
STATE SIZE SIZ4:6 (1)
URBAN-PCT RANGE URB6:6 (-D
[MICHIGAN MINNESOTA]
GND50
STATE REGION WS (0)
STATE SIZE SIZ4:6 (0)
CRIME-RATE RANGE CRI4:5 (0)
[OREGON UTAH]
Figure8-12: Thesamesectionof GBM withOregon.
Generalization-Based Memory so that it can effectively answer questions. This
chapterlooksonlyattheprocessoftakingrepresentationsoftwoobjects(or, equivalency, ageneralizedconceptandconcreteobject)andformingageneralizedconcept.
Therepresentationscomparedhereareframelikeandprimitivebased, concentrating
on the physical relations among the various parts of a complex object. (See Wasserman and Lebowitz, 1983, for a complete description of the representation
scheme.)
--- PAGE 223 ---
LEBOWITZ 209
In the disk drive domain, typical concepts that the generalization process
mightidentifyasbeingusefulwouldbefloppydiskdrivesanddouble-sideddisks. As
with UNIMEM, this mustbe done without specifically providing the program with
examples of these concepts. Instead, instances stored together in GeneralizationBased Memory are recognized as being similar and are generalized.
The use of GBM is morecomplexherethan in the UNIMEM. The "features"
thattwoobjectshaveincommoncanonlybedeterminedbycomparingtwocomplex
objectrepresentations. Thematchingproblemis muchthesameasthatfacedbyWinston in his blocks world learning work (1972). The problem here is in certain ways
bothmoredifficultandeasierthanWinston's. Itismoredifficultbecausemuchmore
GBM
complex representations are involved. However, the existence of an entire
ratherthanamodel ofa single concept will simplifythe matching process. Boththe
complexityofmatchingobjectdescriptionsandtheway inwhich GBM can simplify
the process are described here.
Therepresentationsfortwosimilar, slightlysimplifieddiskdrivepatents, used
to test the initial version ofRESEARCHER'S generalization module, are shown in
figure 8-13. Clearlythetwodiskdrives infigure 8-13 havemuch incommonthatcan
be the source of a new concept derived through generalization-an enclosed disk
drive. Figure 8-14 shows the concept created by RESEARCHER'S generalization
module. The processthat created this generalization, although conceptually similar
to the GBM update algorithm shown in section 8.4, differs in many details, largely
due to the impossibility ofrepresenting complex physical objects as simple sets of
features.
Theideaillustratedinfigure8-14isthat RESEARCHER findsthepartsoftwo
objects that are similar and abstracts them out into a generalized concept. In this
example, the two devices contained similar disk drives and enclosures. Each had a
coverontopofsomeotherobject. These similarities formthebasis ofageneralized
enclosed diskdrive. Onlytheadditional parts and relations ofeach instance needbe
recorded in memory along with the generalization. Currently, the generalization
enclosed-disk-drive1
rdisk-drivel -i enclosure1 v
/ on-top-of \
motor# dis | k# cover# support-member^
spindle# r/w-head#
enclosed-disk-drive2
rdisk-drive2-i enclosure2 t
/ on-top-of\
motor# dis|k# cover# base#
| |
spindle# r/w-head# / surr
b-filter# r-filter#
Figure8-13: Similardiskdrives.
--- PAGE 224 ---
210 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
enclosed-disk-drive#
rdisk-drive#-i enclosure^ ^\
/ on-top-of
motor# dis | k# cover#- < >
spin I dle# r/w- | head#
(enclosed-disk-drivel andenclosed-disk-drive2
storedasvariantsofenclosed-disk-drive#)
Figure8-14: Generalizedencloseddiskdrive.
module ofRESEARCHER, which is integrated with the parser, is able to handle a
moderatenumberofsimpleexamples, includingindexingthenewobjectsasvariants
ofexisting generalizations.
GBM
Adapting for use on complex structural descriptions has proven to be a
difficult problem, even when only the assorted relations among the objects in the
descriptions are considered. One ofthe major problems in generalizing structural
descriptions is theprocess ofmatching two representations (eitheroftwoobjects or
ofan objectandageneralizedobject), thereby determining what parts and relations
correspond. (This waspointedoutforsimplerexamples inWinston, 1972). Clearly,
ifthere are two distinct disk drive representations and one wishes to determine that
the disk mounts in them are similar, then one must determine that they should be
compared with each other. (Note that ifthe similarity is strong enough, it may be
desirabletomodifytherepresentationstopointtoasinglediskmountrepresentation
inmemory.) Sinceonepartofthedescriptionofcomplexobjectsisasetofrelations,
the relations in one object mustbe associated with those in the other.
Thematchingprocesshereisquiteadifficultone. Instructuredobjectsthatare
involved, the parts ofvery similar objects may be aggregated differently in various
descriptions. Forexample, aread/writeheadmightbedescribedasadirectpartofa
diskdrive in one patent but as part ofa read/write assembly in another. This makes
the inherent similarity difficult to identify.
At the moment, this "level problem" is handled with simple heuristics that
allow only a limited amount of "level hopping" during the comparison process (to
avoidtheneedtoconsidereverypossiblecorrespondenceamonglevels), alongwitha
bit ofcombinatoric force.
The ultimate solution to the level problem lies in more extensive use of
Generalization-Based Memory. Ifa new object can be identified as an instance ofa
generalized concept with only a few minordifferences (done with a discrimination
net-based search ofthe sortdescribed in section 8.3), then the levels ofaggregation
will be set. WhenGBM is used, onlya small numberofdifferencesbetween objects
have to be compared, rather than entire complex descriptions. This should allow
RESEARCHER to meet all the performanceconstraints (i.e., generalize pragmatically, be incremental, and handle large numbers ofobjects), even when the complex
representations needed to describe real-world objects are used.
--- PAGE 225 ---
LEBOWITZ 211
In effect, what this work involves is using the generalized descriptions that
have been created to form dynamically a canonical framework for describing new
objects. Such an approach can help solve one of the major problems of canonical
representationssystems. Suchrepresentationschemeshavemanywell-knownadvantages (see Schank, 1972, forexample), including simplifyingthe inferenceprocess.
However, it is often difficult to select the canonical primitives needed for such
schemes, andindomainsthatchangeovertime, it maybeimpossible. A dynamically
createdframeworkofthesortsuggestedherehasthepotentialtocombinetheadvantagesofsystemsbasedoncanonicalprimitiveswiththeabilitytoadapttothedomain
without the problems of initially selecting the primitives.8 A similar approach for
cognitivemodeling-typetasksistakeninSchank(1982), andtheissuesofadynamically changing canonical framework are atopic ofthis author's current research.
CONCLUSION
8.7
The work with Generalization-Based Memory described in this chapter suggests several importantmorals. The firstisthatthedevelopmentofadynamic setof
conceptsisapowerfulapproachtotakewhenlearningfromarichinputdomain.
Itis
notrealistictohopetofindthe "right" setofconceptsallatonce, soitiscrucialthat
theconceptsalready learnedbeconstantly updatedandnewonesconstantly sought.
New information can thus be taken advantage of, and changes in the domain can be
adaptedto. Furthermore, the useoflong-term memory, inthe formofGBM, allows
programstodealwithmanyconceptsatonceandstillretainefficiency. Infact, ashas
been shown, considering many concepts at once often ends up being easier than
learning them one at a time, and it certainly leads to more powerful systems. The
authorbelievesthedevelopmentof UNIMEM andRESEARCHER indicatesthatthe
idea ofGeneralization-Based Memory is a sound one and that these programs can
serve as valuable test-beds forthe pursuit ofimportant issues in concept learning.
ACKNOWLEDGMENTS
This research was supported in part by the Defense Advanced Research ProjectsAgencyunderContractN00039-84-C-0165. CommentsbyKathyMcKeownand
anonymous reviewers onanearlierdraftofthischapterwere mosthelpful. Workon
RESEARCHER and UNIMEM has been greatly advanced by graduate students at
8Althoughaninitialrepresentationfortheinstancesgiventothissystemstillhastobedevelopedforeach
domain,itisnotascrucialasinothersystems,sincemanypropertiesoftherepresentationcanchangeover
time.
--- PAGE 226 ---
212 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
Columbia, including MichelleBaker, AndreaDanyluk, TomEllman, Larry Hirsch,
Laila Moussa, Cecile Paris, KennethWasserman, and UrsulaWolz.
References
Abelson, R. P., "The Structure ofBeliefSystems," in ComputerModels ofThoughtandLanguage,
R. C. SchankandK. Colby (Eds.), Freeman, SanFrancisco, 1973.
Bobrow, D. G. , and Winograd, T., "An Overview ofKRL, A Knowledge Representation Language."
CognitiveScience, Vol. 1, No. 1, pp. 3-46, 1977.
Carbonell, J. G., Subjective Understanding: ComputerModelsofBeliefSystems , UMI Research Press,
AnnArbor, Mich., 1981.
Charniak, E., Riesbeck, C. K., and McDermott, D. V., ArtificialIntelligenceProgramming, Erlbaum.
Hillsdale, N.J. , 1980.
Cohen,P. R.,andFeigenbaum,E.A. (eds.), TheHandbookofArtificialIntelligence, Vol. 3,Kaufmann,
LosAltos,Calif., 1982.
Collins, A., "FragmentsofaTheoryofHumanPlausibleReasoning," TINLAP-2, Urbana-Champaign.
111., 1978.
DeJong,G. F, "AnApproachtoLearningfromObservation,"ProceedingsoftheInternationalMachine
Learning Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois at UrbanaChampaign,pp. 171-76,June22-24, 1983.(Anupdatedversionofthispaperappearsaschap. 19of
thisvolume.)
Dietterich,T. G. , andMichalski, R. S., "DiscoveringPatternsinSequencesofObjects," Proceedingsof
theInternationalMachineLearningWorkshop,R.S. Michalski(Ed.),AllertonHouse.University
ofIllinoisatUrbana-Champaign,pp.41-57,June22-24, 1983. (Anupdatedversionofthispaper
appearsaschap. 4ofthisvolume.)
Hayes, P.J., "OnSemanticNets,FramesandAssociations,"ProceedingsoftheFifthIJCAI. Cambridge.
Mass., pp. 99-107, 1977.
Kolodner, J. L., Retrievaland Organizational Strategies in Conceptual Memory: A Computer Model.
Erlbaum, Hillsdale, N.J., 1984.
Langley, P., "Data-drivenDiscoveryofNaturalLaws," CognitiveScience, Vol.5. No. 1,pp. 31-54. 1981.
Lebowitz, M., "GeneralizationandMemory inanIntegratedUnderstandingSystem." Teehnieal Report
186,DepartmentofComputerScience,YaleUniversity, 1980.(Ph.D.diss..YaleUniversity, 19801
, "CorrectingErroneousGeneralizations,"CognitionandBrain Theory,Vol. 5. No.4.pp. 367-81.
1982.
--- PAGE 227 ---
LEBOWITZ 213
, "GeneralizationfromNaturalLanguageText,"CognitiveScience,Vol.7,No. 1,pp. 1-40, 1983a.
, "Memory-basedParsing,"ArtificialIntelligence, Vol. 21, No. 4, pp. 363-404, 1983b.
"IntelligentInformation Systems," ProceedingsoftheSixthInternational ACMSIGIRConference, Bethesda, Md., pp. 25-30, 1983c.
, "RESEARCHER: An Overview," Proceedings ofthe Third AAA I, Washington, D.C.,
pp. 232-35, 1983d.
, "ClassifyingNumericInformationforGeneralization," CognitiveScience, 1985, inpress.
Michalski, R. S., "PatternRecognitionasRule-guidedInductiveInference," IEEETransactionsonPatternAnalysisandMachineIntelligence, Vol. 2, No.4, pp. 349-61, 1980.
, "ATheory ofMethodologyofInductiveLearning,"ArtificialIntelligence, Vol. 2., pp. 111-61,
1983.
Michalski, R. S., andStepp, R. E., "AutomatedConstructionofClassifications: Conceptual Clustering
VersusNumericalTaxonomy,"IEEETransactionsonPatternAnalysisandMachineIntelligence,
Vol. 5, No. 4, pp. 396-409, 1983.
Minsky, M., "A Framework for Representing Knowledge," in The PsychologyofComputer Vision,
P. H. Winston(Ed.), McGraw-Hill, NewYork, 1975.
Mitchell,T. M., "GeneralizationasSearch,"ArtificialIntelligence, Vol. 18,pp. 203-26, 1982.
, "Learning and Problem Solving," Proceedings ofthe Eighth IJCAI, Karlsruhe, W. Ger.,
pp. 1139-51, 1983.
Mostow, J., "Operationalizing Advice: A Problem-solving Model," Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois at
Urbana-Champaign, pp. 110-16,June22-24, 1983.
Quillian, M. R., "Semantic Memory," in Semantic Information Processing, M. Minsky (Ed.), MIT
Press, Cambridge, 1978.
Quinlan, J. R., "Induction over Large Data Bases," Technical Report HPP-79-14, Computer Science
Department, StanfordUniversity, 1979.
, "Learning from Noisy Data," Proceedings ofthe InternationalMachine Learning Workshop,
R. S. Michalski (Ed.), AllertonHouse, University ofIllinoisatUrbana-Champaign, pp. 58-64,
June22-24, 1983. (Anupdatedversionofthispaperappearsaschap. 6ofthisvolume.)
Riesbeck, C. K., "Failure-driven Reminding for Incremental Learning," Proceedings ofthe Seventh
IJCAI, Vancouver, B.C., pp. 115-20, 1981.
"KnowledgeReorganizationandReasoningStyle," Technical Report270, DepartmentofComputerScience, YaleUniversity, 1983.
--- PAGE 228 ---
214 CHAPTER 8: CONCEPT LEARNING IN A RICH INPUT DOMAIN
Sammut,C,andBanerji,R., "HierarchicalMemories:AnAidtoConceptLearning,"Proceedingsofthe
InternationalMachineLearning Workshop, R. S. Michalski(Ed.),AllertonHouse, Universityof
Illinois at Urbana-Champaign, pp. 74-80, June22-24, 1983. (An updated version ofthis paper
appearsaschap. 7ofthisvolume.)
Schank, R. C, "Conceptual Dependency: A Theory of Natural Language Understanding,"
CognitivePsychology, Vol. 3, No. 4, 1972, pp. 532-631, 1972.
, "Languageand Memory," CognitiveScience, Vol. 4, No. 3, pp. 243-84, 1980.
DynamicMemory:A TheoryofRemindingandLearningin ComputersandPeople, Cambridge
UniversityPress, Cambridge, 1982.
Shortliffe, E. H., Computer-BasedMedicalConsultation:MYCIN, Academic Press, NewYork, 1978.
Wasserman, K., and Lebowitz, M., "Representing Complex Physical Objects," Cognition and Brain
Theory, Vol. 6, No. 3, pp. 333-52, 1983.
Winston, P. H., "Learning Structural Descriptions from Examples," in The Psychology ofComputer
Vision, P. H. Winston(Ed.), McGraw-Hill, NewYork, 1972.
--- PAGE 229 ---
IMPROVING THE GENERALIZATION
STEP LEARNING
Yves Kodratoff
Jean-Gabriel Ganascia
UniversitedeParis-Sud
Abstract
This chapter discusses problems of generalizing expressions and changing
descriptionlanguage. Itconsiders amethod oflearning in which the inventive steps
areaccordedlessattentionthenthedeductive steps. Itis shownthatsomeseemingly
inductive discoveries may well be realized by adeductive system, when this system
doesanin-depthanalysis ofthepropertiesoftheproposedexamples. Theforgetting
operation, which iscentral inlearning, canbeviewedas achange ofdescription. A
good description keeps only the meaningful characteristics for the learning task.
Regardless ofthepointatwhichthechangeofdescriptiontakesplace (eitherduring
orafterthegeneralizationprocess), andchangethatleadstoforgetting somethingor
tocompletingthedescriptionhastobeprecededby ananalysisofthegeneralization
success or failure. The main idea ofthis chapter-that generalization discovers the
significantlinks intheexamplesandexpressesthemas variablebindings-is shown
to be surprisingly efficient when counterexamples are used.
INTRODUCTION
9.1
Theproblemoflearning iscentral toartificial intelligencebecause intelligent
behavior implies an ability to adapt to new situations. Our current robots need to
acquire automatic learning capacities ifthey are to work in a world where changes
may occur.
' 215
--- PAGE 230 ---
216 CHAPTER 9: IMPROVING THEGENERALIZATION STEP IN LEARNING
A description ofthe main ideas relevant to the field ofmachine learning has
beengiveninaparticularlyaccessiblearticlebyDietterichandMichalski (1981).Itis
assumed that the reader is acquainted with this article, since this chapter criticizes
and refines the generalization concept used by these authors. A comprehensive
survey ofconcept learning can also be found in Cohen and Feigenbaum (1982) and
Holte(1984).
Some ofthe ideas presented in this chapterwere initially described by HayesRoth and McDermott (1978) and Vere (1981), buthere a much more explicit version
ofthese ideas is given, and their important consequences are shown. The authors
describe the way one should deduce information from examples in order to find a
suitablegeneralization, givenasetofformulasdescribingtheexamples. Quiteoften,
the formulas must be changed significantly because although they contain useful
implicit information, it is usually difficult to deduce "all" the possible information
without theirbecoming formulas ofinfinite length. This chapter shows what information must be deduced and what must be left implicit. The essential step of the
"information adjustment" process is described in section 9.4.
As a demonstration ofthe usefulness ofthis approach, an example is given in
section 9.6 illustrating how considerable confusion in generalization formulas may
beavoidedwhensomeusefulimplicitinformationhasnotbeenforgotten. Forthis,an
examplesimilartoVere's(seesection9.6)willbegiven, andthesamedescriptionlanguage will be used. Vere was able to find a formulathat would recognize the examples and reject the counterexamples; however, the concept of "one block only on
another block" was not discovered. The method described in this chapter can discoverthisconcept. Moregenerally,theauthorsconsiderit compulsorythatageneralization algorithm be able to discover explicitly the concept that the examples are
intended to illustrate. In case offailure, the description language or the generalization algorithm must be considered faulty.
The methodology used here relies on two main ideas, an intuitive explanation
ofwhich will be presented first. The first idea relates concept discovery to variable
binding. Inasetofexamplesofaconcept, thelinksbetweenthecomponents ofeach
examplemaybepresentbysimplechance. Theroleofgeneralizationisthediscovery
ofthose links that are significant and those that are not. For example, given the
expression
(ON B A) & (ON C B) & (ON D C),
whereON isafunctionthat is notasubjectofgeneralizationandarguments A. B, C.
and D are constants, one can notice the following:
1. B and C appear in two expressions in parentheses (literals).
2. Both B and C occuronce in first position, once in second position.
3. The same argument neveroccurs twice in a literal (i.e.. there is no literal like
(ON A A), for instance).
--- PAGE 231 ---
KODRATOFFANDGANASCIA 217
4. A and D appearonly once.
5. Names ofthe arguments can be alphabetically ordered.
A generalization that takes into accountthese links is the following:
(ONx w) & (ONy x) & (ON z y) &
(DIFFERENT-FROM-EACH-OTHER wx y z) &
(ALPHABETIC-ORDERING wxyz).
Other examples will be used either for further generalization-for example,
replacing the two occurrences of y by different variables-or for further
specialization-forexample, instantiating the variablezto D inall examples.
FollowingHayes-RothandMcDermott(1978)andVere(1981),theauthorswill
state here that the information relative to the difference between two constants is
alwaysimplicitintheexample: forexample, (ON B A)reallymeans(ON B A) &
(DIFFERENT A B). It may seem all too obvious to insist on this fact, but in many
everyday situationsthe sameobject (orperson) is calledby differentnames and differentobjectsarecalledbythesamename. Itisassumedherethatinanydescription,
different objects are given different names. For instance, "my efficient secretary,
VivianeBourotte," canbedesignatedbymanysubsetsoftheabovesentence, andone
can find, at least in France, many "Pierres" who aredifferent persons.
When thesedifferentnames (constants) are generalizedtodifferentvariables,
it mustbe recalledthat some informationhasbeenlost, since/(jc,y) maybe instantiatedby/(A,B)aswell asby/(A,A), whereAandBareconstants. Thisexplains, for
instance, why Ei = (ON A B) and E 2 = (ON C D) will be generalized to
E g = (ON x y) & (DIFFERENT x, y)inordertorecallthatintheexamplesE] and
the variablesxandyare never instantiated tothe same constant.
Intheseexamples, onecouldalsomaketheassumptionthatdifferentvariables
representdifferentobjectsandcannotthereforebeinstantiatedbythesameconstant.
This would force us to specify the cases in which different variables may represent
the same object and to introduce in this case a predicate MAY-BE-THE-SAME.
Choosing DIFFERENT rather than MAY-BE-THE-SAME has no significance,
exceptthatthefirstchoiceistheonemadebythelogicianswhosaythatf(x,y)ismore
generalthan/(x,;t). Theyrecognizethat/(jc,y)hasthemeaning "/(*,}>) & (MAY-BETHE-SAMEx j)," MAY-BE-THE-SAME being implicit, as it is in this chapter.
Whathasbeendoneintheaboveexample, thatis, detectingallthelinksofone
example and afterwards checking their validity forthe other examples, is usually a
verydifficulttask. Inreality, thealgorithmdescribedheredetectslinkscommontoa
set ofexamples.
The second ideaderives fromthe firstone. Ifall links aretobedetected, then
thedroppingcondition rulemustbeusedwithextremecare, as illustratedbythefollowing example: Suppose that some examples exemplify a property P ofa concept
andsomeothersexemplifyapropertyQ. SupposethateverythingrelativetoP isleft,
--- PAGE 232 ---
218 CHAPTER 9: IMPROVINGTHE GENERALIZATION STEP IN LEARNING
butthepartswithpropertyQaredropped; thenimportantinformationabouttheconcepthas been lost. In a "real" world, suppose that a camerapictures a scene ofdifferentblocks from three different directions, but in each picture some ofthe blocks
(notnecessarilythesameones)arenotinsight. Ifthedroppingconditionruleisused,
the missing information in each picture will lead to a loss ofinformation about the
scene.
The method described here, on the other hand, proposes to look for some
stored information aboutthe scene in orderto fill upthe "holes" ofknowledge that
may be present in some pictures. The authors plan to represent all the descriptions
using the same structural pattern before starting the generalization process.
These two ideas have two main implications:
• Firstimplication. Becauseaconcept mustbedefined by the links between the
variables contained in its formulation, a formula containing a disjunction
describes two concepts rather than one, unless strong links between the variablesofthetwodisjointsarefound. A veryimportantsideeffectofavoidingthe
dropping condition rule is that no disjunctive generalization can be obtained
from the usually conjunctive form ofthe examples. This is because all examplesareputunderthe sameform (i.e., examplescontaining fewerliteralsthan
othersareextended, ifpossible, uptothe sizeofthe longerones), which isthe
form ofthe final generalized formula.
• Second implication. Although Vere (1980) and Mitchell (1983) use counterexamplesinverydifferentways, theybothusethemasaspecializationdevice.
Verespecializesanovergeneralizedformula, andMitchellavoidsovergeneralization by using counterexamples. However, the use of counterexamples
describedhereisnotspecificallyaimedatparticularization. Counterexamples
are used in orderto check the existence ofa link between some variables and
someoperators. Theselinks mustnotbepresent in a final generalization; this
fact has been called "pruning the generalization" (Kodratoff, 1983).
Counterexamples maybeusedtoadd furtherlinkstogeneralization. They are
then used forparticularization. Forinstance, supposethata setofexamples leads to
the generalization
Gl = (BLACKx) & (SPHERE v)
and that acounterexample to the set ofexamples is
CGI = (BLACK A) & (SPHERE A).
CGI isan instanceofGl by the substitution (a - A, y *- A). Inorderto avoid this
matching, it isenoughtospecifythatxandymay nothavethe same instantiation. G
is transformed into G2 - (BLACK x) & (SPHERE y) & (DIFFERENT x y).
which is more particular than Gl.
--- PAGE 233 ---
KODRATOFFAND GANASCIA 219
Ofcourse, another way totake into accountthe counterexample is to choose
G3 = (BLACK jc) & (SPHERE y) & (jc * A).
SinceG3 ismoreparticularthanG2, itcouldbepossibletosaythatone isa "better"
generalizationthantheother.
Acompletediscussionofthispointisoutsidethescope
ofthischapter,however, soletussimply(1)acknowledge itasanimportantproblem;
(2) remarkthat it comes from the factthatcounterexamples are now being used ina
way that may not be symmetrical to the way examples are used; and (3) understand
that one cannot really decide which is the best with the present information.
If (BLACK A) & (SPHERE C) is another counterexample, then G3 is the good
generalization.
Another use ofthe counterexamples is for an "overgeneralization" that must
takeplacewhen, onthecontrary, counterexamples matchorpartly matchthegeneralization, that is, when the descriptions ofpositive and negative examples intersect
significantly. Suppose, for instance, that we obtain, foragiven set ofexamples, the
generalization
GG = (EDIBLE
jc)
& (TASTY
jc)
& (TASTY
& (DIFFERENTx
y),
whichmeans, "therearetwodifferentobjects: oneisedibleandtasty;theotheroneis
tasty." Suppose also that the following negative example is given:
CGG = (EDIBLE beef) & (TASTY vegetables),
which says, "beefisedibleandvegetablesaretasty." Inthiscase, CGG canbeaparGG - -
tial instance of with the substitution {jc beef, y vegetables} (notice that
(TASTY jc) in GG cannot match (TASTY vegetables) in CGG because ofthe (DIFFERENT jc y) in GG).
This partial matching says that GG and CGG partially intersect, which indicates thatx edibility andy tastiness are not very important, but, on the contrary, jc
tastiness fully characterizes the examples as opposed to CGG. This puts some
emphasis on (TASTY jc) in GG since it explains why CGG is a counterexample
(because beefis nottasty).
The complete overgeneralization procedure is complicated and will not be
detailed here. There are cases in which such an analysis ofthe partial matchings of
the generalization ofthe examples and each counterexample may give an indication
that someparts ofthis generalization are neverused in ordertoexplain thecounterexample rejection, and therefore they have no interest.
The two main ideas presented in this chapterare as follows:
• Conceptdiscovery is discovery ofrelevant variable bindings.
• Information should be dropped from a formula only with extreme care.
Thefollowingdiscussionwill focusmainlyonthefirstidea, describing methods for
discovery ofvariablebindings. Theauthorswill only hint, in section9.6, atthekind
--- PAGE 234 ---
220 CHAPTER 9: IMPROVING THE GENERALIZATION STEP IN LEARNING
ofcarethatmustbetakenwheninformationisdropped. (Moredetailscanbefoundin
Kodratoffetal., 1984.)
9.2 CONCEPT LEARNING VERSUS THEOREM LEARNING
During generalization, one often replaces some constants by variables whose
quantification is notusually explicitlydefined. One may alsoconsiderthat any variable is universally quantified as are, for instance, PROLOG variables. The authors
maintain that this widely shared approach is not valid in some learning situations.
9.2.1 Theorem Generality
Stating that theorem Tl is more general than theorem T2 is simply a way of
saying that T2 is a logical consequence ofTl orthat T2 is proved as soon as Tl has
been proved (see Plotkin, 1970; Michalski, 1983). When theorems are considered,
the more general implies the more particular.
Example 1. LetTl be a theorem stating thatthe operation * is commutative and T2
be a theorem stating that e is aconstant that commutes with any elements:
Tl = Vjc Vy[jc*y = y*;c]
T2 = Vx [x * e = e * x]
It is clear that T2 is more particular than Tl since for a given *, the proof of Tl
implies the proof of T2. On the contrary, T2 can be true when Tl is false. For
instance, when * is function composition law and e is the identity function, Tl is
false and T2 is true.
Example2. Let Tl', T2\ and T3' be three theorems:
Tl' = Vx V y V z [x * (y * z) = (x * y) * z]
T2' = Vx V y [x * (y * e) = (x * y) * e]
T3' = Vx[x * (x * e) = (x * x) * e]
TV is more general than T2', which in turn is more general than T3'. Letx, y. z be
lists and e be the empty list ( ), and let * be defined by x * y = (APPEND x
(REVERSE y)). For instance, if x = (A B C) and y - (D E). then
jc * y = (A B C E D). One can see that Tl' is false (consider, e.g.. the
instance* = ( ),y = (A B),z = (C))andT2' istrueforanylist. SinceT2' istrue,
one knows that T3' is true.
Let us notice that T2 is obtained from Tl by substituting e fory (and deleting
V y), that T2' is obtained from Tl' by substituting e for : (and deleting V ;). and
T3' isobtained fromT2' by substitutingxfory(anddeleting V y). Thus in this case
one theorem can be obtained from another by simple substitutions.
--- PAGE 235 ---
KODRATOFFANDGANASCIA 221
9.2.2 Concept Generality
LetP(x)bealogicalformulathatcontainssomeunquantifiedvariables. Itisnot
atheorem; otherwiseall ofitsvariables wouldbequantified. Let X bethe setofthe
examples andX 2 be the set ofcounterexamples to P(x). x
X, = {alP(a) = TRUE}, X = {blP{b)= FALSE}
wherea andbareconstants. P(x) canbeviewed as a recognition function. One says
that it recognizes thexbelonging toX, and "rejects" thexbelonging to X 2.
Let us define a conceptthrough the theorem
V x[P(x) => ConceptW],
whereConceptisthenameoftheconcernedconceptandP(x) isitsrecognitionfunction . Ifwedefineaconceptasatheorem , it mustfollowtherulesseeninsection9.2 .1
and the more general concept implies the less general one (as proven in Michalski,
1983).
Let us now consider the recognition functions themselves. Suppose that P(x)
and P'(x) are two recognition functions ofthe same concept. Let
X\ = {a/P'(a) = TRUE}, X!> = {b/P'(b) = FALSE}.
Suppose that X x contains X\ ; then one says that P(x) is a more general recognition
function than P'(x). This definition does not contradict intuition, as the following
example shows.
Example. Let Fl and F2 be two scenes (see figure 9-1; spatial relationships are not
relevant) described by
Fl: (SQUAREa) & (STRIPED a) & (TRIANGLEb)
F2: (SQUARE c) & (STRIPED d) & (TRIANGLEd)
The formula
G(jc, y, z) = (SQUARE x) & (STRIPED^) & (TRIANGLE z)
- - -
is ageneralization ofFl and F2 sincethe substitution (x a, y a, z b) leads
from G to Fl, andthe substitution (x - c, y - d, z - d) leads from G to F2.
The setX, for P = G(x, y, z) is
X, = {F1,F2}
ConsidernowtheformulaP' = G(x,y, d ). ItisclearthatP' isaninstanceofP, since
one obtains P' from Pby replacing variable z ofPby d. X\ forP' is
X\ = {F2}.
Therefore, X, containsX\.
--- PAGE 236 ---
222 CHAPTER 9: IMPROVINGTHEGENERALIZATION STEP IN LEARNING
Fl: F2:
Figure9-1: SceneFl representsastripedsquareunderatriangleandsceneF2 representsastripedtriangleunderasquare.
The above (correct) definition ofgenerality for recognition functions,
"P is more general than P' iff X containsXJ,"
hasbeenthecauseofsomemisunderstandings. SinceX'1 isincludedinX,
itfollows
that P is TRUEwheneverP' is TRUE, whichrecallsthedefinitionoftheimplication.
Itcan lead to the following (wrong) assertion:
"The more particular recognition function implies the more general one."
And, when concepts are identified with their recognition functions, the following
(also wrong) assertion can result:
"The more particularconcept implies the more general one."
Thetwoaboveassertionsaresimplylanguagemisuses, sinceifxisafreevariable,the
validity ofthe formula P' => P is not defined (see, for instance, Chang and Lee,
1973).
The appendix to this chapter contains a definition ofthe i-imptication, which
correctly formalizes the link between "truth domain" inclusion and implication.
9.2.3 What Is Learned During Concept Learning
Ourdefinitionofrecognitionfunctionmay seem incompletesinceno informationaboutthevaliditydomainoftheconceptisintroduced(otherwisesomevariables
would be quantified). Let us further analyze the example in section 9.2.2.
Fl saysthatthereareastripedsquareandatriangle. TheconceptG(.v. v. z) says
that there are three objects, a triangle, a square, and a striped object. These three
objects may be different, but they also may be the same. Therefore G(.v. v. z) states
thatthesquaremaybestriped,thetrianglemaybestriped(allthisinformation iscontained in the examples and correctly generalized in G(.v, y, z)), and that the triangle
may be the square. This last bit ofinformation is. ofcourse, wrong. Looking at the
possible instance ofx, y, z, one sees thatxandy may have the same instance (in Fl)
--- PAGE 237 ---
KODRATOFFANDGANASCIA 223
and thaty andz may have the same instance (in F2), but thatx andz never have the
same instance. Therefore, a better way ofgeneralizing Fl and F2 would have been
G'(X , y, z) = (SQUAREx) & (STRIPED y) & (TRIANGLE z)
& (DIFFERENTx
z),
which implicitly says thatx and >' oryandz may be the same.
Ofcourse, G' isnotthe "best"generalization. Forinstance, onecouldalsoadd
the information that two forms are "blank," that the striped ones are convex polygons, andsoon. Nevertheless, thisexampleshowsthatG' makesthecorrectvariable
bindingonecanexpectfromthe "constantsbinding" implicitinFl andF2. Example
Fl says implicitly thatthe triangle is different fromthe striped square, and example
F2 saysimplicitlythatthesquareisdifferentfromthestripedtriangle, andthisinformation is kept in G'.
The concept G' (and, for that matter, Fl and F2) is not really of striking
interest, but for those who accepted G as a correct generalization, it shows that
finding the correct variable binding is not such a trivial task.
9.2.4 Concept Learning Is Not Enough
Onceasetofconceptsislearned, onehasobtainedrecognitionfunctionslike G
orG' abovethatcontainfreevariables. One mustnowdefinetowhatkindofobjects
theyareworthapplying. Forinstance, thevariablesin G shouldneverbeinstantiated
by Christian names. The learning process should go on by finding the validity
domains of these variables and the links among variables. But this is theorem
learning, which is beyond the scope ofthis chapter.
9.3 GENERALIZATION: A DEFINITION
This section presents a definition ofgenerality, albeit a somewhat weak one.
Theauthors' aimisnottoclaimthatitistheonlygoodonebuttoillustratethatmuch
useful information can be deduced by the use ofthis definition only.
Since this definition has been given in order to make use ofall the deductive
conclusions that can be drawn from the examples, it must be understood that it has
pointsin commonwithothergeneralizationsystems. Inparticular,thelanguageAPC
(Michalski, 1983) contains many deductive as well as inductive rules and has close
relationships withthe methodpresentedhere, which can be seen as akind ofamplified description of the "extending reference rule" (see Michalski, 1984) and the
"inductive resolution rule."
9.3.1 Definitions
Terms. Let V be a countable set ofvariables and F a family offunctions indexed by
thenatural integers. Whenafunction/belongsto Fm onesaysthatthearityof/isn.
The set F offunctions ofarity zero is the set ofthe constants.
--- PAGE 238 ---
224 CHAPTER 9: IMPROVING THEGENERALIZATION STEP IN LEARNING
The set ofterms on Vand F is defined by
1. v E Kis a term;
2. /ft, ... , t„) is a term iff/ E F„ and f,, . . . , tn are terms.
Itcan be seen intuitively that the set ofterms is a set ofexpressions built with functions ofsome arity, constants, and variables.
9.3.2 Term Generalization
Generalization. The term t\ is more general than theterm t2, denotedby t\ < t2, iff
there exists a substitution at\ t2.
e-generalization. This definition does not take into account the properties of the
functions. Let 6 be a set ofaxioms that expresses these properties. When one needs
tousetheseaxiomsinordertorecognizetheequalityoftwoterms, onesaysthatthey
are "e-equal." For instance, the two terms t\ = (2 + 3) and t2 = (3 + 2) are not
considered "equal" but "e-equal" because one needs to use the axiom of +
commutativity
V x V y[(x + y) = (y + *)]
in orderto recognize that t\ ( t2.
This definition may seem counterintuitive, but it is necessary to single out the
use ofaxioms in the context ofan automatic generation ofgeneralizations because
theirusemayleadtoinfinitecomputationloops(usingtheaxiominonedirectionand
then in the other one). This kind ofproblem has been studied extensively (see, for
instance, Stickel, 1981; Hsiang, 1982).
Let =
denotee-equality. Aterm
ismoregeneralthanatermf2 inthetheorye
iffthere exist t\, t2, a such that
t'i =, tu
t'2 = ( t2, and
ot\ t'2.
Depending on e, it may be that the above definition ofe-generalization is not
consistent. Using some ofthe properties, one may find t\ and t'2 such that /, =, t\
and /; , f'2 and there exists 0\ such that cr,/1 , = t2. Using other properties, one may
find /"i and /" : such that /, =, /", and t2 = ( t" 2, but there exists a : such that
'i /" /", even when f| ^ , t:.
Since we want touse the propertiesofthe functionsand furtherdefine the generality offormulas (therefore using the properties ofourconnectors), it is necessary
to find a definition of < -generalization that avoids this difficulty.
--- PAGE 239 ---
KODRATOFFANDGANASCIA 225
Exampleoj'e-generalization(wherepredicatesaretreatedliketerms). Letussuppose
that we work in a world ofobjects with colors and that the following knowledge is
available:
V x 3 y(COLORyx)
This states that each object x has a color named y. In addition, RED is a kind of
COLOR, andthis informationisalsosupposedtobeknown. Thisknowledgeallows
ustotransformanypredicatelike(RED*) intoaninstanceofmoregeneralpredicate
(COLOR RED
x).
Let us compare the generality ofthe concept redsquare C\ and square C
C, = (SQUARE*) & (RED*), C = (SQUARE*)
Applying the above theorem, one knows that for any * of C 2, there is an
unknown color, say y. Therefore C 2 is equivalent to C 2 = (SQUARE *) &
(COLOR vx). Based onthe factthat RED is more particularthan COLOR, onecan
findC,' = C[,C\ = (SQUARE*) & (COLORRED*). Now theusualtermdefinition ofgene e rality can be applied since oC 2 = C\ with a = (y - RED). Therefore
C 2 is more general than C x in the theory containing the above information.
9.3.3 Formula Generalization
LetE x and E 2 be two formulas and e an equational theory.
Generalizedformula. We shallsaythatformulaE\ isageneralizationofformulaE 2if
Condition 1 is fulfilled.
Condition 1: 3E\ such that
E\ = e Ef and 3o 2 such that o 2 E\ = e E 2.
ThisconditionstatesthatthereexistsE\ equivalentto E andthatE\ consideredas
, x, ,
aterm is more general than E 2 considered as a term.
Thenextdefinitiongivesanotherconditionthatinsuresthatformulagenerality
is apartial ordering.
Generalityrelation between twoformulas. We shall saythat E is moregeneralthan
E 2 when Condition 1 and Condition 2 are fulfilled.
Condition 2: V£' 2 such that
E 2 = e E 2 and 3a,, such that
Q\E' E\
2 f
implies
E' =<£,.
This second condition states thatthe first condition meets no contradictions. It says
that ifthere isan E' 2 that isequival - entto E 2 andthat is more general (as aterm) than
E u then all three-E,, E 2, and E' 2 must be equivalent.
--- PAGE 240 ---
226 CHAPTER 9: IMPROVINGTHE GENERALIZATION STEP IN LEARNING
STRUCTURAL MATCHING
9.4
Letus recallthe (restricted)definitionofthegeneralizationasgiven in section
9.3.3:
IfEf,isageneralizationof£,, E 2, . . . , E n, then V ie [\,n] 3 £',3(7,suchthat
Ej = e Ej and <jjEg = El .
The first step is to find for each E the appropriate Ej. This step is called the
detection ofstructural matching between £,, E 2, . . . , E n, and when this step succeeds, the formulas E\ 9 E' 2, . . . , E' n, are saidto match each other structurally.
Thissection isdevotedtothedefinitionandthe intuitive meaning ofthe structural matching. Also described here is a way of obtaining structural matching by
using properties ofthe representation and theorems about the description language.
9.4.1 Definition of the Structural Match
Two formulas structurally match ifthey are identical except for the constants
and the variables that instantiate their predicates.
Definition. E\ andE 2 beingtwoformulas,E\ structurallymatchesE 2iff 3 C 3 o x ,a2
such that
1. <j\C = E\ and o 2 C = E 2;
2. o\ and a 2 never substitute a variable by a predicate ora function.
Example
Let Ei = (COLORx y) & (FORM TRIANGLE v),
E 2 = (COLOR GREEN z) & (FORM u t), and
E = (GREEN
3 z)
£, andE
structurally matchbutE
and£
donot, althoughonecannoticethat E
Ei, in the theory that says that each object has a form.
9.4.2 Determining the Structural Matching
Given a learning set L = £,, E 2, • • • , E n, the first step oflearning is to find,
for each E, belonging to L, a formula £/ = £, such that all EJ structurally match
each other. The structural matching is obtained by progressive meaning-preserving
transformations.
--- PAGE 241 ---
KODRATOFFANDGANASCIA 227
9.4.2.1 Using Representation Laws
A representation language (seetheappendixtothischapter) is given whenthe
properties of each connector in the expressions are known. For instance, let us
assumethattheuniqueconnectorofrepresentation isthelogicalconjunction&. The
properties of & are not only the commutativity and the associativity but also the
idempotency andthe existenceofaneutral element (True). Thefollowing will show
the importance ofa systematic use ofthese last two properties.
Associativityandcommutativityof&. It is quite evidentthat associativity and commutativity of must be used for detecting structural matching. This fact has often
beenused, sinceitallowsonetoconsideraconjunctionofclausesasasetofclauses.
Example
E, = (((RED A) & (SQUARE A)) & ((TRIANGLE B) & (GREEN E)))
E = (((RED C) & (TRIANGLE Q) & ((SQUARE D) & (GREEN D)))
Using associativity and commutativity of &, we may transform E\ into
E\ = ((REDA) & (TRIANGLEE)) & ((SQUAREA) & (GREEN B))), whereE\
structurallymatchesE sincethereexistsaformulaCandtwosubstitutionso ando
2, x 2
such that o\C = E\ and o 2 C = E 2:
C = (((REDjc) & (TRIANGLE v)) & ((SQUARE z) & (GREEN v)))
a = (x - A, v - E, z - A, y - B)
o 2 x = (x - C,y - C,z - D,v - D)
Because ofassociativity andcommutativity oflogical &, the parentheses are meaningless and will be omitted from now on.
& idempotency. Considerthe learning set containing E 3 and E 4:
E = (SQUARE A) & (RED A) & (SMALL A)
£ = (SQUARE B) & (RED B) & (SQUARE C) & (SMALL C)
The classical way ofgeneralizing applied on these formulas (Hayes-Roth and
McDermott, 1978; Michalski, 1983)usesthedroppingconditionrule. Itleadstotwo
different generalizations:
E' g = (SQUARE jc) & (REDjc) & (SMALL};)
En = (RED*) & (SQUARE & (SMALL
g y) y)
These two formulas are not generalizations because there does not exist any
transformationE\ofE A (whereE\ = t E 4)thatisaninstantiationofE' g orE' g'. Theaim
here istoavoidasmuchaspossibletheuseofthedroppingcondition ruleduringthe
generalization processbecause itabstracts somepieceofinformation. Butthedropping condition rule must be used after the generalization process, using counterexamples, to extract some common characteristics ofthe generalization that do not
--- PAGE 242 ---
228 CHAPTER 9: IMPROVINGTHEGENERALIZATION STEP IN LEARNING
belongtothecounterexamples. This secondstepin learning useslinksbetweenvariables in the generalized formula (see section 9.6).
In order to detect the structural matching between £ 3 and E A we must use the
idempotency of&. £ 3 may be rewritten as E' 3, which structurally matches £ 4:
Ei = (SQUARE A) & (RED A) & (SQUARE A) & (SMALL A)
£3 structurally matches E 4 since there exists C,
C = (SQUARE jc) & (REDjc) & (SQUARE v) & (SMALL v),
and there exist a 3 and a 4 such that
oyC = £3 and cr4 = £4,
where (73 = (x - A,y «- A) and a 3 = (x - B, v - C). Novariable is substituted
by a predicate.
Now let us consider the formula £ 3. £' 3 may not be represented by a set of
clauses, as in Vere (1980) orHayes-Roth and McDermott (1978), because the clause
(SQUAREA)appearstwicein £
Thereforewemustuseamultisetofclauses(i.e., a
setwith multiple occurrencesofclauses) to represent formulas whenthe representation ispurely conjunctive. When representation is morecomplicated, more sophisticated data structures are required to represent formulas.
Neutral element. The existence ofa neutral element (True) is used not by itselfbut
with regard to semantical properties of the description language. It indicates that
everytheorem knownas Truemaybeconjunctedtoanyconjunctive formula inorder
toobtainthestructuralmatch. Incaseofpuredisjunctiveclauses, theoremsknownas
False may be inserted.
9.4.2.2 Using Properties of the Description
The properties of the description language are given first by predicate taxonomies (hierarchies), which express generality relationships among predicates,
and second by rewrite rules, which express theorems about the properties of the
predicates.
Hierarchyofpredicates. Generality relationshipsamongpredicatesmustbeencoded
as hierarchies. For instance, suppose that we are in a world where objects are plain
geometrical figures. The form of the objects in such a world is one of their most
important attributes. But their shapes ma) be specified with more Of less generality
depending on the precision ofthe initial description.
--- PAGE 243 ---
KODRATOFFANDGANASCIA 229
Example
Let us suppose we want to generalize two formulas:
E, = (STRIPED A) & (SQUARE A)
E = (STRIPED B) & (TRIANGLE u vB)
E\ is very different from E 2 because a square is, in a sense, a much better defined
shape than a triangle.
In order to find a structural matching between E x and E 2, one must be aware
thatasquareandatrianglehavesomecommonpropertiesbecausebotharepolygons.
This inference is allowed by the hierarchy ofshape predicates shown in figure 9-2.
This hierarchy has been used deliberately to show that "tangled hierarchies" must
sometimes be used.
In order to use predicate taxonomies, one must look for a common ancestor
in the hierarchy; this process has been called "climbing the generalization tree"
(Michalski, 1983). It is used to find the desired new expressions E\ and E' 2 where
E\ = E and E' = E
e { 2 £ 2:
E\ = (STRIPED A) &
(POLYGON QUADRILATERAL RECTANGLE SQUARE A)
E' = (STRIPED B) & (POLYGON TRIANGLE u vB)
Now E\ and E' structurally match.
Let
C = (STRIPEDx) & (POLYGON y z tx)
and
0\C = E\, o C = E'
2 2,
where
a, = (x - A,y - QUADRILATERAL, z - RECTANGLE,
- SQUARE)
and
a 2 = (x +- B,y - TRIANGLE, z - u, t «- v).
Rewriting rules. Rewriting rules areusedtoencodeboththeoremsandproperties of
predicates. Theserulesarethemselvestheorems, sotheyareboundformulas, thatis,
formulasquantifiedbyexistentialoruniversalquantifiers. Forinstance, suppose we
areinablockworldandtherearetwopredicates. Thefirstexpressesthatoneblockis
above another (e.g. (ON A B) expresses that A is onB), andthe secondexpresses
that one block is nearanother (e.g. (NEAR A B) expresses that A is nearB). The
predicate NEAR is commutative (i.e., V jc V y [(NEARx y) = (NEAR yx)]) but
nottransitive(i.e., (NEARx y)and(NEAR y z)donotimply( NEARx z)). Inorder
--- PAGE 244 ---
230 CHAPTER 9: IMPROVING THEGENERALIZATION STEP IN LEARNING
FORM
CIRCLE RECTANGLE RIGHT-ANGLED ISOSCELES
SQUARE ISOSCELES-RIGHT-ANGLED EQUILATER
Figure9-2: Onepossibletangledhierarchyplanegeometrical forms.
NEAR
to take into account the commutativity of during the detection ofthe structural match, we must encode it as a rewrite rule:
ViVy [(NEARx y) - (NEAR y x)], (Rl)
NEAR and ON are linked inany case sinceobjects thatare ON each otherare
also NEAR each other. One has
V x V y [(ONx y) - (ONx y) & (NEARx y)]. (R2)
Theserulesarenotsupposedtobeappliedcarelessly-forinstance, rule(R2)applied
to itselfmay give a formula ofinfinite length. One applies them in orderto obtain a
structural matching; thisfactwillcontroltheprocessofruleapplicationandprevent
infinite loops.
The rewrite rules may alsobe usedtoencodetheorems. Forinstance, in acolored worldeach objecthas acolor, butall objectsdonothave the same color; otherwise the information about color would be empty, and this would be equivalent to
being in a noncolored world. This information may be expressed by
V v V v [True - (COLOR y x)] (R3)
(i.e., each object* has a color named y).
In order to apply the quantified rewrite rules, we must remove the quantifier
according to the following laws:
1. A variablethatappearsunderthescopeofauniversalquantifiermaybe instantiated by any term, constant or variable.
2. A variable that appears under the scope of an existential quantifier may be
instantiated only by a variablethatdoes notbelongtothe formula in which the
rule is applied.
--- PAGE 245 ---
KODRATOFFANDGANASCIA 231
Example
Let£ = (ON E F) & (NEAR E F) and
£4 = (ON G H).
Applying rule (R2) to £ 4 leads to
Ei = (ON G H) & (NEAR G H),
which structurally matches £
Let£ = (RED C) & (NEAR D C) and
£ = (NEAR
6 I J).
Applying rule (R3) in one ofits correct instantiations,
True - (COLOR u J),
we obtain
£' 6 = (COLOR w J) & (NEAR IJ).
Using the colorpredicates hierarchy given in figure 9-3, we obtain
£' = (COLOR RED C) & (NEAR D C),
which structurally matches£' 6.
9.4.3 A Complete Example
In this section a less trivial example is presented. The aim is to prove that
(LOWERx v) is more general than (EQ x 1) & (EQ Y 2). This statement is intuitivelyclear,butitsproofisnotsotrivial. Theneededsemanticrulesabouttheproperties ofthe predicates LOWER and EQ are as follows:
V x[Tme - (EQjcjc)] (R4)
True - (LOWER 1 2) (R5)
Vx V v V z [(LOWERx v) & (EQzx) - (LOWER z y)] (R6)
V x V v V z [(LOWERx v) & (EQzy) - (LOWERx z)] (R7)
COLOR
RED YELLOW GREEN
Figure9-3: Asimple "COLOR" hierarchy.
--- PAGE 246 ---
232 CHAPTER 9: IMPROVINGTHEGENERALIZATION STEP IN LEARNING
LetT, = (LOWER jc,z,) and
T 2 = (EQx 2 \)&(EQy 2 2).
Using rule (R4) twice in T we obtain
T, = (LOWER x, y x) & (EQx, *,) & (EQy, y{).
T may be rewritten in T by applying rule (R5):
2 2
T 2 = (LOWER 1 2) & (EQx 2 1) & (EQy2 2).
T[ and T 2 match structurally. The algorithm ofsection 9.5 will now detect that they
generalize to
T = (LOWERxy)& (EQ vx) & (EQ w y).
Using rules (R6) and (R7), we can rewrite T as T:
T = (LOWER vw) = T
e x.
This shows that T { < T 2 as proven by the definition ofgenerality relation between
two formulas in section 9.3.
9.4.4 Conflicting Structural Matching
Structural matching does not have a unique solution in general, as the following example will show. Considerthe following instances:
£, = (RED A) & (RED B) & (SQUARE B)
E = (RED C) & (SQUARE D)
A structural matchingofE\ andE 2 may be obtained intwoways. The firstway is to
use idempotency to obtain
E' = (RED C) & (RED C) & (SQUARE D).
Now E\ and E 2 structurally match, leading to the generalization
= (RED
jc)
& (RED
& (SQUARE
& (DIFFERENTx
z).
The second way is to use rule (R3)
V x 3 v [TRUE - (COLOR y x)]
with the instantiationx - D to obtain
E'{ = (RED C) & (COLOR v D) & (SQUARE D).
Using the COLOR predicate hierarchy, one also obtains
E\ = (RED A) & (COLOR RED B) & (SQUARE B).
This leads to the generalization
E g = (RED*) & (COLOR v z) & (SQUARE z) & (DIFFERENTx z),
where the fact that v must be different from bothx and z is kept implicit.
--- PAGE 247 ---
KODRATOFFANDGANASCIA 233
The generalizations E and E' have their own qualities, and one cannot state
g g
that one is "better" than the other, since they both contain three variables.
The readerhasprobably noticedthatthetheoremsareappliedhere inthe most
judicious way; for instance, instantiatingxby C in rule (R3) would have led to
E' " = (RED C) & (COLORyC) & (SQUARE D).
The generalization ofEJ" and E\ gives
E g H = (REDx) & (COLOR v y) & (SQUARE z) & (DIFFERENTx z),
which is much toogeneral as compared toE' g or E g.
Section9.5showsawaytodetectthegoodinstantiations. Nevertheless,thefact
that incomparable generalizations may be obtained cannot be avoided, as the above
example shows. From the "point ofview" ofthe colors (i.e., keeping the colors as
consistentaspossible), E gisthebestgeneralization. Fromthe "pointofview" ofthe
objects (i.e., introducing the minimum number ofobjects), E' g is better.
Thepositionoftheauthorsisthatthebestpointofviewisonegiven "atahigher
level," meaningthatitisametaknowledgethatcanbegivenatthesametimethatthe
theorems and the hierarchies are given.
9.5 GENERALIZATION ALGORITHM
As previously seen, the generalization process may be divided into two main
steps. Thefirstisintendedtodetectstructuralmatchingsamongexamplesbelongingto
thelearning set. Findingastructural matching isthe mostdifficult, though preliminary, taskofthegeneralizationprocess, soasketchofthestructuralmatchingdetection algorithm will be produced. The second step, the actual generalization phase,
may beeasily realized; it involves simplydetectingthecommonlinksbetween variables in all the structurally matching formulas.
9.5.1 Structural Matching Detection Algorithm
Ithasbeenshownthatdetectionofstructuralmatchingiscentralinthegeneralization process. To solve this problem, the algorithm presented here operates progressively and in parallel on all examples.
9.5.1.1 Overview of the Algorithm
The algorithm may be roughly described as sequence oftwo alternate operations. The conjunction of these two operations leads to the introduction of new
variables with common occurrences in all examples. Each of these variables is a
generalization variable (GV). Links between variables, or between variables and
constants, keep track ofthe instantiations ofeach variable in each example.
--- PAGE 248 ---
234 CHAPTER 9: IMPROVINGTHE GENERALIZATION STEP IN LEARNING
Thefirstoperationconsistsofchoosingoneconstantoronevariablethatisnot
a GV ineach example andturning it intoanew GV. As will be seen, this choice is a
crucialstepintheprocess,andonethereforeneedstouseheuristicsinordertoleadit.
Dependingonthestrategy, onemayeithermakeonechoiceatatimeorbuildatreeof
choices, each ofthem leading to a generalization formula.
Inthe present implemented state ofthis algorithm, the userhas three choices.
The first is to provide an ordering ofthe predicates, which the system then uses for
findinggeneralizationsthat favorthe "pointofview" (see section9.4) expressedby
this ordering. The second is to choose directly the constantto be turned into a variableandtorepeatthis foreachexample. Thethird istoleavethesystementirely free
and let it choose the generalization that minimizes the combinatory search.
Thesecondoperation isapartial structural matching. Oneconsidersthe setof
predicatescontainingGVsandattemptstorealizeastructuralmatchingofthissubset
oftheexamplepredicates. Eachvariableofthe GVmustbeatthesameoccurrenceof
thesamepredicateinalltheexamples. Otherwise, thestructural matching isconsidered faulty, and a new one is sought.
This operation usually leads to different, partially matching formulas. In this
case one musteitheruse a specific heuristic similarto the heuristic used in the first
operationorbuildsimultaneouslydifferentpartially matching formulas. Operations
1 and 2 are repeated until there are no constants left.
The instantiations ofeach variable are not dropped, since this is very useful
information tobe used further in detecting the links between variables.
9.5.1.2 Description of the Algorithm
Each ofthe steps ofthe algorithm isexemplifiedby the following very simple
example:
Let £, = (ON A B)
E = (ON C D) & (NEAR D F)
The predicates ON and NEAR are those defined in section 9.4.2.2 under
Rewrite rules. Their properties are expressed by rules (Rl) and (R2):
V.v V y [(NEAR Jt>') - (NEARjjc)] (Rl)
V x V y [(ONx v) - (ONx y) & (NEARx y)] (R2)
Operation l is divided into two steps:
Step 1: Oneconstant(ornon-GVvariable) ischosen ineachexampleaccording
to some heuristics. Suppose that the heuristic requires one to choose first the
second argument ofthe predicateON, nextthe firstargument ofON. and then
the first and the second arguments ofNEAR. Applying Step 1 to the example
leads one to turn B and D into a GV.
--- PAGE 249 ---
KODRATOFFANDGANASCIA 235
Step2: A newGV iscreatedandsubstitutedfortheconstantsandthevariables
chosen in Step 1. One keeps track ofthis substitution by using links between
variables. Applying Step 2 to the example, one obtains
E = (ON A *o)[(=*oB)]
E 2 x = (ON C jc ) & (NEARx F)[(= jc D)]
where the formula inside [ ] states the links between variables.
Operation2 is intendedtoforcethepartial structural matchingbetweenexamples. Let us consider the example again: x appears once in E\ (as the second argument ofthe predicate ON) and twice in E 2 (as the second argument ofthe predicate
ON andas the firstargumentofthepredicate NEAR). There is one common occurrenceofjr inE] andE 2, andthere isoneoccurrencethatappears in E 2 andnotin E {.
Operation 2 is alsodivided into two steps:
Step3: TheoccurrenceofthevariablessubstitutedinStep2 isdetectedandthe
factthat these occurrences belong to all the examples is checked.
Step 4: Ifthis is not the case, an attempt is made to generate them using the
transformations introduced in section 9.4.
genera F t o e r it i i n n st E\ a . nc R e u s l i e nc ( e R2 t ) he wi fi t r h st i a ns r t g a u n m t e i n at t i o o f n* NE *- A A R a i n s d x j in - E 2 x , a i n sa a p t p t l e i m e p d t t i o s m E\ a . d O e n t e o
obtains
£, = ( E\ = (ONAjcq) & (NEAR Ai ) & [(= x B)].
x appearsnow inE\ asthesecondargumentofNEAR, andwewanttogenerateitas
the first argument ofjc , which is possible using rule (Rl). Therefore, E\ will be
rewritten as
E'{ = (ON A jc ) & (NEARx A) & [(= x B)].
Finally, ifStep4 did not manage to generate some occurrence ofthe variable
substitutedinStep2, thenStep4killsthisoccurrenceby introducinganewvariable.
Let us go on applying this algorithm tothe example.
Step 1: Suppose that M A and C, the first arguments ofON, are chosen.
Step2: El' = (ON o) & (NEAR* }>o) & [(= y A) & (*x y )
&(=*
B)]
E 2 = (ON y x ) & (NEARx F) & [(= x D) &
(=y C)&(±x
y )]
Step3: Occurrence ofy shared by all examples: (ON y Xo). Occurrence ofy
belonging to E" and not to E 2 : (NEARx y ).
--- PAGE 250 ---
236 CHAPTER 9: IMPROVING THE GENERALIZATION STEP IN LEARNING
Step4: In ordertogenerate (NEAR x v ) rules (Rl) and (R2) are used soE 2 is
rewritten as E'i:
E'{ = (ON y JC ) & (NEAR jc j ) & (NEARx F) &
[(=XoD)&(=xoC)&(*xoy
)].
Step 1: The last constant is F.
Step 2: E'{ = (ON y x ) & (NEARx y ) & [(= x B) & (= v A) &
(±Xoy
E 2 = (ON y x ) & (NEARjc y ) & (NEARjc Zo) & [(= x D) &
(= y C) & (= z F) & (*Xoy ) & (* z *o) & (* z Vo)J
Ste/? 3: The only occurrence ofZo is (NEAR jc Zo) in £ 2Sf<?/? 4: A similar occurrence of z in E" can be obtained by the use of &
idempotency and (NEARx v ) = f (NEARx Zo) & [(= z yo)]:
E\ " = (ON y x ) & (NEARx >'o) & (NEARjc z ) &
[(= x B) & (= jo A) & (= ZoVo) & (* Wo) & (* Zo*o)]
E'{ = (ON JqJCo) & (NEARjcqJo) & (NEARjc z ) & [(= Jc D) &
(= jc F) & (= y C) & ( * xojo) & ( * Zo*b) & ( * z () y ())\
Now the algorithm stops because there is no constant or variable that is not
beinga GV in formulasE\ " andE'{. Onecaneasily verify the structural matchingof
E\ " and E"
9.5.2 Pure Generalization Algorithm
Thepuregeneralizationalgorithmcompares linksbetweenvariables inthis set
ofobtained formulas. Ifalinkexistsinall formulas, it remains inthegeneralized formula; ifnot. it mustbe suppressed. Thereforethe setoflinksbelonging tothe generalized formulaisthe intersectionoflinksbelongingtoeach formula. For instance. E.
being the generalization ofE\ and E
E R = (ON v .v()) & (NEARvoio) & (NEARxbZo) & l( * Jfajto*)].
Onecan now see thatthe restricted definition ofgeneralization, given in section 9.3,
has been used to build up the algorithm ofstructural matching. This algorithm renders explicit (he implicit information contained in the examples that is neeessar\ for
the construction ofa good generalization.
Once this information has been gathered, the dropping rule can he used in
order to obtain a generalization. The readercan now understand what was meant at
the beginning ofthis chapter by the statement that the dropping rule should be used
withextremecare. Inotherwords, thedropping rulecanbeusedafteranattempthas
been made toput the exampleformulas into structural matching.
--- PAGE 251 ---
KODRATOFFAND GANASCIA 237
LEARNING A CONCEPT FROM EXAMPLES AND
9.6
COUNTEREXAMPLES
Inthissectionanexampleisgiventhatshowstheimportanceoftheinformation
gatheredduringstructuralmatching. Suchinformationwillallowustofindanexplanation ofthe differences among examples and counterexamples.
The example used here is taken from Vere (1980) and involves "stacking,
transfer, and unstacking actions for uniform cubic blocks on a table." The counterexamples "illustratethatblock 1 cannotbemovedontopofanotherblock2 ifthereis
ablock3alreadyonblock2" (Vere, 1980). TheformulageneratedbyVere'ssystemis
[(ON t TABLE) & - ((ON u z) ~ ((ONy v) & ~ ((ON z TABLE)
& (ON w
w))))]
(ONx
- (ONx
wherex,>',z, t,u,v, andwarevariablesthatcanbeinstantiatedbythenameofablock
or the constant TABLE; where the shows the modification to be done to the
blocks; andwheretheexpressioninsidethebracketsisthecontextinwhichthismodification isallowed. This formulatakesthevalue TRUE forallthe examples andthe
valueFALSEforallthecounterexamples, butitisapoorillustrationoftheconceptof
stacking and unstacking wherein two blocks cannot be on athird one.
Theexamplesusedhere (see figure9-4) are notthe same as Vere'sbecause his
are somewhat complicated, but it is easy to check that these examples convey the
same idea.
In Vere's notation these examples would be described as follows:
£*,: [(ON A TABLE)](ON B TABLE) - (ON B A)
E< [(ON E TABLE) & (ON F E)](ON G F) - (ON G TABLE)
E 3: [(ON H TABLE) & (ON I H)](ON J TABLE) - (ON J I)
Usingthemethodologydescribedinthischapter, the readerwilleasilyseethat
G: [(ON t TABLE) & (ON t' u) & (DIFFERENT(x(>< z 1 1' u))(y{z t))
(ON - (ONx
(z u)(t' u))] jc y) z)
is ageneralization ofE\, E 2, E 3 where, for instance, (DIFFERENT(v(z t))(z u)) is a
short way of saying (DIFFERENT y z) & (DIFFERENT y t) & (DIFFERENT
zu).
Aswillbeseen, G alsohasnoinstancesuchthattwocubesareoverathirdone.
Ofcourse, G is simpler than Vere's formula, but it is still not satisfactory, because
much ofits information is irrelevant to the concept.
Let us now consider the counterexample E[ shown in figure 9-5. E[ is written as:
E\ = [(ON M TABLE) & (ON P M)](ON N TABLE) - (ON N M).
--- PAGE 252 ---
238 CHAPTER 9: IMPROVINGTHEGENERALIZATION STEPIN LEARNING
/////////
/////////
-V I 1 D' hD
Figure9-4: Examplesofstackingandunstackingcubes.
pD Dn
E[:
777777
Figure9-5: Acounterexampletotheexamplesgiveninfigure9-4.
Thiscounterexample is "nearly" an instance ofG with the following instantiations:
x - N,y - TABLE, z - M, r' - P, m - M, r - M
Thissucceedsonly ifz, m, andrarethesame. FromthesetofDIFFERENT inG, one
sees that u and t may be the same and that therefore the link between the two
M's in
[(ON TABLE)(ON P M)]
may be present in both examples and counterexamples; that z and / may be the same
and that therefore the link between the two A/'s in
(ON M TABLE) & (ON P M)](ON N TABLE) - (ON N M)
--- PAGE 253 ---
KODRATOFFAND GANASCIA 239
maybepresentinbothexamplesandcounterexamples; andthatzanducannotbethe
same and that therefore the link between the twoAfs in
[(ON M TABLE) & (ON PM)](ON N TABLE) - (ON NM)
is the link that rejects the counterexample as the instance ofG. This link expresses
precisely that a new block cannotbe put on another one that is not "clear."
Our concept is therefore described by the formula
G' = [(ON t TABLE) & (ON t' u) & (DIFFERENT z u)]
(ONxy) -* (ONxz),
which is included in G.
A more detailed study ofthe possible matchings of G and other counterexamples would be necessary before one would find the correct generalization,
which is actually more complicated than G'. More details are given elsewhere
(Kodratoffet al., 1984), but it is hoped that the reader now tends to agree with the
statement that concept learning is the discovery ofrelevant variable bindings.
CONCLUSION
9.7
The main goal ofthis chapter has been to describe an algorithm that detects
variablebindingscommontotheexamples. Theimportanceofvariablebindingshas
been exemplified by their use with counterexamples: they are a source ofpossible
"near misses" (Winston, 1975) between examples and counterexamples, and they
maythereforehelpustofindexplanationsforthevalidityofthegeneralization. Inthe
exampleinsection9.6,thelink(DIFFERENTuz)explainswhytwoblockscannotbe
put on top ofthe same third block. The discovery ofthese links can be made using
deductive procedures and a very restricted definition ofgeneralization.
Onemustbeawarethatseveralsetsoflinks, thatis, several independentstructuralmatchings,canleadtodifferentgeneralizations. Eachofthemfavorsthememorization ofsome aspectoftheexamples. Since it requiresthe knowledge ofthe relative importance ofthe predicates describing the examples, the algorithm described
here helps to make explicitthe relationships between agiven generalization and the
aspects it favors.
Finally,theauthorswouldliketostressthatacquiringtheknowledgeoftherelativeimportanceofthedifferentpredicatesdescribingtheexamplesmayalsobeconsideredanactoflearning. Theauthorsarecurrentlydevelopingmethodsofapplying
theideaofstructuralmatchinginordertofindhintsaboutthisimportancebymixing
symbolic and numerical information.
--- PAGE 254 ---
240 CHAPTER 9: IMPROVING THE GENERALIZATION STEP IN LEARNING
APPENDIX: GENERALIZATION AND "l-IMPLICATION"
1. Definitions
Terms. These have been defined in section 9.3.1.
Description language. LetP, be a set ofpredicate symbols ofarity i, and let us call
P = P U P\ . . . U P n where P is TRUE or FALSE.
Thepredicatesmayberelatedbyoneorseveralgeneralizationhierarchies. For
instance, the predicates PLANE-FIGURE, POLYGON, and SQUARE are of
decreasing generality.
Some predicates may have properties or may be related by theorems. For
instance, thepredicate (NEARx y) (which statesthatjcandyare neareach other) is
certainly commutative- V x V y[(NEARx y) => (NEAR yx)]-butitis nottransitive since
3 x 3 y 3 z[ ~[(NEAR jc y) & (NEAR y z) => (NEARx z)]].
Let be the set oftheorems concerning the predicate. Then the triplet (P,H,T) is
called a description language.
Literal. Let tu . . . tn be terms and p e P n; then (p r, . . . t„) or ~ (p t\ , . . . tn) are
called literals.
Representation language. Expressionsdefinedherearealittlemoregeneral thanthe
usualclausesbecausetheauthorsallowmoreconnectorsthanareallowedinclassical
predicate logic. A representation language is defined by a description language and
connectorswiththeirsyntaxandsemantics. Theconnectorsarethoseoflogicaswell
as special connectors describing possible actions.
For instance, when Vere (1980) wants to describe a scene where blocks are
transferred (see figure 9-6), which can be described by
[(ON A TABLE)](ON B TABLE) - (ON B A),
he "invents" two connectors. One ofthem is the set ofbrackets which contains the
context in which the action may be done. In this example the context is [(ON A
/////// V7Z
Figurel>-(y. Anexampleofstacking twocubes
--- PAGE 255 ---
KODRATOFFAND GANASCIA 241
TABLE)]. The otherconnectoristhearrow, which describesthe actionthatmustbe
done. In this example the action is (ON B TABLE) - (ON B A).
The invention of new descriptors is certainly a part of learning, but it is an
extremelydifficulttask. (ForasolutionseeKodratoffetal., 1984; Michalski, 1983).
Formula. Aformulaisanexpressionoftherepresentationlanguageobtainedbyconnecting valid expressions ofthe description language.
2. Term Generalization
Term generalization has been defined in section 9.3.2.
3. Formula Generalization
Formula generalization has been defined in section 9.3.3.
Propertyof < ("moregeneralthan"). Thisisapartialordering. From itsdefinition,
< istriviallyreflexiveandtransitive. Inordertoavoidthee-generalizationdifficulty
described in section 9.3.2 we must prove its antisymmetry.
Suppose that one has togetherE\ < E and E < E\.
2 2
From E\ < E 2 it follows (by Condition 2 ofsection 9.3.3) that if E 2 verifies
E 2 = e E 2 and there exists a substitution o\ such that 0\E 2 = e E\, then
E =
2 ( £,.
From E 2 < E x it follows (by Condition 1 ofsection 9.3.3) that there is an E'{
such t I h t at fo E l 2 low = s e t E ha 2 t a E n 2 d = th e e E re x i = s a ( o E 2 2, su w c h h ic t h ha p t r o o 2 v E e 2 s a = nt e i E s \ ymmetry.
4. Definition of the I-implication
In section 9.2.2, the "truth domain" XI ofapredicate P{x) was definedby
XI = {alP{a) = TRUE}.
This definition can be trivially extended to any formula containing free variables.
Thei-implicationformalizesthefactthattheformulaE ismoregeneralthana
formulaE 2 iffthetruthdomainofE\ containsthetruthdomainofE 2. Inasense, this
expresses the fact that E } is more general than E 2 in a particular universe if E x is
alwaysTRUEwhenE 2 is TRUE inthisuniverse. This isakindofimplicationbythe
instantiations, andthis is why itiscalledherethe i-implication. Asexplainedinsection9.3, formulas with freevariablescannotreally implyeachother; i-implicationis
definedtoformalizetheintuitivefeelingthattruthvaluedomaininclusionhassomething to do with implication.
LetuscallthesetofthepossibleconstantsCandtheinstantiationsbyconstants
/. It will be noted thatthe formulaEhas been instantiatedby constants (i.e., all the
--- PAGE 256 ---
242 CHAPTER 9: IMPROVING THE GENERALIZATION STEP IN LEARNING
variables of£ are replaced by constants) and that this instantiation takes the value
TRUEby 1(E). This is a substitution, and substitution composition is well defined.
LetE\ andE 2 betwo formulas; one says that E x i-implies E 2, E x =>,E 2, ifffor
allinstantiationI
byconstantsofE
suchthatI
(E X),thereexistsaninstantiationI2 of
E 2 such that I2 OI x (E x) andI 2 OI x (E 2) where E 2 = qE 2 and q is a variable renaming.
Example.
Let£, = f(x) &f(y), E 2 = /(y), q = (z - x), q' = (x - z). Trivially,E, => E 2
sinceany instantiationthatgivesthevalueTRUE to E alsogivesthevalue TRUE to
qE 2. One also has E 2 => t E\ f since for any instantiation I 2 such that I 2 (E 2) one can
c fi a n n d I x L f e s i u t n c d I h 2 t b / h , e at f = I o x r O ( i I j n 2 s ( - t E a 2 n ) c Q a e n z dI - s x u O c I C h 2 ( a q n t ' d h E a s x t ) u . pp I o x s O e I t 2 h ( a E t 2 / ) 2 ( = £ 2) /( = C) /(C i ) s is T T R R U UE E . O a n n e d
I x OI 2 (q'E x) = /(C) &/(C) is also TRUE.
5. Properties of the l-lmplication
Property 1: E\= t E 2 implies that E x =>,- E 2 andE 2 =>,- E\.
Property 2: =>, is transitive.
Property 3: oE x = E 2 implies that E 2 =>,- E x
Lemma 1: V E u E 2 (E x < E 2 implies that E 2 =>, E{).
Proof:
B \
= P
€ o
p E
x r
a t
d 1
, a
o s
E u
[ b
t =
c u
£ ti
2 o
m a
p ,
i s
e u
s c
h t a
h t
t £
2 oE x = [ ^
j = o
E e 2 [ E .
pliesthatthereisanE\ suchthat
By Property 3 (applied to the triviality oE[ = oE[), one has oE[ =>, E\.
Since£{ = f £, , theaboveimplicationcanberewrittenas oE{ =>,- E x. Therefore, byProperty 2,
E E
=*,
2 x
E =x
E 2
t £
dition 2'.
Condition 2': ~ (£, =>, £ 2)(i.e., E x does not i-imply E 2).
Then
£, < £ 2 (as defined in section 9.3.3).
Proof
a Since E x = ( £',, Condition 2' also reads: ~(E\ =>, £ 2).
Since E 2 = ( E 2 this gives ~ (£,' =>, £ 2), which will be written as
E 2 such that E 2 = ( E 2 one has: - (£', =>,- £?).
7 On the other hand. Property 3 states that
3o such thai oE'{ = E\ implies £[ =>, £V;
--- PAGE 257 ---
KODRATOFFAND GANASCIA 243
therefore
- (E[ =>, E 2) implies that - (3crsuch that oE 2 = E\).
b It follows tha ~ t A VE 2 such that E 2 = e E 2, one has: ~ ( 3 a such that oE 2 = E[).
Finally, since => (A => B), where
A = ( 3 a such that oE'{ = E[) and B = (£?= £ E[),
one also has A => B, which is exactly Condition2.
In conclusion, this last result and Condition 1 show that E x < E 2.
6. Formula Generalization (Second Version)
Recall the definition ofgeneralization that was given in section 9.3.3. A formula was more general than anotherone provided it fulfilledtwo conditions, called
Condition 1 and Condition 2. From Lemma 2, it follows that Conditions 1 and2 of
the firstversion (ofsection9.3.3) areequivalenttoConditions 1 and2' ofLemma 2.
Theorem. The two definitions using Conditions 1 and 2 or 1 and 2' are equivalent.
Oneshouldnoticethatpoint5oftheproofofLemma2explicitlystatesthatthereisno
other substitution in the reverse direction when 2 or 2' is added to Condition 1.
The counterexample given in Kodratoff(1983), where one finds two terms r,
and t2 such that
t t\,
h tl,
0\t t'l,
t'\ tl,
and
t'i tl,
o 2t2 t'{
is precisely a case where 2 or2' is not verified.
References
Chang,C.L.
andLee,R.C.T., SymbolicLogicandMechanicalTheoremProving,AcademicPress,New
York, 1973.
Cohen, P. R.,andFeigenbaum, E. A., TheHandbookofArtificialIntelligence , Vol. 3, Pittman, London,
1982.
Dietterich,G. T., andMichalski, R. S., "InductiveLearningofStructural Descriptions: EvaluationCriteriaandComparativeReviewofSelectedMethods,"ArtificialIntelligence, Vol. 16, pp. 257-94,
1981.
Hayes-Roth, E, and McDermott, J., 'An Interference MatchingTechnique forInducingAbstractions,"
CommunicationsoftheACM, Vol. 21, pp. 401-11, 1978.
--- PAGE 258 ---
244 CHAPTER 9: IMPROVINGTHEGENERALIZATION STEP IN LEARNING
Holte,R.C, "ArtificialIntelligenceApproachestoConceptLearning,"inAdvancedDigitalInformation
Systems, I. Alexander(Ed.), Prentice-Hall, EnglewoodCliffs, N. J., 1984.
Hsiang,J., "TopicsinAutomatedTheoremProvingandProgramGeneration,"Ph.D.diss., Departmentof
ComputerScience, UniversityofIllinoisatUrbana-Champaign, 1982.
Kodratoff, Y., "Generalizingand ParticularizingasTechniquesofLearning," ComputersandArtificial
Intelligence, Vol. 2, pp.417-41, 1983.
Kodratoff, Y.; Ganascia,J. G.;Clavieras, B.; Bollinger, T.; andTecuci,G., "CarefulGeneralizationfor
ConceptLearning,"ProceedingsoftheSixthEuropeanConferenceonArtificialIntelligence,Pisa,
pp. 483-92, 1984.
Michalski, R. S., "ATheoryandMethodologyofInductiveLearning," inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski, J. G. Carbonell, andT. M. Mitchell (Eds.), Tioga,
PaloAlto,Calif., 1983.
, "InductiveLearningasRule-GuidedTransformationofSymbolic Descriptions: ATheory and
Implementation," AutomaticProgram Construction Techniques, A. Bierman, G. Guiho, and Y
Kodratoff(Eds.), Macmillan, NewYork, 1984.
Mitchell,T. M. , "LearningandProblemSolving,"ProceedingsoftheEighthIJCAI, Karlsruhe,W.Ger.
pp. 1139-69, 1983.
Plotkin,G. D, "ANoteonInductiveGeneralization,"inMachineIntelligence,B. MeltzerandD. Michie
(Eds.), AmericanElsevier, NewYork, 1970.
Stickel,M. E., "AUnificationAlgorithmforAssociative-CommutativeFunctions,"JournaloftheACM,
Vol. 28,423-43, 1981.
Vere, S. A., "MultilevelCounterfactualsforGeneralizationsofRelational Conceptsand Productions,"
ArtificialIntelligence, Vol. 14,pp. 139-64, 1980.
, "ConstrainedN-to-1 Generalizations," Draft,23February 1981.
Winston, P. H., "Learning Structural Descriptions from Examples," in The Psychology ofComputer
Vision, P. H. Winston(Ed.), McGraw-Hill, NewYork, 1975.
--- PAGE 259 ---
PART
THREE
COGNITIVE ASPECTS OF
LEARNING
--- PAGE 260 ---
--- PAGE 261 ---
THE CHUNKING OF GOAL
HIERARCHIES:
A Generalized Model of Practice
Paul S. Rosenbloom
Stanford University
Allen Newell
Carnegie-Mellon University
Abstract
Thischapterdescribesrecentadvancesinthespecificationandimplementation
ofamodelofpractice. Inpreviousworktheauthorsshowedthatthereisaubiquitous
regularity underlyinghumanpractice, referredtoasthepowerlawofpractice. They
alsodevelopedanabstractmodelofpractice, calledthechunkingtheoryoflearning.
This previous work established the feasibility ofthe chunking theory for a single
1023-choicereaction-timetask, buttheimplementationwasspecifictothatonetask.
Inthecurrentworkamodifiedformulationofthechunkingtheory isdevelopedthat
allows a more general implementation. In this formulation, task algorithms are
expressed in terms ofhierarchical goal structures. These algorithms are simulated
within a goal-based production-system architecture designed for this purpose.
Chunking occurs during taskperformance in terms ofthe parameters and results of
the goals experienced. It improves the performance of the system by gradually
reducingthe needtodecomposegoals intotheirsubgoals. This model hasbeen successfully appliedtothetaskemployed inthe previous workandtoa set ofstimulusresponse compatibility tasks.
--- PAGE 262 ---
248 CHAPTER 10: THE CHUNKING OF GOAL HIERARCHIES
INTRODUCTION
10.1
How can systems-both natural and artificial-improve their own performance? At least fornatural systems (people, forexample), we know thatpractice is
effective. A system is engaged inpractice when it repeatedly performs onetask or a
set ofsimilar tasks. Recently, Newell and Rosenbloom (1981) brought together the
evidencethatthereisaubiquitouslaw-thepowerlawofpractice-i\\dXcharacterizes
the improvements in human performance during practice. The law states that when
human performance is measured in terms ofthe time needed to perform a task, it
improvesasapower-lawfunctionofthenumberoftimesthetaskhasbeenperformed
(called the trialnumber). This resultholds overthe entire domain ofhuman performance, including both purely perceptual tasks, such as target detection (Neisser,
Novick, and Lazar, 1963), and purely cognitive tasks, such as supplying justifications for geometric proofs (Neves and Anderson, 1981).
Theubiquityofthepowerlawofpracticesuggeststhatit mayreflectsomething
in the underlying cognitive architecture. The nature ofthe architecture is offundamental importance forboth artificial intelligence and psychology (in fact, for all of
cognitivescience; seeNewell, 1973; Anderson, 1983a). Itprovidesthecontrol structure within which thought occurs, determining which computations are easy and
inexpensive as well as what errors will be made and when. Two important ingredientsintherecentsuccessofexpertsystemscomefromfundamentalworkonthecognitivearchitecture; specifically, the developmentofproduction systems (Newell and
Simon, 1972; Newell, 1973)andgoal-structuredproblemsolving(ErnstandNewell,
1969; Newell and Simon, 1972). This chapterdiscusses recenteffortstotake advantage ofthe powerlaw ofpracticeby using it as agenerator forageneral productionsystempractice mechanism. This is ahighly constrainedtaskbecause ofthe paucity
ofplausiblepracticemodelsthatcanproducepower-lawpracticecurves(Newelland
Rosenbloom, 1981).
Asabeginning, NewellandRosenbloom(1981)developedanabstractmodelof
practicebasedontheconceptofchunking-aconceptalreadyestablishedtobeubiquitous in human performance-and derived from it a practice equation capable of
closely mimickingapowerlaw. Itwashypothesizedthatthismodel formedthebasis
for the performance improvements brought about by practice. In sections 10.2 and
10.3thisworkonthepowerlawandtheabstractformulationofthechunkingtheory
briefly summarized.
Rosenbloom and Newell (1982a, 1982b) took the analysis one step further by
showinghowthe chunkingtheoryoflearningcouldbe implemented fora single psychological task within a highly parallel, activation-based production system called
XAPS2. This work established more securely that the theory is a viable model o\
human practice In showing how it could actually be applied to a task to produce
power law practice curves. By producing a working system, it also established the
theor) 's \ ability as a practice mechanism for artificial systems.
--- PAGE 263 ---
ROSENBLOOM AND NEWELL 249
The principal weakness ofthe work done up to that point was the heavy task
dependence ofthe implementation. Both the representation used for describing the
tasktobe performed andthe chunking mechanism itselfhadbuilt intothem knowledgeaboutthespecifictaskandhow itshouldbeperformed. Theworkreportedhere
is focusedonthe removal ofthis weaknessbythedevelopmentofgeneralized, taskindependent models ofperformance and practice.
Thisgeneralizationprocesshasbeendrivenbytheuseofasetoftasksthatfall
within aneighborhood aroundthe taskpreviously modeled. Thattask sits within an
experimental domain widely used in psychology, the domain ofreaction-time tasks
(Woodworth and Schlosberg, 1954). Reaction-timetasks involvethepresentationto
a subject ofstimulus display-such as an array oflights, a string ofcharacters on a
computer terminal, or a spoken word-for which a specific "correct" response is
expected. Theresponsemaybespoken, manual(suchaspressingabutton, pushinga
lever, ortyping), orsomethingquitedifferent. Fromthesubject'sreactiontime-the
time ittakestomakethe response-anderrorrate, it ispossibletodraw conclusions
about the nature ofthe subject's cognitive processing.
The particular task, as performed by Seibel (1963), was a 1023-choice
reaction-time task. It involved ten lights (strung out horizontally) and ten buttons,
witheachbuttonrightbelowalight. Oneachtrialofthetasksomeofthelightscame
on while the rest remained off. The subject's task was to respond as rapidly as possible by pressing the buttons corresponding to the lights that were on. There were
2'° - 1, or 1023, possiblesituationswithwhichthesubjecthadtodeal(excludingthe
one in which alltenlights wereoff). Rosenbloom (1983) showedthatageneral task
representation based on the concept ofgoal hierarchies (discussed in section 10.4)
could be developed forthe performance ofthis task.
In agoal hierarchy, the root nodeexpresses adesiretodoatask. Ateachlevel
furtherdowninthehierarchy,thegoalsatthelevelabovearedecomposedintoasetof
smallergoalstobe achieved. Decompositioncontinues until the goals at somelevel
canbeachieveddirectly. Thisisacommoncontrolstructureforthekindsofcomplex
problem-solvingsystemsfoundinartificialintelligence,butthisisthefirsttimethey
have been applied to the domain ofreaction-timetasks.
Goalhierarchiesalsoprovidedthebasisformodelsofasetofrelatedreactiontimetasksknownasstimulus-responsecompatibilitytasks. Thesetasksinvolvefixed
sets ofstimuli and responses and a mapping betweenthem that is manipulated. The
main phenomenon is that more complex and/or "counterintuitive" relationships
betweenstimuliandresponsesleadtolongerreactiontimesandmoreerror. Amodel
ofstimulus-responsecompatibility based on goal hierarchies provides excellent fits
to the human reaction-time data (Rosenbloom, 1983).
Thegeneralizedpracticemechanism(describedinsection 10.5) isgroundedin
this goal-based representation of task performance. It resembles a form of storeversus-compute trade-off, in which composite structures (chunks) are created that
relate patterns ofgoal parameters topatterns ofgoal results.
--- PAGE 264 ---
250 CHAPTER 10: THECHUNKING OF GOAL HIERARCHIES
These chunking and goal-processing mechanisms are evaluated by implementing them as part of the architecture of the XAPS3 production system. The
XAPS3 architecture is an evolutionary development from the XAPS2 architecture.
Onlythosechangesrequiredbytheneedsofchunkingandgoalprocessinghavebeen
made. Thisarchitectureisdescribedinsection 10.6. Fromthisimplementationsimulated practice results have been generated and analyzed forthe Seibel and compatibility experiments (section 10.7).]
Following the analysis ofthe model, this work is placed in perspective by
relatingthechunkingtheorytopreviousworkonlearningmechanisms(section
10.8).
The theory stakes out an intermediary position among four previously disparate
mechanisms, bringing out an unexpected commonality among them.
Beforeconcludingand summarizing (section 10.10), some final comments are
presented on ways in which the scope ofthe chunking theory can be expanded to
cover more thanjust the speeding up ofexisting algorithms (section 10.9). Specifically, the authors describe a way in which chunking might be led to perform other
varietiesoflearning, suchasgeneralization,discrimination,andmethodacquisition.
10.2 THE POWER LAW OF PRACTICE
Performance improves with practice. More precisely, the time needed to perform a task decreases as a power-law function ofthe number oftimes the task has
been performed. This basic law, the power law ofpractice, has been known since
Snoddy(1926). Thislawwasoriginallyrecognizedinthedomainofmotorskills, but
it has recently become clear that it holds over a much wider range ofhuman tasks,
possiblyextendingtothefull rangeofhumanperformance. NewellandRosenbloom
(1981) brought together the evidence for this law from tasks involving perceptualmotor skills (Snoddy, 1926; Crossman, 1959), perception (Kolers, 1975; Neisser,
Novick, and Lazar, 1963), motorbehavior (Card, English, andBurr, 1978), elementary decisions (Seibel, 1963), memory (Anderson, 1980), routine cognitive skill
(Moran, 1980), and problem solving (Neves and Anderson, 1981; Newell and
Rosenbloom, 1981).
Practice curves are generated by plotting task performance against trial
number. This cannot be done without assuming some specific measure of performance. There are many possibilities for such a measure, including such things as
quantity produced per unit time and number oferrors per trial. The power law of
'A morecomprehensivepresentationanddiscussionofthese resultscanbe found in Rosenbloom 11983).
--- PAGE 265 ---
ROSENBLOOM AND NEWELL 251
practiceisdefinedintermsofthetimetoperformthetaskonatrial. Itstatesthatthe
time toperform the task (T) is a power-law function ofthetrial number (N):
T= BN
(1)
As shownby the following logtransformofEquation 1, power-law functionsplotas
straight lines on log-log paper:
log(r) = log(fl) + (-a)log(AO (2)
Figure 10-1 shows the practice curve from one subject in Kolers' study (1975) of
reading inverted texts-each line of text on the page was turned upside down-as
plotted on log-log paper. The solid line represents thepower-law fitto this data. Its
linearity is clear (r2 = 0.932).
Many practice curves are linear (in log-log coordinates) over much of their
range but show a flattening at their two ends. These deviations can be removed by
usingafour-parametergeneralizedpower-lawfunction. Oneofthetwonewparameters (,4)takesintoaccountthattheasymptoteoflearningcanbegreaterthanzero. In
general, there is a nonzero minimum bound on performance time, determined by
basic physiological limitations and/or device limitations-if, for example, the subject must operate a machine. The other added parameter (E) is required because
powerlawsarenottranslationinvariant. Practiceoccurringbeforetheofficialbeginningoftheexperiment-evenifitconsistsonlyoftransferoftrainingfromeveryday
experience-will alter the shape ofthe curve, unless the effect is explicitly allowed
i i i 1 1 111 i i i i 1 1 1 11 i i i i 1 1 11
10 100 1000
Pagesread
Figure 10-1: Learningtoreadinvertedtext(log-logcoordinates). Plottedfromtheoriginaldatafor
SubjectHA
(Kolers, 1975).
--- PAGE 266 ---
252 CHAPTER 10:THE CHUNKING OF GOAL HIERARCHIES
forby the inclusion ofthis parameter. Augmenting the power-law function by these
two parameters yields the following generalized function:
T= A + B(N + E)
(3)
A generalized power law plots as a straight line on log-log paper once the effects of
theasymptote (A) are removed from the time(T), and the effective number oftrials
priortotheexperiment(E)areaddedtothoseperformedduringtheexperiment(N):
log(7 - A) = log(fi) + (-a) \og(N + E) (4)
Figure 10-2showsapracticecurvefromtheSeibeltask(Seibel, 1963), asfitby
ageneralizedpower-lawfunction (eachdatapointrepresentsthemean reactiontime
over a block of 1023 trials). This curve, which shows flattening at both ends when
plottedasa simplepowerlaw, is now linearoverthewhole range oftrials. As stated
earlier, similar fits are found across all dimensions ofhuman performance. Though
thesefitsareimpressive, it mustbe stressedthatthepowerlaw ofpractice isonly an
empiricallaw. Thetrueunderlyinglaw mustresembleapowerlaw, but it mayhavea
different analytical form.
10.3 THE CHUNKING THEORY OF LEARNING
Thechunkingtheoryoflearningproposesthatpractice improvesperformance
viathe acquisition ofknowledge about patterns in the taskenvironment. Implicit in
this theory is a model oftask performance based on this pattern knowledge. These
s? 10.00
T = 0.32 + 1673(yV + 2440)
1.00
01 _l l 1 1 - 4-JKMK) K).(KX) lOO.(XX)
Trial number{N + £")
Figure 10-2: Optimal general power law in tothe Seibel data dog-logcoordinates)
--- PAGE 267 ---
ROSENBLOOM AND NEWELL 253
patterns are called chunks (Miller, 1956). The theory thus starts from the chunking
hypothesis:
The chunking hypothesis: A human acquires and organizes knowledge ofthe
environmentby forming and storing expressions, called chunks, that are structured collectionsofthe chunks existing atthetimeoflearning.
The existence ofchunks implies that memory is hierarchically structured as a
lattice (tangledhierarchy, acyclicdirectedgraph, and soon) rooted inasetofpreexistingprimitives. A givenchunkcanbeaccessedinatop-downfashion,bydecodinga
chunkofwhich it is apart, orinabottom-up fashion, by encoding fromtheparts of
the chunk. Encoding is a recognition orparsing process.
Theexistenceofchunksdoes notneedtobejustified solely on the basis ofthe
practice curves. Chunks standontheirownasathoroughly documented component
ofhuman performance (Miller, 1956; DeGroot, 1965; Bower and Winzenz, 1969;
Johnson, 1972; Chase and Simon, 1973; Chase and Ericsson, 1981). The traditional
viewofchunks isthatthey aredata structures representing acombination ofseveral
items. Forexample, inonesetofclassicexperiments, Bowerandcolleagues (Bower,
1972; BowerandSpringston, 1970; BowerandWinzenz, 1969) showedthatrecall of
stringsofnumbersorlettersisstrongly influencedbythesegmentationofthestring.
Ifthe segmentation corresponds to a previously learned grouping ofthe items (for
example, FBI-PHD-TWA-IBM), performance is better than if no such relation
obtains (FB-IPH-DTW-AIB-M). These results were interpreted as evidence for
segmentation-guided chunking offamiliar strings. By replacing a string ofseveral
letterswithasinglechunk, thesubject'smemoryloadisreduced, allowingmoreletters to be remembered. At recall time the chunks are decoded to yield the original
items to be recalled.
The chunking theory oflearning proposes two modifications to this classical
view. The first change is the assumption that there is not a single symbol (chunk) to
whichthe items areencoded and from whichthey can laterbe decoded. As a simple
example, the process of reading the string IBM out loud is more thanjust the encoding ofthe three letters into a chunk, followed by the subsequent decoding to the
three letters. What needs to be decoded is not the visual representation ofIBM, but
the articulatory representation-allowing the subject to say "IBM."
Based on this consideration, the chunking theory assumes that there are two
symbols for each chunk-a stimulus symbol and a response symbol. The process of
usingachunkconsistsofencodingthestimulusitemstothestimulussymbol(amanyone mapping), mapping the stimulus symbol to the response symbol (a one-one
mapping), and decoding the response symbol to the response items ( a one-many
mapping)(seefigure 10-3). Encodinganddecodingarefastparallelhierarchicalprocesses, and the mapping serves as a (serial) point ofcontrol at which the choice of
responsecanbemade. Acquisitionofachunkspeedsupperformancebyreducingthe
numberofmappingstobeperformed. Intheexample infigure 10-3, beforethechunk
--- PAGE 268 ---
254 CHAPTER 10: THECHUNKING OF GOAL HIERARCHIES
Map
I B M "eye" "bee" "em"
Figure 10-3: Athree-partchunkforthearticulationofthevisual string "IBM."
isacquired,threemappings(oneforeachletter)arerequired. Afteracquisitionofthe
chunk, one mapping suffices.
Theseconddifferencebetweenthisproposalandtheclassicalviewofchunking
is the idea that the chunk consists ofthe three processes (encoding, mapping, and
decoding), notjust the symbols. This is reallyjust a shift ofemphasis; chunks are
viewed as the processes ratherthanjust as the results ofthe processes.
GOAL-STRUCTURED PERFORMANCE MODELS
10.4
Initscurrentformulation, chunkingexplainshowperformanceonataskcanbe
spedupwithpractice. Itdoesnotexplainhowtheorganismfirstlearnstodothetask
(but see the discussion in section 10.9). Consequently, each task simulation must be
initialized with a performance model forthetask. What is needed-and is provided
by the notion ofagoalhierarchy is a general, task-independent representation for
theseperformancemodels. Goalhierarchiesarefrequentlyfoundinartificial intelligence systems, but they have not previously been employed in the modeling ofthe
kinds ofreaction-time tasks dealt with here.
Goal hierarchies are built out of goals; each goal is a data structure representing a desired state ofaffairs. A goal is not a procedure for bringing about that
state; it is only a description ofthe state. In order for the goal state to be brought
about, there mustbeamethodassociatedwiththegoal. The methodcouldbe a rigid
algorithm, oritcouldbeoneofthemoreflexibleweakmethods(Newell, 1969), such
as means-endsanalysis (ErnstandNewell, 1969)orheuristic search (Nilsson. 1971).
Inthediscussionthatfollows,theproperlydistinctnotionsofgoalandmethodwillbe
conflated together into a single active concept for the sake of convenience. These
conflated "goals" areactiveprocessesthattakeasetofparametersandreturna set o\'
results.
When agoalcanbedecomposed intoasetofsimplergoals(Nilsson, 1971) and
those goals can be decomposed even further, a goal hierarchy results. In its simplest
AND
form, as an hierarchy, a goal is successful ifall ofits subgoals are successful.
The structure tobe described here moreclosely resemblesan AND/OR hierarchy, in
which some goals succeed only ifall oftheirsubgoalssucceed, andothers succeed if
--- PAGE 269 ---
ROSENBLOOM AND NEWELL 255
any one oftheir subgoals succeed. A terminalgoal is reached when the goal can be
fulfilled directly, without the need for further decomposition.
Weuseadepth-firststrategyforprocessingagoalhierarchy, inwhichthemost
recently generated (the deepest) goal is always selected as the next goal to process.
Withadepth-firstparadigmthereisalwaysexactlyonegoalbeingactivelyworkedon
at any point in time. We will refertothis goal as theactive orcurrentgoal. When a
subgoal becomes the active goal, the parent goal ofthat subgoal is suspendeduntil
controlisreturnedtoitbycompletionofthesubgoal, atwhichpointitagainbecomes
the current goal. On completion, the subgoal will have either succeeded orfailed.
The controlstack specifies the location in the hierarchy at which control currently
resides. Itdoesthisby maintainingthepathfromtherootgoalofthehierarchytothe
currentgoal. Thispath consists ofthe activegoal andall ofits suspendedancestors.
Figure 10-4 shows two different representations of a goal hierarchy for the
Seibel(1963) 1023-choicereaction-timetask. Atthetopofthefigure,thehierarchyis
shown as a tree structure. At the bottom ofthe figure, the goals are shown in their
depth-first processing order. In both representations the boldface goals are the
7*Ks> ^ ^13/j
|l4/j"
| |
- *-Predicate
- s>-Branchonsuccess ram
f>-
Branchonfailure
1. Do-Lights-If-Any(Min-X, Max-X)
2. No-Light-On? (Min-X, Max-X)
if-failedNo-Light-On?then
3. Do-Lights(Min-X, Max-X)
4. No-Light-Off? (Min-X, Max-X)
if-succeededNo-Light-Off?then
5. Do-Press-All-Buttons(Min-X, Max-X)
IF-FAILEDNo-Light-Off?THEN
6. Do-Off-And-On(Min-X, Max-X)
7 One-Light-On?(Min-X, Max-X)
if-succeededOne-Light-On?then
8. Press-Button-Under-On-Light(Min-X, Max-X)
9. Get-On-Light-X (Min-X, Max-X)
10 Get-On-Light-Stimulus(Min-X, Max X)
11. Get-Stimulus-X
12. Press-Button-At-X
if-failedOne-Light-On?then
13. Do-Lights-If-Any(Min-X, (Min-X + Max-X]/2)
14. Do-Lights-If-Any ([Min-X -I- Max-X]/2, Max-X)
Figure 10-4: GoalhierarchyfortheSeibel (1963)task.
--- PAGE 270 ---
256 CHAPTER 10: THE CHUNKING OF GOAL HIERARCHIES
terminals-thosegoalsthatcanbefulfilleddirectly. Thelabeledarrows(eithersorf)
represent branch points in the hierarchy. With simple depth-first processing, the
children ofa node are processed in a strict left-to-right fashion. In this work, this
style ofcontrol has been augmented by allowing the left-to-right processing to be
conditioned on the success or failure ofthe previous child (actually, ofany previous
goal for which this information is still available). For example, goal 2 (No-LightOn?) tests whether there are any lights on within a specific region ofthe display of
lights. Only ifthis goal fails should goal 3 (Do-Lights) be attempted. Ifgoal 2 succeeds,thentherearenolightsonthatneedtobeprocessedandtheparentgoal (goal 1:
Do-Lights-If-Any) can be terminated successfully. Those goals that do not return a
resultarecalledpredicates(denotedbyanasteriskinfigure 10-4) andareusedtotest
thetruthofvariousconditions. Itisonlythesuccessorfailureofpredicatesthatmatters(asthebasisforabranch). Ifthepredicatesucceeds, thentheconditionistrue. If
it fails, the reason could be either that the condition is false or that something went
wrong during the attempt.
The goal structure in figure 10-4 is based on a recursive divide-and-conquer
algorithm in which the stimulus display is broken up into smaller and smaller horizontal segmentsuntilmanageablepiecesaregenerated. Therearethreetypesofhorizontal segmentsthathavebeendefinedasmanageable. Thefirsttypeofmanageable
segment is one in which no lights are on. Such segments require no explicit processing, so the goal just returns with success. The opposite of the first type of
segment-one in which no lights are off-is also manageable. For such a segment,
thesystemgeneratesasingleresponsespecifyingthatapressactionmustoccurinthe
entire regiondefinedby the segment (usingthe Do-Press-All-Buttonsgoal). Specifyingasinglebuttonpressisactuallyaspecialcaseofthis, inwhichthe region isjust
largeenoughtocontaintheonebutton. Allowingmulti-on-lightsegmentstobemanageable implies that sequences ofadjacent on-lights can be pressed simultaneously
even before chunking has begun. Such behavior is seen very early in the trial
sequence forsomesubjects. The remaining manageable segmentsarethosethatcontain exactly one light on. These segments are processed (using the Press-ButtonUnder-On-Light goal) by finding the location ofthat light and generating a button
pressatthatlocation. Ifageneratedsegmentdoesnotmeetanyofthesethreecriteria,
it is unmanageable and is split into two smaller segments.
The recursive aspect ofthe algorithm implies that many different instances of
eachgoal will besimultaneously represented inthe system, although at mostonecan
actually be active. It is necessary to keep track ofwhich goal instance is relevant to
which segment ofthe stimulusdisplay, sothe segment (in terms ofits minimum and
maximum X values) is an explicit parameter to the goals. In addition to the explicit
parameters, agoalcanhaveimplicitlydefinedparameters. Anyobjectexistingbefore
the activation ofthe goal (i.e., as part of the goal's initial state) that is examined
during the processing ofthe goal-suchasastimulus light-is an implicit parameter
ofthegoal. Implicit parametersaredynamicallydeterminedbytheactual processing
ofthe goal.
--- PAGE 271 ---
ROSENBLOOM AND NEWELL 257
second implication ofthe recursion inthis hierarchy is thatthe system does
notstartoffwithaconstantbuilt-ingoalhierarchy. Instead, ithasageneratorofgoal
hierarchies. That chunking works on such a structure is important for any claims
aboutthepotential generality ofthe mechanism (formore onthis, see section 10.9).
The recursion occurs atgoals 13 and 14 in figure 10-4. They are repetitions of
thetopmostgoalinthehierarchy(Do-Lights-If-Any),butthescopeofeachislimited
to one-halfofthe display currently being processed. The numeric computation to
obtainthemiddleofthesegment(involvinganadditionandadivision), whileonthe
surface too powerful a computation to appear where it does, is only intended as an
approximation to a process that divides the stimulus display into two (or three)
roughly equal parts.
Inadditiontothegoalhierarchy, themodelassumesa workingmemoryforthe
short-termstorageofinformationrelevanttotheprocessingthatisgoingon. Foreach
goal, the working memory is logicallypartitioned intotwo components-the initial
stateandthe localstate. The initial state consists ofthe data existing atthe time the
goal is first activated. The remainder ofthe working memory-consisting ofthose
pieces ofdata created during the processing ofthe goal-makes up its local state.
Onlythelocal stateofagoalcanbemodifiedduringtheprocessing ofthatgoal; the
initial state can be examined, but it cannot be modified. The modularity resulting
from this scoping rule increases the likelihood that an arbitrary set ofgoals can be
pursuedwithouttheirinterferingwitheachother. This modularity isalso important
in insuring correct performance ofthe chunking mechanism.
Each datum in the working memory is relevant in a particular temporalprocessingcontext. Aslongasthedatum'screatorgoalisinthecontrolstack, thesystem
isworkingeitheronthatgoalorononeofitsdescendants. Thedatummayberelevant
at any point in this processing. However, once the goal disappears from the control
stack, itisnolongerbeingpursued. Thedatumwillmostlikelynolongerberelevant.
Theprocessing contextofadatum isthatperiodofprocessing during which its creatorgoalisinthecontrolstack. Onceapieceofinformationbecomesoutofcontextitscreatorisnolongerpartofthecontrolstack-itusuallycanbesafelydeletedfrom
the working memory. Datathatcannotbe deleted-becausethey are neededoutside
ofthe context in which they were created-are called the results ofthe goal. Their
continuedpresenceintheworkingmemory isinsuredbychangingtheircontexttobe
the context ofthe parent goal.
10.5 CHUNKING ON GOAL HIERARCHIES
Given atask-independent organization forperformance models, it is possible
toreturntotheoriginalobjectiveofdescribingatask-independentformulationofthe
chunkingtheory. Inthe chunking mechanismdescribedby Rosenbloomand Newell
(1982a, 1982b), chunks related patterns ofstimuli (lights) to patterns ofresponses
--- PAGE 272 ---
258 CHAPTER 10: THECHUNKING OF GOAL HIERARCHIES
(button presses). The goal-oriented formulation is obtained by altering this definition slightly so that chunks relate patterns of goal parameters to patterns of goal
results. Eachchunkimprovestheperformanceofthesystembyeliminatingtheneed
to process fully a specific instance (a combination ofparametervalues) ofa particulargoal. Itreplacesthenormalprocessing(decompositionintosubgoalsfornonterminalgoalsanddirectexecutionofanactionforterminalgoals)withadirectconnection between the relevant parameter values and results. A goal can (and almost
certainly will) have more than one possible chunk; each combination ofparameter
values requires its own chunk.
As with the abstractcharacterization ofthechunking theory, each chunkconsists ofthree components: encoding, decoding, and connection (or mapping). The
goal's parametervalues formthebasis fortheencodingcomponent. Giventhe presenceofthosevaluesintheworkingmemory, theencodingcomponentgeneratesanew
objectrepresentingtheircombination. Encodingisaparallel, goal-independent, datadriven process. Every encoding component executes as soon as appropriate, irrespectiveofwhateverelseishappeninginthesystem. Theresultsofencodingcomponents can themselves become parameter values of other goals, leading to a
hierarchical encoding process.
The results ofthe goal form the basis forthe decoding component. Given the
presence ofan encoded result-object in the working memory, the decoding component generates the actual results returned by the goal. Decoding occurs when the
results are needed. As with encoding, decoding is aparallel, goal-independent process. The set ofdecoding components forms ahierarchical structure in which complex results aredecodedto simplerones, which are then decoded even further.
Theconnectioncomponentofthechunkgeneratestheencodedresult fromthe
encoded parameter. Connections provide a locus ofcontrol by occurring serially,
underthecontrolofthegoals.Thusaconnectioncanbemadeonlywhenthesystemis
workingonthegoalforwhichthechunkwasformed(andaftertheencodingcomponenthasexecuted). Thisinsuresthat, eventhoughencodinganddecodingareuncontrolled, only appropriate results are generated.
A chunkcan becreated foragoal whenthe following twoconditions are met:
(1) the goal hasjust completed successfully, and (2) all ofthe goal's subgoals were
themselvesprocessedbychunks. Thefirstconditioninsuresboththatchunksarecreatedforappropriategoalsandthatthechunkiscreatedatatimewhentheinformation
requiredforthechunkisavailable. Thesecondconditioncauseschunkstobecreated
bottom up in the goal hierarchy. It isthisbottom-upaspect ofchunking that leads to
hierarchical encoding and decoding networks. However, notice that bottom-up
chunking does not imply that all low-level chunks are learned before any high-level
chunksarelearned,oreventhatallofthechunksmustbelearned foraSllbgoaJ before
anycanbelearnedforitsparentgoal. Thesecondconditiononchunkcreationmerely
statesthatchunksmustexist forthegoal'ssubgoalsinthecurrentsituation. Whether
otherchunks exist ordo not exist for the subgoals is irrelevant.
--- PAGE 273 ---
ROSENBLOOM AND NEWELL 259
1. Compute-Average-Of-Two-Numbers
2 Compute-Sum-Of-Two-Numbers
3. Divide-Sum-By-2
Figure 10-5: Asimplethree-goal hierarchy fortheaveragingoftwonumbers.
Givenenough practice withenoughtaskvariations, all ofthe situations forall
of the goals in the hierarchy will be chunked, and asymptotic behavior will be
reachedforthetask. The amountoftimethistakes depends onthe numberofgoals,
the numberofsituations foreach goal, how frequently the different situations arise,
and whether chunks are created wheneverthey can be.
The three-goal hierarchy shown in figure 10-5 provides a simple example of
howchunkingworks. This structurecomputestheaverageoftwonumbers. Thetoplevel goal (Computer-Average-Of-Two-Numbers) takes as parameters the two numberstobeaveragedandreturnstoasingle result, which istheirmean. The firstsubgoal (Compute-Sum-Of-Two-Numbers)performsthefirsthalfofthecomputation.
Ittakesthetwonumbersasparametersandreturnstheirsumasitsresult. Thesecond
subgoalfinishesthecomputationbytakingthesumasaparameterandreturninghalf
ofit as its result.
Supposethatthefirsttaskistoaveragethenumbers3and7. Controlwouldpass
from goal 1 to goal 2. When goal 2 finishes and returns its result of 10, a chunk of
threecomponents iscreated (bottomleftoffigure 10-6). Anencodingcomponent is
createdthatencodesthetwoparameters(3and7) intoanewsymbol (El). Itexecutes
as soon as it is created, because the parameters are in the working memory. A
decoding component is created that decodes from a second new symbol (Dl) to the
result(10).
Aconnectioncomponent(thehorizontallinewiththegoalnameaboveit
and goal number below it) is created that generates the result symbol (Dl) when it
detects boththepresenceoftheencodedparameter (El) andthat goal 2 isthe active
goal. The connection does not execute immediately because goal 2 is already complete when the chunk is created.
Followingtheterminationofgoal2, goal 1 isreactivatedbutthenissuspended
in favor of goal 3 (Divide-Sum-By-2). When this goal terminates successfully
(returningthenumber 5), achunkiscreated forit(bottom rightoffigure 10-6). The
encodingcomponentencodesthe number 10 intothe symbol E2; thedecodingcomponentdecodesfromthesymbolD2tothenumber5; andtheconnectioncomponent
connects E2 to D2 (in the presence ofan active goal 3). In contrastto the chunk for
goal 1,thischunkcanbeusedinmorethanonetasksituation. Itcanbeusedwhenever
goal 1 generatesasumof10, whether itdoesitbyadding3and7, 5and5, oranyother
pair ofnumbers. This is a form oftransferoftraining.
Following the termination ofgoal 3, goal 1 is reactivated and terminates successfully (returning the number 5). No chunk is created for goal 1 because its subgoals were not processed by chunks. Atthis point, the task is complete.
--- PAGE 274 ---
260 CHAPTER 10: THECHUNKING OFGOAL HIERARCHIES
Compute-Average-Of-Two-Numbers
J GoaM
Comoute-Sum-Of-Two-Numbers Divide-Sum-By-2
El w Dl E2 D2
/\ \ V /
Goal 2 Goal 3
3 7 10
Figure 10-6: Samplechunkscreatedforthehierarchy infigure 10-5.
Given whatwas learnedduringthe performanceofthis task, the nexttime the
same task is performed things will go differently. As soon as the task is restarted
(again with the values 3 and 7), the encoding component from the chunk for goal 2
executes, placingEl intheworkingmemory. Goal 1 isactivatedandthensuspended
infavorofgoal2. Atthispoint,theconnectioncomponentforgoal2executes, generating Dl andsuccessfullycompletinggoal2. Dl isdecodedtothe number 10, which
is then immediately reencoded to E2 by the encoding component for goal 3. Followingthe subsequent reactivation and suspensionofgoal 1, goal 3 is activated. The
connection component for goal 3 executes, generating D2 and returning D2 as the
resulttogoal 1. This time, when goal 1 terminates, achunk is created (topoffigure
10-6), because both ofthe subgoals were processed by chunks.
Theencodingcomponentforthischunkbuilds upontheexistingencodingsby
encoding El to a new symbol (E3); it does not go straight from goal l's primitive
parameters (3 and 7). This happens (and causes hierarchical encoding) because, for
this instanceofgoal 1, El isthe implicitparameter, not 3 and 7. Recall from section
10.4that the implicit parameters ofagoal consist ofthose pieces ofthe goal's initial
state thatareexaminedduringthe goal's performance. El isgenerated beforegoal 1
is activated (so it is part ofthe goal's initial state) and examined by the connection
componentforgoal2. Ontheotherhand, neitheroftheobjectsrepresentingthenumbers 3 and7 isexaminedduringthe processingofgoal 1. Therefore, El isan implicit
parameter (and included in the chunk), and the numbers 3 and 7 are not.
The decoding component is created in an analogous hierarchical fashion. It
decodes froma new symbol (D3)toD2. Thisoccursbecause D2 (and not the number
5) is the result ofgoal 1. It never became necessary to decode D2. so it was passed
directly up as the result of both goals 3 and 1. The connection component ofthis
chunk links E3 to 1)3 in a straightforward manner.
--- PAGE 275 ---
ROSENBLOOM AND NEWELL 261
Ifthesametaskisperformedyetagain, theencodingcomponentsimmediately
generate El, followedby E3. Goal 1 is activated, and itsconnectioncomponentexecutes,generatingD3andcompletinggoal
Iftheresultisneededbysomepartofthe
system outside ofthe hierarchy, it will be decoded to D2 and then to the number 5.
The example that we have just gone through outlines the basics of how the
chunkingmechanismworks. Thenextstepistolookatthemorecomplexsituationof
theSeibeltask(figure 10-4). Chunkingstartsinthisstructurewiththeterminalgoals
(numbers2,4, 5, 7, 10, 11, and 12). Considergoal 11 (Get-Stimulus-X) , forexample.
Successfulcompletionofthisgoal requires retrievingfromtheworkingmemorythe
representation ofa stimulus that has been perceived and then generating a piece of
informationrepresentingthehorizontallocation (X) ofthatstimulus. Theparameter
forthisgoal isthe stimulus-animplicitparameter-andthe resultisthe location. In
thechunkforthissituation, theencodinganddecodingcomponentsaretrivial. They
simply recode from the single parameter to a new symbol and from a second new
symbol to the result. The connection component tests for the presence ofan active
Get-Stimulus-X goal and the encoding symbol and produces the decoding symbol.
Goal 10 (Get-On-Light-Stimulus) presents a slightly more complicated case
thangoal 11. Thegoalhasbothexplicitandimplicitparameters. Theexplicitparameters (Min-X and Max-X) define the region ofthe stimulus display in which an onlightshouldbefound. Theimplicitparameteristheactualexternal representation(in
thestimulusdisplay)oftheon-lightthatisfound. Theresultofthisgoalistheinternal
representationoftheon-lightasitappearsintheworkingmemory. Justasbefore, the
implicit parameter and the result form the basis for trivial encoding and decoding
components. However,theexplicitparametersaredifferent. Theydonotexistasseparateentities inthe working memory; insteadthey actas ifthey were augmentations
tothenameofthegoal. Therefore,theyappearalongwiththenameofthegoalaspart
ofthe connection component.
Asa final example ofthe workings ofthechunking mechanism, considerhow
it creates chunks consisting ofsequences oflight-button pairs-the kind ofchunks
producedbythechunkingmechanisminRosenbloomandNewell (1982a). Thelocus
ofthesechunkscanbefoundattherecursivestepinthegoalhierarchy. Theroot(and
recursive)goal inthehierarchy (Do-Lights-If-Any) hasimplicitparametersthatrepresentlightsinthestimulusdisplay, and itgeneratesresultsthatarethebuttonpresses
forthose lights. Theearliest chunks that can be created forthis goal are those at the
bottomoftherecursion-thatis,goalstoprocessmanageablesegmentsofthedisplay.
Eachofthesechunkswill representasingleon-lightinaregion, aregionofsolidonlights, or a region with no on-lights. Once the chunks exist forgoals 13 and 14 (and
their sibling goal 7, the predicate One-Light-On?) in a single situation, the parent
goal (goal6: Do-Off-And-On)canbechunked. Thisyieldsanewchunkforthecombinedsituationinbothsegments. Thisprocesscontinuesupthehierarchyuntilgoal
ischunkedforthatlevelofrecursion. Butgoal 1 atthatlevelisjustgoal 13or 14atthe
next level up. Therefore, gradually (since these chunks are acquired one at a time),
the level ofaggregation ofsegments covered by chunks increases.
--- PAGE 276 ---
262 CHAPTER 10: THE CHUNKING OFGOAL HIERARCHIES
This process always leads to light-button chunks for contiguous light-button
pairs. Itdoesnotleadtochunksfordisjointpatternssuchasthetwoextremerightand
leftlight-buttonpairs. Thisisnotalimitationonthegeneralityofthechunkingmechanism. Instead, itisafunctionofthegoal structureemployed. A differentgoal structure (reflectingadifferentprocessing strategy)couldleadtothecreationofsuchdisjoint chunk patterns.
One of the strong task dependencies present in the Rosenbloom and Newell
(1982a) chunking mechanism was thatencoding productionshadtohaveacondition
addedtothemthat insuredthatnoon-lightappearedbetweenthetwopatterns being
chunked together. This was so even though the information that this condition was
necessary appeared nowhere in the task algorithm. In the hierarchy in figure 10-4.
suchconditionsaregeneratednaturallyfromthegoalsforprocessingsegmentsofthe
display with no on-lights in them. We see that the task dependencies show up in the
goal hierarchy, not in the chunking mechanism.
The following list of points summarizes the key aspects of chunking as it
applies to goal hierarchies:
• Eachchunkrepresentsaspecificgoalwithaspecificsetofparametervalues. It
relates the parameter values to the results ofthe goal.
• Chunks are created through experience with the goals processed.
• Chunks are created bottom up in the goal hierarchy.
• Chunks consist ofencoding, connection, and decoding components.
• Chunk encoding and decoding are hierarchical, parallel, goal-asynchronous
processes that operate on goal parameters and results, respectively.
• Chunk connection is a serial, goal-synchronous process that generates (possibly encoded) results from (possibly encoded) parameters.
• Chunks improve performance by replacing the normal processing of a goal
(and its subgoals) with the faster processes of encoding, connection, and
decoding.
10.6 THE XAPS3 ARCHITECTURE
Modelinggoal hierarchiesandchunking requiresanarchitecture with specific
properties. The XAPS3 production-system architecture is one such architecture.: It
is a new architecture, but it builds upon the work done in the development of the
See Rosenbloom I I983) fora moredetaileddescriptionofthe XAPS3 architecture. A general introduc
nop toproduction systemscan be found in Waterman and Hayes-Roth(1978).
--- PAGE 277 ---
ROSENBLOOM AND NEWELL 263
XAPS2 architecture (Rosenbloom and Newell, 1982a). The presentation ofXAPS2
begins with a discussion ofa set ofconstraints that must be met by any architecture
within which the chunking theory of learning is implemented. These constraints
state thatthe model mustcontainboth parallelismand abottleneck (theparalleland
bottleneck constraints). The parallelism must appear in all aspects ofthe system's
performance including cognitive processing (the cognitive-parallelism constraint),
and the bottleneck must occur after chunk encoding and before chunk decoding (the
encodinganddecodingconstraints). Itisnotclaimedthattheseconstraintsare known
tobe necessary-the arguments are notthattight-butthis permits (andencourages)
attempts to show how the constraints can be circumvented.
Thesameconstraintsstill hold-astheymustiftheyarereallyconstraints-for
thedesignoftheXAPS3 architecture. More recently, twonewconstraintshavebeen
formulated from the consequences ofthe chunking theory. These new constraints
rule out XAPS2 and have led to the design of the XAPS3 architecture. Although
XAPS3 isadirectdescendentofXAPS2, theconstraintshaveforcedanumberofsignificant changes in the XAPS3 design.
The first new constraint is the crypto-information constraint:
Thecrypto-informationconstraint:Anysystemthatlearnsthroughexperience
cannot use crypto-information ifit wants toguaranteecorrectlearning.
Crypto-information-hidden information-is information that the architecture
accesses while making performance decisionsbutthat is not accessible to programs
running withinthe architecture. TheXAPS2 architecture employs aformofcryptoinformation in its use ofactivation-real-valued weights associated to portions of
working memory. These activation values are used by the architecture to decide
which of multiple instantiations ofa production should execute on a cycle, among
other purposes. It is crypto-information because knowledge about activation values
cannot be represented in the productions executed within the architecture. Another
example ofcrypto-information is the information about working memory recency
used forconflict resolution in the OPS languages (Forgy, 1981).
The use ofactivation in XAPS2 is a good example ofhow crypto-information
can lead to incorrectperformance. Supposethere isanobjectA withan activationof
0.5 andanobjectbwithanactivation0.3. Supposealsothatthere isaproductionthat
matchesbothAandB, generatingtwoinstantiations. When thissituationfirstoccurs,
the instantiation that matchesA will execute because ofits greater activation. From
thisexperience, thesystemlearnswhatactiontoperformunderthesecircumstancesthatis,theactionassociatedwiththeAinstantiation. AtsomelaterpointbothAandB
may again be represented, but this time with the activations reversed. Even though
the action associated withB is now the correct one, its previous experience tells the
system to do the action associated with A, because the information about relative
levelsofactivationcouldnotbe representedintherecordofthepreviousexperience.
--- PAGE 278 ---
264 CHAPTER 10: THE CHUNKING OFGOAL HIERARCHIES
Notonly wouldthe previous information be incorrect, but there would in factbe no
way everto learn the correct information.
Itisimportanttonotethatthisconstraintdoesnotruleoutallactivation-based
production-system architectures. Ifthe activation is not used as a basis for an architectural decision, or ifsufficient knowledge about activation levels is representable
withinproductions, thenactivation is notaaproblem, because itis nolongera form
ofcrypto-information.
Though activation was a focal point ofthe XAPS2 design, XAPS3 has been
designedto work without itto meetthe demands ofthis constraint. It is thus a more
traditional, purely symbolic, production-system architecture.
The second new constraint is thegoal-architecturalityconstraint:
The goal-architecturality constraint: The representation and processing of
goalsmustbedefinedinthearchitectureitself; itcannotbeleftuptothediscretionofproductions.3
Thereasonbehindthisconstraintcanbetracedtothechunkingmechanism'sneedto
understand how goals are represented andprocessed. This requirement implies that
goals must be understood by the architecture, because the chunking mechanism is
itselfan architectural mechanism.
One way for the architecture to understand the processing ofgoals is for the
goal-processing algorithm to be defined within the architecture itself. The obvious
alternative-production-definedprocessingofgoals-requiresthechunking mechanismtobeabletoabstractthealgorithmandrepresentationfromtheproductionsand
tuneitselftowhateverschemeisbeingused. Thisrequiresconsiderably moreintelligence than the authors are willing to ascribe to the chunking mechanism or for that
matter to any mechanism built into the architecture. For a system to exhibit truly
adaptivebehavior, it mustbeabletoapplyitsfullknowledgeandlearningcapabilities
to any task requiring intelligence. This is feasible atthe program level but not at the
architectural level.
In response to the goal-architecturality constraint, the goal processing that
occurred at the level ofproductions in XAPS2 has been moved down into the architecture ofXAPS3.
XAPS3 isonearchitecturesittingwithinthedesignspacedelimitedbytheconstraintssofardetermined. TheremainderofthissectiondescribestheXAPS3 architecture(formoredetails, seeRosenbloom, 1983). Itislike XAPS2 inthestructureof
working memory objectsand in the factthat it allows parallelismat the level ofproductionexecution, but itdiffers in itslackofactivationand in itsbuilt-in mechanisms
Vnderson < I1)S2.d has previously made a similarargument for the placement ofgoal processing m the
architecture ol the ACT* productionsystem(Anderson. 1983a).
--- PAGE 279 ---
ROSENBLOOM AND NEWELL 265
for goal processing and chunking.4 This description is divided into sections on the
standardcomponentsofaproduction system-working memory, production memory,
and the cycle ofexecution-followed by sections on the processing ofgoals and the
chunking mechanism.
10.6.1 Working Memory
The XAPS3 working memory consists ofan unordered set ofobjects representingall ofthetypes ofinformation required inan information-processing system
(except for the types ofinformation represented in productions): goals, patterns of
stimuli and responses, intermediate computations, and so forth. Each object is a
token, inthesenseoftheclassicaltype-tokendistinction. Itisuniquelyspecifiedbya
generaltypeandauniqueidentifier. Thetypeisasymbolthatcanbecommonamong
severalobjects, suchasGoal. Theidentifierisauniquesymbolfortheobject, which
allows multiple objects ofthe same type to be present simultaneously in working
memory. A new identifier is generated dynamically by the architecture whenevera
new object is created.
XAPS3 objects also have an optional set of attributes, each of which has
exactly one value. A successfully completed instance of a Press-Button-AtHorizontal-Location goal (with a type of Goal and an identifier of 0bjectl27)
looks like the following example.5
(Goal0bjectl27
[name Press-Button-At-Horizontal-Location]
[status Succeeded] [result-type Response])
The interpretation ofthetwoattributes (name and status) shouldbe obvious. They
tell the system that this goal is an instance of the Press-Button-At-HorizontalLocation goal that has completed successfully. The third attribute (result-type)
specifiesthetypeofobjecttobereturnedasthe resultofthisgoal (see section 10.6.4
forthe details).
During the lifetime ofa working memory object, two types ofauxiliary tags
are kept. The created-by tag marks the identifier ofthe goal that was active-there
being one at most-when the object was created (ifthere was no goal active at the
time, then the special symbol <None> is used). This tag is used for determining
the object's status as part ofeither the local or the initial state ofthe current goal.
The examined-bytag marksthe identifierofthegoal activewhenthe object was last
4SeeMoran(1973),Anderson(1983a),andSauersandFarrell(1982)forotherattemptsatintegratingthe
processingofgoalsintothearchitectureofaproductionsystem.
Theformatofthisobjectandofsubsequentobjectsandproductionshasbeenalteredslightlyforclarityof
presentation.
--- PAGE 280 ---
266 CHAPTER 10: THE CHUNKING OF GOAL HIERARCHIES
examined by a production. This tag, in combination with the first one, allows the
chunking mechanism to determine the implicit parameters ofgoals.
10.6.2 Production Memory
There is a single homogeneous production memory in XAPS3. The productionsinitaresimilartotheproductionsin XAPS2andtothoseintheOPSlanguages.
They have three parts: a name, a list ofone or more conditions, and a list ofone or
more actions. Here is an example production taken from the implementation ofthe
Seibel task (figure 10-4):
(DefProd SubGoal/Do-Lights/Do-Press-All-Buttons
(Goal <Exists) [name Do-Lights] [statusActive]
[MINIMUM-LOCATION = ?Min-Loc]
[MAXIMUM-LOCATION = ?MdX-LoC~\
(Goal {<Local> <Exists>} [nameNo-Light-Off?]
[status Succeeded])
(Goal <New-Object> [NameDo-Press-All-Buttons]
[status Want] [result-type Response]
[MINIMUM-LOCATION = ?Min-Loc]
[MAXIMUM-LOCATION = ?McLX-Loc])))
The name ( SubGoal/Do-Lights/Do-Press-All-Buttons ) is purely for the
convenience oftheprogrammer; itdoesn'taffectthe processing ofthe system in any
way. Each condition is a pattern to be matched against the objects in working
memory. Aconditionpatterncontainsatypefield(specifyingthetypeofobjecttobe
matched), an identifierfield (containingseveraldifferenttypesofinformation), and
an optional set of patterns for attributes. In the first condition in production
SubGoal/Do-Lights/Do-Press-All-Buttons the type is Goal, the identifier is
<Exists>, and there are four attributes (name, status, minimum-location, and
maximum-location) with associated values (Do-Lights, Active, = ?Min-Loc,
and = ?Max-Loc, respectively).
Conditionpatternsarebuiltprimarily fromconstantsand variables. Constants
are signified by the presenceoftheappropriatesymbol inthe patternandonly match
objects that contain that symbol in that role. Some example constants in production
SubGoal/Do-Lights/Do-Press-All-Buttons are Goal, minimum-location, and
Succeeded. Variables are signified by a symbol (the name ofthe variable) preceded
by an equal sign. They match anything appearing in their role in an object. An
example is = ?Min-Locinthesampleproduction. All instancesofthesame variable
within a production must be bound to the same value in order for the match to
succeed.
--- PAGE 281 ---
ROSENBLOOM AND NEWELL 267
The identifier in the first condition ofproduction SubGoal/Do-Lights/DoPress-All-Buttons is specifiedby the special symbol <Exists>. Suchacondition
succeedsifthereisanyobjectinworkingmemorythatmatchesthecondition. Ifthere
is more than one such object, the choice is conceptually arbitrary (actually, the first
objectfoundisused). Onlyoneinstantiationisgenerated. Iftheidentifierisspecified
asavariable, theconditionstillactslikeanexistscondition, buttheidentifiercanbe
retrieved, allowing the object tobe modified by the actions ofthe production.
The complete opposite of an exists condition is a not-exists conditioncommonly called a negated condition. When the symbol <Not-Exists> appears in
the identifier field, it signifies that the match succeeds only ifthere is no object in
working memory that matches the remainderofthe condition pattern.
Thesecondconditioninthesampleproductioncontainsthesymbol <Local) as
well as <Exists> inthe identifierfield (they areconjoined syntactically bybraces).
When this symbol is added to an identifier pattern ofany type-constant, variable,
exists, or not-exists-it signifies that the condition should be matched only against
thoseobjectsinworkingmemorythatarelocaltotheactivegoal. Thisinformationprovided by the objects' created-by tags-is a means by which the production that
works on a goal can determine which objects are part oftheir local context.
The remainder of the condition specifies a pattern that must be met by the
attribute-valuepairsofworkingmemoryobjects. Thesepatternsarebuiltoutofconstants, variables, built-in predicates (such as <Greater-Than>), and general LISP
computations (viaanescapemechanism). Inaddition, anyoftheabove formscanbe
negated, denoting that the condition only matches objects that do not match the
pattern.
There isonlyonekindofactionin XAPS3-modifyingworkingmemory. The
interface to the outside world is through working memory rather than through production actions. Actions cancreate new objects in working memory and, undercertain circumstances, modify existing objects. When the action has an identifier of
<New-0bject>
anewobjectiscreatedbyreplacingtheidentifierwithanewlygeneratedsymbol, instantiatingthevariableswiththeirvaluescomputedduringthematch,
and replacing calls to LISP functions with their values (via another escape mechanism). An existing object can be modified by passing its identifier as a parameter
from acondition. As discussed in section 10.4, only objects localtothe current goal
can be modified.
There are no production actions that lead to the deletion ofvalues or objects
from working memory. A value can be removed only ifit is superseded by another
value. As discussed in section 10.4, objects go away when they are no longerpart of
the current context.6 No explicit mechanism for deletion has proved necessary, so
none has been included.
sThismechanismissimilartothedampeningmechanismintheACTarchitecture(Anderson, 1976).
--- PAGE 282 ---
268 CHAPTER 10: THE CHUNKING OFGOAL HIERARCHIES
10.6.3 The Cycle of Execution
XAPS3 followsthetraditional recognize-actcycleofproduction-systemarchitectures, with a few twists thrown in by the need to process goals. The recognition
phase begins with the match and finishes up with conflict resolution. The cycle
number-simulated time- is incremented after the recognition phase, whether any
productions are executed or not. During the act phase, productions are executed.
10.6.3.1 Recognition
The match phase is quite simple. All of the productions in the system are
matched against working memory in parallel (conceptually). This process yields a
setoflegal instantiationswithatmostoneinstantiationperproduction, becauseeach
conditiongeneratesatmostoneinstantiation. Eachinstantiationconsistsofaproduction name and a set ofvariable bindings for that production that yield a successful
match. Thissetofinstantiationsisthenpassedinitsentiretytotheconflictresolution
phase,7wheretheyarewinnoweddowntothesettobeexecutedonthecurrentcycle.
This winnowing is accomplished via apairofconflict resolution rules.
The first rule is goal-context refraction. A production instantiation can fire
only oncewithinany particulargoalcontext. Itisaformofthe standardOPS refractory inhibition rule (Forgy, 1981), differing only in how the inhibition on firing is
released. With the standard rule, the inhibition is released whenever one of the
workingmemoryobjectsonwhichtheinstantiationispredicatedhasbeenmodified.
With goal-context refraction, inhibition is released whenever the system leaves the
context in which the instantiation fired. If the instantiation could not legally fire
beforethecontextwasestablishedbutcouldfirebothwhilethecontextwasactiveand
afterthecontextwasterminated,thentheinstantiationmustbebased, atleastinpart,
on a result generated during the context and returned when the context was left.
Therefore,theinstantiationshouldbefreetofireagaintoreestablishthestill-relevant
information.
The second rule-the parameter-passing bottleneck-states that only one
parameter-passing instantiation can execute on a cycle (conceptually selected arbitrarily). This rule first appeared in a slightly different form as an assumption in the
HPSA77 architecture (Newell, 1980a). It will bejustified in section 10.6.5.
10.6.3.2 Action
All of the instantiations that make it through the conflict resolution phase
are fired (conceptually) in parallel. Firing a production instantiation consists of
I<u purposesOlefficiency,thesetwophasesareactuallyintermingled. However,thisdoesnotchangethe
behavioi ol the systemal the level al which wearc interested.
--- PAGE 283 ---
ROSENBLOOM AND NEWELL 269
(1)overwritingtheexamined-bytagsoftheworkingmemoryobjectsmatchedbythe
instantiation, with the identifier ofthe active goal; (2) replacing all variables in the
actions by the values determined during the match phase; (3) evaluating any LISP
forms; and (4) performingthe actions. Thiscan resultinthe addition ofnew objects
toworkingmemoryorthemodificationofexistinglocalobjects. Ifconflictingvalues
are simultaneously asserted, the winner is selected arbitrarily. This does notviolate
thecrypto-informationconstraintbecausethearchitectureisusingnoinformationin
making the decision. Ifthe performance system works correctly, even when it can't
dependonthe outcomeofthedecision, thenthe learned informationwill notleadto
incorrect performance.
10.6.4 Goal Processing
Insection 10.4,theprocessingofgoalswasdescribedatanabstractlevel. Inthis
section how that processing is implemented within the XAPS3 architecture is
described. The goals themselves, unlike chunks, arejustdata structures in working
memory. A typical XAPS3 goal goes through four phases in its life. The current
phase ofagoal is represented explicitly at all times by the value associated with the
status attribute ofthe working memory object representing the goal.
Inthefirststageofitslife,thegoalisdesired:atsomepoint,butnotnecessarily
right then, the goal should be processed. Productions create goal desires by generatinganewobjectoftypeGoal. Each newgoal objectmusthaveaname attribute, a
statusattributewithavalueofWant, andaresult-typeattribute. Inaddition, it may
have any numberofotherattributes, specifying explicitparameters to the goal. The
value of the result-type attribute specifies the type of the results that are to be
returnedon successful completionofthegoal. All local objectsofthattype areconsidered to be results of the goal. Results are marked explicitly so they won't be
flushedwhenthecontextisleftandsothechunkingmechanismwillknowtoinclude
them in the chunks for the goal.
Leavingtheexpressionofgoaldesires underthecontrol ofproductionsallows
goals tobeprocessedby the architecture, whilethe structure ofthe hierarchy is still
leftunderprogram(thatis, production)control. Thearchitecturecontrolsthetransition to the second, active phase ofthe goal's life. At most, one goal is active at any
pointintime. Thearchitectureattemptstoactivateanewgoalwheneverthesystemis
at a loss about how to continue with the current goal. This occurs when there is an
emptyconflictset; thatis, thereisnoproductioninstantiationthatcanlegallyfireon
thecurrentcycle. When thishappens, thesystemlooks inworkingmemorytodetermine ifthereareany subgoalsofthecurrentgoal-thosegoalscreatedwhilethecurrentgoal wasactive-thataredesired. Ifsuchasubgoalisfound, itis madetheactive
goal,andtheparentgoalissuspendedbyreplacingitsstatuswiththeidentifierofthe
newlyactivatedsubgoal. Ifmorethanonedesiredsubgoal isfound, oneisarbitrarily
selected (actually, the last one found is used).
--- PAGE 284 ---
270 CHAPTER 10: THE CHUNKING OFGOAL HIERARCHIES
Suspensionisthethirdphaseinthelifeofagoal(itoccursonlyfornonterminal
goals). Replacing the status ofthe parent goal with the subgoal's identifier accomplishes two things: it halts work on the goal, because the productions that process
goals all checkfora status ofActive, and it maintains the control stack forthe goal
hierarchy.
Asuspendedgoalremainssuspendeduntilitsactivesubgoalterminates,
which time it returns to being active. Ifa goal has more than one subgoal, the goal
will oscillate between the active and suspended states.
Ifnoprogresscanbemadeontheactivegoalandtherearenodesiredsubgoals,
thenthesystemhasnoideahowtocontinue makingprogress; itthereforeterminates
theactivegoalwithastatusofFailed. Followingtermination,thegoalisinitsfourth
and final phase oflife. In addition to a failure termination, goals can be terminated
with a status ofSucceeded. There are no uniform criteria fordetermining when an
arbitrary goal hascompleted successfully, so ithasbeenlefttoproductionstoassert
thatthishas happened. This isdoneviathecreation ofanobjectoftype Succeeded.
When the architecture detects the presence ofsuch an object in working memory, it
terminates the current goal and reactivates its parent.
Atgoalterminationtimeanumberofactivitiesoccurinadditiontotheobvious
oneofchangingtheactivegoal. The firsttwoactivitiesoccuronly onthe successful
termination ofagoal. As will bediscussed in the following section, the first step is
thepossiblecreationofachunk. Thesecondstepistoreturnthe resultsoftheterminatinggoaltoitsparentgoal. Thisisaccomplishedbyalteringthecreated-bytags of
the results sothat itlooksas ifthey werecreatedbytheparentgoal. Thethirdstep is
to delete all ofthe objects from working memory created during the processing of
this goal. The fourth and final step is toenable result decoding ifit is appropriate.
10.6.5 Chunking
As was seen in section 10.5, chunking improves performance by enabling the
systemtouse itsexperience withprevious instances ofagoal toavoidexpanding the
goal tree below it. In this section is detailed how this has been implemented within
the XAPS3 architecture-yielding a working, task-independent production-system
practice mechanism. This section begins with a description ofhow chunks are used
and concludes with a description ofhow they are acquired.
10.6.5.1 The Use of Chunks
The key to the behavior ofa chunk lies in its connection component. When a
goal is proposed in a situation in which a chunk has already been created for it. the
connection component of that chunk substitutes for the normal processing o( the
goal. This is accomplished in XAPS3 by having the connection component check
--- PAGE 285 ---
ROSENBLOOM AND NEWELL 271
thatthegoal'sstatusisdesired. Theconnectionisaproductioncontainingacondition
testing fortheexistence ofadesiredgoal, aconditiontestingfortheencodingofthe
goal's parameters, and any relevant negated (<Not-Exists>) conditions. It has two
actions: one marks the goal as having succeeded, and the other asserts the goal's
encodedresult. Ifthereisadesiredgoalinworkingmemoryinasituationforwhicha
connection production exists, the connection will be eligible to fire. Whether (and
when) itdoesfiredependsonconflictresolution. Connectionproductionsaresubject
to the parameter-passing-bottleneck conflict resolution rule because they pass the
identifierofthedesiredgoalasaparametertotheactionthatmarksthegoalashaving
succeeded. Thisconflictresolutionruleservesadualpurpose: (1)itimplementspart
ofthebottleneckconstraintbyinsuringthatonlyonegoalcanbeconnectedatatime,
and(2)it removesasourceofpossibleerrorbyinsuringthatonlyoneconnectioncan
execute forany particulargoal instance.
Iftheconnectiondoesfire, it removestheneedtoactivateandexpandthegoal,
becausethegoal'sresultswillbegenerateddirectlybytheconnection(anddecoding)
andthegoalwillbemarkedashavingsucceeded. Ifinsteadnoconnectionproduction
isavailable forthecurrentsituationofadesiredgoal, theneventuallytheproduction
system will reach an impasse-no productions eligible to fire-and the goal will be
activated and expanded. Therefore, we havejust the behavior required ofchunksthey replace goal activation and expansion, ifthey exist.
This behavior is, ofcourse, predicated on the workings ofthe encoding and
decoding components ofthe chunk.8 Theencodingcomponentofachunkmustexecute before the associated connection can. In fact, it should execute even before the
parent goal is activated, because the subgoal's encoded symbol should be part of
the parent goal's initial state. Recall that encodings are built up hierarchically.
Iftheparentgoalsharesalloftheparametersofoneofitssubgoals,thentheencoding
ofthe parent goal's parameters should be based on the encoding generated for the
subgoal. This behavior occurs in XAPS3 because the encoding components are
implementedasgoal-freeproductionsthatdonotpassparameters. Theyfire(concurrently with whatever else is happening in the system) whenever the appropriate
parameter values for their goal are in working memory (subject to refraction). If
the parameters exist before the parent goal becomes active, as they must ifthey are
parameters to it, then the encoded symbol becomes part ofthe parent goal's initial
state.
As stated in section 10.4, the decoding component must decode an encoded
resultwhen itwillbeneeded. Eachdecodingcomponentisaproductionthatkeysoff
the nonprimitive result pattern generated by the connection production and offan
8Forefficiency, theencodinganddecodingcomponentsarenotcreatedifthereisonlyoneparameteror
result, respectively.
--- PAGE 286 ---
272 CHAPTER 10: THECHUNKING OF GOAL HIERARCHIES
objectoftypeDecode.Whenthearchitecturedeterminesthatdecodingshouldoccur,
it places a Decode object in working memory, with the type of the object to be
decoded specified by the type attribute.9 The actions of the decoding production
generatethecomponentresultsoutofwhichthenonprimitivepatternwascomposed.
10.6.5.2 The Acquisition of Chunks
A complete specification of the chunk acquisition process must include the
details ofwhen chunks canbe acquiredand from what informationthey are built. A
chunkcanbeacquiredwhenthreeconditionsaremet. Thefirstconditionisthatsome
goal must havejust been completed. The system can't create a chunk for a goal that
terminated at some point in the distant past, because the information on which the
chunkmustbebased is nolongeravailable. Chunksalsoare notcreatedpriortogoal
completion(onpartialresults). Chunksaresimpletocreateinpartbecausetheysummarize all ofthe effects ofa goal. Ifchunks were created partway through the processingofagoal-forpartial resultsofthe goal-thenasophisticatedanalysis might
be required in order to determine which parameters affect which results and how.
Thisisnotreallyalimitationonwhatcanbechunked,becauseanyisolableportionof
the performance can be made into a goal.
Thesecondconditionisthatthegoalmusthavecompletedsuccessfully. Partof
theessenceofgoalfailure isthatthe systemdoes notknow why the goal failed. This
meansthatthesystemdoesnotknowwhichparametervalueshaveleadtothefailure;
thus, it can't create a chunk that correctly summarizes the situation. Chunking is
success-driven learning, as opposed to failure-driven learning (see for example.
Winston, 1975).
The third and final condition for chunk creation is that all of the working
memory modifications occurring sincethe goal was first activated mustbe attributabletothatgoal, ratherthantooneofits subgoals. Thiscondition is implementedby
insuringthat noproductionswere firedwhileany ofthegoal's subgoals were active.
All ofthe subgoals must either be processed by a chunk or fail immediately after
activation-failure of a subgoal, particularly of predicates, does not necessarily
imply failure ofthe parent goal.
To summarize, a chunk is created after the system has decided to terminate a
goal successfully butbefore anything is done about it (such as marking the goal succeeded, returning the results, or flushing the local objects from working memory).
At that point the goal is still active, and all ofits information is readily available.
Most ofthe information on which the chunk is based can be found in working
memory (but see below). The first piece ofinformation needed forthe creation o\'a
chunk isthe name (and identifier)ofthegoalthat isbeingchunked. This information
'Matters arc actuallj somewhai morecomplicated (Roscnbloom, 1983).
--- PAGE 287 ---
ROSENBLOOM AND NEWELL 273
is found by retrieving the object representing the active goal. The goal's explicit
parameters are also available as attribute-value pairs on the goal object. Given the
goal's identifier, the system finds its implicit parameters by retrieving all ofthe
objects in working memory that were part ofthe goal's initial state-that is, their
created-by tag contains an identifierdifferent from that ofthe active goal-and that
were examined by a production during the processing ofthe active goal. This last
type ofinformation iscontained inthe objects' examined-by tags. The goal's results
are equally easy to find. The architecture simply retrieves all ofthe goal's local
objects that have a type equal to the goal's result-type.
Becausethegoalparameterandresultinformationisdeterminedfromtheconstant objects in working memory, chunks themselves are not parameterized. Each
chunk represents a specific situation for a specific goal. However, two forms of
abstractionareperformedduringthecreationofachunk: (1)theinclusionofonlythe
implicitparametersofagoalandnottheentireinitialstate, and(2)thereplacementof
constant identifiers (found in the working memory objects) with neutral <Exists>
specifications. These abstractions allowthe chunkstobe applicable in any situation
thatis relevantly identical, notmerelytotally identical. Differentchunks are needed
only for relevant differences.
Theonecomplicationinthecleanpictureofchunkacquisitionsofarpresented
involves the use of negated conditions during the processing of a goal. When a
negated condition successfully matches working memory, there is no working
memoryobjectthatcanbemarkedashavingbeenexamined. Therefore, someofthe
information required forchunkcreation cannotbe represented in working memory.
Thecurrentsolutionforthisproblemisnotelegant, butitworks. Atemporaryauxiliary memory is maintained, into which is placed each nonlocal negated condition
occurring on productions that fire during the processing ofthe goal (local negated
conditions can be ignored becausethey do nottestthe initial state). This memory is
reinitializedwheneverthegoalthatiseligibletobechunkedchanges. Beforetheconditions are placed in the memory they are fully instantiated with the values bound
to their variables by the other conditions in their production. As discussed in
Rosenbloom (1983), including a negated condition in an encoding production can
leadtoperformanceerrors, sotheseconditionsareallincludedintheassociatedconnection production.
10.7 RESULTS
InthissectionsomeresultsderivedfromapplyingtheXAPS3 architecturetoa
set ofreaction-time tasks will be presented. A more complete presentation ofthese
resultscanbefoundinRosenbloom (1983)
Thefirstexperimentdescribedhereisthe
Seibel task. Two different sequences oftrials were simulated, of which the first
--- PAGE 288 ---
274 CHAPTER 10:THE CHUNKING OF GOAL HIERARCHIES
sequenceisthesameastheoneusedinRosenbloomandNewell(1982a). Thesimulation completed268 trials before it wasterminated.10A total of682 productions was
learned. Onthesecondsequenceoftrials-fromanewlygeneratedrandompermutationofthe 1023possibilities-259trialswerecompletedbeforetermination. Forthis
sequence, 652 productions were learned.
Figure 10-7showsthefirstsequenceasfitbyageneralpowerlaw. Eachpointin
the figure represents the mean value over five data points (except for the last one,
which only includes three).1' For this curve, the asymptote parameter (A) has no
effect. Only E, the correction forprevious practice, is required to straighten outthe
curve. At firstglance, it seems nonsensical totalkaboutpreviouspractice forsuch a
simulation, buta plausible interpretation is possible. In fact, there are two independent explanations-either orboth may be responsible.
The firstpossibility is thatthe level at which the terminal goals are defined is
toohigh (complex). Ifthe "true" terminalsaremoreprimitive, thenchunking starts
at a lower level in the hierarchy. One view ofwhat chunks are doing is that they are
turning theirassociated (possibly nonterminal) goals into terminal goals forparticular parameter situations. During preliminary bottom-up chunking, the system
would eventually reach the lowest level in the current hierarchy. All ofthe practice
prior to that point is effectively previous practice forthe current simulation.
1000 c
T = 0.0 + 5187(/V + 33)"
i i i i i i i
10 100 1000
Trial number(N -+ E)
Figure 10-7: General power-law fitto268simulatedtrialsoftheSeibel(1963)task.
'"Atthispointthe FRANZLISPsystem-whichappearednottobegarbagecollectinginthefirstplace -
refused toallocateany more memory forthesimulation.
1 hesedataappearnoisierthanthehumandatafromSeibel(1963)showninfigure 10 2 Thisisaccounted
fol In thefactthateachpointinfigure 10 2wasthemeanof1023datapointsandeachpointinthisfigureis
the mean of Iivedata points
--- PAGE 289 ---
ROSENBLOOM AND NEWELL 275
Theothersourceofpreviouspracticeisthegoalhierarchyitself. Thisstructure
is posited at the beginning of the simulation, hence it is already known perfectly.
However, there must exist some process ofmethodacquisition by which the subject
goes from the written (or oral) instructions to an internal goal hierarchy. Though
methodacquisitiondoes notfallwithinthedomainofwhatisheredefinedas "practice," a scheme will be proposed in section 10.9 whereby chunking may lead to a
mechanism for method acquisition.
In addition to the Seibel task, the system so far has been applied to fourteen
tasks from three different stimulus-response compatibility experiments (Fitts and
Seeger, 1953; Morinand Forrin, 1962; Duncan, 1977). Asasampleofthese results,
figure 10-8 shows a pair of simulated practice curves for two tasks from Fitts and
Seeger (1953). These curves contain fifty trials each, aggregated by five trials per
data point.
This chapter is not the appropriate place to discuss the issues surrounding
whetherthesimulatedpracticecurvesaretrulypowerlawsorsomethingslightlydifferent (such as exponentials). Suffice itto say that a mixture ofexponential, power
law, andambiguous curvesisobtained. These results roughly followthepredictions
of the approximate mathematical analysis ofthe chunking theory appearing in
Rosenbloom (1983). Theyalsofitwiththeobservationthatthehumancurvestendto
be most exponential for the simplest tasks-the compatibility tasks are among the
simplest tasks for which we have human practice data. For more on this issue, see
Newell and Rosenbloom (1981) and Rosenbloom (1983).
r* 100.0
* " • - "*•-.. v X
^^«^0 X
10.0 : x" - --x .
^o x
1.0 .: O O
. K/^J < j - - > \ C < o -• n • d • i . t • ioni o >A-iK-kA: n / r - 4aJ-,.O y /\VT-CU\8,il£l
X • -x ConditionSB-RA: T = 67.5AT -57
10 100
Trial number(TV)
Figure 10-8: SimulatedpracticecurvesforconditionsSA-RAandSB-RAfromFittsandSeeger(1953).
Thelattercurveistheaverageovertwohierarchyvariations.
--- PAGE 290 ---
276 CHAPTER 10:THE CHUNKING OF GOAL HIERARCHIES
10.8 RELATIONSHIP TO PREVIOUS WORK
The current formulation ofthe chunking theory oflearning provides an interesting point ofcontact among four previously disparate concepts: (1) classical
chunking; (2) production composition (Lewis, 1978; Neves and Anderson, 1981;
Anderson, 1982b); (3)tablelook-up-memo functions (Michie, 1968) and signature
tables (Samuel, 1967); and (4) macro-operators (Fikes, Hart and Nilsson, 1972;
Korf, 1983). Classical chunking has already been discussed in section 10.3, so this
sectioncoversonlythelatterthreeideas, followedbyaproposalabouttheunderlying
commonality among these concepts.
10.8.1 Production Composition
Production composition (Lewis, 1978; Neves and Anderson, 1981; Anderson,
1982b) isalearningschemewherebynewproductionsarecreatedthroughthecombination ofold ones. Given apairofproductions thatexecute successively, the system
createstheircomposition fromtheirconditionsandactions (figure 10-9). Theconditionsideofthenewproductionconsistsofalloftheconditionsofthefirstproduction
(C|, C 2, andC 3), plusthoseconditionsfromthe secondproductionthatdonot match
actions of the first production (C 5). The conditions ofthe second production that
matchactionsofthefirstproduction(C
matchesA
arenotincludedinthecomposition (removing the serial dependency between the two productions). All of the
actions fromboth productionsarecombined intheaction sideofthe new production
(A 4 and A 6). The resulting composition is a single production that accomplishes the
combined effects ofthe olderpairofproductions. As learning continues, composed
productions can themselves be composed, until there is a single production for an
entire task.
In some recent work with the GRAPES system (Saures and Farrell. 1982;
Anderson, Farrell, and Saurers, 1982; Anderson, 1983b), production composition
was integrated with goal-based processing. In GRAPES, specific goals are designated by the programmertobe ones forwhich composition will occur. When such a
C, c 2 C 3 A 4
c c
4 5
C, C C C A A,
a 3 5 4
Figure 10-**: Anexampleof productioncomposition.
--- PAGE 291 ---
ROSENBLOOM AND NEWELL 277
goal completessuccessfully, alloftheproductionsthatexecutedduringthattimeare
composed together, yielding a single production that accomplishes the goal.
Because the main effects of chunking and goal-based composition are the
same-the short-circuiting of goals by composite productions- it is probably too
earlytoknowwhichmechanismwillturnouttobethecorrectoneforageneralpractice mechanism. However, there are a number ofdifferences between them worth
noting. We willfocusonthethreemostimportant: (1)theknowledgerequiredbythe
learning procedure: (2) the generality of what is learned; and (3) the hierarchical
nature ofwhat is learned.
10.8.1.1 Knowledge-Source Differences
With chunking, all ofthe information required for learning can be found in
working memory (modulo negated conditions). With production composition, the
informationcomes fromproduction memory (andpossibly fromworking memory).
Beingabletoignorethestructureofproductionshastwoadvantages. Thefirstadvantage is that the chunking mechanism can be much simpler. This is both because
working memory is much simpler than production memory-productions contain
conditions, actions, variables, function calls, negations, and other structures and
information-andbecause, withcomposition, thecomplexprocessofmatchingconditions oflater productions to actions ofprevious productions is required, in order
that conditions that test intermediate products not be included in the composition.
Chunking accomplishesthisby only including objects that arepart ofthe goal's initial state.
The second advantage of the chunking strategy, of learning from working
memory, isthatchunkingisapplicabletoanygoal, nomatterwhatitsinternalimplementation is (productions or something else). As long as the processing ofthe goal
leaves marks on the working memory objects that it examines, chunking can work.
10.8.1.2 Generalization Differences
Theproductsofchunkingarealwaysconstantproductions(exceptfortheidentifiers of objects) that apply only for the situation in which they were created
(although, asalreadydiscussed,twoformsofabstractionsareperformed). Withproduction composition, the variables existing in the productions to be composed are
retainedinthenewproduction. Thenewlylearnedmaterialisthusmoregeneralthan
that learned by chunking. The chunking mechanism definitely has more ofa table
look-up flavor. Section 10.8.2 contains a more thorough discussion ofchunking as
table look-up, and section 10.9 discusses how achunking mechanism couldpossibly
learn parameterized information.
--- PAGE 292 ---
278 CHAPTER 10: THE CHUNKING OF GOAL HIERARCHIES
10.8.1.3 Hierarchical Differences
Bothmechanismslearnhierarchicallyinthattheylearnforgoalsinahierarchy.
Theydifferinhowthey decideabout whichgoalstolearn and inwhetherthelearned
material is itselfhierarchical.
Chunkingoccursbottomupinthegoalhierarchy. Productioncomposition-in
GRAPES at least-works in isolation on any singlegoal in the hierarchy. Forthis to
work, subgoalsarekeptasactionsinthenewproductions. Thecompositionapproach
is more flexible, but the chunking approach has two compensating advantages. The
firstadvantageisthat, withchunking, theencodinganddecodingcomponentscanbe
themselveshierarchical, basedontheencodinganddecodingcomponentsofthepreviouslychunked subgoals. Productionsproducedbycompositiontendtoaccumulate
huge numbers ofconditions and actions because they are flat structures.
Thesecondadvantage isagain simplicity. When information islearnedabouta
goal at an arbitrary position in the hierarchy, its execution is intermingled with the
execution of its subgoals. Knowing which information belongs in which context
requiresacompletehistorical traceofthechanges madetoworking memoryandthe
goals that made the changes.
10.8.2 Table Look-up
It has been seen that from one point ofview chunking resembles production
composition. From another point of view it resembles a table look-up scheme, in
which atable ofinput parametersversus results isgradually learned foreach goal in
the system. As such, it has two important predecessors-memo functions (Michie.
1968; Marsh, 1970) and signature tables (Samuel, 1967).
10.8.2.1 Memo Functions
A memo function12 is a function with an auxiliary table added. Whenever the
function isevaluated, thetableisfirstcheckedtoseeifthereisaresultstoredwiththe
current setofparametervalues. Ifthere is, it is retrieved asthe valueofthe function.
Otherwise, the function is computed and the arguments and result are stored in the
table. Memo functions have been used to increase the efficiency of mathematical
functions (Michie, 1968; Marsh, 1970) and oftree searches (Marsh, 1970).
Memo functions themselves are derived from the earlier work b> Samuel (1959) on creating role
memor) foi the valuesoi board positions in thegameofcheckers
--- PAGE 293 ---
ROSENBLOOM AND NEWELL 279
Chunking can be seen as generating memo functions for goals. But these are
hierarchical memofunctions, andonesinwhichtheargumentsneednotbespecified
explicitly. Chunking also provides a cleaner implementation ofthe ideas behind
memo functions because the table is not simply an add-on to a different processing
structure. Itisimplementedbythesame "stuff" (productions)asisusedtorepresent
the othertypes ofprocessing in the system.
10.8.2.2 Signature Tables
Signaturetables weredevelopedasameansofimplementing nonlinearities in
anevaluationfunctionforcheckers(Samuel, 1967). Theevaluationfunctionisrepresentedasahierarchical mosaicofsignaturetables. Eachsignaturetablehadbetween
two and four parameters, each of which had between three and fifteen possible
values. The parameters to the lowest-level tables were measures computed on the
checkerboard. Foreachcombinationofparametervaluesanumberwas storedinthe
table representing how good that combination was. There were nine ofthese tables
arranged in a three-level hierarchy. The values generated by lower tables were fed
intohighertables. The final value oftheevaluation function wasthe numbergenerated by the root (top) table.
Signaturetablescapturethetablelook-upandhierarchical aspectsofchunking,
though only for encoding. There is no decoding because signature tables are not a
modelofaction;theyactsimplyasaclassifierofboardpositions. Anotherdifference
betweenchunkingandsignaturetablesisthatinformationisstoredinthelatternotas
a direct function ofexperience, but as correlations overa numberofexperiences.
10.8.3 Macro-Operators
A macro-operator is a sequence of operators that can be viewed as a single
operator. Oneclassical systemthatmakesuseofmacro-operators is STRIPS (Fikes,
Hart, andNilsson, 1972). STRIPSworksbytakingataskandperformingasearchto
findasequenceofoperatorsthatwillaccomplishthetask. Givenasolution, STRIPS
firstgenerates ahighly specific macro-operatorfromthe sequence ofoperators and
thengeneralizes itby figuringoutwhichconstantscanbereplacedbyvariables. The
generalized macro-operator is used as a plan to guide the performance ofthe task,
and it can be used as a primitive operator in the generation ofa macro-operator for
anothertask.
Each STRIPS operator is much like a production: it has a set ofconditions (a
preconditionwff)andasetofactions(consistingofanaddlistandadeletelist). Each
macro-operator is represented as a triangle table representing the conditions and
actions for all subsequences ofthe operators in the macro-operator (preserving the
--- PAGE 294 ---
280 CHAPTER 10: THECHUNKING OF GOAL HIERARCHIES
orderofexecution). Thisprocessisverymuchlikeaproductioncompositionscheme
thattakesalloftheproductionsthatfireduringtheprocessingofagoalandcreatesa
compositionforeverypossiblesubsequenceofthem. STRIPSdiffersfromthemechanismsdescribedabove inexactly how it represents, selects, and uses what it learns,
but it shares a common strategy ofstoring, with experience, meaningful (based on
thetask/goal) compositesthatcan reducetheamountofprocessing requiredby subsequent tasks.
Another form of macro-operators can be found in Korf's (1983) work on
macro-operator-based problem solving. Korf presents a methodology by which a
table ofmacro-operators can be found that can collectively solve all variations on a
problem. Forexample, onetableofmacro-operators is sufficientforsolvingany initialconfigurationofRubik'scube. Korf'stechniqueisbasedonhavingasetofdifferences between the goal state and the current state. The differences are ordered and
then solved one at a time. During the solution ofa difference, solutions to previous
differences can be destroyed, but they must be reinstated before the current difference is considered solved. Rather than learn by experience, Korf's system preprocessesthetasktolearnthemacro-operatortablecapableofhandlingall variationsof
thetask. Itdoesthisintimeproportionaltowhat it wouldtaketosearchforasolution
to one variation ofthe task without the table ofmacro-operators.
Even though the macro-operators are nonvariabilized, a single table with size
proportionaltotheproductofthenumberofdifferencesandthenumberofvaluesper
differenceistotally sufficient. Thisisbecauseateachpointinthesolutionwhat isto
bedonedependsonlyonthevalueofthecurrentdifferenceandnotonanyoftheother
differences. Itispossibletobemoreconcreteandtobringoutthe relationshipofthis
mechanism to chunking by viewing the sequence ofdifferences as a goal hierarchy.
Thetop-levelgoal istosolveallofthedifferences. Ingeneral,tosolvethefirstx + 1
differences one firstprocessesa subgoal forsolvingthe firstxdifferences; one then
processesasubgoalforsolvingthefirstx +
differencesgiventhatthefirstxdifferences have been solved. These latter conditional goals are the terminal goals in the
hierarchy. Moreover,eachonehasonlyoneparameterthatcanvary-thevalueofdifferencex + 1 -soonlyavery fewmacro-operatorsneedbecreated forthegoal (the
numberofvaluesthatthedifferencecantake). Korf'smacro-operatorsareessentially
chunksfortheseterminalgoals. Theyrelatetheparametersofagoal(thesetofdifferencesalready solvedandthevalueofthenextdifference)tothecomposite result (the
sequence of operators to be performed). Korf avoids the combinatorial explosion
implicit in the tasks by creating macro-operators only for these limited terminal
goals. Ifthechunking mechanism weredoingthe learning, it would begin by chunking the terminals, but it would then proceed to learn about the nonterminal goals as
well. Korf's work is a good example ofhow choosing the right goal hierarchy (and
limiting the setofgoals forwhich learningoccurs)canenableasmall set ofnonvariabilized macro-operators (or chunks) to solve a large class ofproblems.
--- PAGE 295 ---
ROSENBLOOM AND NEWELL 281
Summary
10.8.4
Although classical chunking, production composition, table look-up, and
macro-operators were proposed in quite different contexts and bear little obvious
relationshiptoeachother, thecurrentformulationofthechunkingtheoryoflearning
has strong ties to all four. (1) It explains how classical chunks can be created and
used; (2)itresultsinproductionssimilartothosegeneratedbygoal-directedproduction composition; (3) it caches the results of computations, as in a table look-up
scheme; and(4) itunitizessequencesofoperatorsintohigher-levelmacro-operators.
Thechunkingtheorydiffersfromthese fourmechanisms inanumberofways, butat
the same time it occupies a common ground among them. This leads the authors to
propose that all five mechanisms are different manifestations ofa single underlying
ideacenteredonthe storageofcomposite informationratherthan its recomputation.
The chunking theory has a number ofuseful features, but it is probably too early to
know whatformulationwillturnouttobethecorrectoneinthelong runforgeneral
practice mechanism.
10.9 EXPANDING THE SCOPE OF CHUNKING
Inthis work it has been shown how chunking canprovide a model ofpractice
fortasks that can already be accomplished. Performance is sped up but not qualitatively changed. It is interesting to ask whether chunking can be used to implement
any of the other, more complex forms of learning, such as method acquisition,
concept formation, generalization, discrimination, learning by being told, and
expectation-driven learning. Chunking does not directly accomplish any of these
forms of learning, and on first glance the table look-up nature ofchunking would
seem to preclude its use in such sophisticated ways. However, four considerations
suggestthe possibility thatthe scope ofchunking may extend much further.
The first two considerations derive from the ubiquitous presence of both
chunkingandthepowerlawofpracticeinhumanperformance. Chunkingisalready
implicatedatleastinhigher-levelcognitiveprocesses, andthepowerlawofpractice
has been shown to occur overalllevels ofhuman performance. Thus, ifthe current
chunkingtheoryturnsoutnottobeextendable, thelimitationwillprobably beinthe
detailsofthe implementation itselfratherthan inthe moreglobal formulationofthe
theory.
The remaining twoconsiderations stem from the interactionofchunking with
problem solving. The combination of these two mechanisms has the potential for
generating interesting forms oflearning. The strongest evidence ofthis potential to
date can be found in the work of Anderson (1983b). He has demonstrated how
--- PAGE 296 ---
282 CHAPTER 10: THECHUNKING OF GOAL HIERARCHIES
production composition (a mechanism that is quite similartochunking, as has been
shownhere), whencombinedwithspecificformsofproblemsolving, caneffectively
perform both production generalization and discrimination. Generalization comes
aboutthroughthecompositionofananalogyprocess,anddiscriminationcomesfrom
the composition ofan error-correction procedure.
The final line ofevidence comes from the work of Newell and Laird on the
structureofageneralproblem-solvingsystembasedontheproblemspacehypothesis
(Newell, 1980b), a universal weakmethod(Laird and Newell, 1983), and universal
subgoaling(Laird, 1983). Theproblemspacehypothesisstatesthatintelligentagents
arealways performing in aproblem space. Atany instant, the agentwill have agoal
that it is attempting to fulfill. Associated with that goal is aproblem space in which
thegoalcanbepursued. Theproblemspaceconsistsofasetofstates, aproblem (initial and desired states), a set ofoperators that move the agent between states, and
searchcontrolinformationthatassistsinguidingtheagentefficientlyfromtheinitial
to the desired state. Added to this problem space structure are (1) a universal weak
method, whichallowsthebasicproblem-solvingmethods, such asgenerate-and-test
and depth-first search, to arise trivially out ofthe knowledge ofthe task being performed; and(2) universal subgoaling, whichenablesaproblemsolverautomatically
to create subgoals for any difficulties that can arise during problem solving. The
resultofthislineofworkhasbeenaproblem-solvingproduction-systemarchitecture
called SOAR2 that implements these three concepts (Laird, 1983).
Tosee thepowerofintegratingchunking with suchaproblem-solving system,
considertheproblemofmethodacquisition; givenanarbitrarytaskorproblem, how
doesthe systemfirstconstructamethod (goal structure) for it? This is the prototypicalcaseof"hard" learning. Thereareatleasttwowaysinwhichchunkingcanassist
SOAR2 inmethodacquisition: (1)bycompilingcomplexproblem-solvingprocesses
into efficient operators, and (2) by acquiring search control information that eliminatesirrelevantsearchpaths. A singlegoal-chunkingmechanismcanacquirebothof
these types ofinformation; the difference is in the types ofgoals that are chunked.
Thecompilationprocessisanalogoustothekindsofchunksthatarecreated in
XAPS3: inefficientsubgoal processingisreplacedbyefficientoperatorapplication.
Givenatask alongwith itsassociatedgoalsandproblem spaces, SOAR2 attempts to
fulfill the task goal through a repeated process ofelaborating the current situation
with information and selecting a new goal, problem space, state, or operator. The
system applies operators to a state by creating a new state, elaborating it with the
results ofthe operator, and selectingthe new state. Ifthe application ofthe operator
requiresproblemsolving itself, itwill notbepossibletoapply ittoastatedirectly via
a set ofelaborations. Instead, a difficulty will arise for which SOAR2 will create a
subgoal.
One way this subgoal can be pursued is by the selection of a problem space
within which the task of applying the problematic operator to its state can be
--- PAGE 297 ---
ROSENBLOOM AND NEWELL 283
accomplished. Thesubgoalisfulfilledwhentheoperatorhasbeenappliedandanew
stategenerated.
Achunkcouldbecreatedforthissubgoalthatwouldbeapplicablein
any statethatdefinesthe same set ofparameters forthatoperator. The nexttimethe
operatoristrieditwillbeapplieddirectly, sothedifficultywillnotoccurandthesubgoal will not be needed.
Anotherwaythatadifficultycanarisein SOAR2 isifthereisuncertaintyabout
whichoperatortoapplytoastate. Aswithallsuchdifficulties, SOAR2automatically
creates a subgoal to work on this problem. It then employs an operator selection
problem space within which itcan evaluate and compare the set ofcandidate operators. The difficulty is finally resolved when a set ofpreferences-statements about
which operators are preferred to which other operators-has been created that
uniquely determines which ofthe original operators should be selected.
A subgoalthatdealswithanoperatorselectionproblemhasasetofparametersthose aspects ofthe goal and state that were examined during the generation ofthe
preferences. Italsohasasetofresults-thepreferences. Shouldachunkbecreatedfor
thisgoal, it wouldbeapieceofsearchcontrol fortheproblem spacethatallows itto
pick the appropriate operator directly. As the system acquires more search control,
the methodbecomes moreefficientbecauseofthe resulting reduction intheamount
ofsearch required.
Onelimitationofthecurrentchunkingmechanismthatsuchamethodacquisition scheme could alleviate is the inability ofchunks to implement parameterized
operators. Chunking always creates a totally specified piece ofknowledge. As currently formulated, it cannot create the kinds ofparameterized operators used as
terminal nodes in the goal hierarchies. We have seen that chunking does create
abstracted knowledge, and Korf's (1983) work shows that nonvariabilized macrooperatorscanattainagooddealofgenerality fromthegoal hierarchy itself(see section 10.8.3), but fully parameterized operators are outside the current scope. On the
otherhand, problem spacesareinherently parameterizedbytheirinitial anddesired
states. Thereforeit maybethatitisnotnecessary forchunkstocreateparameterized
operators. Theseoperatorscancomefromanothersource(problemspaces). Chunks
would only be responsible for making these operators more efficient.
In summary, the ubiquity ofboth chunking and power-law learning indicates
that the chunking model may notbe limited in its scope to simple speedups. Examining three "hard" types oflearning reveals that generalization and discrimination
are possible viathe combination ofa similarlearning mechanism (production composition) and specific types of problem solving; additionally, method acquisition
appears feasibleviachunking inproblem spaces. Ifthisdoes workout, it mayprove
possible to be able to formulate a number ofthe other difficult learning problems
within this paradigm. The complications would appear as problem solving in
problem spaces, and the chunking mechanism would remain simple, merely
recording the results generated by the problem-solving system.
--- PAGE 298 ---
284 CHAPTER 10: THE CHUNKING OF GOAL HIERARCHIES
CONCLUSION
10.10
Atthe beginning ofthis investigation the authors set outto develop a generalized, task-independent model ofpractice, capable ofproducing power-law practice
curves. The model wastobebasedon theconceptofchunking, and it wastobeused
asthebasis(andasourceofconstraint) foraproduction-systempracticemechanism.
All ofthishasbeenaccomplished. Thegeneralizedmodelthathasbeendeveloped is
based on a goal-structured representation of reaction-time tasks. Each task has its
own goal hierarchy, representing an initial performance algorithm.
Whenagoal issuccessfullycompleted, athree-partchunkcanbecreatedforit.
The chunk is based on the parameters and results ofthegoal. Theencoding component ofthe chunk encodes the parameters ofthe goal, yielding a new symbol representingtheircombination. Theconnectioncomponentofthechunkties theencoded
parameter symbol to an encoded symbol for the results ofthe goal. The decoding
componentofthechunkdecodesthenewresultsymboltotheresultsoutofwhich
itis
composed.
Thechunk improvestheperformanceofthe systemby eliminating the needto
process the goal fully; the chunktakes care ofit. Theprocess ofchunking proceeds
bottomupinthegoalhierarchy. Oncechunksarecreatedforallofagoal'ssubgoalsin
aspecificsituation, itispossibletocreateachunkforthegoal. Thisprocessproceeds
upthehierarchy until there isachunkforthetop-levelgoal forevery situationthat it
could face.
Mechanisms for goal processing and chunking have been built into a new
production-system architecture that fits within a set ofcontraints developed for the
architectureofcognition. Thisarchitecturehasbeenappliedtoanumberofdifferent
reaction-timetasks(thoughnotalloftheseresultsarepresentedhere). Itiscapableof
producing power-law practice curves.
Ascurrentlyformulated,thechunkingtheorystakesoutapositionthatisintermediary among four previous disparate mechanisms: classical chunking, memo
functions, production composition, and macro-operators. These five ideas are differentmanifestationsofasingleunderlyingideacenteredonthestorageofcomposite
information rather than its recomputation.
And finally, a research path has been outlined by which the chunking theory.
when integratedwithaproblem-solvingsystem,canpotentiallybeexpandedtocover
aspects of learning, such as method acquisition, outside of the domain of pure
practice.13
Tor follow up work along tins path, see Laird, Rosenbloom, and Newell 1984)
--- PAGE 299 ---
ROSENBLOOM AND NEWELL 285
ACKNOWLEDGMENTS
This research was sponsored by the Defense Advanced Research Projects
Agency (DOD), ARPAOrderNo. 3597, monitoredby theAirForceAvionics Laboratory under Contract F33615-78-C-1551. The views and conclusions contained in
thisdocumentare those ofthe authors and should not be interpreted as representing
theofficial policies, eitherexpressedorimplied, ofthe DefenseAdvanced Research
Projects Agency orthe U.S. Government.
TheauthorswouldliketothankJohnLairdforinnumerablehelpfuldiscussions
about this material.
References
Anderson, J. R., Language, Memory, andThought, Erlbaum, Hillsdale, N.J., 1976.
, Privatecommunication, 1980.
, Privatecommunication, 1982a.
, "AcquisitionofCognitiveSkill,"PsychologicalReview, Vol. 89, pp. 369-406, 1982b.
, TheArchitectureofCognition, HarvardUniversity Press, Cambridge, 1983a.
, "Knowledge Compilation: The General Learning Mechanism," Proceedings ofthe Machine
Learning Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois at UrbanaChampaign,pp.203-12,June22-24, 1983b. (Anupdatedversionofthispaperappearsaschap. 11
ofthisvolume.)
Anderson,J.R.,Farrell,R.,andSauers,R., "LearningtoPlaninLISP,"TechnicalReport,Departmentof
Psychology, Carnegie-MellonUniversity, 1982.
Bower, G. H., "PerceptualGroupsasCodingUnitsinImmediateMemory,"PsychonomicScience, Vol.
27, pp. 217-19, 1972.
Bower,G. H.,andSpringston,E, "PausesasRecodingPointsinLetterSeries,"JournalofExperimental
Psychology, Vol. 83. pp. 421-30, 1970.
Bower, G. H., andWinzenz, D., "GroupStructure, Coding, and Memory forDigit Series," Journalof
ExperimentalPsychologyMonograph, Vol. 80, Pt. 2, pp. 1-17, May 1969.
Card,S.K.
English,W.K.
andBurr,B., "EvaluationofMouse,RateControlledIsometricJoystick,Step
Keys, andTextKeysforTextSelectiononaCRT,"Ergonomics, Vol. 21, pp. 601-13, 1978.
Chase, W. G., and Ericsson, K. A., "Skilled Memory," in Cognitive Skills and TheirAcquisition,
J. R. Anderson(Ed.), Erlbaum, Hillsdale, N.J., 1981.
Chase, W. G. andSimon, H. A., "Perception in Chess." CognitivePsychology, Vol. 4, pp. 55-81, 1973.
--- PAGE 300 ---
286 CHAPTER 10: THECHUNKING OF GOAL HIERARCHIES
Crossman, E. R. F. W., "ATheoryoftheAcquisitionofSpeed-Skill," Ergonomics, Vol. 2, pp. 153-66.
1959.
DeGroot, A. D., ThoughtandChoicein Chess, Mouton, TheHague, 1965.
Duncan,J., "ResponseSelectionRulesinSpatialChoiceReactionTasks,"AttentionandPerformanceVI,
Erlbaum, Hillsdale, N.J., 1977.
Ernst,G.W.
andNewell,A.
GPS:ACaseStudyinGeneralityandProblemSolving, ACMMonograph,
Academic Press, New York, 1969.
Fikes,R. E.,Hart, P. E.,andNilsson,N.J., "LearningandExecutingGeneralizedRobotPlans,"ArtificialIntelligence, Vol. 3, pp. 251-88, 1972.
Fitts, P. M., and Seeger, C. M., "S-RCompatibility: Spatial CharacteristicsofStimulusand Response
Codes,"JournalofExperimentalPsychology, Vol. 46, pp. 199-210, 1953.
Forgy,C. L., "OPS5User'sManual,"TechnicalReportCMU-CS-81-135, DepartmentofComputerScience, Carnegie-MellonUniversity,July 1981.
Johnson, N. F, "Organization and the Concept ofa Memory Code," in Coding Processes in Human
Memory, A. W. MeltonandE. Martin(Eds.), Winston, Washington, DC, 1972.
Kolers, P. A., "Memorial Consequences ofAutomatized Encoding," Journal ofExperimental Psychology:HumanLearningandMemory, Vol. 1, No. 6, pp. 689-701, 1975.
Korf, R. E., "LearningtoSolve Problemsby Searching for Macro-Operators," Ph.D. diss., CarnegieMellonUniversity, 1983. (AvailableasTechnicalReportNo. 83-138,DepartmentofComputerScience, Carnegie-MellonUniversity, 1983.)
Laird,J. E., "Universal Subgoaling," Ph.D. diss., Carnegie-MellonUniversity, 1983.
Laird,J. E., andNewell, A., "AUniversalWeakMethod,"TechnicalReportNo. 83-141, Departmentof
ComputerScience, Carnegie-MellonUniversity, 1983.
Laird,J.E.,Rosenbloom,P.S.,andNewell,A.,"TowardsChunkingasaGeneralLearningMechanism,"
inProceedingsofAAAI-84, Austin,Tex., pp. 188-192, 1984.
Lewis,C.H.,"ProductionSystemModelsofPracticeEffects,"Ph.D.diss.,UniversityofMichigan. 1978.
Marsh, D, "MemoFunctions,theGraphTraverser,andaSimpleControlSituation," inMachineIntelligence5, B. Meltzerand D. Michie(Eds.), AmericanElsevier, NewYork. 1970.
Michie, D, "'Memo' Functionsand MachineLearning,"Nature, Vol. 218, pp. 19-22. 1968.
Miller, G. A., "The Magic NumberSeven Plus or MinusTwo: Some LimitsonOurCapacity for Processing Information," PsychologicalReview, Vol. 63, pp. 81-97, 1956.
Moran, T. P.. "TheSymbolic Imagery Hypothesis: An Empirical InvestigationviaaProductionSystem
SimulationofHumanBehaviorinaVisualizationTask,"Ph.D.diss.,Carnegie-MellonUniversity,
1973.
. "Compiling Cognitive Skill." AIPMemoNo. 150, XeroxPARC. 1980.
Morin, R. E., and Forrin, B., "MixingTwoTypesofS-RAssociation inaChoice ReactionTime Ihsk"
JournalofExperimentalPsychology, Vol. 64. pp. 137-41. 1%:.
--- PAGE 301 ---
ROSENBLOOM AND NEWELL 287
Neisser, U., Novick, R., and Lazar, R., "Searching for Ten Targets Simultaneously," Perceptualand
MotorSkills, Vol. 17, pp. 427-32, 1963.
Neves, D. M., andAnderson, J. R., "Knowledge Compilation: Mechanisms forthe Automatizationof
CognitiveSkills,"inCognitiveSkillsandTheirAcquisition,J.R.Anderson(Ed.),Erlbaum,Hillsdale, N.J., 1981.
Newell, A., "Heuristic Programming: Ill-Structured Problems," in Progress in OperationsResearch,
Vol. 3,J. Aronofsky (Ed.),Wiley, NewYork, 1969.
, "Production Systems: Models of Control Structures," in Visual Information Processing,
W. G. Chase(Ed.), AcademicPress, NewYork, 1973.
, "Harpy, Production Systems and Human Cognition," in Perception andProduction ofFluent
Speech, R. Cole(Ed.),Erlbaum,Hillsdale,N.J., 1980a. (AlsoavailableasTechnicalReportNo.
CMU-CS-78-140, DepartmentofComputerScience, Carnegie-MellonUniversity, 1978.)
, "Reasoning, Problem Solving and Decision Processes: The Problem Spaceas a Fundamental
Category," inAttention andPerformance VIII, R. Nickerson (Ed.), Erlbaum, Hillsdale, N.J.,
1980b. (Also available as Technical Report CMU CSD, Department ofComputer Science,
Carnegie-MellonUniversity, 1979.)
Newell,A.andRosenbloom,P.S., "MechanismsofSkillAcquisitionandtheLawofPractice,"inCognitiveSkillsandTheirAcquisition, J. R. Anderson(Ed.), Erlbaum, Hillsdale, N.J., 1981.
Newell,A., andSimon, H. A.,HumanProblemSolving, Prentice-Hall, EnglewoodCliffs, N.J., 1972.
Nilsson, N. J.,Problem-SolvingMethodsinArtificialIntelligence, McGraw-Hill, NewYork, 1971.
Rosenbloom, P. S., "The Chunking ofGoal Hierarchies: A Model ofPractice and Stimulus-Response
Compatibility," Ph.D. diss., Carnegie-Mellon University, 1983. (Available as Technical Report
No. 83-148, DepartmentofComputerScience, Carnegie-MellonUniversity, 1983.)
Rosenbloom, P. S., andNewell, A., "LearningbyChunking: AProduction-SystemModelofPractice,"
Technical Report No. 82-135, Department of Computer Science, Carnegie-Mellon University,
1982a.
"LearningbyChunking:SummaryofaTaskandaModel,"ProceedingsofAAAI-82,Pittsburgh,
Pa., pp. 255-257, 1982b.
Samuel, A. L., "Some Studies in Machine Learning Using the Game ofCheckers," IBMJournal of
ResearchandDevelopment, Vol. 3,pp. 210-29, 1959.
, "SomeStudies in Machine Learning UsingtheGameofCheckers, II-RecentProgress," IBM
JournalofResearchandDevelopment, Vol. 11, pp. 601-17, 1967.
Sauers, R., and Farrell, R., "GRAPES User's Manual," Technical Report, DepartmentofPsychology,
Carnegie-MellonUniversity, 1982.
Seibel, R., "DiscriminationReactionTimefora 1,023-AlternativeTask,"JournalofExperimentalPsychology, Vol. 66, No. 3, pp. 215-26, 1963.
Snoddy, G. S., "LearningandStability,"JournalofAppliedPsychology, Vol. 10, pp. 1-36, 1926.
Waterman, D. A.,andHayes-Roth, F. (Eds.),Pattern-DirectedInferenceSystems, AcademicPress, New
York, 1978.
--- PAGE 302 ---
288 CHAPTER 10: THECHUNKING OF GOAL HIERARCHIES
Winston, P. H., "Learning Structural Descriptions from Examples," in The Psychology ofComputer
Vision, P. H. Winston(Ed.), McGraw-Hill, NewYork, 1975.
Woodworth,R. S.,andSchlosberg, H.,ExperimentalPsychology, rev.ed.,Holt, RinehartandWinston,
New York, 1954.
--- PAGE 303 ---
KNOWLEDGE COMPILATION:
The General Learning Mechanism
John R. Anderson
Carnegie-Mellon University
Abstract
The ACT learning mechanismsofknowledgecompilation, consisting ofcomposition and proceduralization, are discussed. Composition operates by collapsing
multipleproductionsintoasingleproductionthathastheeffectofthe set. Proceduralizationoperatesbybuildingintoproductionsinformationthatpreviouslyhadtobe
retrieved fromlong-termmemory. Itis shownhowthesetwomechanismscan simulatetheinitial stagesofskillacquisitioninthedomainoflearninghowtoprogram. It
is also shown thatthese mechanisms can reproduce the effects thathavebeen attributedtoinductivelearning mechanisms involving generalizationanddiscrimination.
Generalizations and discriminations emerge as consequences ofcompiling the processes ofanalogy formation and errorcorrection.
11.1 INTRODUCTION
One of the oldest intellectual issues is whether all forms of learning can be
accounted forby associative learning-thatis, learningby associating co-occurring
elements (seeAndersonandBower, 1973, forahistorical review). Oneconsequence
ofournewtechnical agehasbeentorefinethisquestionpractically outofexistence.
However, theissuehasreceivedanewembodimentintheworldofmachinelearning
inwhichthere isasetofproposalsaboutlearningby inductionanddiscovery. These
learningmechanismsassumeimplicitlythatitisnotpossibleforadequatelearningto
be achievedby means ofassociationby contiguity. Approximately halfthepapersat
the 1983 Machine Learning Conference had this character.
--- PAGE 304 ---
290 CHAPTER 11: KNOWLEDGE COMPILATION
Thebasicassumptionisthatanyadequatelearningsystemhastohaveaspartof
its basic architecture the ability to compare noncontiguous examples, look forcommonalitiesanddifferences, formulatehypotheses, andactonthesehypotheses. Two
papers, however, seemedtobethe intellectual heirstotheassociation-by-contiguity
position; one wasby Rosenbloomand Newell, andtheotherwas by this author. The
twoproposals were quitedifferent in detail but similar in spirit. The common spirit
consistedoftwoassumptionsaboutthebasicarchitecture. Thefirstwasthatbehavior
was controlled according to hierarchical goal structure-that the basic category of
behaviorwasproblemsolving, notinduction. Inbothpaperstheproblemsolvingwas
encoded in a production system. The second assumption was that the only form of
learningconsistedofcreatingone-stepoperatorsthathadthe sameeffectasthe multiplestepsofinformationprocessingintheoriginalproblemsolution. Thegoalstructure is used in deciding which steps belong together and should be collapsed into a
single operator.
Thepurposeofthischapteristoarguethatthesetwoarchitecturalassumptions
areadequatetoaccountforinductivelearning. Thisargumentwillbemadewithinthe
context of the ACT* theory of learning (Anderson, 1983a). The ACT* theory
includes both inductive learning mechanisms and operator-collapsing mechanisms.
It will be shown here that operator collapsing can account for the phenomena that
wereattributedtotheinductivemechanismsinACT*. Ofcourse,thisleavesopenthe
questionofwhetherthereareotherinductiveprocesses, notinACT*,thatarebeyond
the scope ofoperatorcollapsing. However, it will be lefttoothersto show that such
phenomenaexist.
Knowledgecompilationisthenamegiventotheprinciplesin ACT thatgovern
operator collapsing (Anderson, 1982, 1983a; Anderson, Sauers, and Farrell, 1982;
NevesandAnderson, 1981). Theseprinciples areconcerned with how anew skill is
acquired, such as generating aproofingeometry (Anderson, 1983b). Inthe general
frameworka learner isviewedasbeginning withdeclarative information relevantto
theexecutionofaskill. Forinstance, inthecaseofgeometry, thestudentmightlearn
aboutthepropertiesoftwo-columnproofsandvarioustheoremsandpostulates. This
information isstored indeclarativeform-thatis, asfactsaboutthedomain. Forthis
knowledge to be used, general interpretive procedures must be applied to it. Two
types ofinterpretive procedures commonly observed in human subjects are general
problem-solving procedures and general analogy procedures. Knowledge compilation operates on the traces ofsuch procedures, creating more efficient procedures
that are specific to the task domain.
Thischapterwilldiscussknowledgecompilationandgiveanexampleofitsuse
tosimulatethe learningofinitial programming skills. The restofthechapterwill be
devoted to discussing how knowledge compilation can produce inductive processes
ofdiscrimination and generalization (Anderson, Kline, and Beasley. 1979; HayesRoth and McDermott, 1976; Langley, 1985; Michalski and Stepp. 1983; Mitchell.
1978; Vere, 1975). It will be argued further that there is evidence in the human
--- PAGE 305 ---
ANDERSON 291
case that knowledge compilation is the process that underlies generalization and
discrimination.
11.2 INTRODUCTION TO KNOWLEDGE COMPILATION
Knowledge compilation mechanisms are defined with respectto aproduction
system like ACT (Anderson, 1983a), which has a separate long-term declarative
memory to represent facts and a production memory to represent procedures. The
knowledgecompilationmechanismsoperateonthetracesofproductionapplications
tocreatenewproductions. Beforethedetailsofafullexampleorofthe implementation are presented, a briefoverview will be given.
Theknowledgecompilationprocessesin ACTcanbedividedintotwosubprocesses. The first, called composition, takes a sequence ofproductions that follow
eachotherinsolvingaparticularproblemandcollapsesthemintoasingleproduction
thathasthesameeffectasthesequence. Theideaofcompositionwasfirstdeveloped
byLewis (1978). Compositionspeedsuptheprocessingbycreatingnewproductions
thatembody the sequence ofsteps used in aparticularproblemdomain. The second
process,proceduralization, buildsversionsoftheproductionsthatnolongerrequire
thedomain-specificdeclarativeinformationtoberetrievedintoworkingmemoryso
the informationcanbe matchedbythegeneral interpretiveproductions. Thus itcreates new productions that collapse the formerly separate processes of information
retrieval and production matching.
The basic processes ofcompilation can be illustrated with the task ofdialing
telephone numbers. It has been noted (Anderson, 1976) that one develops a special
procedure fordialing a frequently dialed telephone number. Sometimes declarative
access tothe number is lost, and the only access one has to the number is through a
procedure fordialing it.
Consider the following two productions that might serve to dial a telephone
number:
PI IF the goal is to dial ?telephone-number
and ?digit is the first digit of?telephone-number
THEN dial ?digit.
P2 IF the goal is to dial ?telephone-number
and ?digitl hasjustbeen dialed
and ?digit2 is after ?digitl in ?telephone-number
THEN dial ?digit2.
Compositioncreates"macroproductions"thatperformtheoperationofapairofproductions that occurred in sequence. Applied to the sequence ofPI followed by P2,
composition would create
--- PAGE 306 ---
292 CHAPTER 11: KNOWLEDGECOMPILATION
P1&P2 IF the goal is to dial ?telephone-number
and ?digitl is the first digit of?telephone-number
and ?digit2 is after ?digitl in ?telephone-number
THEN dial ?digitl and then ?digit2.
Compositionslikethiswill reducethenumberofproductionapplicationstoperform
the task.
A composedproduction like P1&P2 still requiresthatthe information (inthis
case, the telephone number) be retrieved from long-term memory, held in working
memory, and matched to the second and third clauses in P1&P2. Proceduralization
eliminates clauses in the condition of a production that require information to be
retrieved from long-term memory and held in working memory. In P1&P2, the
second and third condition clauses would be eliminated. The variables that would
havebeen bound in matching these clauses are replaced by the values to which they
areboundinthespecialcase. IfthisproductionisappliedindialingMary'stelephone
number, which is 432-2815, the variables in P1&P2 would be bound as follows:
?telephone-number -» Mary's number
?digitl -> 4
?digit2 -> 3
Substitutingthesevaluesforthevariablesandeliminatingthesecondandthirdcondition clauses transform the production into
P1&P2* IF the goal is to dial Mary's number
THEN dial 4 and then 3.
Byfurthercompositionandproceduralization, aproductioncanbebuiltthatdialsthe
full number:
P* IF the goal is to dial Mary's number
THEN dial 4-3-2-2-8-1-5.
Itshould beemphasizedthatformingthisproductiondoes not necessarily imply the
loss ofthedeclarative representation norofthe ability touse it interpretively. In the
few reported cases where peoplecan dial a numberbut not report it, the declarative
knowledge probably has ceased to be used and has simply been forgotten.
Elsewhere (Anderson, 1982, 1983a; Neves and Anderson, 1981) the evidence
for knowledge compilation fromthe literature ofexperimental psychology has been
discussed. The major issuetobeexplained here is howthese mechanismsare implementedand used in relativelycomplex problem-solvingdomains. Theauthor's work
on this topic has been done in the context ofthe GRAPES simulation of the ACT
theory (Sauersand Farrell, 1982). GRAPES isasystemdevotedtosimulatingACT in
the context ofnovice LISP programming. It simulates the problem solving and programming ofnovices writing LISP functions, as well as the way novices learn from
their problem-solving episodes. A typical GRAPES simulation ofa subject will be
--- PAGE 307 ---
ANDERSON 293
describedhere first, andthenthemechanismsofknowledgecompilationunderlying
that simulation will be examined.
11.3 A SIMULATION OF LISP PROGRAMMING
Oneofourconsistentobservationsaboutnovicesisthattheyarenotabletoread
instructions ofeven modest complexity and then generate the instructed behavior
withouterror. Thisis not surprisinggiventhe ACT theory. Accordingtothattheory,
instructions are initially stored in a declarative form, but behavior requires procedures that are represented as productions. Instructions cannotdirectly setupprocedurestoperformtheskill. Generalinterpretiveproductionsmustconvertthisknowledge into behavior. Many ofthe problems arise because ofthe indirection through
these interpretive productions.
Theproblem-solvingepisodedescribedhereisoneoftheearlyonessimulated
by the author. (For simulations of more complex programming, see Anderson,
Sauers, andFarrell, 1982.)Thedifficultiesexperiencedinthisepisodearetypicalof
the difficulties people have in making thetransition from instructionto experience.
The subject, BR, had read the instruction on pages 33 to 37 ofWinston and Horn
(1981) on function definition, but she extracted virtually nothing from the text
instruction. What she did extract was a template for how to write a function
definition:
(DEFUN< function name>
(<parameter 1> <parameter 2> <parameter n>)
. . .
<process description>
Winston and Horn assertthat "angle brackets delineate descriptions ofthings." She
also studied some examples offunction definitions to which she referred. The most
important ofthese converted Fahrenheit to Celsius:
(DEFUN F-TO-C (TEMP)
(QUOTIENT (DIFFERENCE TEMP 32) 1.8))
BR'sfirstproblemwastodefinethefunction FIRST, whichreturnsthe firstelement
ofalist. Sheknewthefunction CARandhowtouseit wheninteractingwiththemonitorinLISR CARreturnsthefirstelementofthelistthatisitsargument. Sheknew, for
instance, that ifshe typed (CAR '(ABC)), the monitor would returnthe answerA.
Thus this problem is really an exercise in using the syntax of function definition
ratherthan one in defining a novel function.
BR'smethodofsolvingtheseproblemsreliedheavilyontryingtousethestructure ofthe template and the examples to guide her function writing. This process is
referredtoasstructuralanalogy. A production systemwascreated in GRAPES that
wouldsimulatethisprotocol. Theonlyproductionsrequiredforthissimulationwere
productionsthatcoulddostructuralanalogyandproductionsthatcouldusetheLISP
functions CAR and CDR at the top level. The first type ofproduction represents a
--- PAGE 308 ---
294 CHAPTER 11: KNOWLEDGECOMPILATION
general abilitythatcanbeused in many contexts (for instance, in filling out income
taxforms). ThesecondtypewasacquiredfromworkwithearlierchaptersinWinston
and Horn.
Figure 11-1 illustratesthegoaltreegeneratedin simulatingthisexample. Each
boxinfigure 11-1 representsagoal, andeacharrowemanatingfromabox represents
a GRAPES production trying to achieve the goal. Ifthe production generates subgoals, it is connected to goal boxes below. The simulation starts with the goal of
writing the function and chooses to use the template for function definition as a
guide. This is referredtoasmappingthe template. The first subgoals that GRAPES
processedinfigure 11-1 involvedmappingDEFUNand <functionname> inthetemplate. Productions responding to these goals wrote out ' ' (DEFUN FIRST" without
difficulty.
Likeoursubject, GRAPES was notableto write outthe parameterpart ofthe
functiontemplatedirectlybecauseGRAPESdidnotknowwhata "parameter" was.
In cases likethis, GRAPES' analogy productions will resortto aconcrete example.
TheconcreteexampleretrievedbyGRAPES isthedefinitionofF-TO-Cgivenearlier.
GRAPES solvedtheanalogyasfollows: Xisto F-TO-Castheparameterlist-that is,
(<parameter 1> <parameter 2> . . . <parameter n> ) - is to the abstract
template, and it retrieved (TEMP) as the value for X. Thus it decided (TEMP) was
serving the parameter role in F-TO-C. Then it solved the analogy X is to FIRST as
(TEMP) istoF-TO-Candcameupwiththeanswer (LIST1)
which itputintothefunction definition; that is GRAPES decided (LISTl) servedthecomparable role in the
function it was defining as (TEMP) was serving in F-TO-C.
Then GRAPES turned to the process definition. Being unable to interpret
directly what is meant by <process description> , it looked to its concrete
exampleF-TO-CandsawthatLISPcodefilledthisslot,whichperformedthefunction
operations. In analogy, GRAPES set its goal to write code that would perform the
operationsrequiredbyFIRST. AproductionforusingCARatthetoplevelappliednext
in GRAPES, but there is no production to specify how to write the argument to the
function CARinthiscontext. GRAPESandthesubjectknowthat CARwilloperateon
LISTl,buttheydonotknowthesyntaxforspecifyingtheargumentLISTl. GRAPES
again turns to its concrete example F-TO-C. It solves the analogy (CAR ARG) is to
(QUOTIENT X) and retrieves (DIFFERENCE TEMP 32) , which is the first argument to
QUOTIENT. It then solves theanalogy problemofwhat it mustdoto LISTlto make it
like (DIFFERENCE TEMP 32) anddecides it should embed LISTl in parentheses. The
subject made the sameerror. The function definition at this point as written by both
subject and GRAPES is
(DEFUN FIRST (LISTl)
(CAR (LIST)))
There are twothings to note at this point. First, the subject has read information inthetextthatwouldhaveenabled hertoknowthatshe should notembed LISTl
--- PAGE 309 ---
ANALOGY ANALOGY
BETWEEN BETWEEN
TEMPLATE TEMPLATE
ANDF-TO-C ANDF-TO-C
ANALOGY CODE
BETWEEN FIRST
F-TO-CAND / RELATIONSHIP
FIRST
ANALOGY
BETWEEN
F-TO-C CODE MAP
ANDFIRST
TOP-LEVEL CODE
EXAMPLE
'\ (LIST 1)
r /
WRITE
VARIABLE
COMPILED
ASC2
LIST1
Figure 11-1: ArepresentationofthegoalstructureinsubjectBR'ssolutiontotheproblemofwriting
thefunction FIRST. Theboxesrepresentgoals, andthearrowsindicatethataproductionhas
decomposedthegoalaboveintothesubgoalsbelow. Checksindicatedsuccessfulgoals, andX's
indicatefailedgoals. Thedottedlinesindicatepartsofthegoaltreecombinedincomposition.
--- PAGE 310 ---
296 CHAPTER 11: KNOWLEDGECOMPILATION
inparentheses,butthishasnoimpactonherbehavior. Second, onpreviousoccasions
she had correctly specified variable arguments when evaluating functions atthe top
level. Eventually, theexperimenterusedthissecondfact-thatthesubjectcoulddoit
correctly at the top level-to guide herto a correct solution. Both ofthese observationsillustratetherelativeisolationofknowledge; thatis, knowledgestudiedorused
in one context is not available in anothercontext.
When the subjecttries her functiondefinition, anerror message is generated:
' 'LIST1: undefined function object.' ' GRAPES received the same error messagewhen ittriedoutthesamefunctiondefinition. TheerroroccurredbecauseLISP
treatsthefirstthinginsideaparenthesizedexpressionasafunction. GRAPES associatedthiserrorwiththe failureto specifytheargumentto CARcorrectly. Onprevious
occasions BR had encountered the same error at the top level while typing in commandslike (CAR (A)
wheretheargumentto CARistobetakenliterally ratherthan
evaluated. Inthepast shehadalways repairedtheseerrorsby quotingthe argument;
thatistheargumentisprecededbyasinglequotationmark: ( CAR ' (A) ). Itisassumed
that both the subject and GRAPES have compiled from previous experience a rule
and that the way to repress this error is by quoting the argument, which stops LISP
from evaluating. The function definition at this point is
(DEFUN FIRST (LISTl)
(CAR '(LIST)))
When this new function is tried on an example, LISP returns the CAR of
' ( LISTl) , whichisLISTl, ratherthanthefirstelementofthevalueofLISTl. Thisis
thepointatwhichtheexperimenterintervenedand remindedthe subjectofhow she
would solve the problem at the top level. Ifthe subject were writing code at the top
level she would have used (CAR LISTl) rather than (CAR (LISTl)) or (CAR
'(LISTl)). This intervention was simulated in GRAPES by refocusing it on the
code-argumentgoal infigure 11-1 andputting (CARLISTl) asatop-levelexample in
working memory. Then GRAPES and the subject both mapped this code to its current function definition and came up with the correct code.
11.3.1 Knowledge Compilation
After finally solving this problem, the knowledge compilation mechanism
formedtwoproductionsthataideditssolutionofthe secondproblem. These productions summarized much ofthe problem solving that took place:
CI IF the goal is to write ?function defined on ?variable
THEN write (DEFUN ?function ( ?variable)
and set as a subgoal to code ?relation calculated by 'function
and then write).
--- PAGE 311 ---
ANDERSON 297
C2 IF the goal is to code ?argument
and ?argumentcorresponds to ?variable of?function
THEN write ?variable.
The portions ofthe goal tree that are summarized by each ofthese productions are
encircled in figure 11-1. The firstproductioncapturesthetop-level syntax ofa function,andthesecondsummarizesthesearchinvolvedindiscoveringhowtospecifyan
argumenttoafunction. Withtheseproductions, GRAPESwasabletowriteasecond
function much moreeasily, as wasthe subject. This function, called SECOND, wasto
return the second element ofa list.
11.3.2 Conclusions
There are a number ofconclusions to be drawn from BR's protocol and the
GRAPES simulation. The first concerns the importance of structural analogy in
bridgingthegapbetweencurrentknowledgeandtheneededbehavior. Therearetwo
sources for the structure from which the analogy is made. One is templates and
worked-outproblemsprovidedinthetext. Theotherisstructuresthatthesubjectcan
generate; forinstance, the subjectgenerated (CARLIST1) asatop-level solutionand
then used this in her function definition.
Asecondconclusionisthataproblem-solvingepisodeisorganizedasahierarchicalgoalstructureinwhichthegoalsareexpandedinadepth-firstandleft-to-right
fashion. Jeffries et al. (1981) note this hierarchical structure in the programming
behaviorofexperts, althoughtheirsubjectsusebreadth-firstexpansionincontrastto
the depth-firstexpansionusedby these novice subjects.
The third conclusion concerns the importance of knowledge compilation in
extracting new production rules from an example problem. These rules streamline
the solution of later problems. As the protocol shows, the learning can be accomplishedonthebasis ofasingleexample. Itshouldbe stressedthatthe lessons ofthis
example "stuck"; that is, BRon laterdays did nothave the same difficulty with the
basic syntaxoffunctiondefinition orwith argument specification. It should alsobe
stressedthatcompilationdependscriticallyonthestructureofgoaltrees; thatis, the
structureofthegoaltreeidentifieswhatpartsoftheproblem-solvingepisodebelong
together and what can be collapsed into a single rule.
Inthesethreefeatures-structuralanalogy, hierarchicalgoaltrees, andknowledge compilation-we have one complete solution to the issue ofhow the learner is
able to make the transition to a new cognitive behavior. The important question is
whether there is anything in this transition that might be called induction. In the
opinion of the author, there is not. The process of analogy formation is purely a
problem-solving effort to make the structure ofthe current solution similar to the
structure of the old solutions. Knowledge compilation just puts steps that had
occurred in the original problem into single operators. The system does not try to
formanygeneralization-and, indeed,howcouldit,workingfromasingleexample?
--- PAGE 312 ---
298 CHAPTER 11: KNOWLEDGECOMPILATION
Still, the result isapairofgeneral operators forwriting LISPfunctions. Thisalmost
has the flavorof"blackmagic." In latersectionsthisblackmagic will be explained.
11.4 FURTHER DISCUSSION OF COMPILATION
Before a discussion ofhow generalizations occur is presented, some further
examples ofproceduralization and composition will be considered.
11.4.1 Proceduralization
Proceduralizationcanbeillustratedinitspureformbythefollowingexample.
In GRAPES there is a production that will retrieve function definitions from longterm memory and apply them as follows:
IF the goal is to code ?relation on ?argument
and there is ?functionthatcodes ?relation
THEN use ?function with ?argument
and set as a subgoal to code ?argument.
Inthisproduction, ?relation, ?function, and ?argumentarevariables thatallow
theproductiontomatchdifferentdata. Thesecondlineoftheconditionmightmatch,
forinstance, ' ' CAR codes the firstmemberofalist r with ?functionboundto
CAR and ?relationboundto first member. Ifthis rule is proceduralized toeliminate the retrieval ofthe CARdefinition, it becomes
C3 IF the goal is to code the first member of?argument
THEN use CAR of?argument
and set as a subgoal to code ?argument.
Thisisachievedbydeletingthesecondclauseinthefirstproduction, whichrequired
memory retrieval, andmakingthe restoftheproduction specific tothe relationfirst
element and the function CAR. Now a production has been created that can directly
recognize the application of CAR. The amount of information that has to be maintained in working memory is thus reduced.
11.4.2 Composition
As an example ofpure composition, let us suppose we wanted to add the first
member of LIST1 to LIST2. Then the following two operators would apply in
sequence:
IF the goal is to add ?element to ?list
THEN CONS ?element to ?list
and set as subgoals to code ?element
and to code ?list.
IF the goal is code the first memberof'.'argument
--- PAGE 313 ---
ANDERSON 299
THEN use CAR of?argument
and set as a subgoal to code ?argument.
The first rule above wouldapply binding ?element to "the firstmemberofLIST1"
and ?list to "LIST2." The second production would apply binding ?argument to
"LIST1." A simplecaseofcompositionwouldinvolvecombiningthesetwoproductions togetherto produce
C4 IF the goal is to add the first member of?argumentto ?list
THEN CONS the CAR of?argument to the ?list
and set as subgoals to code ?argument
and to code ?list.
Suchcompositionscollapse repeated sequences ofcoding operations tocreate
macro-operators. Theresultisaspeedupincoding. Amajorissueconcernsdeciding
whichproductionstocomposetogether. Theaboveexampleisafairlysimplecaseof
collapsingtwolevelsofagoaltreeintoone. However, insomecases, suchasthatpresented in figure 11-1, many productions can be collapsed. GRAPES determines
whichproductionstocollapsebyinspectingthegoaltree. Compositiondistinguishes
between two types ofgoals: inherent goals andplanning goals. Inherent goals are
intrinsicpartsoftheprogrammingtask. Forcurrentpurposes, inherentgoalsareall
variantsofwritingcode. The importantfeatureofinherentgoals isthat inachieving
them one achieves part ofthe original task. Planning goals produce results that are
usedtoguidethe solutionoftheoriginal problem, butthe resultsthemselves are not
partoftheoriginalproblem. Infigure 11-1 theinherentgoalsare "codethefunction,"
"code the relationship," "codethe argument," and "write the variable"; all the rest
are planning goals.
Composition collapses productions in one oftwo ways. One is by eliminating
the planning goals that are intermediate between two inherent goals. This is what
happened when production CI, given earlier, was formed for figure 11-1. Compositionformedarulethatwentfromthegoalof"codethefunction" tothegoalof"code
the relationship." Indoingthis, itcompiledoutthe planning process and simply left
intheproductsofthatplanning. ThesameprocessunderliestheformationofC2from
the goal tree in figure 11-1.
The secondwaycompositioncollapsesproductions is illustrated inthecaseof
C4. Here composition starts with the goal ofadding the first element ofone list to
another, skips overthe intermediate goal ofcoding the first element ofthe list, and
sets the goals ofcoding the two lists. In doing this it is basically creating macrooperators similar to those in STRIPS (Fikes and Nilsson, 1971). This learning
schemerequiresthatthelearnerbeabletoidentifywhichsubgoalsareessentialtothe
problem solutions and which are only intermediate to the final solution.
Itneedstobeemphasizedthatneitherproceduralizationnorcompositioneliminates the original production rules from which they were built. Rather, the new
--- PAGE 314 ---
300 CHAPTER 11: KNOWLEDGECOMPILATION
compiled rulesjust serve as supplemental rules that produce better performance in
certain circumstances.
The effect ofthe knowledge compilation process is to create a set ofproductions that mirror the structure ofLISP. They may explicitly involve LISP functions
like CARorCONDorLISPprogrammingtechniquesliketail-recursion. TheseproductionswillpreservetheinherentgoalsthatarespecifictoLISPanddeletetheplanning
goalsinvolvedindomain-generalprocesseslikestructuralanalogy. Thusrepresentative productions become the following (see Anderson, Sauers, and Farrell, 1982):
C5 IF the goal is tocode the second memberof?list
THEN use CADR and set a subgoal
to code ?list.
C6 IF the goal is to obtain all the elements which have
?relationto any member of?list
MAPCONC
THEN use and set as subgoals
1. To code ?function that will return all the elements that have
?relation to ?argument.
2. Tocode?list.
11.5 GENERALIZATION
GeneralizationisamechanismforlearningnewproductionsinACT*. It isthe
learning mechanism in ACT* that is most transparently inductive. As it is typically
formulated(see, e.g., Anderson, Kline, andBeasley, 1980), ittakesapairofproductions and generates what is called the maximal common generalization. This is the
most specific production that will apply everywhere that the original productions
would and that will have the same effect as the original productions. For a simple
example, considerthe following pairofproductions:
51 IF Fred is rich
and Fred is ugly
and Fred is smart
and Fred is ofmedium height
THEN Fred is in club 1.
52 IF Gail is rich
and Gail is ugly
and Gail is stupid
and Gail is ofmedium height
THEN Gail is in club 1.
The following production would be formed as the generalization ofthe two:
Gl IF '/person is rich
and ?person is ugly
--- PAGE 315 ---
ANDERSON 301
and ?person is ofmedium height
THEN ?person is in club 1
where ?personisavariable. Thepairofproductionscanbeviewedasspecificobservations and Gl as a generalization formed from these observations. Note that Gl is
formed both by deleting condition clauses and by replacing constants by variables.
These are the two transformations that occur when one moves to generalizations.
The interesting observation is that compilation results in clause deletion and
replacementofconstantswithvariables. Compilationdeletesclausesassociatedwith
omittedgoalsandwithplanning. Variablesfromplanningproductionscanremainin
thecompiledproductions. Thisis howtheeffectofgeneralizationisobtainedthrough
compilation.
Specifically, itappearsthatgeneralizationscanbeformedthroughtheprocess
ofcompilinganalogies. Thebasicframeworkisasfollows: Thesystemhasinstances
committedtomemory asdeclarative facts. Whenanewinstance isencountered, the
systemcomparesittoamemorizedinstanceandusesthestructureofthememorized
instancetoguideresponse. Compilingthiscompare-and-respondbehaviorproduces
a production that is a generalization from the two instances. This is basically what
happenedinthecaseofformingCI andC2 fromfigure 11-1. A simplerexamplewill
be presented here to make this process more transparent. Then psychological evidence will be introduced indicatingthatthis isthecorrectconception ofgeneralization in humans.
Consider a very simple production set that will classify new instances according to their similarity to studied instances:
PI IF the goal is to classify ?object
and ?reference has been studied
THEN initialize the measure ofoverlap.
and setas subgoals to compare ?objectto ?reference
and to determine if?object is in the same category as ?reference.
P2 IF the goal is to compare ?object to ?reference
and ?object has ?feature
and ?reference has ?feature
THEN incrementthe measure ofoverlap.
P3 IF the goal is to compare ?objectto ?reference
andthere are no more matching features
THEN POPthe goal.
P4 IF the goal is to determine if?object is in the same category as
?reference
and the measure ofoverlap is above threshold
and ?reference is in ?category
THEN ?object is in ?category.
--- PAGE 316 ---
302 CHAPTER 11: KNOWLEDGE COMPILATION
P5 IF the goal is todetermine if?object is in the same category as
?reference
and the measure ofoverlap is below threshold
THEN POP failure.
Thisproductionsetwillkeepcomparingtheobjecttobeclassifiedtocandidatereferences from memory until ahigh-similarity reference is found. Then itwill place the
object in the same category as the high-similarity reference.
Suppose the system has committed to memory that Fred is in category 1 and
thatheisrich,ugly, smart, andofmediumheight. ThenthesystemisaskedtocategorizeGail, who isrich, ugly, stupid, andofmediumheight. Figure 11-2 illustratesthe
goal treegeneratedby GRAPES in applyingthis production setto the classification
problem. Itfoundmatchesonthreefeaturesandamismatchonone, which, itwillbe
assumed, was sufficient to exceed threshold. The compilation process regarded all
the similarity comparisons as planning goals and compiled all ofthis out, forming
the following single production:
IF the goal is toclassify ?object
and ?object is rich
Match Match
Rich Match Medium
Ugly Height
Figure 11-2: A representationofflowofcontrol intheclassificationofGailby analogs toFred.
--- PAGE 317 ---
ANDERSON 303
and ?object is ugly
and ?object is ofmedium height
THEN ?object is in club 1.
This production is essentially identical to the generalization Gl given earlier. Thus
compiling the process ofmaking an analogy will result in generalization.
Thereareanumberoffeaturesthatdistinguishthispathtogeneralizationfrom
astandardinductionparadigmsuchastheACT*generalizationmechanism.
First, it
isbasedonasingleexample. Second, beingtheresultofconsciousproblemsolving,
itismoreflexible. Ifthesystemthoughtcertainfeatures (suchasappearance) should
be discounted in analogy formation, this could be accommodated and a different
analogy wouldappear. Ifthesystemthoughtthatwhatwas importantwasnumberof
extremely positive features (e.g., rich, smart), again this could be accommodated
and a different generalization would appear.
Whathasbeendescribedisaprocedureforproducingthesameeffectasgeneralization. Whatreasonistheretobelievethatthisprocedureistheprocessunderlying
generalization rather than the more direct generalization mechanism? First, it is
moreplausiblewithinthe ACTproductionsystemframework. Toproduceageneralizationin ACT itis necessaryto startoutwithvery specific productions like SI and
S2 given earlier. This is because the generalization mechanism only works on productions. Thesehighly specificproductionsarepatently nonintuitive, andmoreover
theACTtheorywouldclaimitisnotpossibletoformsuchproductionsdirectly. Thus
an ACT theory that produces generalization through compilation is more plausible
than one thatuses the directgeneralization mechanism.
Second, independent ofthe ACT framework, the evidence is against an automatic generalization mechanism that has no strategic component. Both Elio and
Anderson (1984) and Kline (1983) have shown that the generalizations learners
emergewithdependonboththeirproblem-solvingsetandtheorderoftheexamples.
Thus itdoesnotseemthathumansextractalwaysandonlythemaximalcommongeneralizations. The generalizations they do extract are determined by what they are
lookingfor. Thisisbettermodeledbyasystemliketheprecedingonethatleavessimilarity detection atthe strategic level.
Third, notice that the system can classify on the basis of a single studied
instance. A generalization scheme requires atleasttwo instances to formageneralization before novel instances can be classified. Elio and Anderson (1981) showed
thatsubjectscanclassifynovelinstancesonthebasisofsimilaritytoasinglestudied
instance when there is no possibility ofgeneralization. This is consistent with the
currentconception. Inaddition, ElioandAndersonshowedthatsubjectscouldbetter
classify a novel instance when there was a relevant generalization that could be
formed from two studied instances-even when overall similarity of studied
instances to novel instances was keptconstant. This showsthat generalizations also
--- PAGE 318 ---
304 CHAPTER 11: KNOWLEDGE COMPILATION
are made and that all classification is not a matter ofanalogy. Again the evidence is
consistent with the current scheme.
Furthermore, experimentshavebeendoneinwhichsubjectscanclassifynovel
instances after studying instances while unaware of their classification structure
(Brooks, 1978). Forinstance, subjectslearntoassociateanimalsandcitiestostimuli
inasimplepaired-associateexperiment. Afterdoingthepaired-associatetask, they
aretoldthatallthestimuliassociatedtonew-worldanimalsandcitiesformonecategory and all stimuli associated to old-world responses form another category. Subjects were unaware ofthe old-world/new-world dimension during study. They can
nowreliablyclassifynewinstancesasold-ornew-worldstimuli. Sincesubjectswere
not aware ofthe classification structure at study there was no opportunity to form
generalizations. Theclassificationmustbeonthebasisofcomparingspecificstudied
stimuli to specific test stimuli.
Carbonell provides an elaborate discussion ofhow analogy might be used to
guide problem solving that goes beyond the discussion here (1983). He also speculates on how generalizations might emerge from the analogy process. He proposes
thatthelearnerwouldstoreanalogical solutionsandgeneralizefromtheseinjustthe
way onetypically generalizes fromexample. This discussion shows thatgeneralizations can emerge as aby-productofthe analogy process without a separate generalization phase.
11.6 DISCRIMINATION
Lessworkhasbeendoneondiscriminationintheknowledgeacquisitionliterature, andthereislessconsensusabouthowdiscriminationisdone. However, thegeneralsituationcallingfordiscriminationisoneinwhichthereisanoverlygeneralrule
andthusaneedtorestrictitsrange. Itisdoubtfulthatthereisasinglewaytoformdiscriminations, and this probably accounts for the lack ofconsensus in the literature.
Certainlythereisnotauniformdiscriminationprocessinhumans. However, forpurposesofdiscussionakindofdiscriminationcalledactiondiscrimination inthe ACT
theory (Anderson, 1982, 1983a) will be presented here. Again for clarity it will be
illustrated with simple classifications, but Anderson (1982) can be consulted for
more complex applications.
Suppose we have the following general production:
G2 IF ?person is rich
?person is ofmedium height
THEN ?person is in club 1.
Suppose this rule is applied to David, who is smart, good-looking, rich, and of
medium height. It would classify David as being in club 1. However, suppose it is
subsequently learned that David is in club 2. Thus an error has been made, and the
system sets out to form a discrimination that will correct this error.
--- PAGE 319 ---
ANDERSON 305
In forming a discrimination the system looks for some past instance to which
therulecorrectlyapplied. SupposeinthiscaseFred, who issmart, ugly, rich, andof
mediumheight, isretrievedassomeonecorrectlyclassifiedbytherule. Thediscriminationmechanismlooksforsomefeaturethatwastrueofthesuccessfulinstancebut
nottrueoftheunsuccessful instance. Inthiscase, theonlydifferenceisthatDavidis
good-looking and Fred is ugly. Therefore, a new production is formed that has this
added feature:
Dl IF ?person is rich
and ?person is ofmedium height
and ?person is good-looking
THEN ?person is in club 2.
Thisiscalledanactiondiscriminationbecauseithasadifferentactionfromtheproduction (G2) from which it was formed. Di does not replace G2, butbecause ofthe
specificity ordering ofproductions inACT, Dl willtakeprecedenceoverG2 wheneverboth match. G2 willonly apply whentheperson isnotgood-looking. Notethat
informinganactiondiscriminationthesystembothaddstotheconditionofaproduction and changes the action.
Theinterestingthingtonoteisthatcompilationcanhavetheeffectofaddingto
the condition ofaproduction and changing the action. When we compose two productions, PI followedby P2, we can addtothe conditions ofPI some ofthe conditions ofP2, and we can change the action ofPI to the action ofP2. Thus it appears
that we mightbe able to getthe effect ofdiscrimination through compilation.
Thebasic schemeforgettingdiscriminationthroughcompilationistohavethe
system consciously and deliberately go through the steps involved in forming a discrimination. Discrimination is done by problem-solving productions rather than as
an automatic process above the production system. Thus a sequence ofproductions
willcomputeadiscrimination. Compilingthatsequencewillresultinadiscriminate
production. As will be seen, doing this is not as straightforward technically as the
generalization case. This isbecause it wouldbe necessary to inspectthe contents of
productionstomimicperfectlytheautomaticdiscriminationprocess, whereasthisis
not necessary in order to mimic generalization. In the ACT theory one production
cannotinspectthecontentsofanother. Still, wecangetnearlytheeffectofautomatic
discrimination in a fairly plausible way.
Asnotedearlier, therearemultiplewaysofformingdiscriminations. Thissection will focus onthe action discriminationasjust sketched out. However, this general schemewillprobablyextendtoothertypesofdiscrimination. Inthisschemethe
productions compute the discrimination and then compile that computation. After
thecompilationofanactiondiscriminationisillustrated, theevidencethatthisview
ofdiscrimination is psychologically correct will be considered.
This example will be developed within the framework ofproductions P1-P6
given earlierforclassifying objectsby analogy. The learning situation here is one in
--- PAGE 320 ---
306 CHAPTER 11: KNOWLEDGE COMPILATION
which the learner makes a classification ofan object, receives feedback, and ifthe
classification is incorrect tries to find a discriminating feature. Assume the learner
has compiled the following too-general production:
G2* IF the goal is to classify ?object
and ?object is rich
and ?object is ofmedium height
THEN ?object is in club 1.
ThisisavariantofG2givenearlierbutnowsetupforthecurrentframework. Asearlier, assume itmisappliestoclassifyDavid, who issmart, good-looking, rich, andof
mediumheight, asbeinginclub 1. Anerrorisdetected, andagoal is settoreclassify
David.
The followingthreeproductionsarerelevanttothecorrectionofamisclassification:
P7 IF the goal is to classify ?object
and ?object is in ?categoryl
but ?object was classified as in ?category2
and ?reference is in ?category2
THEN remove the classification of?object
and setas subgoals to find a difference between ?object and
?reference
and then to reclassify ?object
and then to correct the classification of?object.
P8 IF the goal is to find a difference between ?object and ?reference
and ?object is in ?category1
and ?object has ?featurel on ?dimension
and ?reference has ?feature2 on ?dimension
and ?featurel is different from ?feature2
THEN conclude ?object is in ?categoryl because of?featurel.
P9 IF the goal is to correct the classification of?object
and ?object has been classified as in ?category2
but ?object is in ?categoryl because of?featurel
THEN change the classified category of?object to ?categoryl.
Production P7 will applytocorrectthe situation. It selects some instance that was in
the incorrectcategory. Supposeagain itselects Fred, who is rich, ugly, smart, andof
medium height. This isan instance theovergeneral production G2* would have correctly classified. There is no guarantee that the instance selected by P7 will be one
that fits the overgeneral production, because P7 cannot inspect G2*. However, the
spreadingactivation retrieval mechanismsin ACT (see Anderson, 1983a) wouldtend
to select an instance thatoverlapshighly with thecurrent instanceand hence the rule
--- PAGE 321 ---
ANDERSON 307
thatclassifiedthe current instance. P7 sets goals to find adifferencebetween David
and Fred and then to reclassify David.
Production P8 will note that David is good-looking and Fred is ugly and so
enterintomemorythefactthat"Davidisinclub
becauseheisgood-looking." Then
the system will return tothe goal ofclassifying Davidonceagain. Again G2* would
applytomisclassifyDavidasbeinginclub
butnowproductionP9canapplytocorrectthe classification ofDavid. P7 followed by P8 followed by G2* followedby P9
constitutes agoal treethatcanbe composedtogetherandproceduralized. The result
will be the following production:
Dl* IF the goal is to classify ?object
and ?object is rich
and ?object is ofmedium height
and ?object is good-looking
THEN ?objectisinclub2.
This is essentially the same as Dl formed earlier by the automatic discrimination
mechanism.
There are two essential features that distinguish this means to discrimination
from the automatic discrimination ofthe current ACT theory. First, it requires that
thelearnerinitiallymakeaclassificationoftheobjectandthencorrectthatclassification ifit is inerror. Second, it requires thatthe learnermake aconscious hypothesis
about what distinguishes the current instance from prior instances that were in the
hypothesized category. Both ofthese features have been confirmed in two series of
experiments by Lewis and Anderson (1985). One series of experiments involved
rulesforprovingtrianglescongruentingeometry, andtheotherseriesinvolvedrules
fortravelingthroughamaze. Inbothseriessubjectsweregivenovergeneralrulesthat
hadtobe discriminated. Subjects whopassively studied instances failedto learnthe
discriminating features. Subjects learned only when they made active hypotheses
aboutthecorrectrule, whichcouldthenbedisconfirmed. Furthermore,theonlysubjects to learn were those who had some conscious awareness ofwhat the discriminating features were. Thus subjects discriminated only when both conditions ofthe
current scheme ofdiscriminating through compilation were satisfied.
The example illustrated above is somewhat unrealistic since it assumes the
learnerboth findsthediscriminating featureanduses itto reclassifythe item. However,thisiswhatisrequiredifadiscriminatedproductionistobeformedonthesame
trial as the discriminating feature is identified. It seems more reasonable to assume
thatthisisstretchedoutovermultipletrials-ononetrialthelearnerformsadeclarative proposition about the importance of a discriminating feature like "goodlooking," and on another trial the learner acts on this information. The discriminating production wouldbeacquiredonthelatertrial when the learneracted. There
isnoguaranteeinthissituationthattheproductionlearnedwouldbeidenticaltowhat
--- PAGE 322 ---
308 CHAPTER 11: KNOWLEDGE COMPILATION
isformallydefinedasanactiondiscriminationinACT*,butthelearningwouldbein
the general direction ofdiscrimination.
GENERAL CONCLUSIONS
11.7
There is ample evidence for the existence of something close to knowledge
compilation as a basic process of human learning (Anderson, 1982; Anderson,
Farrell,andSauers, 1983; NevesandAnderson, 1981). Theevidencehadalwaysbeen
rather scarce forthe details ofthe ACT mechanisms ofgeneralization and discrimination, andmorerecently somerathernegativeevidencehasbeengathered. Clearly,
humans can approach the task ofextending experience as typical problem solving.
Generalized or discriminated productions appear as the product ofcompiling this
problem solving.
It is something ofan embarrassment that the author worked so long with the
ACT mechanisms before realizing how the knowledge compilation mechanisms
couldbe recruitedtoprovidetheeffectofgeneralizationanddiscrimination. This is
because he thought of knowledge compilation as simply making existing paths of
processingmoreefficientratherthanenablingnovelpathsofprocessing. Togetnovel
behaviorit seemedthatinductivemechanismsoflearning suchasgeneralizationand
discriminationwereneeded. Whatwasnotrecognizedwasthatiftheresultsofacting
onthebasis ofsimilarity detectionand differencedetection werecompiled, productions could be produced that enabled novel behavior.
Thefundamentalpointthenisthattheinductionprocessoccursasaconscious
problem-solving effortto find abasis fordealing with a new case. In compiling the
results ofthis problem solution, productions are formed that will extend to the new
situation. The fundamental category ofbehavior is problem solving, not induction.
This theory is one of learning not by temporal contiguity but by contiguity in the
problem-solving goal structure. There is no such thing as unconscious induction of
features. Recently, Dulany, Carlson, andDewey(1984)havedemonstratedthatinsituations in which subjectsare supposedlyengaged inunconscious inductionthey can
be shown to have conscious inductive hypotheses on which they are acting.
References
Anderson, J. R. Language, Memory, andThought, Erlbaum, Hillsdale, N.J., 1976.
, "AcquisitionofCognitiveSkill," PsychologicalReview, Vol. 89, pp. 369-406, 1982.
. IheArchitectureofCognition. Harvard University Press. Cambridge 1983a.
. "Acquisition of Proof Skills in Geometry," in Machine Learning: An Artificial Intelligence
Approach. R. S. Michalski,J. G. Carbonell,andT. M. Mitchell (Eds). Tioga. PaloAlto. Calif .
1983b.
Anderson. J. R .. and Mower. O. H . HumanAssociativeMemory. Winston. Washington. DC . 1973
--- PAGE 323 ---
ANDERSON 309
Anderson, J. R., Kline, P. J., andBeasley, C. M., "A General Learning Theory and Its Applicationto
SchemaAbstraction,"inThePsychologyofLearningandMotivation,Vol. 13,G.H.Bower(Ed.),
AcademicPress, NewYork, 1979.
, "Complex Learning Processes," inAptitude, Learning, andInstruction, Vol. 2, R. E. Snow,
P. A. Frederico,andW. E. Montague(Eds.), Erlbaum, Hillsdale, N.J., 1980.
Anderson, J. R., Sauers, R., and Farrell, R., "Learning to Plan in LISP," Technical ReportONR82-1,
Carnegie-MellonUniversity, 1982.
Brooks, L. "NonanalyticConceptFormationandMemoryforInstances," inCognitionandCategorization, E. RoschandB. B. Lloyd(Eds.), Erlbaum, Hillsdale, N.J., 1978.
Carbonell,J. G., "LearningfromAnalogy: FormulatingandGeneralizingPlansfromPastExperience,"
inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski,J. G. Carbonell, and
T. M. Mitchell(Eds.),Tioga,PaloAlto,Calif., 1983.
Dulany,D.E. , Carlson,R.A. , andDewey,G.I., "ACaseofSyntacticLearningandJudgment:HowConsciousandHow Abstract?"JournalofExperimentalPsychology: General, Vol. 113, pp. 54-55,
1984.
Elio, R., and Anderson, J. R., "The Effects ofCategory Generalization and Instance Similarity on
SchemaAbstraction,"JournalofExperimentalPsychology:HumanLearningandMemory,Vol
.7,
pp. 397-417, 1981.
"TheEffectsofInformationOrderandLearningModeonSchemaAbstraction," Memoryand
Cognition, Vol. 12,pp. 20-30, 1984.
Fikes, R. E., andNilsson, N. J., "STRIPS: ANewApproachtotheApplicationofTheoremProvingto
ProblemSolving,"ArtificialIntelligence, Vol. 2, pp. 189-208, 1971.
Hayes-Roth, F,andMcDermott,J., "LearningStructuredPatternsfromExamples,"Proceedingsofthe
ThirdInternationalJointConferenceonPatternRecognition,Coronado,Calif. pp.419-23, 1976.
Jeffries, R.; Turner, A. A.; Poison, P. G.; andAtwood, M. E., "The Processes Involvedin Designing
Software," in CognitiveSkillsandTheirAcquisition, J. R. Anderson(Ed.), Erlbaum, Hillsdale,
N.J., 1981.
Kline, P. J., "ComputingtheSimilarityofStructuredObjectsbyMeansofaHeuristicSearchforCorrespondence," Ph.D. diss., UniversityofMichigan, 1983.
Langley, P., "A General Theory ofDiscrimination Learning," in Self-modifying Production System
ModelsofLearningandDevelopment, D. Klahr, P. Langley, andR. T Neches (Eds.), Bradford
Books, Cambridge, Mass., 1985, inpress.
Lewis,C.H. , "ProductionSystemModelsofPracticeEffects,"Ph.D.diss., UniversityofMichigan, 1978.
Lewis,M.,andAnderson,J. R., "TheRoleofFeedbackinDiscriminatingProblem-solvingOperators,"
CognitivePsychology, 1985, inpress.
Michalski, R. S., and Stepp, R. E., "Learning fromObservation: Conceptual Clustering," inMachine
Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and
T M. Mitchell(Eds.), Tioga, PaloAlto, Calif., 1983.
Mitchell,T. M., "VersionSpaces:AnApproachtoConceptLearning,"Ph.D.diss.,StanfordUniversity,
1978.
--- PAGE 324 ---
310 CHAPTER 11: KNOWLEDGECOMPILATION
Neves, D. M., and Anderson, J. R., "Knowledge Compilation: Mechanisms forthe Automatization of
CognitiveSkills,"inCognitiveSkillsandTheirAcquisition,J.R.Anderson(Ed.),Erlbaum,Hillsdale, N.J., 1981.
Sauers, R., and Farrell, R. "GRAPES User's Manual," Technical ReportONR-82-3, Carnegie-Mellon
University, 1982.
Vere,S.A.
"InductionofConceptsinthePredicateCalculus,"ProceedingsoftheFourthIJCAI,Tbilisi,
Georgia, USSR,pp. 281-87, 1975.
Winston, P. H., andHorn, B. K. P.,LISP, Addison-Wesley, Reading, Mass., 1981.
--- PAGE 325 ---
LEARNING PHYSICAL DOMAINS:
Toward a Theoretical Framework
Kenneth D. Forbus
Dedre Gentner
UniversityofIllinois
at Urbana-Champaign
Abstract
This chapter presents a theoretical framework that is being developed in an
attempttoconstructacomputationalaccountofhumanlearningofphysicaldomains.
QualitativeProcesstheoryisusedtomodelportionsofpeople'sphysicalknowledge,
and Structural Mappingtheory isusedtocharacterizethecomputationsthatmovea
learnerfromonerepresentationtoanother. Thechapteroutlinesthecomponenttheories and proposes a learning sequence forphysical domains.
INTRODUCTION
12.1
Peopleuseandextendtheirknowledgeofthephysicalworldconstantly. Understanding how this fluency is achieved would be an important milestone in understanding human learning and intelligence, as well as auseful guide forconstructing
machinesthatlearn. Theauthors' purposeistoconstructacomputationalaccountof
human experiential learning in physical domains.
This work is still at the stage where questions are being refined rather than
answers provided. In many cases, there is no direct evidence for the claims made
here. In other instances, support for the theory is obtained by combining evidence
from several different areas, including developmental psychology, studies of
--- PAGE 326 ---
312 CHAPTER 12: LEARNING PHYSICAL DOMAINS
learning, and other psychological research. No one of these is adequate by itself.
Whenextrapolatingfromadultlearningresearch, wemustkeepinmindthatcasesof
pure experiential learning are rare in adult life; some sort of instruction or prior
expectationistypically involved. Developmental researchprovides agood sourceof
data, since much ofyoung children's learning is truly from direct experience. Yet
whendevelopmental resultsareapplied, it mustbe rememberedthatchildrenarenot
only learningbutalsomaturing. Therefore, inordertoisolateand studyexperiential
learning, the existing empirical findings must be examined, filtered, and carefully
fittedtogether. Although spacedoesnotpermitdetailingall the relevantlinesofevidence, theauthors willtrytogivethe readersomejustification forourclaims whenever possible.
As this volume attests, the past few years have seen significant progress in
machine learning. However, to construct programs that learn as well as (or better
than)peopledo, itisimportanttounderstandhowhumanlearningworks. Ultimately
both psychological studies and directcomputational experiments (i.e., constructing
programs)willbenecessarytoprovideafullaccount. Tothisend, theauthorswilltry
when possible to indicate how techniques developed in machine learning might be
used to implement such programs.
12.1.1 Overview
A briefprologue may help to organize the material. Three key ideas underlie
the theory: (1) the centrality ofphysicalprocesses in mental models ofscience; (2)
theimportanceofanalogy inlearning; and(3)theprimacyofrich, contextually specific representations. The ideathatthe notion ofprocess is central tohuman knowledge about physical domains is the chieftenent ofQualitative Process (QP) theory
(Forbus, 1981, 1984). This is not to say that notions ofprocess are there from the
beginning. Rather, it is hypothesized that a person's experiential knowledge of a
domainbeginsasacollectionofscenariosthatdescribeparticularphenomena, outol
whichisdevelopedavocabularyofprocessesthatprovidesanotionofmechanismfor
the domain. The second key idea concerns the role ofcomparisons among related
knowledgestructures. Theauthorsconjecturethatmuchofexperiential learningproceeds through spontaneous comparisons-which may be implicit or explicit
between acurrent scenarioand priorsimilaroranalogous scenarios that the learner
hasstored inmemory. Structure Mappingtheory (seeGentner, 1980. 1983)describes
these kinds ofcomparisons.
Thethirdideaisaratherparadoxicalclaim: inhumanprocessing, more isoften
easier. Rich, perceptuallybasedrepresentationsareacquiredearlierin learningthan
'It should be noted thai psychologists by no means generally agree with this claim Consequently, the
authorstried tobe Fairly explicit in presentingevidence torthis position.
--- PAGE 327 ---
FORBUSAND GENTNER 313
sparse abstract representations. That is, early domain representations differ from
more advanced representationsofthe samedomain incontaining more information,
especially perceptual information specific to the initial context of use. A second
aspect ofthe "more is easier" claim concerns comparisons: it is suggested that, for
humans, similarity comparisons are easier when there is more overlap between the
two knowledge structures being compared.
On the basis of these three ideas, the authors propose a canonical learning
sequence. The claim is that human learning ofphysical domains can be viewed as a
sequence ofdifferent mental models: (1)protohistories, (2) the causal corpus, (3)
naivephysics, and (4) expert models. Briefly, protohistories are rich, contextually
specific, highly perceptual representations of phenomena, capturing expectations
abouttypicalphenomenologicalpatterns-forexample, "IfIturnthekey,thecarwill
start." Withthecausal corpus, theexpectationsofmechanismenters; heretherepresentation consists of simple statements that some sort of causal connection exists
betweenvariables-"Ifthecarhasnogas, itwillnotstart."Inthenaivephysicsstage,
processes are introduced to provide the mechanism underlying the causal corpus-
"Gas must flow fromthe tanktothecarburetorandmix with airsothatthe mixture
canbe ignitedbythespark." Thedisparatelocalconnectionsofthecausalcorpusare
replacedwith qualitative models organized aroundthe notionofprocess. Finally, in
the expert models stage, quantitative representations are created-for example,
models ofthe effects ofdifferent mixtures ofoxygen and gasoline.
In this chapter the authors discuss their conjectures about these models and
about how a learner constructs one type ofmodel from another. First, however, the
component theories that underlie this framework are briefly summarized: Qualitative Processtheory, whichprovidesconceptsneededtorepresentthemodels(particularlyinthenaivephysicsstage), andStructureMappingtheory, whichcharacterizes
the kinds ofcomputations that move the learner from one representationto another.
Then theoverall role ofstructure-mappingcomparisons is examined intheprogression from rich to sparse representations. With these foundations in place, the four
stages oflearning for physical domains are then described.
12.2 QUALITATIVE PROCESS THEORY
The first requirement ofthis work is a language in which to describe people's
commonsenseknowledgeaboutphysical situations. Peopleknowaboutagreatmany
kinds ofphysical changes: things move, collide, bend, break, heat up, cool down,
flow, andboil. Intuitively wethinkoftheseasprocesses. Qualitative Processtheory
attemptstoformalizethisnotionofprocesstoprovideacommonformforqualitative
theories ofdynamics. As will become clear lateron, the authors do not believe that
the firstmodelspeopleconstructofadomaintakethe formofprocess, noreventhat
theybecomeknowledgeableenoughtoconstructthesemodelsforeverydomainthey
--- PAGE 328 ---
314 CHAPTER 12: LEARNING PHYSICAL DOMAINS
experience. Nevertheless,
someoftheconceptsofQPtheorywillbeusefulindescribing models in other stages as well.
In QP theory, aphysical situationismodeledasacollectionofobjectsandrelationships among them, with processes responsible forcausing changes. The continuous parameters ofan object, such as temperature and pressure, are represented by
quantities. A quantityhastwoparts, anamountandaderivative. Amountsandderivativesarebothnumbers. The model tokeep in mind fornumbers isthatofthe reals,
but it is important to note that in QP theory particular numerical values are never
used. Instead,thevalueofanumber isdescribedintermsofitsquantityspace-acollectionofinequalitiesthatholdbetween itandotherquantities. Figure 12-1 illustrates
a quantity space for the level ofliquid in a container. The quantity space is a useful
qualitative representation because processes typically start and stop when inequalities between parameters change.
Figure 12-2 illustrates a typical process, called LIQUID-FLOW. A process has
fiveparts: individuals preconditions quantityconditions relations, andinfluences.
, , ,
Roughly speaking, the individualspartdescribeswhereinstancesofaprocess might
occur, the preconditions and quantity conditions tell when it will be acting, and the
relations and influences describe what holds as a consequence ofit acting. In more
detail, foranycollectionofobjectsthatmatchestheindividual specificationsthereis
aprocess instance that represents the potential for that process to occur between
those individuals in a particular way. For example, there will be two instances of
LIQUID-FLOWbetweentheliquidinthecontainersinfigure 12-2,eachcorresponding
to flow in a particulardirection.
A process instance is active whenever both its preconditions and its quantity
conditionsaretrue. Thedistinctionbetweenpreconditionsandquantityconditionsis
that quantity conditions can be determined within QP theory but preconditions
cannot. Quantityconditionsconcernwhatinequalitiesholdandwhatotherprocesses
(orindividual views, whichare introducedbelow) areactive. Preconditionsconcern
- -
BOTTOM(a) LEVEL(wa) ^TOP(a)
LEVEL(wb)
Figure 12-1: Aquantity spacedescribesthe valueofa numberb\ the inequalin relationshipsthat hold
between
andothernumbers. Anarrowindicatesthatthenumberatitsheadisgreaterthanthenumberat
itstail. Thus LEVEL(wa) islessthanLEVEL(wb) andgreaterthanBOTTOM(a),andLEVEL(wb) andTOP(a)
aie unordered.
--- PAGE 329 ---
FORBUSAND GENTNER 315
Process LIQUID-FLOW
Individuals:
source, a CONTAINED-LIQUID
dest, a CONTAINED-LIQUID
path, a FLUID-PATH, FLUID-CONNECTION(source, dest, path)
Preconditions:
ALIGNED(path)
Quantity Conditions:
A[PRESSURE(source)] > A[PRESSURE(dest)
Relations:
Let flow-rate, diff be quantities
diff = PRESSURE(source) - PRESSURE (dest)
flow-rate ocQ+ diff
Influences:
I + (AMOUNT-OF(dest), A[flow-rate])
I - (AMOUNT-OF(source), A[flow-rate])
Figure12-2: Atypicalprocess.Thisprocessspecificationdescribesasimplekindofliquidflow. Itcan
occurbetweentwocontainedliquidsthatareconnectedbyafluidpath,wheneverthepathisaligned-that
is,allvalvesinthepathareopen-andthepressureintheonetakenassourceisgreaterthanthepressurein
thecontainedliquidtakenasdestination.ThequantitytypeAMOUNT-OFrepresentshowmuch"stuff"there
isinanobject. (RecallthefunctionofAmapsaquantityintothenumberthatisitsamount,anumber, as
opposedtoAMOUNT-OF, whichisafunctionthatmapsapieceofstuffintoaquantity.)
any relevant factors other than quantity conditions, such as spatial boundaries. For
example, in "real"physicswecansolveequationstofigureouthowfastaballwillbe
movingwhen ithitsthefloor,buttheequationswillnottellusaprioriwherethefloor
is. Or, returningtothepresentexample, if weknowthatallthevalvesinthefluidpath
between the two containers are open (i.e., the fluid path is aligned), then fluid will
flow, but we cannotpredict within QP theory when or ifsomeone will walkby and
turn offa valve. Because these factors still affect dynamical conclusions, preconditions must be explicitly represented.
Whenever a process instance is active, its influences and relations hold. The
influences componentofaprocess specifies thedirecteffects ofaprocess; the relations component describes other facts that are true while the process is active. The
directeffects-calleddirectinfluences-lake the form
I + (Q, n)
I - (Q, n)
--- PAGE 330 ---
316 CHAPTER 12: LEARNING PHYSICAL DOMAINS
dependingonwhethernisapositiveornegativecontributiontothederivativeofQ . If
aquantity isdirectly influenced, itsderivativewill bethe sumofall thedirect influences on it. Returning to the description ofLIQUID-FLOW, for example, we see that
when an instance ofLIQUID-FLOW is active, there will be a positive influence on the
amountofliquid inthedestinationandanequal, negativeinfluenceontheamountof
liquid in the source.
Therelationsfieldcandescribenewindividualsthatarecreatedbyvirtueofthe
processbeingactive(suchasthesteamproducedbyboilingwater), aswellasproperties needed by representations outside QPtheory (such as the appearance ofboiling
water). Anespecially importantkindoffactexpressed inthe relations component is
functional dependency between quantities. Functional dependencies between quantities are expressed by
Ql « Q+ Q2
(read "Ql is qualitativelyproportional to Q2," or informally, "Ql q-prop Q2"),
meaning there exists a function that determines Ql and is strictly increasing in its
dependence on Q2. oc Q _ indicates that the dependence is strictly decreasing. Note
that qualitative proportionalities express partial information, since the exact nature
ofthe function relatingtheparameters isnotknownandthefunctionmayormay not
depend upon otherquantities.2 Ifaquantity Qlis functionally dependent on a quantity Q2, and ifQ2 is influencedbyaprocessP, thenwewill saythatPindirectlyinfluences Q1R; that is, when P is acting it can cause Ql to change. If, for instance, the
PRESSUREandLEVELofaliquidarequalitativelyproportionaltotheAMOUNT-OFofthe
liquid, then LIQUID-FLOW will indirectly influence both PRESSURE and LEVEL
because it directly influences AMOUNT-OF. It is important to note that the only way a
quantity can change is ifit is directly or indirectly influenced. This means that one
can reasonby exclusion: Ifnothing is influencingtheamountoffluid inacontainer,
then it isn't changing, but ifthe amount is changing, something must be influencing
it. Nochangeshappenbythemselves. Furthermore, wecantracethepossiblepathsof
influencesinasituationanddeterminewhetherornotparticularkindsofchangescan
occur.
Twootherimportanttypesofdescriptionsshouldalsobementionedhere. Individualviewsaredescriptionsusedto representbothobjects whoseexistenceare subjecttodynamicalconstraintsandstatesofobjects. "Thewaterinacup," forexample,
isdescribedasa CONTAINED-LIQUID(seefigure 12-3)because wecanget ridofit by
QPtheoryalsopro\ideswaystospecifydependenceonpropertiesthaiarenotquantities(suchasshape,
relatingthelevelofaliquid inacontainertoitsvolume)amitomakestrongerstatementsabout functional
relationships, such.is " ..l dependsonQ2directly, withnointerveningparameters-'and "(J dependson „1
and Q2and nothingelse" when required lor Framingstrongerhypothesesaboutadomain. Ho\\c\er. precise specificationsOffunctions (eg., Ql = |Q2|*2) are not permitted
--- PAGE 331 ---
FORBUSANDGENTNER 317
INDIVIDUAL-VIEW CONTAINED-LIQUID
Individuals:
c a CONTAINER
s a SUBSTANCE
Preconditions:
CAN-CONTAIN-SUBSTANCE(c,s
Quantity Conditions:
A[AM0UNT-0F-IN(s, c)] > ZERO
Relations:
THERE IS g, a PIECE-OF-STUFF
HAS-QUANTITY(g, AMOUNT-OF)
AMOUNT-OF(g) = AM0UNT-0F-IN(s, c)
HAS-QUANTITY(g, LEVEL)
LEVEL(g) a Q+ AMOUNT-OF(g)
HAS-QUANTITY(g, PRESSURE)
PRESSURE(g) ocQ+ LEVEL(g)
Figure12-3: Thistypicalindividualviewdescribesapieceofliquidinacontainer,usingtheontologyfor
liquidsdescribedinHayes (1979a).THEREISisjust"syntacticsugar"forstatingthatwheneverthepreconditionsandquantityconditionsaretrue, gwillexist.
reducing its amount to zero (perhaps by making it the source of an instance of
LIQUID-FLOW). Anotherexample is a model ofa spring. Springs havethree statesrelaxed, compressed, and stretched-each of which can be modeled by individual
views. Individualviewsare specifiedinthe sameway asprocesses, inthatthey have
individuals, preconditions, quantity conditions, and relations. However, they do not
have an influence component; directly influencing quantities is the sole prerogative
ofprocesses.
The other kind of description is the encapsulated history. How an object
changesthroughtimeisrepresentedbyitshistory(Hayes, 1979b). Historiesareannotated pieces ofspace-time; thus they are object centered, have finite spatial extent,
andextendovertime.3As its namesuggests, anencapsulatedhistoryisaschematized
description ofsome fragments ofhistories for a collection ofobjects. Encapsulated
historiesareusefulforsummarizingbehaviorandfordirectlydescribingphenomena
that have not been accounted for by process descriptions. An example ofthe latter
usage is describing collisions between moving objects. A very simple way to model
3Bycontrast,theclassicsituationalcalculusdescriptionofchange(McCarthyandHayes. 1969)consistsof
situationsthatdescribethewhole univei I soi partial!, instantoftime.
--- PAGE 332 ---
318 CHAPTER 12: LEARNING PHYSICAL DOMAINS
collisionsistosaythattheverynextthingthathappensafter, say, anobjecthitsawall
isthat itsvelocity reversesand itstarts movingtheotherway. Givenhow rapidlycollisionsoccur,thismodel isquiteadequateformostpurposes, andencapsulatedhistories allow itto be written this way.
A reasoner's theory ofdynamics for a particular domain is characterized in
terms of(1) aprocess vocabularythat describes the kinds ofprocesses the reasoner
believes can occur and (2) a view vocabulary that describes dynamical objects and
relevantstatesofobjects. Allchangesareassumedtobedirectlyorindirectlycaused
- -
by processes the sole mechanism assumption which provides a strong constraint
ontheformofdynamicaltheories. Importantly, thecontentofdynamical theories is
not tightly constrained-incorrect theories can be expressed as easily (and sometimesmoreeasily!) thancorrecttheories. Forexample versionsofNewtonian, Aristotelian, and Impetus theories ofmotion have all been encoded using QP theory.
QP theory sanctions several basic deductions. For example, the kinds ofprocesses that might occur in a situation can be determined by using the process and
viewvocabulariestoconstructinstancesrepresentingthedifferentpossibilities. The
collectionofprocesses actingatanytimecharacterizes "what ishappening" then in
that situation, and these processes can be found by evaluating the preconditions and
quantity conditions forthese instances.
Consideragain the example in figure 12-1. There will be two instances ofthe
LIQUID-FLOWprocess, and sincethe level in wb is greaterthan wa, the LIQUID-FLOW
instancerepresentingflowfromwbtowawillbeactive. Bytakingintoaccountallthe
influencesoneachquantity (calledresolvingitsinfluences), wecanoftendetermine
thesignofitsderivative. Thesignofthederivativeisimportantbecause itrepresents
how the amount ofthe quantity is changing-increasing, decreasing, or remaining
constant. In this example there is only one process instance acting, which makes
thingssimple. AMOUNT-OF(wb) isdirectly influenced, andsincethisinfluence isnegative, itwilldecrease. Bythe ocQ+ statementsintheCONTAINED-LIQUIDdescription,
LEVEL(wb) and PRESSURE(wb) will be indirectly influenced and thus will also
decrease. Similarly, AMOUNT-OF(wa) , LEVEL(wa) , andPRESSURE(wa) will increase.
From the ways the quantities are changing we can determine how the process
and view structures themselves might change, since they depend in part on the inequalities statedasquantityconditions. Thiscomputation iscalled limitanalysis. In
the example two things might happen-the pressures in wb and wa might equalize,
or AMOUNT-OF(wb) could become zero, thus ending wb's existence (the geometry of
this example rules out the latter).
The basic deductionsofQPtheory canbecombined toperform more complex
reasoning tasks. Two examples ofmore complex deductions are qualitative simulation (Forbus, 1984)and measurement interpretation(Forbus. 1983). Qualitative simulation consists ofperforming limit analysis repeatedly. It is useful for making predictions, lor instance, that boiling water in a sealed container could cause an
explosion. Measurement interpretationprovidesalink between physical theoriesand
--- PAGE 333 ---
FORBUSAND GENTNER 319
observations; forexample, it mightbehypothesizedthatthe level ofafluidinacontaineris droppingbecausethe fluid is flowing out somewhere. Measurementstaken
at a single instant may be interpreted by searching through the space ofprocess and
view structures for situations in which the results ofinfluence resolution match the
observations. Algorithms for interpreting measurements taken over a span oftime
are still under development.
12.3 COMPARISONS AND STRUCTURE MAPPING
Sofarthischapterhasconsideredhowportionsofaperson's knowledgeabout
thephysicalworldmightberepresented. Letusnowturntothequestionofhowsuch
domain models might be learned. The authors conjecture that a major process in
experiential learning is comparing the current situation with stored descriptions.
Considertheexampleofapersonwhohasjustmovedtoacoldclimateandislearning
tooperateafurnace. Supposethatatfirsthewronglybelievesthatthehousewillget
warmfasterifthethermostatissettoatemperaturehigherthanthedesiredtemperature. (Kempton showsthatthisview isquitecommon; Kempton, 1985.) Howcanhe
reachthecorrectconclusionthattherateofheatingdoesnotdependonthetemperaturesetting?Thereareatleastthreedifferentways, eachbasedonadifferentkindof
implicitcomparison. First, hecouldcomparehispastfurnaceexperienceswitheach
otherandnoticearegularity intherateofheatingthatis independentofthethermostat setting. Second, he may comparethe furnace situation with known abstractions
and realize that it is bestdescribedas apositional-actioncontroller (as opposedtoa
proportional-action controller). Third, he may use an analogy, comparing the furnacesituationwithadescriptionfromanotherdomain, suchas fluidflow, tosuggest
governingprinciples. Eachofthesewaysoflearningreliesonsomeformofcomparison, eitherwithastoredrecordofliterally similarevents, withastoredabstraction,
orwith a storeddescription thatcan function as an analogy.
Structure Mapping theory is concerned with such comparisons (see Gentner
1980, 1982, 1983; Gentner and Gentner, 1983). The theory describes the rules that
are used to import a descriptive structure from one domain (the base domain) into
another (the target domain). The central intuition is that an analogy implies that a
predicate structure from one domain can be applied in another domain with arbitrarily different objects and surface appearances. Literal similarity, analogy, mere
appearance mappings, and abstraction mappings (applications ofgeneral laws) are
viewed as different kinds ofmappings betweendescriptions. The types ofcomparisonsaredefinedsyntactically, intermsoftheformoftheknowledgerepresentation,
not in terms ofits content. Each type ofcomparison will be considered in turn.
1. Ananalogyisacomparisoninwhichrelationalpredicates, butfewornoobject
attributes, aremappedfrombasetotarget. Theparticularrelationsmappedare
determined by systematicity, as defined by the existence of higher-order
--- PAGE 334 ---
320 CHAPTER 12: LEARNING PHYSICAL DOMAINS
constraining relations that can themselves be mapped.4 The correspondences
betweenobjectsofthebaseandobjectsofthetargetarethusdeterminedbythe
roles ofthe objects in the relational structure, not by any intrinsic similarity
between the objects themselves.
2. A literalsimilaritystatementisacomparisoninwhichalargenumberofpredicates, both attributes and relations, can be mapped from base to target. Here,
the model is based on one proposed by Tversky (1977), which states that the
similarity between A and B increases with the size ofthe intersection oftheir
feature sets and decreases with the size ofthe intersection ofthe two complement sets.5 Thus, there are many more shared predicates than nonshared
predicates.
3. An abstract mapping is a comparison in which the base domain is an abstract
relational structure. Predicatesfromtheabstractbasedomainare mapped into
thetargetdomain. As inanalogy, the mappedpredicates arearelational structure. Abstractiondiffers fromanalogy inthe nature ofthe basedomain. There
are almost no object attributes in the base, so there are few, ifany, one-place
predicates to be left behind. Applying a rule to a situation is an example of
abstraction mapping. Sometimes the relational structure so mapped will also
be referred to as anabstraction.
4. A mereappearance match isacomparison inwhichtheobjectattributesmatch
but the relational structure does not. In a sense it is the opposite ofanalogy.
Suchmatchesareeasilymade, buttheyguaranteenothingbeyondsimilarity in
appearance.
A series ofrelated examples using the analogy between heat flow and water
flowwill illustratethesedistinctions. Figures 12-4aand 12-4bshowawater-flowsituation and the corresponding heat-flow situation (adapted from Buckley, 1979,
15-25). Figure 12-5 showsapossiblerepresentationapersonmighthaveofthewater
situation. Notice that the description contains both object-attribute predicates, such
as CYLINDRICAL(beaker), and relational predicates, such as GREATER-THAN
*Objec1attributesarcpredicatesthattakeoneobjectasanargument, suchasRED(x) . Relationsarepredicatesthat taketwoormorearguments, suchasCOLLIDE (x, y). Wedefinetheorderofapropositionas
follows: Constants have order zero, asdo functionson them. Theorderofa proposition is one plus the
maximumoftheordersol itsarguments. ThusC0LLIDE(x, y) wouldbefirstorderifxandyaredomain
Objects, and CAUSE(COLLIDE(x, y), BREAK(x)) wouldbesecondorder. Exampleso\ higher-orderrelationsare IAUSI and implies.
"AgainaccordingtoTversky, the negativeeffectsofthetwocomplement setsare notequal: forexample.
giventhequestion Hov« similar is A to B?, theset (B - A)-featureso\ Bnot sharedb\ A-counts more
thanthe set (A B)
--- PAGE 335 ---
FORBUSANDGENTNER 321
Figure12-4: ThesetwophysicalsituationsinvolvingflowwillbeusedtoillustratethekindsofcomparisonssanctionedbyStructureMappingtheoryandlatertoillustratehowQP-styledomaindescriptionscan
beusedinanalogies: (a) Awater-flowsituation; (b)thecorrespondingheat-flowsituation.
[PRESSURE(water, beaker) , PRESSURE(water, vial) ] . Let us consider the comparison types as exemplified here:
1. The analogy Heat is like water conveys that certain aspects of the water
description can be mapped onto the heat domain. In particular, (1) object
attributes shouldbedropped; (2) some relational predicates should becarried
over; and (3) systematicity determines which relations should be mapped.
Thus, CYLINDRICAL(beaker) is dropped, along with other object attributes;
--- PAGE 336 ---
322 CHAPTER 12: LEARNING PHYSICAL DOMAINS
PLIES)
condition implication
object path source
Figure12-5: Arepresentationofthewatersituation.Thisnetworkrepresentsaportionofwhataperson
mightknowaboutthewatersituationillustratedinfigure 12-14. Inthisandotherfigures, predicatesare
writteninuppercaseandcircled.Objectsarewritteninlowercaseanduncircled.Asimplifiedrepresentationisusedtoillustratetherulesofanalogy. Amoredetailedmodelwillbeshown.
that is, the target objects do not have to resemble their corresponding base
objects. Some relations are carried across, such as, GREATER-THAN
[PRESSURE(water, beaker), PRESSURE(water, vial)] . Yet not all relations
arecarriedacross. Bythesystematicityprinciple,thisGREATER-THANrelationis
preservedbecause itispartofthemappablechaingovernedbythehigher-order
relation IMPLIES. In contrast, the relation GREATER-THAN[DIAMETER
(beaker), DIAMETER(vial) ] is not carried across, since it is not part ofam
mappable system of constraining relations in this representation of the base
domain.
--- PAGE 337 ---
FORBUSANDGENTNER 323
Figure12-6: Arepresentationoftheheatsituationthatresultsfromtheheat/wateranalogy.Thisnetwork
representstheknowledgeapersonwouldmapacrossintotheheatdomainfromthewatersituationillustratedinfigure 12-5.Asinthatfigure,asimplifiedrepresentationisusedhere.Amoredetailedtreatment
ofthisanalogyispresentedlater.
Figure 12-6 shows the representation in the target domain ofheat flow
thatresultsfromtheanalogicalmapping. Giventhearbitraryobjectcorrespondences heat/water, beaker/coffee, vial/ice cube, pipe/bar, and PRESSURE/
TEMPERATURE,6 systematicity operates to enforce a tacit preference for coherence and predictive power. The systematic relational structure in the water
domain
6Inthisanalogy,thefirst-orderpredicateofPRESSUREinthewaterdomainmustbereplacedbyTEMPERATURE in the heat domain. Although systems ofrelations can often be imported into the target without
change, substitutions oflower-order relations, as well as ofobjects and theirattributes, are sometimes
madeinordertopermitmappingalargersystematicchain.
--- PAGE 338 ---
, ,
324 CHAPTER 12: LEARNING PHYSICAL DOMAINS
IMPLIES
GREATER-THAN[PRESSURE (water, beaker)
PRESSURE(water, vial)],
FLOW(water, pipe, beaker, vial))
is mapped into
IMPLIES(GREATER-THAN[TEMPERATURE(heat, coffee)
TEMPERATURE(heat, ice cube)],
FL0W(heat, bar, coffee, ice cube))
2. The literal similarity comparison Kool-Aidis like waterconveys that most of
the water description can be applied to Kool-Aid. In literal similarity, both
object attributes, such as FLAT-TOP(water), and relational predicates, such as
the systematic chain discussed above, are mapped over.
TheabstractionHeatisathrough-variablemightbeavailabletoastudentwho
knows some system dynamics. This abstraction conveys the idea that heat can
be thought of as something that flows across a difference in potential (i.e.,
somesortofacross-variable-inthiscase,temperature). Thisis muchthesame
relational structure as conveyed by the analogy in 1, above; the difference is
that in the abstract base domain ofthrough-variables and across-variables,
there are no concrete properties ofobjects to be left behind in the mapping.
Amereappearancematchisamatchwithoverlapchieflyinlower-orderpredicates, suchasobjectattributes, butlittleorno relational match. Anexample is
The tabletop gleamed like water. Such a match typically yields little or no
useful information about the target; here, for example, little can be learned
about the table by mapping across knowledge about water. These matches,
however, cannot be ignored in a theory oflearning, because a novice learner
may be unable to tell them from true literal similarity matches.
Table 12-1 summarizes the kinds ofpredicate overlap that characterize literal
similarity, analogy, abstraction, and mere appearance matches, as well as one other
kind of comparison, anomaly. An anomaly is a match with little or no predicate
overlap; it is included simply forcompleteness.
It should be clear that the contrasts described here are continua, not dichotomies. Forexample, analogy and literal similarity lieonacontinuum. Given that two
domainsoverlap in relational structure, then the comparison becomes more a literal
similarity match to the extent that their object attributes also overlap and more an
analogy totheextent that few ornoobject attributes overlap. A different sort ofcontinuum exists between analogies and general laws. In both eases, a relational Structure is mapped from base to target. If the base representation included concrete
--- PAGE 339 ---
FORBUSANDGENTNER 325
Table 12-1: Kinds ofDomain Comparisons
OBJECT
ATTRIBUTES RELATIONS EXAMPLE
LiteralSimilarity Many Many Milkislikewater.
Analogy Few Many Heatislikewater.
Abstraction Few Many Heatflowisathroughvariable.
Anomaly Few Few Coffeeislikethesolar
system.
MereAppearance Many Few Theglasstabletopgleamed
likeapoolofwater.
objects whose individual attributes mustbeleftbehind inthemapping, the comparisonisananalogy. Astheobjectnodesofthebasedomainbecomemoreabstractand
variablelike, the comparison is seen more as a general law.
STRUCTURE MAPPING AND LEARNING
12.4
The role of a comparison in learning depends on at least two things: (1)
accessibility-the likelihood that the match will be noticed-and (2) usefulnesswhatcanbededucedfromthematchifitisaccessed. Accessibility, inturn, depends
at least on (a) thefamiliarity ofthe base description and (b) the overall similarity
between the base description and the current target. The immediate usefulness ofa
match depends, ofcourse, on whetherthe content ofthe match is appropriate to the
task at hand. In addition, the usefulness ofa match depends on the inspectabilityof
thematchingcontent-thedegreetowhich itcanbeconsciouslyanalyzedandarticulated. The comparisons discussed above behave very differently with respect to
accessibility and inspectability.
Fornovicelearners, literalsimilaritymatchesarethemostaccessiblecomparisons,andabstractionsaretheleastaccessible. Incontrast, abstractionmatchesarefar
more inspectable than literal similarity matches. Onboth dimensions, analogies are
intermediate. That is one reason that analogy is crucial in learning. Some evidence
forthese conjectures will now be reviewed.
Literalsimilaritymatchesarehighlyaccessible. Ithasbeenshownineducation
andtraining literaturethatthemoresimilaranewsituationistoanoriginal situation
the more readily transfer oftraining occurs (cf. Brown and Campione, 1985). The
term generalization gradient expresses the fact that a learned response generalizes
more readily the more similar the new situation is to the original situation. In contrast, subjects are often quite slow to use an available analogy. In research done by
--- PAGE 340 ---
326 CHAPTER 12: LEARNING PHYSICAL DOMAINS
Reed, Ernst, andBanerji(1974)andlaterbyGickandHolyoak(1980, 1983), subjects
were asked to solve a rather difficult problem, such as how to cure an inoperable
tumorwith radiationwithoutkillingthefleshalongthepathofthe rays. Justpriorto
receivingtheproblemsomeofthesubjectsreadmaterialthatcontainedananalogous
solution, such as a story about a general who splithistroopsup so thatthey all converged simultaneously on a fortress he wished to capture
Therearethreeinterestingresultshere. First, agoodanalogycanbeverypowerful ifitis noticed. Withouttheanalogy, only about 10percentofthe subjectscould
solvetheproblem. Oncetheexperimental subjectsweretoldtousethepriorstoryas
an analogy, 80to90percentofthem solvedtheproblem correctly. Second, apotentiallypowerfulanalogycaneasilygounnoticed. Beforetheanalogywaspointedout,
onlyaboutathirdofthesubjectsspontaneouslynoticedandusedit. Itcannotbetaken
forgrantedthatapotential analoguewillbe spontaneously noticedandused. Third,
literal similarityisfarmoreaccessiblethantrueanalogy. Inoneoftheirstudies, Gick
and Holyoak (1983) happened to set up a literal similarity match between the story
andproblem. Subjectshadtosolveaproblemthatinvolvedtyingtworopestogether,
andthestorythey weregiveninvolvedtyingtwo ribbonstogether. Inthiscase, 70to
80 percent ofthe subjects were able to access the matching story spontaneously.
There is also developmental evidence that literal similarity matches appear
prior to analogies and abstraction matches in learning. One example is early word
learning. In spontaneous labeling, one-year-old children frequently apply words to
objects that closely resemble the original referent ofthe word; for example doggie
will be applied to another dog or to a cat, and carto cars, trucks, or other vehicles
(Clark, 1973). Trulyanalogousormetaphoricalusagesareseldomhearduntiltheage
oftwo or three years; for example, a three-year-old child remarks about his dirty
bedraggled blanket, "It's out ofgas" (Gentner and Stuart, 1984; Winner, 1979).
Childrenaresaidtomovefromrich, concreterepresentationstomoreabstract,
rule-based systems (cf. Bruner, Olver, and Greenfield, 1966; Gibson, 1969). Even
three-year-olds can sort objects into perceptually similar categories; for example,
theycangroupacatandadogandexcludeahen. However, notuntil they are fiveor
sixyearsold canthey succeed ifthe match is more abstract; forexample, acategory
like "living thing" requires grouping perceptually dissimilar things.
Inthesamevein, researchonthenovice-expertshiftinadultlearninghasdemonstratedthatwhereasnovicesciencestudentstypicallymatchsituationsonthebasis
ofsurfacefeatures, expertsusedeeperandmoreabstractcriteria(Larkin, 1983). For
example, Chi, Feltovich, andGlaser(1981)haveshownthatwhennovicephysicsstudents are asked to classify problems into similar groups they put together problems
withsimilarsurfacefeatures, suchas "inclinedplanes" or"pulleys." Experts, onthe
other hand, use categories like "force problems" and "energy problems."
One final indicationoftheeasewithwhich literal similarity matchesare made
involvesan indirect, butvery important, lineofargument. Intherealmofobjectconcepts, there is some evidence that people automatically perform literal similarity
--- PAGE 341 ---
FORBUSAND GENTNER 327
comparisonstocombineperceptuallysimilarexperiencesintocompositeprototypes
(see Posnerand Mitchell, 1967; Rosch, 1973, 1975, 1978; Smith and Medin, 1981).7
InthePosnerandMitchellstudy, peopleclassifieddotpatternsintocategories. After
they had sorted the patterns, they were asked to remember which patterns they had
seen. Although the task called simply for accessing verbatim memory, subjects
showed systematic misrecognitions: they falsely rememberedhaving seenprototypical patterns that were never presented. Thus without being told to do so, people
formedcompositementalrepresentations, apparentlybasedonimplicitcomparisons
among the patterns that they saw. The virtually automatic nature of prototype
learning is further evidence that the literal similarity matches on which they are
based are highly accessible-indeed, evidence that making such comparisons is a
passive, essentially automatic process (see also Reber, 1967, 1976).
However, prototypes also illustrate the limited usefulness ofliteral similarity
matches. Althoughtheseimplicitcompositesareoftensufficientforrecognizingand
categorizing situations, they are oflimited use inderiving causal principles. This is
because(1)amatchbasedlargelyonperceptualcommonalitieswilloftenfailtocontainthecorrectprinciplesand(2)evenwhensomeofthecorrectrelationsarepresent,
literal similarity matches are too rich to be inspectable. There is some evidence,
albeitindirect, forthisnotionofrich, noninspectablerepresentations. Nickersonand
Adams (1979) studied people's memory of the common penny. Despite the overwhelmingamountofexperiencethatthesubjectshadwithpennies, anddespitetheir
evident ability to recognize and categorize pennies, they were remarkably poor at
recallingorrecognizingthedetailsofhowpennieslook. Thisdemonstratesthatpossessingadescriptionsufficienttorecognizeaclassofobjectsisnoguaranteethatthe
description can be articulated.
Studies ofyoung children show that similarityjudgments can be difficult to
decompose. Shepp (1978) has found that three- and four-year-olds appear to base
their similarity judgments on some kind ofoverall comparison; they are typically
unable tojudge one dimension independently ofanother. For example, they cannot
ignoreheightwhenjudgingwidth. Unlikeadults, they areunabletotreatlength and
width as separable.
Bycontrastanappropriateabstractionmatchislikelytobeextremelyusefulin
both respects: it should contain the correct principle, and the match should be
inspectable. Butabstractionsareoftennotparticularlyaccessible,especiallyfornovices. Novicelearnersmaynotknowtheappropriateabstraction, orit maybesounfamiliar that they will not retrieve it when appropriate. Thus abstraction mappings,
7Thetermprototypehasbeenusedinvariouswaysinpsychology. Hereitisusedtorefertoastructured
compositeobject.
--- PAGE 342 ---
328 CHAPTER 12: LEARNING PHYSICAL DOMAINS
although ultimately important, areunlikelytoplayamajorrole intheearly stagesof
learning.
Analogies lie between the highly accessible literal similarity matches and the
highlyusefulabstractionmatches. Potentialanalogiesarelessaccessibleinexperientiallearningthanliteralsimilarity matches. This isbecauseanalogy requiresthatthe
learner's database be accessed via relational matches; object matches are oflittle or
no use. However, once found, an analogy should be more useful than a literal similarity match in deriving the key principles, since the shared data structure is sparse
enoughtopermitanalysis. (Ofcourse, educatorsoftenexplicitlyintroduceanalogies
inteachingbeginnersforexactlythisreason. Inthiscase, theproblemofnoticingthe
analogical match is bypassed.) Moreover, by the systematicity principle, the set of
overlapping predicates is likely to include higher-order relations, such as causality
and logical implication. Thus analogy can function to reveal principles in a domain
that previously lacked the appropriate abstractions (Burstein, chap. 13 of this
volume; Carbonell, chap. 14; Clements, 1982; Darden, 1983; Gentner, 1980, 1982,
1983; Gentnerand Gentner, 1983; Gickand Holyoak, 1983; Hoffman, 1980). Winston's system (see Winston, 1980, 1982), which derives if-then rules by abstracting
the predicates common to two analogues, is a case in point.
Theanalogicalshifthypothesisconcernstheroleofthesecomparisonsinexperiential learning. In the earliest stages most ofthe spontaneous matches are either
mereappearancematches(andthuserroneous)orliteralsimilarity matchesbasedon
massivefeatureoverlap. Thisistosaythatinitiallearningissurfaceorientedandconservative, based on rich, specific-case kinds of matches. As the domain becomes
familiar, moredistantcomparisonsbegintooccur; matchesare madeinwhichfewer
object attributes are shared. These sparse comparisons lead to the kinds ofbinary
connectionsthatformthebulkofthecausalcorpus-forexample "Lighterthingsgo
farther when thrown." Analogy also serves as a means of introducing structured
mental models. Successful analogies may yield abstractions that can be stored and
accessed (Gick and Holyoak, 1983; Winston, 1980, 1982). Thus, analogy plays an
important role in the middle and later stages oflearning. In the final stages, when
learning is well advanced, abstraction mappings play a major role.
12.5 STAGES OF UNDERSTANDING
The authors suspectthat fourkindsofmental modelsare generated in the process of understanding physical domains. The sequence of models proposed here is
developmental, in that the theories ofeach stage are generated both by the phenomena being understood andbythetheoriesofthe stage before it. It is not proposed
that every persongothroughevery stage forevery domain, northat a person is at the
same stage in every domain at the same time.
--- PAGE 343 ---
FORBUSANDGENTNER 329
12.5.1 Stage 1: Protohistories
Supposesomenewphysicalphenomenonisbeingobserved. Ifthereisnoprior
model, allonecando isobserveand rememberwhatishappening. Theauthorsconjecture that the simplest physical models ofa domain areprotohistories-prototype
historiesthatserveas summariesofexperience.8Likeobjectprototypes, protohistoriesarethe "mosttypical instances" ofphenomena. Theterms inthesedescriptions
areobservables, andtheirdeductiveimportcanberoughlyexpressedas, "IfIseeX,
then Y will happen (has happened)."
Considerabalancebeamorseesaw. Ifaweightisplacedoneachsideofthefulcrum, the seesaw will either tilt counterclockwise, tilt clockwise, or not tilt at all.
Most people have had enough experiences with seesaws to have formed protohistories concerning their behavior. By the conjecture described here, a protohistory is
automatically available whenever they encounter a seesaw. From it, they can often
predict which way the particular seesaw will move. For example, they may have a
protohistory thatdescribes what happens ifa smallperson gets onthe seesaw opposite a large person.
However, the predictive power ofprotohistories is quite limited. There is no
guarantee that the features matched actually correspond to relevant factors. For
example, an observer will be fooled when a large person sits close to the fulcrum if
theobserver's seesawprotohistorieshavebeen formed from watchingpeople sitting
at equal distances. Massive overlap in features is needed for reliable use, which
means protohistories will yield conclusions in fewer situations than a true theory
would. Consider, forexample, twoweightshungfromoppositeendsofastickthatis
suspendedbyastring. Theprincipleinvolvedisthesame, yetthesituationslookdissimilar enough that the protohistories for seesaws would not match. Furthermore,
there is nocertain way todecidebetweenconflicting results ifmorethan oneprotohistory matches a situation.9
12.5.1.1 Learning Protohistories
Theprocessofconstructingprotohistoriesinvolvesdividingupexperienceinto
classes according to literal similarity and abstracting a summary for each class.
Therehasbeenlittledirectresearchonthisprocess. However, investigationsintothe
process ofconstructing object prototypes provide some hints. First, people seem to
be able implicitly (i.e., unconsciously) to compute a kind of component match.
Second,thisintersectionisnotmerelyasimplefeatureintersection; rather, itappears
8SomeofdiSessa'sphenomenologicalprimitives(1983)appeartoberepresentableasprotohistories.
'There areofcourse heuristic criteria, such as using the protohistory that has worked most often. The
problemwith suchheuristicsisthatlittleislearnedfrommistakes.
--- PAGE 344 ---
330 CHAPTER 12: LEARNING PHYSICAL DOMAINS
that configurations among features are important in the prototype. Third, once this
prototype iscomputed, ithaspowerfuleffectsonthe subsequentprocessingofexperience. As mentioned previously, once people abstract aprototype from a set ofpatterns, they may be more confident ofhaving seen the prototype-which was never
presented-than they are ofhaving seen the patterns actually presented (Posner and
Mitchell, 1967). Finally, peoplemaynotbeawareofformingprototypes, exceptasa
general sense ofincreased familiarity with a category.
In summary, ifprotohistories behave like object prototypes, then they should
befoundto(1)becomputedimplicitly; (2)actascompositeconcepts; (3)besensitive
toperceptualconfigurationsamongevents; and(4), oncecomputed, showtherecognition strength and other psychological privileges ofprototypes.
The machinelearning researchthatmostcloselycapturesthistype oflearning
isconcernedwithconceptualclustering(seeMichalskiandStepp, 1983). Sofar, such
research has focused on classifying objects that can be characterized mainly by differing attributes. Extending suchtechniquestodescribe situations thatdependcriticallyonrelationaldescriptionscouldprovideamethodforcomputingprotohistories
(Stepp and Michalski, chap. 17).
12.5.2 Stage 2: The Causal Corpus
Protohistories summarize the phenomena, but they do not constitute a theory
ofthem. Building adetailedtheory directly canbe quite difficult. The space ofpossible models connecting all observable (and possible) parameters in a typical situation can be quite large. The authors conjecture that weaker theories, theories that
characterize which parts ofthe situation are relevant to desired conclusions, are
formedfirst. Inparticular, itisconjecturedthatacollectionofCAUSEstatements, the
causal corpus, is computed from prototype objects and protohistories.
CAUSE is viewed here as an approximate concept, a weak form ofontological
commitment. In particular, saying CAUSE(A, B) expresses beliefin the existence of
some mechanism, specified by some theory T, such that IMPLIES [(AND AT), B].
Many, perhaps most, of the causal corpus relations are binary relations among
variables-forexample, "Biggerobjectsweighmore" (Piaget, 1951; Carey, 1985),or
"Smaller objects have higherpitch when struck" (diSessa, 1983).
The notion of mechanism in the causal corpus is quite primitive: the causal
beliefs need be neither explicit nor internally consistent. Later in the learning
sequence, aswillbeseen,processeswill assumethe roleofmechanisms forphysical
domains. Nevertheless, the authors conjecture that even at this early stage, the
learner makes a distinction between mechanistic connections and, say. definitional
connections.10 Further, they suspect that many ofthe initial causal connections are
Fbi example, the statement CAUSE(TRlANGLE(f), HAS-THREE-SIDES(f)) is not legitimate use of
CAUSEh> tins account, since the required axiomsofgeometr) donot specif) mechanism.
--- PAGE 345 ---
FORBUSAND GENTNER 331
incorrect. Novicesoftenincludediagnosticandcorrelationalrelationsintheircausal
corpus. Forexample, askedifan increase intheevaporationratewill causeachange
inthetemperatureofthewater, anovicemayreply, "Yes, becauseit wouldhavetobe
hotterto evaporate more."
CAUSE, then, isastatementofbeliefinsome mechanisticconnection. Thedistillation ofexperience from protohistories into the causal corpus serves three purposes. First, itservesasameansofdatareduction. Second, itprovidesacollectionof
heuristics that can be used directly to draw inferences. Even ifthe learner doesn't
have firm grounds to considerthe CAUSE statements complete or correct, the CAUSE
statementsmayoftensufficeforthedesiredclassofinferences. Third, thecollection
ofCAUSE relationscanbeusedtoguidethe search foradeepertheory ofthedomain.
The CAUSEstatementssuggestconnectionsamongvariousaspectsofthedomainthat
adeepertheory must eitherexplain orexplain away.
Returningtotheseesawexample, supposethecausalcorpusis nowappliedtoa
balancebeambuiltoutofblocks. Supposethetwoblockson itarecalledaandb. The
causal corpus might be as follows:
CAUSE(BIGGER(a, b), TILT-TOWARDS(a)
CAUSE(FARTHER(a, b), TILT-TOWARDS(a)
Thesestatementscanbeinterpretedasrulesinseveralways: Ifblockaisbiggerthan
blockb,onecanpredicttilt, andifoneseestilt, onemayhypothesizethatoneblockis
fartheroutthananother. Thesestatementsaremorebroadlyapplicablethanprotohistoriessincetheyrefertofewerproperties. Unlikeprotohistories, thecausalcorpusis
sparse enough to be debugged to some degree.
However, the approximate nature ofthe CAUSE relation limits the learner's
ability to discriminate between conflicting predictions. With the causal corpus
above, for instance, ifblock a is bigger andblock b is fartherout, we will have two
predictions. Inhelder and Piaget (1985) and Siegler (1976, 1981) have documented
suchastageinthedevelopmentofunderstandingaboutthebalancebeam(withanalogous developmental sequences in other domains). Initially, children focus only on
weight. Butthere is an interesting second stage when they come to realize thatboth
weightanddistance are importantbutthey do notyet know the interrelations. They
can manage either property by itselfifthe other is constant; but ifboth properties
vary, they tend to focus on one or the other inconsistently. Eventually they become
abletocoordinate weightanddistance inthebalancebeamproblem. Atthis stage, if
notbefore, theyhavegonebeyondthecausalcorpus. Aswillbediscussed, inorderto
make more precise inferences the learner must eventually uncover the mechanisms
whose behavior is described by causal corpus.
--- PAGE 346 ---
332 CHAPTER 12: LEARNING PHYSICAL DOMAINS
12.5.2.1 Learning the Causal Corpus
The authors suspect that there are three techniques for computing and debugging a causal corpus. The first technique is to hypothesize causality from
co-occurrence:
If you always see A before B,
then hypothesize CAUSE (A, B)
and
If A is true whenever B is true,
then hypothesize CAUSE (A, B)
Theserules makecertainassumptionsabouttheformofmemory, namely, that some
numberofcircumstancescanberememberedandthattheycanberememberedinsufficient detail that A and B are either explicitly stored or computable from what is
stored. Protohistories should serveasameansofinitial datareduction from which a
causal corpus can be constructed.
Itisnotclearexactlyhowthelearnerabstractsoutparticularvariablesfromthe
richrepresentationoftheprotohistorystage. Howeverthisisdone, thesimplification
achieved with the causal corpus is considerable. Another study by Siegler (1978)
shows the power offocusing on particular variables. Three-year-old children were
shownabalancebeam, askedtopredictwhichway
wouldtilt, andthenshownwhat
actually occurred. Even after large numbers oftrials, their performance failed to
improve. Butwhentheyweretaughttothinkofthedomainintermsofafew relevant
variables-weightandlength-theirperformancedid improve with experience. The
moral to be drawn is that the pace oflearning is greatly accelerated when a small
numberofvariables can be abstracted from all the possibly relevant factors.
As suggested earlier, many ofthe early causal relations will be incorrect. The
authorssuspectthatthereexistsaclassofrulesthatareusedtodebugacausalcorpus
in the face of new information (cf. Sussman, 1976). Each rule corresponds to a
hypothesisaboutabuginthestructureofthecausalcorpus, suchasamissingprecondition. Theauthorsbelievethatthetaskofjudgingacausal corpus forconsistency is
an example of an important but relatively neglected kind of learning, coherencedriven learning. Coherence-driven learning is learning that is driven not by a mismatch between the model and the world but by inconsistencies within the model
itself. Williams, Hollan, and Stevens (1983) found evidence ofsuch learning. The)
studied a subject whowas learningabouta heatexchangerand noted that one source
of insight was a "boggle" experience, in which the person noticed that a current
inference contradicted a priorbelief. The authors are still examining the criteria for
--- PAGE 347 ---
FORBUSAND GENTNER 333
judging the consistency ofa causal corpus.11 Such criteria will play a major role in
controlling the dubugging rules and the mixture ofgeneration and debugging that
occurs.
Analogy provides the third technique for extending a causal corpus (see
GentnerandGentner, 1983; Stevens, Collins,andGoldin, 1979). TheCAUSErelations
fromonedomaincanbemappedintoanother, sinceCAUSEqualifiesasahigher-order
constraining relation (see also Winston, 1982).
12.5.3 Stage 3: Naive Physics
ThenaivephysicsmodelsreplaceCAUSEstatementswiththeoriesaboutthespecificmechanismsofchange. Theontologyisextendedbyaddingprocessestoexplain
observed changes. The ontology also includes properties and objects that are not
directlyobservable(forexample, heatandheatflow)andthenewrelationships(such
as fluid path and heat path) requiredto reason aboutthem.
An important advantage ofthese models is that they allow one to reason by
exclusion. Unlike the previous stages, predictions that fail still yield information
about the situation. For instance, iffluid is flowing into a container and the level is
not rising, then it is reasonable to hypothesize that fluid is flowing out ofitthrough
some unknown path.
Returning to the balance beam example, a process SWING might be used to
describerotationaroundacontactpoint(seefigure 12-7). Thepreconditionsdescribe
thegeometricconfigurationofthesystem, andthequantityconditionsaysthatSWING
will occur whenever there is a nonzero angular velocity. SWING directly influences
theangularpositionofthebeam. Thusapredictionconcerningtiltbecomesaprediction about which instance, ifany, ofthe SWING process will be active.12
What influences ANGULAR-VELOCITY? The existence of an ANGULARACCELERATIONprocess (seefigure 12-8)thatdirectly influences ANGULAR-VELOCITY
wheneverthere is a net torque will be assumed. It is furtherassumed that
For-All (x) For-All (y)
PHYSOB(x) and CONTACT-POINT(cp)
implies NET-TORQUE(x, cp) = SUM-0F(T0RQUES-0N(x, cp))
"With LanceRipsoftheUniversityofChicago, theauthorsare investigatingthe roleofintransitivesin
debuggingcausaldescriptions.
12Analternate,andequallygood,representationforSWINGwouldleavedirectionsimplicitinthesignofthe
velocity. Inthatvocabulary, thebalancebeamwouldgiverisetoonly oneinstanceofSWING, anddeterminingwhichwaythebeammovesrequiresthatonedeterminefirstwhethertheinstanceofswingisactive
andifitis, whatthesignoftheangularvelocity is.
--- PAGE 348 ---
334 CHAPTER 12: LEARNING PHYSICAL DOMAINS
Process SWING
Individuals:
b a PHYSOB
c a PHYSOB
cp a CONTACT-POINT
dir a DIRECTION
Preconditions:
MOBILE(b)
not MOBILE(c)
CONNECTED(b, c, cp)
ROTATION-FREE(b, c, cp)
DIRECTION-OF(dir, ANGULAR-VELOCITY(b, cp))
Quantity Conditions:
An[ANGULAR-VELOCITY(b, cp)] > ZERO
Influences:
I + (ANGULAR-POSITION(b, cp), A[ANGULAR-VELOCITY(b, cp)])
Figure 12-7: A SWING process describes rotation ofan object around another object. For the balance
beamtherewillbetwoinstancesofthisprocess,differingonlyintheirbindingsforthedirectiondir. In
eachinstancebwillbeboundtothebeam,cwillbeboundtothefulcrum,andcpwillbeboundtothecontactpointbetweenthem.
It is assumed that each physical object (PHYSOB) has quantities to represent its angular position and
velocitywithrespecttoeachpointofcontactwithotherobjects. Directionswillbenotedbythesymbols
CW, CCW,andNULL,correspondingtoclockwiserotation,counterclockwiserotation, andnorotation.
In other words, the nettorque on an object around a contactpoint is the sum ofthe
torquesonthatobjectmeasuredaroundthatcontactpoint. Themassofthebeamwill
be ignored, andthepull ofgravity ontheblocks oneach side ofthe fulcrum will be
assumedtobetheonly source oftorques. Figure 12-9 describes this inducedtorque
by means of an individual view. Notice that the factors illuminated in the causal
corpus ofBIGGERand FARTHERhavebecomethequantities MASS and DISTANCE, and
theirrole in producing swinging hasbeenexplicated. In particular, these properties
determine how much torqueeachblockplaces onthebeam. The sum ofthe torques
determines the net torque, which can cause the beam to accelerate and thus swing.
This model comes one step closer to a model that can always determine which
way something will tilt. There will still be cases in which exactly what will happen
cannotbedetermined(e.g., ifthemassononesideis increasedand
isbroughtcloser
tothe pivot), but this is a precise hypothesis about what all the relevant factors are.
12.5.3.1 Learning Naive Physics
The majorproblem in learning a naive physics isconstructing a vocabulary of
processes that consistently describes experience. The learner must strip away the
--- PAGE 349 ---
FORBUSANDGENTNER 335
Process ANGULAR-ACCELERATION
Individuals:
b a PHYSOB
c a PHYSOB
cp a CONTACT-POINT
dir a DIRECTION
Preconditions:
MOBILE(b)
not MOBILE(c)
CONNECTED(b, c, cp)
ROTATION-FREE(b, c, cp)
DIRECTION-OF(dir, NET-TORQUE(b, cp))
Quantity Conditions:
Am[NET-TORQUE(b, cp)] > ZERO
Relations:
Let ace be a quantity
ace ocQ+ NET-TORQUE(b, cp)
ace oc Q_ MASS(b)
Influences:
I + (ANGULAR-VELOCITY(b, cp), A[acc])
Figure 12-8: AnANGULAR-ACCELERATIONprocess.
irrelevantpredicates that arepartofhis orherprotohistories and causal corpus and
construct more appropriate descriptions. In addition, the learner must sometimes
hypothesize the existence ofobjects and properties that are not directly observable.
Research in machine learning has developed several techniques for inductive
learning that should prove useful (see Dietterich and Michalski, 1983; Mitchell,
1982; Michalski, 1983). Theseproblems are startingtobeaddresseddirectly in the
study ofscientific discovery (Langley, etal., 1983).
The causal corpusprovides a search space forpotential process vocabularies.
Each statementinthecausalcorpusmustbeelaboratedintoaconsequenceofaprocessvocabulary. Itappearsthatthereareonlyasmallnumberofdistinctwaystoperform the elaboration, depending on the particular form of the arguments. For
example, the statement
The decrease in AMOUNT-OF q
causes the LEVEL OF Q to fall
--- PAGE 350 ---
336 CHAPTER 12: LEARNING PHYSICAL DOMAINS
Individual View GRAVITY-INDUCED-TORQUE
Individuals:
b a PHYSOB
c a PHYSOB
d a PHYSOB
cp a CONTACT-POINT
Preconditions:
CONNECTED(b, c, cp)
0N(d, b)
Relations:
Let f be a quantity
f ELEMENT-OF TORQUES-ON(b, cp)
f ocQ+ DISTANCE(C-M(d), cp)
f ocQ+ MASS(d)
; Assign positive torques to CW, negative torques to CCW
0N(C-M(d), SIDE-0F(CW, b, cp)) iff As [f] = 1
0N(C-M(d), SIDE-OF(CCW, b, cp)) iff As [f] = -1
0N(C-M(d), SIDE-OF(NULL, b, cp)) iff As [f] =
Figure 12-9: Adescriptionofgravity-inducedtorque.
indicates that some active process (or individual view) in the situation contains the
statement
LEVEL(q) ocQ+ AMOUNT-OF(q)
in its relations.
Hypothesizingaprocessvocabularyfromacausalcorpusshouldbemuchsimplerthan workingfromprotohistoriesordirectobservation. Yetitstillappearsdifficult. The authorsconjecturethatthere are several constraints that make the problem
moretractable. First, peopleareapparentlyconservativeintheintroductionofunobservedproperties. Forexample, somesubjectshaveamodelofadomainthatappears
to be organized around one parameter-a "generalized strength" attribute. In reasoningabout fluids, forinstance, they appeartousepressure,flowrate, and velocity
asdifferentnamesforthesamething. Inelectricity, theyuse voltage, current,power,
potential, and velocity ofelectrons interchangeably. The advantage of this theor\
generation strategy is, ofcourse, thatsimplermodelswill beexplored first, with furtherdistinctions madeonly when necessary. Second, some physical laws are used as
constraints on what process vocabularies are possible. Conservation ofenergy, for
example, demands that ifa process directly influences a quantity representing some
formofenergy, it mustalsodirectly influencesomeotherquantity representing some
form ofenergy, but in the opposite direction.
--- PAGE 351 ---
FORBUS AND GENTNER 337
Once again, analogy can provide a constructive mechanism. It can be used to
import candidate processes from previously understood domains-for example,
whenoneunderstandsheatflowintermsoffluidflow. Thisisanespeciallypowerful
mechanism because ifthe model fortheprevious domain is consistentwithphysical
laws, thenitsuggeststhatthemodelforthenewdomainmaybesoaswell. Recallthe
liquid flow model presented in section 12-2. Figure 12-10 illustrates a collection of
assertionsthatdescribestheconsequencesofaparticularinstanceofLIQUID-FLOW.13
Supposeapersonhypothesizesthatthereisaprocessofheatflowanalogousto
the process ofliquid flow. By Structure Mapping theory, this means thattheperson
suspects that a similar relational structure holds among the objects in the heat-flow
situation (the coffee, the ice cube, the silver bar, and the instance ofheat flow) as
holdsamongtheobjectsintheliquid-flowsituation(thewaterinthebeaker,thewater
in the vial, the pipe, and the instance ofliquid flow). Mapping the systematic relational structure (see figure 12-11) leads to several predictions that the person can
check to see whether the analogy is correct. For example, it can be determined
whether or not the temperature ofthe ice cube is rising and the temperature ofthe
coffee falling. The structure-mapping rules for analogy have provided an initial
model forthe process ofheat flow; in particular, the preconditions, quantity conditions, relations, and influences are all carried across from liquid flow. Note that to
make the analogy really work, a new kind ofobject-a HEAT-PATH-must be postulated. Thus analogy can provide candidates forextending ontologies.14
12.5.4 Stage 4: Expert Models
The models generated so far have two important limitations. First, they still
containfundamentalambiguities, ambiguitiesthatareinherentinthenatureofqualitative representations.15 Second, they lack domain-independent generalizations
(except in the raw form ofthe representation-CAUSE statements, processes, and so
on). The final stage oflearning consists ofovercomingtheselimitations, ofdiscovering ways to resolve ambiguities and to construct powerful generalizations.
Clearly several kindsofknowledgeare involved, andthepotential complexity
of the models in this stage is open-ended (it includes the whole of mathematical
physics, for example). Examples of the kinds of knowledge involved include
l3TheassertionsweregeneratedbyanearlyversionofGIZMO,acomputerprogrambeingconstructedto
explorethecomputationalaspectsofQPtheory. GIZMOwasdesignedtomakepredictionsandinterpret
measurements, not to be a learning system. In particular, these descriptions were not generated with
learningoranalogy inmind.
I40fcourse, suchextensionsarenottobemadelightly. Theauthorssuspectthatnewtypesofobjectsare
postulated inthetargetdomainonlywhennecessarytopreserveamuchlargersystematicstructure.
'Thenatureofambiguity inqualitativedescriptionsisdiscussedbydeKleer(1979)andForbus(1984).
--- PAGE 352 ---
PRESSURE AMOUNT-OF PRESSURE AMOUNT-OF
C-S (WATER. BEAKER) C-S (WATER.VIAL)
pipe
Figure 12-10: Relational structure for an instance ofliquid flow. These important conclusions follow
from thedefinition o\ liquid How presented in figure 12-2 and the assumption that an instance ofliquid
How exists involvingthe liquids inthetwocontainers. Specifically, the\ describe theconditions forand
consequenceso{the process instance pi = 0'sbeingactive.
--- PAGE 353 ---
TEMPERATURE HEAT TEMPERATURE HEAT
coffee icecube
bar
Figure12-11: Relationalstructuretransferredtoheatflow.Heretherelationalstructuredescribingasituation involving liquid flow has been transferred to a situation involving heat flow. Notice the systematicityoftherelational structure, asindicatedbythenestedchainsofimplications.
--- PAGE 354 ---
340 CHAPTER 12: LEARNING PHYSICAL DOMAINS
equationstodescribe relationshipsbetweenparameters, "rulesofthumb" tospecify
useful default resolutions for ambiguities, and new ontologies to allow reasoning
about more complex systems. The importance of mathematical models is fairly
obvious. Therulesofthumbarelessobviousbutequally important(see, e.g., Lenat,
1982). In physical domains they include empirical knowledge about the circumstances under which certain processes can be ignored (such as evaporation when
waterispouredfromoneglasstoanother)andwhattheirneteffectis(suchasBlack's
law for the temperature of mixtures). Finally, different ontologies are sometimes
necessary to deal with certain types of complex systems. In the process-oriented
physics discussed here, describing flow requires finding flow paths. Finding flow
paths in complex networks such as electrical circuits can quickly become computationally intractable; switchingtoadevice-centeredphysics such as that described in
deKleerandBrown (1983) can reducethecomputational burden tomanageable proportions for such systems.
Inthebalancebeamexample,
itis
knownthattheforceofablockonthebeam
qualitatively proportional to the mass ofthe block and to the distance from the fulcrum. Ifit is also known that the torque is the product ofdistance and weight, then
providing numerical values for these quantities will allow an unambiguous prediction abouttilt.
12.5.4.1 Learning Expert Models
Thetransitiontoexpertmodelsinvolvesseveralkindsoflearning. Someaspects
ofthistransitionprobablylieoutsidethescopeofexperientiallearning; forexample,
peopletypicallylearnmathematicalmodelsbybeingtaughtratherthanbydiscovery.
Some aspects of this learning-such as developing new ontologies-involve
improving the content ofthe representations. Other aspects ofthe transition from a
naivephysicstoanexpertphysicsarebetterdescribedastranslatingtheexistingqualitative representations into quantitative statements, using mathematics to express
laws. By converting a physical theory into a mathematical model, the learner gains
the ability to make precise predictions and to recognize powerful generalizations
moreeasily. An importantpartofthis refinement istoelaborate oc statements into
constraintequations. Langley (1979; Langley, Zytkow, Simon, and Bradshaw, 1983)
describestechniquesthatshouldbeuseful forconvertingqualitativelawsintomathematical relations.
Developing rulesofthumbmeansknowingnotjustwhat ispossiblebut what is
probable. The learnermustdiscoverwhichoutcomes raisedby qualitative reasoning
arelikelyorunlikelyandwhichpotential interactionscanbeignored. Thetechniques
developed in machine learning foracquiring heuristics should bedirectly applicable
(cf. Lenat, 1982; Mitchell etal., 1981). In addition, the authors suspect the possible
behaviors raised by naive physics are compared against known protohistories. Hypothesized outcomes that have no corresponding protohistory are judged unlikely,
--- PAGE 355 ---
FORBUSANDGENTNER 341
and those corresponding toa highly familiarand accessible protohistory arejudged
very likely (see Tversky and Kahneman, 1973).
Further, it seems likely that at least some expert rules ofthumb derive from
learning new protohistories. This intuition is based in part on research in automaticity (Schneider and Fisk, 1983). It has been demonstrated that, given an orderly
domain and sufficient practice, adult subjects can learn a new response pattern well
enoughsothatit becomesessentiallyeffortless(seealsoAnderson, 1982; Rumelhart
and Norman, 1978). Moreover, there is some transfer from the learned material to
newsimilarmaterial. Theselearnedsequenceshavemanyoftheessentialqualitiesof
protohistories. First, they are triggered by recognition (in the terms used here, by a
literalsimilaritymatchbetweenthepresentsituationandastoredsituation). Second,
computingandcarryingouttheproceduresthatfollowfromthematchareautomatic;
virtually no attentional resources are required. Third, these computations are
implicit; subjects are typically poorat introspecting about what they are doing, and
when they do introspect, it can interfere with the response (Brooks, 1978; Reber,
1967, 1976). It may be too simplistic toviewprotohistories as a special caseofautomatic pattern-responsecombination, butthere isenoughoverlaptoallow someconfidence that protohistories can continue to be learned at all stages ofexpertise. Of
course, the contentsofexpertprotohistories maybedifferent fromthoseofnovices,
since experts' protohistories may reflect a more advanced ontology, as discussed
below. However, the mechanismofaperceptually triggeredautomatic match should
be the same.
Theauthors suspectthatontological shiftisdrivenbothbythedesiretounderstand more complex physical systems and by the emergence ofdomain-independent
mathematical abstractions. As an example ofthe firstkind, considertheproblem of
reasoningaboutfluidflowinacomplexsystem, suchasasteamplant. Hayes(1979b)
hasdistinguishedtwoseparateontologiesforliquids: acontained-liquidontology, in
whichliquidisthoughtofasthefluidinaplace, andamolecularcollectionontology,
in which wateristhoughtofaslittlebits offluidthatmovearound insidethe system.
The contained-liquid ontology is appropriate ifthe goal is to determine what flows
canoccur. However,thisviewofwaterisnotusefulifonewantstoknowhowchanges
inthepropertiesoftheworkingfluidinonepartofthesystem(say,therisingtemperatureofthe inletwaterinaboiler)affectpropertiesofthefluid inanotherpartofthe
system (say, temperatureofthesteamcomingoutoftheboiler's superheater). Inthis
case, liquidmustbeviewedintermsofmolecularcollectionsthatmovearoundinside
the system. Conversely, establishing flows using the molecular collection view is
very difficult. A learner with only one ofthese two ontologies will have a difficult
time with certain questions, and such difficulties may drive the search for a new
ontology.
Mathematical abstractionsprovideanotherimportantdrivingforceinontological change. In system dynamics, forexample, physical systems involving fluid elements, mechanical elements, thermal elements, and acoustical elements are viewed
--- PAGE 356 ---
342 CHAPTER 12: LEARNING PHYSICAL DOMAINS
as variations on a common, abstract theme. This means that the analysis and synthesis tools developed for abstract mathematical models can be used to solve problems in several domains. This isapowerful motivation, as evidencedby the wave of
interest in attempting diverse applications evoked by the publication ofcertain new
mathematical formalisms (e.g., catastrophe theory and fractal geometry).
SUMMARY
12.6
Theauthorshavedescribedtheirprogress inweavingtogetherStructure MappingtheoryandQualitativeProcesstheoryintoaframeworkthataimstoaccountfor
learning in physical domains. The learning sequence is built around three ideas.
First, developmentproceeds from richtosparseand fromconcrete toabstract-that
is, initial representations differ from later representations in containing more, and
more context-specific, information. Second, after sufficient experience people
developexperiential modelsthatarecenteredaroundthe notion ofphysical process,
as described by Qualitative Process theory. Third, the process of comparing and
mappingbetweenstoredknowledgeandthecurrent situation, asdescribed in Structure Mapping theory, is central to experiential learning.
Four stages ofexperiential learning have been laid out: protohistories, the
causalcorpus, naivephysics, andexpertmodels.,6Thefirststage, thatofprotohistories, embodies the idea that early representations are rich and context specific; this
stage attempts to capture a combination ofevidence from developmental patterns,
similarityjudgments,basic-levelcategories,andobjectprototypes. Thethirdstageis
theprocess-centeredstagedescribedbyQualitativeProcesstheory. The fourth stage
buildsonthethird-stagemodels, addingdomain-independentgeneralizationsand in
some cases mathematical models. There is some evidence for the third and fourth
stages in the research on the novice-expert shift (Chi, Feltovich, and Glaser, 1981;
Larkin, 1983).
Thesecondstage, thecausalcorpus, isthe mostspeculative. There is nodirect
evidence for its existence, nordothe authors currently have a detailed theory ofthe
kinds ofcausal statements thatcanenter intothe representations. Moreover, detailing
how the causal corpus emerges from protohistories will not be easy. But something
like the causal corpus seems necessary: a collection of simplistic, mostly binary,
directed regularities amongdimensionsandquantitiesthat begintobedifferentiated
"Inderiving this sequence oflearning stages, theauthors have been influenced b) Piaget's well known
theoryofcognitivedevelopment(see Piaget. 1954;orforan introductiontothe work, sec Ravel, 1963
However,thefourstagesoflearningpresentedheredifferconsiderably fromPiaget"sfour-stageaccount.
One difference, for example, is that the authors view their stages as domain specific, whereas Piaget's
Stagesarc intended asgeneral stageso(intellectual development.
--- PAGE 357 ---
FORBUSAND GENTNER 343
outofthetangled representationsoftheprotohistory stage. Thelearnercan nowuse
these simple assertions as grist for further progress.
Whathappenstopriorstagesasnewstagesoccur?First, storedrepresentations
have to be distinguished from new learning. The authors conjecture that learners
retainmuchoftheirstoredknowledgeevenwhentheygobeyondthestageatwhich
was formed. Thus, a hydraulics engineer still uses the same protohistory he or she
formedasatoddlertodecidehow fastonecancarryaglassofwaterwithoutspilling
it.
And, asdeKleerpointsout (1979),expertphysicistsdonotalwaysresorttoquantitative models (fourth stage); frequently the answer they want can be obtained by
using a good qualitative model (third stage).
But what about new learning? Does new learning occur only at the leading
edge,ordopeoplecontinuetolearnatlevelsbelowthemostadvancedstagetheyhave
attained? The authors suspectthat even experts continue to learn atall prior stages,
with the possible exception ofthe causal corpus. As described earlier, there is evidence thatevenexpertscontinuetolay down new protohistories. Similarly, learners
who are operating at the fourth stage, that ofexpert models, may continue to learn
andrefinetheirnaivephysics. Thisisbecausethemathematicalmodelsofthefourth
stagearenotasubstitutefortheprocessmodelsofthethirdstage.17Improvementsto
anaivephysicsareuseful whetherornotmathematical modelsarealsoavailable. As
expertise increases the least new learning is expected within the causal corpus.
Ofthe fourlevels, thecausal corpus has the least claim tocontinued independentexistenceinanadvancedexpert. Thecausalcorpusisnotreliableforprediction,
nordoes itpossesstheadvantagesofautomaticity.18 In summary, theoverall picture
isthatalearnermovesfromrich, perceptualprotohistoriestothesparserrepresentation ofthe causal corpus. Thecausal corpus serves as a staging area in which rough
connections among variables can be stored until they can be subsumed into a true
system. If learning continues, a person develops a process-centered naive physics
and, for some domains, expert models.
l7Historically,philosophersofsciencehavedifferedaboutwhetherthebestconceptionofadomainisprovidedbyamathematical model orby amechanical model. Foranextendeddiscussionofthishistorical
debate,seeHesse(1966)
Thepositiontakenhereisthatbothmathematicalmodelsandmechanicalmodels
areimportanttofull understandingofadomain.
l8Thisdiscussion, ofcourse, concernsdomainsforwhichthe learnereventually acquiresexpertknowledge. Wesuspectthatpeoplerelyheavilyoncausalcorpusknowledgeindomainsinwhichtheyareinexpert. Further,therearemanydomains, suchaschild-rearingorgettingrich, thatlackdefinitivemodels.
Collins'sworkonplausible reasoning (1978) suggeststhat inthesedomains, people rely heavilyonthis
causalcorpusknowledge. SeealsoSalter'swork(1983)ontacittheoriesofeconomics.
--- PAGE 358 ---
344 CHAPTER 12: LEARNING PHYSICAL DOMAINS
ACKNOWLEDGMENTS
The authors gratefully acknowledge the intellectual and financial support of
Schlumberger-Doll Research. TheythankAllanCollins, DaveChapman, Ed Smith,
AlbertStevens, andDanWeldfortheirinsightfulcommentsonanearlierdraftofthis
paper.
References
Anderson, J. R., "Acquisition ofCognitive Skill," PsychologicalReview, Vol. 89, No. 4, pp. 369-406,
1982.
Brooks,L., "NonanalyticConceptFormationandMemoryforInstances,"CognitionandCategorization,
B. B. LloydandE. Rosch(Eds.), Erlbaum, Hillsdale, N.J., 1978.
Brown,A. L.,andCampione,J. C, "ThreeFacesofTransfer: ImplicationsforEarlyCompetence. Individual Differences, and Instruction," in Advances in Developmental Psychology, M. Lamb,
A. Brown, andB. Rogoff(Eds.), Erlbaum, Hillsdale, N.J., 1985, inpress.
Bruner,J. S., Olver, R., andGreenfield, R, Studiesin CognitiveGrowth, Wiley, NewYork. 1966.
Buckley, S., Sun UptoSunDown, McGraw-Hill, NewYork, 1979.
Burstein, M. H., "ConceptFormationbyIncrementalAnalogicalReasoningandDebugging,"Proceedings oftheInternationalMachineLearning Workshop, R. S. Michalski (Ed.), Allerton House.
UniversityofIllinoisatUrbana-Champaign,pp. 19-25,June22-24, 1983. (Anupdatedversionof
thispaperappearsaschap. 13ofthisvolume.)
Carbonell,J. G., "AComputationalModelofProblem-SolvingbyAnalogy,"ProceedingsoftheSeventh
IJCAI, Vancouver, B.C., pp. 147-52, 1981.
, "Derivational Analogy in Problem Solving and Knowledge Acquisition," Proceedings ofthe
InternationalMachineLearning Workshop, R. S. Michalski(Ed.),AllertonHouse, Universityof
Illinois at Urbana-Champaign, pp. 12-18, June 22-24, 1983. (An updated version ofthis paper
appearsaschap. 14ofthisvolume.)
Carey, S., "Are Children Fundamentally Different Kinds ofThinkers and Learners Than Adults?" in
ThinkingandLearningSkills, Vol. 2,S.Chipman,J. Segal,andR. Glaser(Eds.).Erlbaum,Hillsdale, N.J., 1985, inpress.
Case, R., "Intellectual DevelopmentfromBirthtoAdulthood: ANeo-PiagetianInterpretation." inChildren's Thinking: WhatDevelops?R. S. Siegler(Ed.), Erlbaum, Hillsdale. N.J.. 1978.
Chi, M.T. H., Feltovich, P. J.,andGlaser, R., "CategorizationandRepresentationofPhysicsProblems
by Expertsand Novices," CognitiveScience, Vol. 5, No. 2. pp. 121-51. April-June 1981.
Clark,E.V, "What'sinaWord?OntheChild'sAcquisitionofSemanticsinHisFirstLanguage." inCognitive DevelopmentandtheAcquisition ofLanguage, T. E. Moore (Ed.). Academic Press. Now
York. 1973.
Clements, J . "Analogical Reasoning Patterns in Expert Problem-solving" Proceedings ofthe Fourth
innual Conferenceofthe CognitiveScienceSociety, Berkeley, Calif., pp. 137-40. 1982
--- PAGE 359 ---
FORBUSAND GENTNER 345
Collins, A., "FragmentsofaTheory ofHuman Plausible Reasoning," in TheoreticalIssues inNatural
LanguageProcessingII, D. Waltz(Ed.), UniversityofIllinoisatUrbana-Champaign, 1978.
Collins,A.;Warnock.E.H.;Aiello,N.;andMiller,M.L.,"ReasoningfromIncompleteKnowledge,"in
Representation and Understanding: Studies in Cognitive Science, D. Bobrow and A. Collins
(Eds.). AcademicPress, NewYork, 1975.
Darden.L.. "ReasoningbyAnalogyinScientificTheoryConstruction,"ProceedingsoftheInternational
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois at
Urbana-Champaign. pp. 31-40,June22-24, 1983.
deKleer, J., "CausalandTeleological Reasoning inCircuitRecognition," Technical ReportNo. 529, AI
Lab. MIT, 1979.
deKleer,J., andBrown,J. S., 'AssumptionsandAmbiguitiesinMechanisticMentalModels," inMental
Models, D. GentnerandA. L. Stevens(Eds.), Erlbaum, Hillsdale, N.J., 1983.
Dietterich. T, and Michalski, R. S., "A Comparative Review ofSelected Methods for Learning from
Examples,"inMachineLearning:AnArtificialIntelligenceApproach,R.S. Michalski,J.G.Carbonell, andT. M. Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
diSessa. A., "PhenomenologyandtheEvolutionofIntuition," inMentalModels, D. GentnerandA. L.
Stevens(Eds.), Erlbaum, Hillsdale, N.J., 1983.
Flavell,J. H., TheDevelopmentalPsychologyofJeanPiaget, VanNostrandReinhold, NewYork, 1963.
Forbus. K., "Qualitative ReasoningaboutPhysical Processes," ProceedingsoftheSeventhIJCAI, Vancouver, B.C., pp. 326-30, 1981.
"MeasurementInterpretationinQualitativeProcessTheory,"ProceedingsoftheEighthIJCAI,
Karlsruhe, W. Gen, pp. 315-20, 1983.
, "QualitativeProcessTheory," TechnicalReportNo. 789, AILab, MIT, 1984.
Garner,W. R., "AspectsofStimulus:Features,Dimensions,andConfigurations,"inCognitionandCategorization, B. B. LloydandE. Rosch(Eds.), Erlbaum, Hillsdale, N.J., 1978.
Gelman, R., "CountinginthePreschooler: WhatDoesandDoesNotDevelop," inChildren's Thinking:
WhatDevelops?R. S. Siegler(Ed.), Erlbaum, Hillsdale, N.J., 1978.
Gentner, D., "TheStructureofAnalogicalModelsinScience,"TechnicalReportNo.4451,BoltBeranek
andNewman, Cambridge, Mass., 1980.
"AreScientificAnalogiesMetaphors?"inMetaphor:ProblemsandPerspectives,D.Miall(Ed.),
HarvesterPress, Ltd., Brighton, England, 1982.
, "Structure-Mapping: ATheoreticalFrameworkforAnalogy," CognitiveScience, Vol. 7, No.2,
pp. 155-70, 1983.
Gentner,D., andGentner,D.R., "FlowingWatersorTeemingCrowds: MentalModelsofElectricity,"in
MentalModels, D. GentnerandA. L. Stevens(Eds.), Erlbaum, Hillsdale, N.J., 1983.
Gentner, D., and Stuart, P., "Metaphor as Structure-Mapping: What Develops," Technical Report
No. 315, CenterfortheStudyofReading, UniversityofIllinois. 1984.
Gibson, E. J., Principles ofPerceptualLearning andDevelopment, Prentice-Hall, Englewood Cliffs,
N.J., 1969.
--- PAGE 360 ---
346 CHAPTER 12: LEARNING PHYSICAL DOMAINS
Gick, M. L., and Holyoak, K. J., "Analogical Problem Solving," Cognitive Psychology, Vol. 12,
pp. 306-55, 1980.
, "SchemaInductionandAnalogicalTransfer," CognitivePsychology, Vol. 15, pp. 1-38, 1983.
Hayes, P. J., "The Naive Physics Manifesto," in ExpertSystems in the MicroelectronicAge, D. Michie
(Ed.), Edinburgh University Press, Edinburgh, 1979a.
, "NaivePhysics 1: Ontology forLiquids," Memo, Centrepourlesetudessemantiquesetcognitives, Geneva, Switzerland, 1979b.
Hesse,M.B., ModelsandAnalogiesinScience,UniversityofNotreDamePress,NotreDame,Ind., 1966.
Hoffman, R. R., "MetaphorinScience," in ThePsycholinguisticsofFigurativeLanguage, R. P. Honeck
andR. R. Hoffman(Eds.), Erlbaum, Hillsdale, N.J., 1980.
Inhelder,B., andPiaget,J., TheGrowthofLogicalThinkingfromChildhoodtoAdolescence,BasicBooks,
NewYork, 1958.
Kahneman, D., andTversky,A., "SubjectiveProbability: AJudgmentofRepresentitiveness," Cognitive
Psychology, Vol. 3, pp. 430-54, 1972.
Kempton, W. , "Two Theories Used for Home Heat Control," in Cultural Models in Language and
Thought, D. Holland and N. Quinn (Eds.), Cambridge University Press, Cambridge, 1985, in
press.
Langley, P., "RediscoveringPhysicswithBACON.3,"ProceedingsoftheSixthIJCAI, Tokyo,pp. 505-7.
1979.
Langley,P.;Zytkow,J.;Simon,H.;andBradshaw,G., "MechanismsforQualitativeandQuantitativeDiscovery," Proceedings ofthe InternationalMachine Learning Workshop, R. S. Michalski (Ed.),
AllertonHouse,UniversityofIllinoisatUrbana-Champaign,June22-24, 1983. (Anupdatedversionofthispaperappearsaschap. 16ofthisvolume.)
Larkin, J., "The RoleofProblem Representation in Physics," in MentalModels, D. Gentnerand A. L.
Stevens(Eds.), Erlbaum, Hillsdale, N.J., 1983.
Lenat, D, "The NatureofHeuristics,"ArtificialIntelligence, Vol. 19, pp. 189-249, 1982.
, "TheNatureofHeuristicsII,"ArtificialIntellgence, Vol. 20, pp. 31-59, 1983.
, "TheNatureofHeuristicsIII,"ArtificialIntelligence, Vol. 20, pp. 61-98, 1983.
McCarthy, J., and Hayes, P., "Some Philosophical Problems from the Standpoint ofArtificial Intelligence," inMachineIntelligence4, Edinburgh University Press, Edinburgh. 1969.
Michalski, R. S., "ATheory and Methodology ofInductive Learning." ArtificialIntelligence, Vol. 20.
pp. 111-61, 1983.
Michalski, R. S., and Stepp, R. E., "Learning from Observation: Conceptual Clustering" in Machine
/.earning: An Artificial Intelligence Approach, R. S. Michalski. J. G. Carboncll. and 1 M
Mitchell (Eds.),Tioga, PaloAlto. Calif.. 1983.
Mitchell,T. M.. "GeneralizationasSearch."ArtificialIntelligence, Vol. is. pp. 203 26, 1982
--- PAGE 361 ---
FORBUSANDGENTNER 347
Mitchell, T. M.; Utgoff, P. E.; Nudel, B.; and Banerji, R., "Learning Problem-solving Heuristics
ThroughPractice," ProceedingsoftheSeventhIJCAI, Vancouver, B.C., pp. 127-34, 1981.
Nelson,K., "HowChildrenRepresentKnowledgeofTheirWorldinandoutofLanguage:APreliminary
Report," in Children's Thinking: WhatDevelops?R. S. Siegler(Ed.), Erlbaum, Hillsdale, N.J.,
1978.
Nickerson,R.S., andAdams,M. J., "Long-termMemoryforaCommonObject,"CognitivePsychology,
Vol. 11, pp. 287-307, 1979.
Palmer, S. E., "Fundamental Aspects ofCognitive Representation," in Cognition and Categorization,
B. B. LloydandE. Rosch(Eds.), Erlbaum, Hillsdale, N.J., 1978.
Piaget,J., TheChild'sConceptionofPhysicalCausality, Routledge&KeganPaul, London, 1951.
, TheConstructionofRealityinthe Child, BasicBooks, NewYork, 1954.
Posner, M. I., and Mitchell, R. F, "Chronometric Analysis ofClassification," PsychologicalReview,
Vol. 74, pp. 392-409, 1967.
Reber, A. S., "Implicit Learning ofArtificial Grammars," Journal ofVerbal Learning and Verbal
Behavior, Vol. 6, pp. 855-63, 1967.
"ImplicitLearningofSynthetic Languages: The RoleofInstructional Set,"JournalofExperimentalPsychology:HumanMemoryandLearning, Vol. 2, pp. 88-94, 1976.
Reed, S. K., Ernst,G. W. , andBanerji, R., "TheRoleofAnalogyinTransferBetweenSimilarProblem
States," CognitivePsychology, Vol. 6, pp.436-50, 1974.
Rosch,E., "OntheInternalStructureofPerceptualandSemanticCategories,"inCognitiveDevelopment
andtheAcquisitionofLanguage, T. E. Moore(Ed.), AcademicPress, NewYork, 1973.
"CognitiveRepresentationsofSemanticCategories,"JournalofExperimentalPsychology:General, Vol. 107, pp. 192-233, 1975.
, "Principles ofCategorization," in Cognition and Categorization, E. Rosch and B. B. Lloyd
(Eds.), Erlbaum, Hillsdale, N.J., 1978.
Rumelhart, D., andNorman, D. A., "Accretion,Tuning,andRestructuring: ThreeModesofLearning,"
inSemanticFactorsinCognition,J.W.CottonandR.L.Klatzky(Eds.),Erlbaum,Hillsdale,N.J.,
1978.
Schneider,W.
andFisk,A.D.,"AttentionTheoryandMechanismsforSkilledPerformance,"inMemory
andControlofAction, R. A. Magill (Ed.), North-Holland, NewYork, 1983.
Shepp,B.E., "FromPerceivedSimilaritytoDimensionalStructure:ANewHypothesisaboutPerspective
Development,"inCognitionandCategorization,E.RoschandB.B.Lloyd(Eds.),Erlbaum,Hillsdale, N.J., 1978.
Siegler, R. S., "ThreeAspectsofCognitive Development," CognitivePsychology, Vol. 8, pp. 481-520,
1976.
, "TheOriginsofScientificReasoning," inChildren'sThinking: WhatDevelops?inR. S. Siegler
(Ed.), Erlbaum, Hillsdale, N.J., 1978.
, "Developmental Sequences Within and Between Concepts," Monographs ofthe Societyfor
Researchin ChildDevelopment, Vol. 46, No. 2, 1981.
--- PAGE 362 ---
348 CHAPTER 12: LEARNING PHYSICAL DOMAINS
Smith, E. E.,andMedin, D. L., CategoriesandConcepts, HarvardUniversity Press, Cambridge, 1981.
Stepp, R. E.,andMichalski, R. S., "HowtoStructureStructuredObjects,"ProceedingsoftheInternationalMachineLearning Workshop, R. S. Michalski(Ed.),AllertonHouse, UniversityofIllinois
atUrbana-Champaign,pp. 156-60,June22-24, 1983.(Anupdatedversionofthispaperappearsas
chap. 17ofthisvolume.)
Stevens, A., Collins, A., andGoldin, S. E., "Misconceptions in Students' Understanding," Journalof
Man-MachineStudies, Vol. 11, pp. 145-56, 1979.
Sussman, G. J., A ComputationalModelofSkillAcquisition, Springer-Verlag, NewYork, 1976.
Tversky, A., "FeaturesofSimilarity," PsychologicalReview, Vol. 84, pp. 327-52, 1977.
Tversky, A., andGati, I., "Similarity, Separability andtheTriangleInequality," PsychologicalReview,
Vol. 89, No. 2, pp. 123-54, 1982.
Tversky,A.,andKahneman,D., "Availability:AHeuristicforjudgingFrequencyandProbability,"CognitivePsychology, Vol. 5, pp. 207-32, 1973.
Williams, M., Hollan, J., andStevens, A. L., "Human ReasoningaboutaSimple Physical System," in
MentalModels, D. GentnerandA. L. Stevens(Eds.), Erlbaum, Hillsdale, N.J., 1983.
Winner, E., "NewNamesforOldThings,"JournalofChildLanguage, Vol. 6, pp. 469-91, 1979.
Winston, P. H., "Learningand Reasoningby Analogy," CommunicationsoftheACM, Vol. 23, No. 12,
pp. 689-703, 1980.
, "Learning New Principles from Precedents and Exercises," Artificial Intelligence, Vol. 19,
pp. 321-50, 1982.
--- PAGE 363 ---
PART
FOUR
LEARNING BY
ANALOGY
--- PAGE 364 ---
--- PAGE 365 ---
CONCEPT FORMATION
BY INCREMENTAL ANALOGICAL
REASONING AND DEBUGGING
MarkH. Burstein
Yale University
Abstract
Thischapterpresentsamodel oflearningby analogical reasoning. The model
is based on twomain ideas, namely, (1) thatthe analogies used in learning aboutan
unfamiliar domain depend heavily on the use ofpreviously formed causal abstractionsinafamiliarorbasedomain; (2)thattheseanalogiesareextendedincrementally
to handle related situations. CARL is a computer program that learns about the
semantics of assignment statements for the BASIC programming language. It is
describedasanillustrationofcausallydrivenanalogicalreasoningandlearning. The
model mapsanddebugs inferencesdrawn from severalcommonlyusedanalogiesto
assignment in response topresented examples.
13.1 INTRODUCTION
It has often been said among AI researchers that learning something new
requiresknowingalotaboutitalready. Thisiscertainlytrueforlearningbyanalogy.
This chapter shows how prior knowledge can be applied in one specific kind of
learningbyanalogy, namely, theformationofnewconceptsinanunfamiliardomain
fromanalogiespresented inatextorbyateacher. Acomputerprogram, CARL, will
be described to illustratethis kind oflearning. CARL learns about the semantics of
assignment statements forthe BASIC programming language when given analogies
like those found in introductory computer programming textbooks. The model was
--- PAGE 366 ---
352 CHAPTER 13: INCREMENTALANALOGICAL REASONING
motivatedinpartbyobservationsofhowstudentsbehavedwhentheywerefirstintroducedtoBASIC usingtheseanalogies. In fact, it was oftentheerrors madeby these
students thatprovided the most useful insights into analogical reasoning processes.
Some unresolved problems with earlier models of analogical reasoning are
addressed here. Because of the close relationship between everyday notions of
analogyandsimilarity, severalmodelsofanalogicalreasoninginAIhavebeendeveloped around forms ofpartial pattern matching. Algorithms likethose developed by
Evans (1968) and Winston (1980) were based on the assumption that a best partial
match could be found by accumulating evidence for each ofa number ofpossible
object-to-object mappings between representations of two situations and then
choosingtheonethatscoredhighest. Inthesesystems,evidenceforamatchconsisted
essentiallyofthenumberofrelationalconnectionspreservedbetweencorresponding
objectsforagivenalignmentofobjects. Theobjectalignmentthatplacedthelargest
numberofrelations and attributes incorrespondencewas consideredthe best match
and thus the "correct" analogical interpretation.
Thisapproachhasseveralmajordrawbacksasamodelforanalogicallearning.
First, itpresupposesthatwell-defined,boundedrepresentationalmodelsofthesituations in both the base (i.e., familiar) domain and the targetdomain are available as
inputs. Inalearningsituation, however, therequiredpriorrepresentationsofobjects
and relations inthetargetdomain may bewrong or inconsistent withthe analogy. If
thedomainistotallyunfamiliar, theremaynotevenbeanyfragmentsofausefulrepresentationavailable. Thepointofpresentingananalogytostudentsistoaidthem in
the construction ofa representation ofa target situation orto correct problems in a
prior representation. Since matching cannotbe used to construct such a representationwheretherewasnonebefore, itcannotbethebasisofageneraltheoryoflearning
by analogy.
Another problem with theories based principally on the matching ofdescriptions, particularly as the complexity ofthese descriptions increases, is that conceptual representations for real situations may contain many objects thatdon't take part
in a specified analogy. Winston has suggested that attention to important relations,
suchasthoseinvolvingcausallinks,canreducethenumberoflinksandthusthecomputational complexity of the matching process to some degree (Winston, 1980,
1982). Yet even in strictly causal models, subsystems can quite often be usefully
expanded to greater and greater levels ofdetail (de Kleer and Brown, 1981 ; Collins
and Gentner, 1982); new objects and relationships that may or may not play roles in
theanalogyarethusintroduced. Asystemthattakesasinputincompletedescriptions
ofanalogical situations, suchasthose presented intexts, but that has a large body of
background causal knowledge and other knowledge for "filling out" such descriptions, wouldstill needmethodsfornarrowingthe focusofthecomparison. Inparticular, it must be possible to find analogical relationships between situations without
pairing detailed specificationsofallofthe objects potentially present in representations ofeach situation.
--- PAGE 367 ---
BURSTEIN 353
To address these objections, we have replaced bottom-up matching with an
approach based on analogical mapping. Such an approach uses a set ofheuristics to
delimitwhat istobe "imported" fromabasetoatargetdomainatagiventime. One
such heuristic involves mapping previously formed abstractions, such as those
embodying causal and planning rules. It has been argued independently that such
structuresarenecessarywhenoneisinterpreting,planning,andreasoningaboutsituations in familiar domains (Wilensky, 1983; Sacerdoti, 1975). Focusing on such
structures andtheirassociated special cases and knownproblems allows foramuch
more top-down form ofanalogical reasoning. This is exactly what is required when
prior knowledge ofthe target domain is severely impoverished.
13.2 Student Protocols Used as Guidelines
Analogiesfoundintypicalintroductorytextsgenerallyincludestatementssuggestingcorrespondences betweenclasses ofobjects in the domains tobe related. To
beuseful,thesestatedcorrespondencesmustbepresentedalongwithatargetdomain
situation described in terms ofa. plausible situation in the familiar domain. This is
illustrated in figure 13-1, which depicts the method used by one authorto introduce
the notion ofacomputer variable.
Theanalogyshowninfigure 13-1 canbeparaphrasedasfollows: "Avariableis
like a box in that numbers can be inside variables similar to the way objects can be
insideboxes." Ifthis analogy istobeappliedeffectively by a studentlearning about
assignment, it will be helpful ifsome valid actions involving variables in this relationship are explicitly introduced, as by the statement "Topwfthe number 5 in the
variableX, type 'X = 5'." Regardless ofhow it is presented, however, this kind of
given informationmustbecombined with a student'sability toaccess knowledge of
the "box" domain, includingmanyspecificconceptsandinferencesdevelopedfrom
A B C D E F G H
3 12
J K L M N P Q R
S T U V W X Y Z
Toillustratetheconceptofvariable, imaginethatthereare26littleboxesinsidethecomputer.
Eachboxcancontainonenumberatanyonetime(Albrecht, Finkel, andBrown, 1978).
Figure 13-1: Atextbookintroductiontovariables.
--- PAGE 368 ---
354 CHAPTER 13: INCREMENTALANALOGICAL REASONING
experienceinthatdomain. Inthissimplecase, astudentmustatleastbeabletointerpretstatementsaboutputtingobjectsinboxesandtopredicttheeffectsofsuchactions
undervarying conditions.
This author found that students learning introductory BASIC generated a
numberofplausible, thoughincorrect,explanationswhenaskedtopredicttheeffects
ofpresented examples or to solve simple problems using what they had been told
about the "programming" domain. Their answers were plausible in the sense that
they were based on valid or plausible scenarios for events in one ofthe analogical
base domains that they knew. The errors occurred even when they were analyzing
extremely simple assignment statements. For example, statements like "X = 7"
were misinterpreted as indicating that the variable Fwas to be "placed inside" the
variableX. Thiscanbe seenasamapping oftheplausible scenariothataboxcanbe
placed inside another, largerbox.
Errorsofthistypearealmostinevitablewhenoneisextrapolatingfromananalogical base domain, since analogies are, almostby definition, useful but imperfect
correspondences between situations. Although these errors were not the intended
effects ofthe teacher who presented the analogy, they may nevertheless be taken as
examples of analogical reasoning in humans with limited knowledge of a target
domain. They weretherefore included inthedata treated by the process model presented here.
Suchexamplesmake itclearthatanimportantpartoftheprocessofdeveloping
new concepts by analogy must be the incremental debugging of the inferences
derived from the analogy. Sources of alternate hypotheses, including additional
analogies, can also be quite useful in this debugging process. The following protocols with one subject (Perry, age ten) illustrate this quite clearly. After Perry read a
paragraphcontainingtheboxanalogy, takenfromatextbook, thedialogue shownin
figure 13-2 occurred.
Here, three common analogies to variables and assignment are mentioned
(showninitalicsinthefigure): (1)puttingnumbersinvariablesislikeputtingobjects
Teacher: Supposethere'saboxcalledXandwe'regoingtostorethenumber5 inthere. Howwould
youdothat?
Pupil: Thevariables uhX X no
. . . . . . . . . . . .
Teacher: Youhavetotell it . . .
Pupil: Putthenumber5 inthevariableX.
Teacher: Youhavetogiveitacommandthatwill make itdothat. Now, herewehaveanexample
. . .
supposeItype "fl=10."
Tea P c u h p e i r l : : O N h o , w a I nd ty i p f e y d ou tha w t an in t , t s o o s n to o r w et it h ' i s s g 1 o 0 in y g ou to wr r i e t m e e . mb . e . rthat, ok?
Pupil: ThatBequals 10.
Teacher: It'sgotaboxcalledB, andinside it is 10.
Pupil: Soyou writetheboxandthenthenumberthatyouwanttostore it in?
Teacher: Yes. andyou put inanequal signtotell it todothat.
Figure 13-2: An informal introductionofseveral relatedanalogies.
--- PAGE 369 ---
BURSTEIN 355
inboxes, (2)computersusevariablestorememberthingstheyaretold, and(3)assignment statements are like algebraicequalities. At first glance, this might seem to be
more confusing than helpful. Yet each analogy canbe shown to play auseful, often
complementary, role in developing a working understanding ofthe idea ofassignment. Tutorialtextbooksoftenuseatleasttwooftheseanalogies. Ifthey arenotpresentedexplicitly as analogies, theirpresence is suggestedby the normal language of
the computer science domain. Computermemory is often referred to, and the equal
sign is used in a number oflanguages to denote assignment.
The real test ofatutorial analogy is how it affects one's understanding ofnew
situations. The followingexample showsoneway inwhich having several analogies
can be more helpful than havingjust one. As the dialogue above continued, a point
about transferring values from one variable to another was illustrated. The teacher
typed "P = 10" and then "g = P." Figure 13-3 shows what happened. Perry
clearly seemstohave madethe inferencethatif "Q = P" was analogoustomoving
anobjectfromoneboxtoanother, thenthenumberthathadbeenin Pmustnowbein
Q. Sinceobjects,whenmoved,arenolongerattheiroriginallocation, Pmustnowbe
"empty."
Whenpressed, Perrywasuncertainaboutthisconclusionandcameupwithan
alternate explanation based on an entirely different line of reasoning, using a different analogy. Iftyping "g = P" is treated as a statementtothecomputerthatthe
twovariables have equal values, then, according tothe normal algebraic interpretation, onecan conclude fromthe factthatPwas originally 10that Q is also 10, while
P's value stays the same.
Applyinghisknowledgeofalgebrainthisexplanationdidnot stopPerryfrom
furtheruseoftheboxanalogy. Hecontinuedtousebothmodelsinanalyzingnew situations. Algebrasimplyprovidedwhatseemedlikeamoresatisfactoryanswertothis
particularproblem.
Teacher: Sowhat'sinPnow?
Pupil: Oh. Nothing.
Teacher: Nothing?
Pupil: 10! andthenQ isalso.
Teacher: Whatdoyouthinkitis?Isitnothingor 10?
Pupil: Let'sfindout. Firstlet'ssee . . .
Teacher: Well, whatdoyouthinkitis?
Pupil: Ifyouhavetwoboxes, andyoumoved
. . .
Youmovedoritequalsto?Youmovedwhat'sinPtoQsothere'snothinginit, ordidyou
onlyputthesamenumberinQthat'sinP? Ithinkit's 10.
Teacher: Youthinkit's 10?
Pupil: Becauseyoudon'tsaythat, uh, movePatall
. . .
takePout. YouonlysaidthatQequalsthe
sameasP. Soifitequals, ithastobe 10,becauseifthere'sno 10init, Qwouldn'tbeequalto
it.
Figure 13-3: Twoanalogiesinvolvingconflictingpredictions.
--- PAGE 370 ---
356 CHAPTER 13: INCREMENTALANALOGICAL REASONING
Sucherrorsandtheirexplanationsplayedanimportantroleinthedevelopment
of the process model implemented in CARL. Because these incorrect hypotheses
were arrived at by extending existing analogies to cover plausible, related basedomainsituations, they wereassumedtohavebeenproducedbythe same analogical
reasoning process thatproducedcorrectanswers in othercases. Thus the analogical
reasoning model described below was developed in an attempt to account for these
errors as well as forcorrect analogical inferences.
13.3 A PROGRAM THAT LEARNS ABOUT ASSIGNMENT
Manyaspectsoftheoveralllearningmodelareillustratedinthebehaviorofthe
computer program CARL, which learns about different kinds ofassignment statementsbybuildingandcombininginferencesdrawnfromseveraldifferentanalogies.
Figure 13-4 is the equivalent in English of one portion of a dialogue with CARL
(actual inputsareinanextendedversionofConceptualDependency; seeSchankand
Abelson, 1977).
With the aid of a tutor CARL develops semantic representations for most
common types ofassignment statements. It forms rules concurrently about how to
parse them, how to infer the logical results oftheir execution, and how to generate
them as components ofsimpleplans. Inthe process, it makes a number ofthe same
mistakes madeby people learning the same material. (See Burstein, 1981, for other
examples.)
Teacher Thecomputerremembersanumberbyputtingitinavariable.
Teacher Avariableislikeabox.
Teacher Toputthenumber5 inthevariablenamedX, type "X=5."
Teacher Howdoyougetthecomputertorememberthenumber9?
CARL Type "X=9."
Teacher SupposeIthentype "X=6."
Teacher Nowwhat'sinthevariableX?
CARL 6isinXand9isinX.
Teacher No, the6replacesthe9inX.
CARL OK.
Teacher SupposeItype "y=5." andthen "X-Y."
Teacher Nowwhat'sinthevariableX?
CARL YkinX.
Teacher No, itmeansputthenumberin KintoX.
CARL OK.
Teacher Nowwhat'sinthevariable Y?
CARL Nothing.
Teacher Guessagain.
CARL 5 is in K(byanalogytoequality).
Teacher Correct.
CARL OK.
Figure 13-4: Adialogue withCARL.
--- PAGE 371 ---
BURSTEIN 357
13.4 AN INITIAL STRUCTURE MAPPING THEORY
The analogical reasoning process used in CARL was strongly influenced by
somepsychological studies ofanalogical learning. Gentnerhas outlinedacognitive
model oflearning from scientific or "explanatory" analogies involving some ofthe
problems mentioned here, though not others (Gentner, 1982; Gentner and Gentner,
1982; see also chap. 12 of this volume). The analogies considered by Gentner
included such statements as:
The hydrogen atom is like the solar system.
Electricity flows through a wire like waterthrough a pipe.
The model Gentner proposed for learning from such analogies, unlike those
basedprincipallyonpatternmatching, didnotrequireafulldescriptionofthetarget
domainbeforehand. Inhermodel, relations, orpredicatesconnectingseveralobjects
orconcepts, are mapped identically from one domain tothe otherundera prespecified object-object correspondence. After identical first-order relations have been
mappedtorelatecorrespondingobjectsinatargetsituation, second-orderpredicates,
such as causal links between relations, are also mapped.
Although this model does suggest a way to map new structures into an unfamiliardomain, itdoesnotgiveagoodaccountofhowcorrespondingobjectsarefirst
identified, nordoesitconstrainwhichrelationsaremapped. Italsodoesnotallowfor
mappings between nonidentical relations, which is often necessary.
Theneedtoconstrainthesetofrelationsmappedbyananalogycanbeseenby
examining Gentner's representation ofa mental model for the solar system and the
mapping her system predicts to an analogous model for the atom (fig. 13-5). In the
diagram, the sun is relatedtoeachplanetby thepredicate HOTTER-THAN, as well
YELLOW, HOT, MASSIVE + (doesn'tmap)
(propewrties)
SUN - NUCLEUS
ATTRACTS I REVOLVES-AROUND ATTRACTS REVOLVES-AROUND
MORE-M | ASSIVE-/ HOTTER-THAN-^-^ \ MORE-MASSF
THAN / \ THAN
ELECTRON
BaseDomain TargetDomain
Figure 13-5: Gentner'srepresentationofRutherford'satomicmodel.
--- PAGE 372 ---
358 CHAPTER 13: INCREMENTALANALOGICAL REASONING
as by ATTRACTS and REVOLVES-AROUND, two predicates that are themselves
causally related (not shown). The problem Gentner noticed was that the HOTTERTHAN relationdoesnotseemtobemappedtotheatomicmodel; thatis, peopledon't
generally believethattheanalogy implies thatthe nucleus ofan atom mustbe hotter
than its surrounding electrons. Gentner's formal specification ofthe mapping processcouldnotpredictthis failureto mapcertain relations. This is apotentially large
problem, since many other attribute comparisons, such as BRIGHTER-THAN,
YELLOWER-THAN, and so on, could also be part of a description of the solar
system. Presumably, those relations are notbeing mapped either.
The explanation provided by Gentner for this phenomenon was in terms ofa
general condition onthe mapping process, which shecalledthesystematicitycondition. Thisconditionisessentiallythat "predicatesaremorelikelytobeimportedinto
thetargetiftheybelongtoasystemofcoherent, mutuallyconstrainingrelationships,
theothersofwhich aremapped" (GentnerandGentner, 1982). Inthiscase, theconstraint system is the causal relationship connecting the relations ATTRACTS and
MORE-MASSIVE-THANtothemotionREVOLVES-AROUND.
Sincethesepredicates are related together, the systematicity condition predicts that they are more
likely tobe mapped. Although this example clearly shows the need to constrain the
mapping to a subset of all the possible relations between a given set of objects.
Gentner's systematicity condition is not easily integrated into her proposed process
model, as stated.
In CARL, this general condition is reformulated as part ofa set oftop-down
constraints on the relations considered for mapping. When a causally connected
structurecanbefoundinmemorytosupportadescribedbase-domainsituation, only
relations taking part in that structure are considered for mapping. By this rule, the
only relationsconsidered for mapping in an analysis ofthe solar system analogy are
thosecontributingtothelearner'spriorcausal modeloftherelativemovementofthe
planetary bodies. The rule is not simply a restatement ofGentner's condition, since
the discourse context and the active goals ofthe learner may influence the type of
causalexplanation retrievedforagivensituationandhencetheeffectofthe rule. For
example, this model suggests that there will be times when a description of an
analogy is underspecified (no causal structure found) or ambiguous (several causal
structures found).1
A second mapping constraint used in CARL operates on the structured setsof
relationsretrieved frommemory. Thisconstraintstatesthat simpleattributecompar-
:Interpretingthesolarsystemanalogywhengiventhestatement "EachatomislikeIsolarsystem" ispresumabK madeeasier it one isalsogiven some Statement like "Electronsmoveaboutthe nucleustheway
planets move about the sun.'- or il an accompanying diagram is provided that focuses one's attention
directK on the similai motions ot the twosystems.
--- PAGE 373 ---
BURSTEIN 359
isons (like HOTTER-THAN, LARGER-THAN, and so on) are not mapped if the
objects in the targetdomaincannotbe compared onthe same relational scale and no
corresponding attributes have been suggested by the teacher. For example, in
applyingtheanalogy betweenboxesandvariables, CARL decidesthattheprecondition that numbers must "fit" inside variables should be dropped from the causal
structure for assignment because there is no common size scale for numbers and
variables.
Of course, limiting the set of relations considered for mapping in this way
requires thatthe causal structures governingbase-domain situations are made available to the analogical reasoning system at the appropriate time. All ofthis is made
easierinthecontextofalanguage-understanding systemthatactivates such memory
structuresasanormalpartofitslanguageinterpretationprocess. WhenCARL ispresented with a statement ofa novel analogy, it uses the object and predicate assumptions given in the analogy statement to turn an analogical description of a targetdomain event into a description of a base-domain situation that it can support as
plausible with causal and goal/plan structures retrieved from memory in that
domain. It then maps the retrieved causal structure to the target domain.
Theresultofmappingacausallyconnectedstructurefoundundertheseconditions is the formation ofa new, parallel, causal structure characterizing the target
example. Objects in the target example are made to fill roles in the newly formed
structure; knownobjectcorrespondencesbetweenthedomains areusedwhenavailable. When known correspondences are not available, object correspondences
betweenthedomainsareonlyformedbyvirtueofrolecorrespondencesbetweenthe
mappedstructures. So, forexample, fromacausalstructureindicatingthattheresult
ofputting aphysical objectinacontaineristhe state "theobject is INSIDEthecontainer," CARL concludes that one result ofan assignment is a parallel relationship
betweenvariableandnumbers. Onthebasisofcorrespondingrolesintheserelations,
an indirect correspondence is formed between physical objects (that go into boxes)
and numbers. This causally directed mapping process forms new target-domain
causal structures where none existed before, while allowing correspondences
between relations to be formed with some consideration of what is known of the
objects and relations in the targetdomain.
Theapproachtoanalogical reasoningandconceptformationtakenhereparallels Carbonell's work on analogical problem solving (Carbonell, 1983; see also
chap. 14 ofthis volume). Carbonell outlined a problem-solving process whose first
stepwastoberemindedofasolutiontoasimilarproblem. Theprocessmodelheproposed then modified the components ofthe recalled plan to satisfy the needs ofthe
new problem using a set ofoperators that preserved, as much as possible, the temporal and logical goal/subgoal structure ofthe original solution. Both Carbonell's
model and that ofthe author were strongly influencedby Schank's theory ofhuman
memory organization (Schank, 1982) andtheeffectsofthatorganizationontheprocesses ofinterpreting, planning, and learning about new situations.
--- PAGE 374 ---
360 CHAPTER 13: INCREMENTALANALOGICAL REASONING
13.5 MAPPING TO NONIDENTICAL RELATIONS
Gentner's model also claimed that all relations are mapped "identically"
between analogous situations. Although this might be true in analogies between
purely spatial descriptions ofsituations, including the standard geometric analogies
discussedby Evans (1968), itis muchtoo strong aclaim in general. Whenanalogies
are formed between physically realizable situations and purely abstract ones, as in
mathematicsandcomputerprogramming, itisimpossibletomaintainthe "identical
predicate" mapping model.
Probably the most importantthing implied by the analogy betweenboxes and
variables is the fact that variables can "contain" things; that is, the relationship
between abox and an object inside the box is, in some ways, similartothe relationshipbetweenavariableandthenumberassociatedwiththatvariable. Yet itis notthe
physical properties ofboxes that are preserved by this analogy. Variables don't have
sidesand abottomthatacontainedobjectcan rest on. The relations related by mappingaresimilarprimarilybecauseoftheactionsandplansinwhicheachisinvolved.
One canputthings in boxes, andassignmentprovides a way to "put" numbers "in"
variables as well. The principle function ofthe containment relation for variables is
its role in abstract plans like STORE-OBJECT.
Students learningtoprogram are generally aware that computers can manipulatenumbersandthatthereasononelearnstoprogramistobeabletodirectthecomputer's actions. This knowledge may be used to infer that the action of putting a
numberinavariablewillbeusedinplanstomanipulatenumbers. Whetherornotthis
inferenceoccurs immediately, however, the factthattheanalogy betweenboxes and
variables relates physical objects to abstract concepts (numbers) suggests that the
actual preconditionsand sideeffectsofassignment inthe programmingdomain may
bequitedifferentfromtheconditionsonplacingobjects inboxes. Although students
only gradually discover how these situations differ, it is important that such differences not invalidate the analogy entirely. In a computational inference system, this
must be reflected in the predicates representing the relations in each domain.
The problem from the standpoint ofGentner's model is that the relationship
thatis mappedfromthe "boxworld" tothe "computerworld" isexactlythatofphysical containment. When this relation is copied into the programming domain, the
interpretationthat results isthata number isphysically INSIDEofavariable. Under
normal circumstances, people learningtoprogram may have no idea what computer
variables are, but they should know that numbers are not physical objects and they
should not expect that all the inferences involving the relation INSIDE will appl)
when numbers are placed "in" variables.
This problem can be characterized as one oflevels ofabstraction. Depending
onhow much is knownabouttheobjectsinthetargetdomainwhentheanalog) ispresented, it may ormay not be reasonabletomapthe most specific version ofa relation
from onedomain toanother. When mapping a relation identically leads to the viola-
--- PAGE 375 ---
BURSTEIN 361
tion ofa constraint on one ofthe slots in that relation, then the relation meant in the
targetdomainmustbeonesharingsomeofthepropertiesofthebase-domain relation
but not others.
When an attempt to map a relation directly results in such a constraint violation, CARLformsavirtualrelationinthetargetdomainthatisa "sibling" ofthecorresponding base-domain relation or an ancestor at some higher level in the generalizationhierarchyofrelationalpredicates. Theconstraints initiallyplacedontheslots
invirtual relationsaredeterminedprimarilyfromtheclassesoftheobjectsrelatedin
the target-domain examples presented.
WhenCARL isgiventheboxanalogy, itfindsthatmappingobjectstonumbers
violates a normal constraint on the INSIDE relation. Instead, CARL forms a new
predicate to relate variables and their "contents." This relation, hereafter called
INSIDE-VAR, is initially given the constraints thatthe "contents" slot be a number
andthe "container" slotbeavariable, basedonthetypesoftheobjectsintheaccompanying example, "X = 5." Inferences are associated with this new relation as they
are successfully mapped from the box domain, learned independently in the new
domain, or inherited from other analogies.
The final resultofmapping the structure PUT-IN-BOX, describing the causal
relations involved in putting an object in a container, looks roughly as shown in
figure 13-6. Notice particularly that the PTRANS predicate indicating physical
transfer was also replaced by the more general predicate TRANS (general state
change) because the object "moved" in the target domain was notaphysical object.
BoxDomain ProgrammingDomain
Template:
PUT-IN-BOX PUT-IN-VAR
role-variables:
R-ACTOR (a HUMAN) PIV-ACTOR (a COMPUTER)
R-BOX (a BOX) PIV-BOX (a VARIABLE)
R-CONTENTS (a PHYSOBJ) PIV-CONTENTS (a NUMBER)
actions:
(PTRANS (TRANS
actor R-ACTOR actor PIV-ACTOR
object R-OBJ object PIV-OBJ
from (unknown) from (unknown)
to (INSIDE of R-BOX)) to (INSIDE-VAR of PIV-BOX))
preconditions:
(*not* (INSIDE or R-BOX IS R-OBJ)) (*not* (INSIDE-VAR of PIV-BOX is PIV-OBJ))
(SMALLER than R-OBJ is R-BOX) - dropped -
results:
(INSIDE of R-BOX is R-OBJ) (INSIDE-VAR of PIV-BOX is PIV-OBJ)
Figure 13-6: Mappingasimplecausal structure.
--- PAGE 376 ---
362 CHAPTER 13: INCREMENTALANALOGICAL REASONING
OVERVIEW OF THE ANALOGY-MAPPING PROCESS
13.6
In general, CARL develops simple causal or inferential structures in a target
domain by retrieving structures in memory from a familiar domain and adapting
them using a top-down mapping process that preserves the causal/temporal links
explicitly specified in those structures. The predicates mapped are subject to transformation within their abstraction hierarchy, as described above. Subsequent use of
an analogy may occur when new examples are presented for which no explanation
can be found in the target domain or when problems are presented requiring the
retrieval ofplansoractionstoachieve specific, analogically statedgoals. The latter,
in CARL's subsetoftheprogrammingdomain, isgenerally a request forthe generation ofan assignment statement satisfying some specific goals or constraints.
Inansweringaquestion, CARLalwayslooksfirstformemorystructuresinthe
domain it is learning about. Ifthis fails, it tries known analogies. Thus subsequent
accesstobasedomains isalways forthepurposeofmapping new, related structures:
related action situations or more detailed, context-specific versions of previously
mapped structures that account for additional predications or results.
Themappingprocesstriestoformstructuresinthetargetdomainunderthefollowing general constraints:
• Correspondingpredicatesmustbeofthesameclass(action, relation,planstep,
plan, goal, and so on).
• Corresponding predicates inthe target structure are related togetherby causal
or temporal links corresponding to those in the base-domain structure.
• Corresponding case slots ofanalogically related predicates must consistently
be filled by corresponding roles ofthe two structures related.
CARL keeps a record called an AMAP detailing all ofthe object, role, and
predicate correspondences developed.
AMAPS
are extended as needed to include
new correspondences as they are found.
AMAP
Because the uses role correspondences as well as object class correspondences, the relationships between objects oftwo domains can actually change
quite subtly when new problems are being handled. Several bugs in protocols
observedbytheauthordependonthisdistinction. Thequestion "Howwouldyouput
one more in X?" has been answered with "X = I" by a student, and the question
"What is in X afteryou type 'X = 7' and then 'X = 6'?" has been answered. "13."
These responsescanbe interpreted intermsofavariantoftheboxanalogy, by which
the numberassigned to the variable correspondstothe numberofobjects placed in a
box. Thus, for the first problem, "X = 1" simply places an object in a box that
already contained some objects, causing it to contain "one more." while putting six
objects in a box that contained seven others results in one with thirteen.
These solutions can both be explained by an analogical correspondence
between contents ofboxes and contents ofvariables that maps a set ofobjects to the
--- PAGE 377 ---
BURSTEIN 363
cardinalityofthatsetratherthanmappinganobjecttoanumber. Thisisaslightlydifferent analogical mapping, though the same roles ("contents") of each base- and
target-domain situations are being related. Modeling these responses requires that
the analogy formation process primarily relate objects in terms oftheir functional
roles in specific situations and only secondarily determine how object features correspond.
INCREMENTAL ANALOGICAL REASONING
13.7
Even when analogies are based on simple actions, the specific inferences
retrieved in supportofnewexamples may vary considerably, depending on the context. Forexample, throwingarockatabrickwallandthrowingoneataglasswallare
immediately known to have very different consequences. Although an analogy to a
thrown rock mightimply indirectlythateachofthesealternatecontexts hadacorrelate in a target domain, in practice each potential class oftarget domain situations
must be explored.
Extending analogies inthis fashion is anerror-proneprocess. Intheprotocols
examinedby the author, attemptstoextend analogies tovarianttarget-domain situationsoccurredbothwhensubjectswererespondingtopresentedproblemsandexamplesandalsowhentheyweregeneratingexamplesinindependentattemptstofindout
moreaboutwhatwaspossibleintheprogrammingdomain. Perrydisplayedthelatter
behavior early in his first session when he began asking questions to find out what
could go "inside" a variable. He asked whether it was possible to put one "box" in
another. Thisisclearlyareasonablethingtodowithrealboxes, butit'simpossiblein
BASIC.
CARL extendsanalogiesby mappingcontext-specific inferenceslikethisone,
butonlywhentheyformpartoftheinterpretationofapresentedexample. Inthecomputerdialogueshown infigure 13-4, CARL firsttriesusingtheboxanalogytointerpret "X = Y" intermsoftheaction "moveonebox intoanother." When thisiscorrectedbythetutor'sstatementthatthecontentsofFaremoved, CARL triesmapping
a causal chain describing the transfer of an object from one container to another,
much as Perry did in figure 13-3.
Both ofthese hypotheses about statements like "X = Y" are generated after
CARL'sinitialapplicationoftheboxanalogytotheBASICstatement "X = 5." Each
subsequentuseoftheanalogyalsoinvolvesthemappingofacausaldescriptionfrom
AMAP
thebasetothetargetdomain. Informationsavedinthe fromthatearliermappingofthecausal prototype "putan objectinabox" is firstusedina "reversemapping" processtoconstructabase-domaindescriptionofthenewproblem. Thisbasedomain description is used totriggerthe retrieval ofa base-domain causal structure
explainingit. TheAMAP isthenusedagaintomapthenewcausalchainintotheprogramming domain. This results in a model for "X = Y" containing the "bug" that
the "moved" number is no longer in Y.
--- PAGE 378 ---
364 CHAPTER 13: INCREMENTALANALOGICAL REASONING
Thememoryorganizationandretrievalsystemusedin CARL forknowledgeof
simpleaction-baseddomains involving familiarobjects is an extension ofan objectbased indexing system described by Lehnert (1978; Lehnert and Burstein, 1979) for
naturallanguage-processingtasks. Sothat CARL couldalsoretrieveavarietyofspecial case situations, its memory retrieval process was augmented using discriminationnetworksbasedonthespecificationhierarchy modelofepisodicmemorydeveloped by Lebowitz (1980) and Kolodner (1980). In addition, precondition and result
indices were added so that actions and simple plans could also be retrieved in
responseto requests forthe achievementofspecific goals. Any orall ofthese forms
ofindexingmaybeusedinfindingasuitablestructuretomap. Forfamiliardomains,
thesystemassumesthatalargesetoffairly specificcausal inferencestructuresexists
in memory at the beginning ofthe learning process. No attempt is currently being
madetoconstructcompositecausal structuresonthefly, althoughclearlythatmight
be necessary with more complex analogies.
Figure 13-7 shows partofCARL's networkofcausal structures describing the
effects ofsome simple actions involving containers.
Inthecomputersession shown in figure 13-4, an initial mapping fromthe box
domainwas formedfromthecausal structure PUT-IN-BOX. Thereafter, specializations ofthat structure wereavailable foruse when new examples were presented. In
addition, once the new containment relation was formed for variables, expectations
were established for the other "primitive" situations involving containers. Thus,
fromthe factthatvariablescan "contain" numbers, CARLexpectedthatthey might
also be "put in" or "removed."
Afterconsideringanumberofexamples, CARLdevelopsasimilarnetworkof
causal structuresin itstargetdomain. Manyofthestructuresformedby the mapping
processcontainerroneousinferencesatfirst. Thesestructuresare "debugged" inthe
targetdomain, ifpossible,orreplaced,dependingonthenatureofthecorrectionsuggestedby thetutor. Onceastructuredescribing someclassofassignment statements
has been debugged, corrections made to that structure propagate downward to any
subsequently formed specializations ofit. Thus, once there is a debugged prototype
for statements like "X = 5," the fact that prior values of X are replaced rather than
accumulated alsoappliestocausal structures mapped whencases like "X = K" are
analyzed. Theinheritancemechanism in CARL thathandlesthis isactiveonly when
new structuresareformed, soit wasimportantin "teaching" CARL toshow it these
Situationswith BOXasaCONTAINER:
(PUTIN-BOX OBJ-IN-BOX TAKE-FROM-BOX)
\ /
TRANSFER-OBJ-BETWEEN-BOXES
PUT-BOX-IN-BOX \
PUTMORE-IN-BOX SWAP-OBJ-IN-BOXES
Figure 13-7: Pan ofthe specialization network forthings "INSIDE"' boxes
--- PAGE 379 ---
BURSTEIN 365
bugs early on. This model seems to suggestat leastone reason why itisbestto keep
initialanalogicalprototypesas simpleaspossibleandtocorrectproblemswiththem
quickly.
CARL alsodevelopsparsingandgeneration rules foreachclass ofassignment
statementsuccessfully represented. Theserulesaredevelopedduringthe final stage
ofthe analysis ofeach example.
13.8 USING MULTIPLE ANALOGIES
CARL isoftenabletopredicttheeffectsofassignmentsbetterbyusingthesimilarityofassignmentstatementstoequalitiesthanbyusingtheboxmodel. Ingeneral,
the box model does not help much in interpreting statements containing arithmetic
expressions. However, many assignment statements can be interpreted correctly if
they are first considered as algebraic equalities, particularly ifall ofthe variables
appearingtotherightoftheequalsignhaveknownvaluesandthevariableontheleft
has none.
CARL first notices that algebra might be useful in learning about an assignment when it sees the "=" sign in statements like "X = 5." As it builds a new
meaning for "=," it discovers this earlier definition in its dictionary. Reparsing
"X = 5" as an equality, CARL forms an interpretation with the conclusion that
the value ofthe algebraic variable X is 5. However, because the statement is also a
communicativeact, theeffectofthe statementonthe "BASIC computer" istocause
it to store a new fact, namely, that "the value ofthe variable X is 5." This interpretation thus depends both on the algebraic rule that statements of the form
"variable = number" imply that the value ofthe variable is the specified number
and on a partially formed analogical model ofthe computer as a humanlike interpreter offacts and requests.
It should be emphasized here that a causal/temporal element must be introduced when CARL moves from algebra to assignment. In algebra, variables do not
have changeable values. However, for the algebraic and box models to be related to
eachothersuccessfully,acomparablecausaleffecthadtobefound. Forthisreason, it
was important thatthe algebra model be applied in conjunction with a model ofthe
machine as an active agent or interpreter ofstatements.
SinceCARL
representstheinferenceaboutX'svalueasoccurringasaresultof
thestatementbeingtypedintoacomputer, ithasacausal/temporaleffectthatcanbe
relatedtoaneffectofthe "physical" model ofthe same assignment statementdeveloped using the box analogy; that is, both interpretations ofthe statement cause an
association to be formed between a variable and a value. By comparing these two
descriptions ofthe causal effects ofinterpreting this one statement, CARL forms a
mapping from the predicate VAR-VALUE in the algebra domain to the predicate
INSIDE-VAR that it had previously constructed using another analogy.
--- PAGE 380 ---
366 CHAPTER 13: INCREMENTALANALOGICAL REASONING
Once this analogical association is formed, CARL can interpret the effects of
assignment statements involving arithmetic expressions by first parsing them as
equalities. When it does this during an attempt to determine the effect ofan unfamiliarkindofassignment, ituses rulesofalgebratodeterminethevalueofthe variable on the left. The result is then mapped onto a causal structure describing that
effectonacorrespondingprogrammingvariable. Structuresfromalternateanalogies
in this fashion may be used either to replace erroneous inferences developed using
otheranalogiesortomodelsituationsthathadnodirectcounterpartsintheotheranalogical domains. However, since the relationship between algebraic equalities and
assignmentswasbasedonlyonthediscoveryofsimilarcausaleffects, CARL'srepresentation ofthe "storing action" inthese new descriptions ofclasses ofassignments
is still based onthe action model developedprimarily from the box analogy and the
correctionsmadetoit. Theresultisa "mixed" model, butonethatenablesCARL to
interpretcorrectly assignment statements ofthe form "X = X + 1," which itcould
not do using any ofits analogies independently.
Theforegoinganalysisdependsonacausal model ofactions involving objects
likecontainers, rulesofequalityandarithmeticfromalgebra, and rulesaboutagents
and information processors that are used primarily in interacting with other human
beings. Each contributesapiecetothepuzzle. In learning aboutassignment, CARL
makes only limited use ofthethird analogy, relating computers tohumans as information processors with an ability to communicate, manipulate numbers, and
rememberthings. However,theauthorhasarguedthatthisanalogydoesplayarolein
relating knowledge ofalgebrato its causal model forassignment. This may explain
why references to these last two analogies often appear together, both in textbooks
and in informal dialogues like that shown in figure 13-2.
TheinteractionsamongthethreeanalogiesusedbyCARL areroughlysummarizedinfigure 13-8. Itshouldbenotedthateachanalogy isrepresentedandrelatedat
several levels ofdescription but that the functions served by the analogies are quite
different. Theboxmodelprovidestheinitialcausalmodeloftheassignmentdomain.
Thealgebradomainprovidesknowledgeofnumbers, theoperationsthatcanbe performedonthem, andthesymbolsforrepresentingthem. Thehumanprocessormodel
isactiveprimarilyattheplanninglevel, providingreasonsformanyoftheoperations
thatcomputerscan performandexpectationsthat it willbecapable ofothers. It also
playsaroleinearly modelsofmanyofthecomputercommands, especially inputand
output functions. Forexample, when Perry wanted to check the value ofa variable,
which he knew was done with the PRINTcommand, he would often say, "Let's ask
him."
CONCLUSIONS
13.9
CARL illustrates how analogical learning in a new domain can be accomplishedbyacombinationofincrementalanalogical reasoningandtheuseofmultiple
--- PAGE 381 ---
BURSTEIN 367
analogical models. The authorhasalsoargued thateffective analogical mapping for
learning requires focusing onpreviously known abstractions in abasedomain. This
wasfoundtobenecessary in formingrulesaboutassignmentinCARL, bothtolimit
the analogical reasoning required to create initial models of concepts in the new
domain and to allow for incremental debugging ofthe many errors that can result
fromtheuseofanalogies. Theprocessdescribedhereisheavilyteacherdirected, but
itallowsforfairlyrapiddevelopmentofaworkingunderstandingofbasicconceptsin
a new domain.
This chapter describes an attempt to model the learning ofa common human
cognitive task, given the kinds of information that students often receive when
learning the same task. It also investigates a number ofpotential problems in prior
modelsofanalogicalreasoningandtheirroleinlearning. Thereareseveralproblems
that must be addressed more closely before the algorithms used in CARL can be
appliedinageneralmachinelearningsystem, evenoneoperatinginasimilar, "tutorial"mode. Oneofthemostimportantandpotentiallyusefuloftheseisthecontinued
investigationofthewaysthatmultipleanalogiesinteractandcontributetotheformationofacoherenttarget system. Analogical reasoningandhypothesisgenerationare
intrinsically error-prone processes, requiring continuous monitoring and debugging. Because ofthis, effectiveanalogical learning systems canbenefit fromthe use
ofmultiple analogies. Theability ofa systemtomakeeffective useofseveral analogies in learning should reduce the number ofdetailed explanations and corrections
thatneedtobesuppliedbyateacher. Thisalonewouldimprovetheviabilityofthese
systems.
BoxDomain ProgrammingDomain AlgebraDomain
PlanLevel:
STORE(obj, container) STORE(num, var) REMEMBER(concept)
part-plan part-plan
ActLevel:
PUT-IN-BOX(obj,box) PUT-IN-VAR(num,var) EQUATE(expl,exp2)
result: INSIDE(box,obj) result: IN-VAR(var,num) result: VALUE-OF(var,num)
described-by described-by
WordLevel: I
"<var> = <nwn>" '<var> = <num>
Figure 13-8: Interactionsamongthreeanalogies.
--- PAGE 382 ---
368 CHAPTER 13: INCREMENTALANALOGICAL REASONING
ACKNOWLEDGMENTS
The author would like to thank Dr. Chris Riesbeck and Larry Birnbaum for
many helpful comments on drafts ofthis paper.
The work reported in this chapter was supported in part by the Advanced
ResearchProjectsAgencyoftheDepartmentofDefenseandmonitoredbytheOffice
ofNaval Research under Contract No. N00014-75-C-111.
References
Albrecht, R., Finkel, L.,andBrown,J. R., BASICforHomeComputers, Wiley, NewYork, 1978.
Burstein, M. H., "ConceptFormationthroughtheInteractionofMultiple Models," Proceedingsofthe
ThirdAnnualConferenceoftheCognitiveScienceSociety, pp. 271-74, August 1981.
Carbonell,J. G., "LearningbyAnalogy: FormulatingandGeneralizingPlansfromPastExperience,"in
Machine Learning: An ArtificialIntelligence Approach, R. S. Michalski, J. G. Carbonell, and
T. M. Mitchell(Eds.), Tioga, PaloAlto, Calif., 1983.
Collins,A.
andGentner,D., "ConstructingRunnableMentalModels,"ProceedingsoftheFourthAnnual
ConferenceoftheCognitiveScienceSociety, pp. 86-89, August 1982.
deKleer,J., andBrown,J.S., "MentalModelsofPhysicalMechanismsandTheirAcquisition,"Cognitive
SkillsandTheirAcquisition,J. R. Anderson(Ed.), Erlbaum, Hillsdale, N.J., 1981.
Evans,T. G., "AProgramfortheSolutionofGeometricAnalogyIntelligenceTestQuestions,"Semantic
InformationProcessing, M. L. Minsky(Ed.), MITPress, Cambridge, 1968.
Gentner,D., "StructureMapping:ATheoreticalFrameworkforAnalogyandSimilarity,"Proceedingsof
theFourthAnnualConferenceoftheCognitiveScienceSociety, pp. 13-15, August 1982.
Gentner,D.,andGentner,D.R., "FlowingWatersorTeemingCrowds:MentalModelsofElectricity."in
MentalModels, D. GentnerandA. L. Stevens(Eds.), Erlbaum, Hillsdale, N.J., 1982.
Kolodner,J. L., "RetrievalandOrganizationalStrategiesinConceptualMemory: AComputerModel."
Technical ReportNo. 187, DepartmentofComputerScience, YaleUniversity, 1980.
Lebowitz, M., "GeneralizationandMemoryinanIntegratedUnderstandingSystem," Ph.D. diss.. Yale
University, 1980.
Lehnert, W. G., "RepresentingPhysicalObjectsinMemory," TechnicalReportNo. 131. Departmentof
ComputerScience, YaleUniversity, 1978.
Lehnert,W. G., andBurstein, M. H., "TheRoleofObjectPrimitivesinNatural LanguageProcessing."
ProceedingsoftheSixthIJCAI, Tokyo, pp. 522-24, 1979.
Sacerdoti. E. D, "AStructureforPlansandBehavior," TechnicalReportNo. 109. SRI Artificial IntelligenceCenter, 1975.
Schank.R.C.
DynamicMemory:A TheoryofLearninginComputersandPeople. CambridgeI imerMt>
Press, Cambridge. 1982.
--- PAGE 383 ---
BURSTEIN 369
Schank, R. C, and Abelson, R., Scripts, Plans, Goalsand Understanding, Erlbaum, Hillsdale, N.J.,
1977.
Wilensky,R.,PlanningandUnderstanding:AComputationalApproachtoHumanReasoning, AddisonWesley, Reading, Mass., 1983.
Winston, R, "Learning and Reasoning by Analogy," Communications ofthe ACM, Vol. 23, No. 12,
pp. 683-703, December 1980.
, "Learning New Principles from Precedents and Exercises," Artificial Intelligence, Vol. 19,
pp. 321-50, 1982.
--- PAGE 384 ---
--- PAGE 385 ---
DERIVATIONAL ANALOGY:
A Theory of Reconstructive Problem Solving and
Expertise Acquisition
Jaime G. Carbonell
Carnegie-Mellon University
Abstract
Derivational analogy, a method ofsolving problems based on the transfer of
pastexperiencetonewproblemsituations, isdiscussedinthecontextofothergeneral
approaches to problem solving. The experience transfer process consists of recreating lines ofreasoning, including decision sequences and accompanyingjustifications, that proved effective in solving particular problems requiring similar initial
analysis. The role ofderivational analogy in case-based reasoning and in automated
expertise acquisition is discussed.
14.1 INTRODUCTION: THE ROLE OF ANALOGY IN PROBLEM
SOLVING
Thetermproblemsolvinginartificial intelligencehasbeenusedtodenotedisparate forms of intelligent action to achieve well-defined goals. Perhaps the most
common usage stems from the work ofNewell and Simon (1972) in which problem
solving consists ofselecting a sequence ofoperators (from a preanalyzed finite set)
thattransforms an initial problem state intoadesiredgoal state. Intelligentbehavior
consistsofafocusedsearchforasuitableoperatorsequenceinvolvinganalysisofthe
--- PAGE 386 ---
372 CHAPTER 14: DERIVATIONALANALOGY
states resulting from the application ofdifferent operators to earlier states.' Many
researchers have adopted this viewpoint (Fikes and Nilsson, 1971; Sacerdoti, 1974;
Nilsson, 1980).
However, a totally different approach has been advocated by McDermott
(1967) and by Wilensky (1978, 1983) that views problem solving as plan instantiation. Foreachproblemposedthereareoneormoreplansthatoutlineasolution, and
problem solving consists of identifying and instantiating these plans. In order to
select, instantiate, or refine plans, additional plans that tell how to instantiate other
plans orhowto solve subproblems are broughttobear in a recursive manner. Traditional notionsofsearcharetotallyabsentfromthisformulation. Somesystems, such
asthecounterplanningmechanism inPOLITICS (Carbonell, 1981b, 1981c), provide
a hybrid approach, instantiatingplans wheneverpossibleand searching toconstruct
potential solutions in the absence ofapplicable plans.
A third approach is to solve a new problem by analogy to a previously solved
similarproblem. This processentails searching for related past problems and transforming their solutions into solutions potentially applicable to the new problem
(Polya, 1945). Suchamethodwasdevelopedandadvocatedbytheauthor(Carbonell,
1982, 1983) primarily as a means ofbringing to bear problem-solving expertise
acquired from past experience. The analogical transformation process itself may
requiresearch, as itisseldom immediatelyclearhowasolutiontoasimilarproblem
can be adapted to a new situation.
A usefulmeansofclassifyingdifferentproblem-solvingmethodsistocompare
them in terms ofthe amount and specificity ofdomain knowledge they require.
• If no structuring domain knowledge is available and there is no useful past
experience to draw upon, weak methods such as heuristic search and meansend analysis are the only tools that can be brought to bear. Even in these
knowledge-poor situations, information about goal states, possible actions,
their known preconditions, and theirexpected outcomes is required.
• Ifspecific domain knowledge in the form ofplans or procedures exists, such
plans may be instantiated directly, recursively solving any subproblems that
arise in the process.
• Ifgeneral plansapply but nospecificonesdo, thegeneral planscanbe used to
reduce the problem (by partitioning the problem or providing islands in the
searchspace). Forinstance, incomputingthepressureataparticularpoint in a
fluid staticsproblem, one may usethegeneral planofapplyingthe principle of
'In inc.ms ends analysis, the current state is compared to the goal state, and one or more operators that
reduce the differencearc selected, whereas in heuristic search, the present state isevaluated in isolation
andcompared toalternatestates resulting fromtheapplicationofdifferent operators (tostatesgenerated
earlier in the search), and the search torasolutioncontinues from the highest-rated slate
--- PAGE 387 ---
CARBONELL 373
equilibriumoffereesatthepointofinterest(thevectorsumoftheforces =
0).
But the application of this plan only reduces the original problem to one of
findingandcombiningtheappropriateforces, withouthintingat howthatmay
be accomplished in a specific problem (Carbonell, Larkin, and Reif, 1983;
Larkin, Reif, and Carbonell, 1985).
• If no specific plans apply but the problem resembles one solved previously,
analogical transformation can be applied to adapt the solution ofthat similar
past problem to the new situation. For instance, in some studies it has proven
easier for students to solve mechanics problems by analogy to simpler, solved
problems than by appealing to first principles or by applying general procedures presented in a physics text (Clements, 1982). As an example ofanalogy
involving composite skills rather than pure cognition, consider a person who
knowshowtodriveacarandisaskedtodriveatruck. Suchapersonmayhave
nogeneralplanorprocedurefordrivingtrucksbutislikelytoperformmostof
the steps correctly by transferring much of his or her automobile-driving
knowledge. Wouldthatwehadrobotsthatweresoself-adaptabletonew, ifrecognizably related, tasks!
Clearly, these problem-solving approaches, illustrated in figure 14-1, are not
mutuallyexclusive; forinstance, a "first-principles" approachcanbeusedtoreduce
aproblemto simplersubproblems, which inturncanbe solvedby analogy torecognizably similarpastproblemsorbyanyoftheothermethods. Infact, ageneralinference engine for problem solving in the natural sciences that combines all four
General Plan
SpecificPlans
OldProblems
Solved
Figure14-1: Problemsolvingmayinvolvethefollowing: (a)instantiatingspecificplans,(b)usinganalogicaltransformationtoaknownsolutionofasimilarproblem, (c)applyinggeneralplanstoreducethe
problem,(d)applyingweakmethodstosearchheuristicallyforapossiblesolution,or(e)usingacombinationoftheseapproaches.
--- PAGE 388 ---
374 CHAPTER 14: DERIVATIONALANALOGY
approachesisbeingdeveloped(Carbonell, Larkin, andReif, 1983; Larkin, Reif, and
Carbonell, 1985).
As discussed earlier, only direct plan instantiation and weak methods have
received substantial attention by AI practitioners. For instance, Laird and Newell 's
recentformulationofauniversal weakmethod (LairdandNewell, 1983)asageneral
problem-solvingengineisdevelopedcompletelywithinthesearchparadigm. Expert
systems, for the most part, combine aspects ofplan instantiation (often broken into
small rule-size chunks ofknowledge) and heuristic search in whatever manner best
exploits the explicit and implicit constraints of the specific domain (Feigenbaum,
Buchanan, and Lederberg, 1971; Shortliffe, 1976; Duda et al., 1979; McDermott,
1980, 1982). The author is more concerned with the other two approaches, as they
could conceivably provide powerful reasoning mechanisms not heretofore analyzed
inthecontextofautomatingproblem-solvingprocessesandofallowing theproblem
solvertolearnfromexperience. Therestofthischapterfocusesonanewformulation
oftheanalogicalproblem-solvingapproachanditsroleinautomatingtheknowledge
acquisition process.
14.2 ANALOGY AND EXPERIENTIAL REASONING
The term analogy often conjures up recollections of artificially contrived
problems invariouspsychometricexams, suchas, "X isto Yas Z isto?" Thisaspect
of analogy is far too narrow and independent of context to be useful in general
problem-solvingdomains. Instead,thefollowingoperationaldefinitionofanalogical
problem solving is proposed consistent with past AI research efforts (Kling, 1971;
Winston, 1978, 1979; Gentner, 1980; Carbonell, 1981a, 1983):
Definition:Analogicalproblemsolvingconsistsoftransferringknowledgefrom
pastproblem-solving episodes to newproblems that share significant aspects
withcorrespondingpastexperienceandusingthetransferredknowledgetoconstructsolutionstothenewproblems.
Inordertomakethisdefinitionoperational, theproblem-solving methodmust
specify the following:
• What it means for problems to "share significant aspects"
• What knowledge is transferred from past experience to the new situation
• Precisely how the knowledge transfer process occurs
• Howanalogically relatedexperiencesareselectedfromapotentiallyvast longterm memory ofpast problem-solving episodes.
There are two distinct approaches to analogical problem solving. The first
approach, called transformational analogy, has been successfully implemented in
ARIES (Analogical Reasoning and Inductive Experimentation System) (Carbonell.
1983). The second approach, called derivationalanalogy, is a reconstructive rather
--- PAGE 389 ---
CARBONELL 375
than atransformational method, and it isthe topic ofthis chapter. Both methods are
analyzed with respect to the four criteria listed above.
14.2.1 Analogical Transformation of Past Solutions
Ifaparticularsolution has been foundto workon aproblem similartothe one
athand, perhapsitcanbeused, withminormodification, forthepresentproblem. By
solution is meant only a sequence of actions that if applied to the initial state of a
problembringsaboutitsgoal state. Simplethoughthisprocessmay appear, aneffective computer implementation requires that many difficult issues be resolved,
namely:
1. Descriptions ofpast problems and oftheir solutions mustbe remembered and
indexed for later retrieval.
2. The new problem must be matched against a large number ofpotentially relevant past problems to find closely related ones, if any. An operational similarity metric is required as a basis for selecting the most suitable past
experiences.
3. The solution to a selected old problem must be transformed to satisfy the
requirements ofthe new problem statement.
In order to achieve these objectives, the initial analogical problem solver
(Carbonell. 1983) requiredapartial matcherwithabuilt-in similaritycriterion, aset
ofpossible transformations to mapthe solution ofone problem into the solution to a
closely related problem, and a memory-indexing mechanism based on a MOPs-like
memoryencodingofeventsandactions(Schank, 1982). The solutiontransformation
process was implemented as a set ofatomic transform operators and a means-ends
problem solver that searched for sequences of atomic transformations that, when
appliedtotheretrievedsolution, yieldedasolutiontothenewproblem. Theresultant
system, called ARIES, turned out to be far more complex than was originally envisioned. Partial pattern matching ofproblem descriptions and searching in the space
ofsolution transformations are difficult tasks in themselves. Figure 14-2 illustrates
the transformational analogy process.
Intermsofthe fourcriteria, thesolutiontransformationprocessmaybeclassified as follows:
1. Two problems share significant aspects ifthey match within a certain preset
thresholdinthe initialpartial matchingprocess, accordingtothebuilt-in similarity metric.
2. Theknowledgetransferredtothenew situationisthe sequence ofactions from
theretrievedsolution, whetherornotthatsequenceislatermodifiedintheanalogical mapping process.
3. Theknowledgetransferprocessisaccomplishedbycopyingtheretrievedsolution andperturbing it incrementally according tothe primitive transformation
--- PAGE 390 ---
376 CHAPTER 14: DERIVATIONALANALOGY
Partial
New Mapping Previously
Q Solved
Problem
Problem
Derivation
Solution Solution
toNew toOld
Problem Transform Problem
Process
Figure14-2: Thetransformationalanalogyprocess. Solutionstocloselyrelatedproblemsareretrieved
andmodifiedtosatisfytherequirementsofthenewproblem.
stepsintheheuristicallyguidedmanneruntilitsatisfiestherequirementsofthe
new problem. (See Carbonell, 1983, for details.)
4. Theselectionofrelevantpastproblemsisconstrainedbythememory-indexing
scheme and the partial pattern matcher.
Sinceasignificantfractionofproblemsencounteredinmundanesituationsand
in areas requiring substantial domain expertise (but not in abstract mathematical
puzzles)bearscloseresemblancetopastsolvedproblems, theARIESmethodproved
effective when tested in various domains, including algebra problems and routeplanning tasks. An experiential learning component was added to ARIES that constructed simple plans (generalized sequences ofactions) for recurring classes of
problems, henceallowingthesystemtosolvenewproblemsinthisclassbythedirect
plan instantiation approach. However, no sooner was the solution transformation
method implementedandanalyzedthan someofits shortcomingsbecame strikingly
apparent. In response to these deficiencies, more sophisticated methods ofdrawing
analogies were analyzed, as discussed in the following section.
14.3 THE DERIVATIONAL ANALOGY METHOD
In formulating plans and solving problems, aconsiderable amount ofintermediate information is produced in addition to the resultant plan or specific solution.
Forinstance, formulationofsubgoal structures, generationand subsequent rejection
ofalternatives, and access tovarious knowledge structures all typicalh take place in
the problem-solving process. But the solutiontransformation method outlined above
LglM CS an such information, focusing only upon the resultant sequence of actions
--- PAGE 391 ---
CARBONELL 377
and disregarding the reasons for selecting those actions. Why should one take such
extra information into account? It would certainly complicate the analogical
problem-solving process; nevertheless, what benefits would accrue from such an
endeavor?Perhapsthebestwaytoanswerthisquestionisbyanalysisoftheshortcomings ofthe simple solution transformation process and ofways that such problems
maybealleviatedorcircumventedbypreservingmoreinformationfromwhichqualitatively differentanalogies maybedrawn. Thegeneral ideaofderivational analogy
is depicted in figure 14-3 and examined in greaterdetail below.
14.3.1 The Need for Preserving Derivation Histories
Consider, forinstance, thedomainofconstructingcomputerprogramstomeet
asetofpredefinedspecifications. Intheautomaticprogrammingliterature, perhaps
the most widely used technique is one of progressive refinement (Barstow, 1977;
Balzer, 1975; Kant, 1981). In brief, progressive refinement is a multistage process
that starts from abstract specifications stated in a high-level language (typically
Englishorsomevariantoffirst-orderlogic)andproducesprogressively moreoperational or algorithmic descriptions ofthe specification committing to control decisions, datastructures, andeventually specific statements inthetargetcomputerlanguage. However, humans (at least this writer) seldom follow such a long and
painstakingprocess, unlessperhapsthe specificationscall foratruly novelprogram
Partial Mappings
-e * Previous
Problem^
Previous
Problem,
Derivati
Solution Solution
toNew
Problem Problem^
Solution
Problem
Figure 14-3: The derivational analogy process. The derivational traces ofsimilar past problems are
replayedandwherenecessary modifiedtoconstructasolutiontoasimilarnewproblem.
--- PAGE 392 ---
378 CHAPTER 14: DERIVATIONALANALOGY
unlikeanythingintheirpastexperience. Instead, acommonpracticeistorecallsimilar past programs and reconstruct the new programming problem along the same
lines. Forinstance,oneshouldbeabletoprogramaquicksortalgorithminLISPquite
easily ifone has recently implemented quicksort in Pascal. Similarly, writing LISP
programs that perform tasks centered around depth-first tree traversal (such as
testingequality ofS-expressions orfindingthe nodewith maximal value) are rather
trivialforLISPprogrammersbutsurprisinglydifficultforthosewholacktheappropriate experience.
The solution transformation process proves singularly inappropriate as a
means ofexploiting past experiences in such problems. A Pascal implementation of
quicksort may look very different from a good LISP implementation. In fact,
attempting to transfer corresponding steps from the Pascal program into LISP is
clearly notagoodwaytoproduceany reasonable LISPprogram, letaloneanelegant
or efficient one. Although the two problem statements may have been similar, and
although the problem-solving processes may preserve much of the inherent similarity, the resultant solutions (i.e., the Pascal and LISP programs) may bear little if
any direct similarities.
Theusefulsimilaritieslieinthealgorithmsimplementedandinthesetofdecisions and internal reasoning steps required to produce the two programs by successivelyrefiningthegeneralspecificationoftheproblem. Therefore, theanalogymust
takeplacestartingatearlierstagesoftheoriginalPascalimplementation, and it must
be guided by a reconsideration ofthe key decisions in light ofthe new situation. In
particular, thederivationoftheLISPquicksortprogramstartsfromthesamespecifications, retaining the same divide-and-conquer strategy, but it may diverge in the
selectionofdatastructures(e.g., listsversusarrays)orinthemethodofchoosingthe
comparison element, depending on the tools available in each language and their
expected efficiency. However, future decisions (e.g., whether to recurse or iterate,
what mnemonics to use as variable names, and so on) that do not depend on earlier
divergent decisions can still be transferred to the new domain rather than recomputed. Thus, the derivational analogy method walks through the reasoning steps in
theconstructionofthe past solutionand considers whetherthey are still appropriate
inthenewsituationorwhethertheyshouldbereconsideredinlightofsignificantdifferences between the two situations.
The difference between the solution transformation approach and the derivationalanalogyapproachjustoutlinedcanbestatedintermsoftheoperational knowledgethatcan be brought tobear. The formercorresponds toa person who has never
before programmed quicksort and is given the Pascal code as an aid in constructing
theLISPimplementation; whereasthelatterisakintoapersonwhohasprogrammed
the Pascal version and therefore has a better understanding of the issues involved
before undertaking the LISP implementation. Swartout and Balzer (1982) and Reif
and Scherlis (1982) have argued independently in favor of working with program
derivations as the basic entities in tasks relating to automatic programming. The
--- PAGE 393 ---
CARBONELL 379
advantages ofthe derivational analogy approach are quite evident in programming
becauseofthefrequentinappropriatenessofdirectsolutiontransformation; buteven
indomains in which the latterisuseful, onecan envisionproblemsthatdemonstrate
the need forpreserving or reconstructing past reasoning processes.
14.3.2 The Process of Drawing Analogies by Derivational
Transformation
Let us examine in greater detail the process ofdrawing analogies from past
reasoningprocesses. Theessential insightisthat usefulexperience isencoded inthe
reasoningprocessusedtoderivesolutionstosimilarproblems, ratherthanjustinthe
resultant solution. Additionally, a method ofbringing that experience to bear in the
problem-solvingprocessisrequiredinordertomakethisformofanalogyacomputationally tractable approach. Here we outline such a method:
1. When solving a problem by any means, store each step taken in the solution
process, as illustrated in figure 14-4, including the following:
• The subgoal structure ofthe problem
• Each decision made (whether a decision to take action, to explore new
possibilities, orto abandon present plans), including the following:
o Alternatives considered and rejected
o The reasons forthe decisions taken (with dependency links to the
problem description or information derived therefrom)
o Thestartofafalsepathtaken(withthereasonwhythisappearedto
beapromisingalternativeandthereasonwhy itprovedotherwise,
againwithdependency linkstotheproblemdescription. Notethat
thebody ofthe falsepathandotherresultant information neednot
be preserved)
o Dependencies oflaterdecisions on earlier ones in the derivation.
• Pointerstotheknowledgethatwasaccessedandthatprovedusefulinthe
eventual construction ofthe solution
• The resultant solution itself
o Intheeventthattheproblemsolverprovedincapableofsolvingthe
problem, store the closest approach to a solution, along with the
reasonswhynofurtherprogresscouldbemade(e.g., aconjunctive
subgoal that could not be satisfied).
o Intheeventthatthe solutiondepends, perhaps indirectly, on volatileassumptions not stated in theproblemdescription (such as the
cooperation ofanother agent or time-dependent states), store the
appropriate dependencies.
--- PAGE 394 ---
380 CHAPTER 14: DERIVATIONALANALOGY
Problem w = Decisions
I \ ! 4 I V f ^Y 1 (Elaborate) / \ = Justifications
Failure-Cause
Propagation
(Decompose)
Cause (InstantiatePlan)
(Failure)
(SelectSubgoal)
(ApplyOperator)
(Instantiate . . .)
Figure14-4: Aderivationaltrace.Eachreasoningstepisjustifiedintermsofpreviousreasoningstepsor
externalknowledge.Whenasolutionattemptfails,thecauseoffailureispropagatedbacktothebranching
pointfromthesuccessful pathandretained.
When a new problem is encountered that does not lend itself to direct plan
instantiationorotherdirectrecognitionofthesolutionpattern, starttoanalyze
the problem by applying general plans or weak methods, whichever is appropriate to the situation.
Ifafter the analysis ofthe problem is commenced, the reasoning process (the
initial decisions madeandthe informationtaken intoaccount) parallelsthatof
pastproblemsituations, retrievethe full reasoningtracesandproceed with the
derivational transformationprocess. Ifnot, considerthepossibilityofsolution
transformation analogy or, failing that, proceed with the present line ofnonanalogical reasoning.
• Two problems are considered similar iftheir analysis results in equivalent reasoning processes, at least in its initial stages. This replaces the
more arbitrary context-free similarity metric required for partial
matching among problem descriptions in drawing analogies by direct
solutiontransformation. Hence, past reasoning traces (henceforth called
--- PAGE 395 ---
CARBONELL 381
derivations)areretrieved iftheirinitial segment matchesthatofthefirst
stages ofthe analysis ofthe present problem.
• Theretrievedreasoningprocessesarethenusedmuchasindividual relevant cases in medicine are used to generate expectations and drive the
diagnostic analysis. Reasoning from individual cases has been recognized as an important component ofexpertise (Schank, 1983), but little
has been said ofthe necessary information that each case must contain,
letaloneprovidingasimplemethodofretrievingtheappropriatecasesin
a mannerthatdoes not rely on arbitrary similarity metrics. The stand is
takenherethatcasesmustcontainthe reasoningprocessusedtoyieldan
answer,togetherwithdependenciestotheparticularcircumstancesofthe
problem, pointers to data that proved useful, a list of alternative reasoning paths not taken, and failed attempts (coupled with both reasons
fortheirfailureandreasonsfortheirhavingbeentried). Case-basedreasoning is nothing more than derivational analogy applied to domains of
extensive expertise.
• Itisimportanttoknowthatalthough onemay viewderivational analogy
as an interim step in reasoning fromparticularpastexperiences as more
generalplansareacquired, itisamechanismthatremainsforeveruseful,
sinceknowledge isalwaysincomplete, andexceptionstothebestformulatedgeneralplansrequirerepresentationanduseofindividualreasoning
episodes.
4. Applyaretrievedderivationtothecurrentsituationasfollows: Foreachstepin
the derivation, starting immediately after the matched initial segment, check
whether the reasons for performing that step are still valid by tracing dependenciesinthe retrievedderivationtorelevantparts oftheoldproblemdescription orto volatile external assumptions made in the initial problem solving.
• Ifparts ofthe problem statement or external assumptions on which the
retrieved situation rests are also true in the present problem situation,
proceed to check the next step in the retrieved derivation.
• Ifthereisaviolatedassumptionorproblemstatement, checkwhetherthe
decisionmadewouldstillbejustifiedbyadifferentderivationpath from
thenewassumptionsorstatements. Ifso, storethenewdependenciesand
proceed to the next step in the retrieved derivation. The idea oftracing
causal dependencies and verifying past inference paths borrows heavily
from TMS (Doyle, 1979) and some ofthe nonmonotonic logic literature
(McDermottand Doyle, 1980). However, the role played bydatadependencies in derivational analogy is somewhat different and more constrained than it is in maintain! g global consistency in deductive data
bases.
--- PAGE 396 ---
382 CHAPTER 14: DERIVATIONALANALOGY
• Iftheolddecisioncannotbejustifiedbythenewproblem situation, proceed as follows:
o Evaluate the alternatives not chosen at thatjuncture and select an
appropriate one in the usual problem-solving manner, storing it
along with itsjustifications, or
o Initiatethesubgoalofestablishingtherightsupportsinorderforthe
old decision to apply in the new problem2 (clearly, any problemsolving method can be brought to bear in achieving the new subgoal), or
o Abandonthisderivationalanalogyinfavorofanother, moreappropriateproblem-solvingexperiencefromwhichtodrawtheanalogy
or in favor ofother means ofproblem solving.
• If one or more failure paths are associated with the current decision,
check the cause of failure and the reasons these alternatives appeared
viable in the context ofthe original problem (by tracing dependency
links when required). In the case that their reasons for failure no longer
apply but the initial reasons for selecting these alternatives are still
present, consider reconstructing this alternate solution path in favor of
continuing to apply and modify the present derivation (especially if
quality ofsolution is more important than problem-solving effort).
• Intheeventthatadifferentdecisionistakenatsomepointintherederivation, do not abandon the old derivation, since future decisions may be
independentofsomepastdecisionsormay stillbevalid(viadifferentjustifications) in spite of the somewhat different circumstances. This
requires that dependency links be kept between decisions at different
stages in the derivation.
• In the event that a preponderance ofthe old decisions are invalidated in
the new problem situation, abandon the derivational analogy. Exactly
whattheperseverancethresholdshouldbeisatopicforempirical investigation, as it depends on whether there are other tractable means of
solving thisproblem and ontheoverheadcostofreevaluating individual
pastdecisionsthatare nolongersupported andthat may ormay not have
independentjustification.
:Thisapproach worksonly ifthe missingorviolated premise relatestothat part oftheglobal state under
controloftheproblemsolver, suchasacquiringamissingtoolorresource, ratherthanthatpartunderthe
controlofanuncooperativeexternalagentorarecalcitranten\ironment. ThediscussionofStrategy-based
counterplanning gives a more complete account ofsubgoaling to rectify unfulfilled expectations (Carbonell. !9Klb. 1981c).
--- PAGE 397 ---
CARBONELL 383
5. Afteranentirederivationhasbeenfoundtoapplytothe newproblem, storeits
divergence from the parent derivation as another potentially useful source of
analogiesandasan instance fromwhich moregeneral planscanbe formulated
ifalarge numberofproblems share acommon solutionprocedure (Carbonell,
1983).
14.3.3 Efficiency Concerns
An importantaspectofthederivational analogyapproach istheabilitytostore
and trace dependency links. It should be noted that some ofthe inherent inefficiencies in maintaining global consistency in a large deductive database do not apply, as
thedependencylinksareinternaltoeachderivationwithexternalpointersonlytothe
problemdescriptionandtoany volatileassumptions necessitated inconstructingthe
resultant solution. Hence, the size ofeach dependency network is quite small compared to a dependency network spanning all memory. Dependencies are also stored
amongdecisionstakenatdifferentstages inthetemporal sequenceofthederivation,
thus providing the derivational analogy process access to causal relations computed
atthe time the initial problem was solved.
The analogical rederivation process is not inherently space inefficient,
although it may so appear at first glance. The sequence ofdecisions in the solution
path of a problem is stored, together with necessary dependencies, the problem
description, theresultantsolution, andalternativereasoningpathsnotchosen. Failed
pathsare notstored; onlytheinitialdecisionthatwastakentoembarkuponthatpath
and the eventual reason for failure (with its causal dependencies) are remembered.
Hence, the sizeofthe memory forderivational traces isproportional to the depth of
the search tree ratherthantothe numberofnodes visited. Problems that share large
portions oftheir derivational structure can be so represented in memory, saving
spaceandallowing similarity-based indexing. Moreover, whenageneralizedplan is
formulated for recurring problems that share a common derivational structure, the
individualderivationsthataretotally subsumedbythemoregeneral structurecanbe
permanently masked or deleted. Those derivations that represent exceptions to the
general rule, however, are precisely the instances that should be saved and indexed
accordingly for future problem solving (Hayes-Roth, 1983).
14.3.4 Summarizing the Derivational Process
Derivational analogy bears closer resemblance to Schank's reconstructive
memory (1980, 1982) and Minsky's K-lines (1980) than to traditional notions of
analogy. Althoughderivationalanalogyislessambitiousinscopethaneitherofthese
theories, it is a more precisely defined inference process that can lead to an operational method ofreasoning from particularexperiential instances. The key notion is
to reconstruct the relevant aspects of past problem-solving situations and thereby
transfer knowledge to the new scenario, where that knowledge consists ofdecision
--- PAGE 398 ---
384 CHAPTER 14: DERIVATIONALANALOGY
sequences and theirjustifications rather than individual declarative assertions. In
summary,considerhowtheprocessofderivationalanalogycanbedescribedinterms
ofthe fourcriteria for analogical reasoning presented in the previous section:
1. Twoproblems share significantaspects iftheir initial analysis yields the same
reasoning steps, that is, ifthe initial segments oftheir respective derivations
start by considering the same issues and making the same decisions.
2. Theearlierderivationmaybetransferredtothenewsituation, inessencerecreating the significant aspects of the reasoning process that solved the past
problem.
3. Knowledgetransfer is accomplishedby reconsidering olddecisions in lightof
thenewproblem situation, preservingthosethatapply, andreplacingormodifying those whose supports are no longer valid in the new situation.
4. Problemsandtheirderivationsarestoredinalargeepisodic memoryalongthe
lineofSchank'sMOPs (1982), andretrievaloccursbyreplicationofinitialsegments ofdecision sequences recalling the past reasoning process.
14.4 INCREMENTAL EXPERTISE ACQUISITION
Derivationalanalogy isafertilecomputationalparadigmthatsupportsvarious
knowledge acquisition and skill refinement strategies. Thus far the focus here has
beenonthebasicproblem-solvingaspects, butamajormotivationbehindthe reconstructive derivational strategy is the natural manner in which it can be extended to
includeincrementalacquisitionofdomainexpertise. First, case-basedreasoningasa
major component ofhuman expertise will be briefly considered. Then, some concrete methods for acquiring and refining expertise from experience will be examined, based upon the derivational analogy model.
14.4.1 Case-Based Reasoning as a Model of Human Expertise
The vast majority ofpresent-day expert systems encode their knowledge as a
large, amorphous set ofdomain-specific rules (Feigenbaum, Buchanan, and Lederberg, 1971; McDermott, 1980, 1982; Shortliffe, 1976; Waterman, Hayes-Roth, and
Lenat, 1983). The "knowledge engineering" task is defined as one of extracting
fromthehumanexpertthesetofrulesthatcomprisehisorherexpertise inparticular,
well-defined domains. The task is by no means easy-quite the contrary. It can take
yearsoflaboriouseffortsbyteamsofdomainexpertsand AI researchers inan iterative processofformulating, evaluating, reformulating, discarding, and refining a set
ofrules todevelopthe knowledge base ofa particularexpert system. Observing this
phenomenon, Edward Feigenbaum uttered his now-famous proclamation. "In the
knowledge lies the power." How right he was! Fortunately, however, the tacit
assumption that domain knowledge must necessarily be represented as large sets o(
--- PAGE 399 ---
CARBONELL 385
context-independent rules isprovingtobeonly anearly engineeringdecision, anda
very limiting one at that. The knowledge must be captured, but the question ofthe
best means ofacquiring and representing it in a computationally effective manner
remains.
Whatthenwouldbeanalternativemeansofrepresentingandacquiringdomain
knowledge? In order to address this question, the author set out to build an expert
system and gain first-hand experience, keeping in perspective all the different
problem-solving methods and machinelearningparadigms. In lessthanayear, with
thehelpofoneprogrammerandtwodomainexperts, SMOKEY (Carbonell, 1985), a
prototype fire diagnosis expert system, was produced. In essence, SMOKEY polls
multipleremotesensors(heatdetectors, smokedetectors, airpressuredetectors, andso
on), and calculates the location, expected spread, and critical nature of a fire on a
buildingoraship. Fromthisassessmentit recommendsactions, suchassignalingsafe
exit routes free ofsmoke, closing down air circulation ducts before they spread toxic
smoke to unaffected areas, selecting equipment forthe fire-fighting team appropriate
tothenatureofthefire,andsoon. Severallessonswerelearnedfromthisendeavor,and
here the focus will be on the central one: the utility ofcase-based reasoning.
Whennavalexpertswereinterviewedabouton-boardfirediagnosissituations,
it was found that for sizable fires, they are swamped with too much information
comingfromallthepotentially relevantsensors. Therefore, videotapesimulationsof
several fires wereplayedat much-reduced speeds. The results were amazing: previously sloppy decisions ignoring crucial information vanished, and elaborate
problem-solving protocols were recorded, including fairly complete justifications
foreach actionordecisiontaken. Unfortunately, real fires cannotbe played in slow
motiontoallowforhumanreactiontimeandmemorylimitations. Thus, theneedfor
a SMOKEY-like system on a fast processor was established. Now the question
remained ofhow the excellent (slow-motion) problem-solving traces could be convertedintotheknowledgebaseofanexpertsystem. Thekey insightwasthatperhaps
they need not be converted-only encoded appropriately and fed to a derivational
analogyproblemsolverandlearningmodule. SMOKEYwasbuiltconcurrentlywith
the development of the derivational analogy method, so the expertise acquisition
steps discussed below were carried out largely by hand, rather than in a completely
automated fashion.
Human experts are incredibly poor at producing general deductive rules that
account fortheirbehavior. When forced to do so by insistent knowledge engineers,
they try hard and produce faulty rules. When they are later faced with a problem in
whichtherulefails,thetypicalresponseis: "Well, Ididn'tthinkofthatsituation,but
perhaps I can fix the rule ... oradd a new one . . . ." This adhoc iterative process,
slowand frustratingly inefficientas it maybe, usuallyconverges uponanacceptable
knowledge base. However, a much more efficient and humane approach is to letthe
experts do whatthey dobest: solve problems in theirdomain ofexpertise. The only
added burden is a reporting requirement. Each problem-solving step, including
--- PAGE 400 ---
386 CHAPTER 14: DERIVATIONALANALOGY
references to static domain knowledge or to heuristics of the domain, must be
reported explicitly, along with the reason why such knowledge was used. This process provides external derivational traces that a derivational analogy inference
enginecanusetosolvesimilarfutureproblems inaneffectivemanner. Althoughthe
derivational method was originally conceived as a means to reason and learn from
one's ownpastexperience, a more knowledgeable external source, such as ahuman
expert or a worked-out problem example in a textbook, can prove even more
effective.
Case-basedreasoningisparticularyprevalentinlaw-atleastintheBritishand
American systems ofjurisprudence-and in medical diagnosis and treatment. The
idea of case-based reasoning in expert systems is not new. Schank (1983), for
instance, advocates this method as superior and closer to human reasoning than
present expert systems. Doyle (1984) proposes the notion ofemulating the human
master-apprentice process as a means whereby the latter (human or computer) can
acquire expertise by replicating the reasoning processes ofthe former. Here, a concretecomputationalmechanism-thederivationalanalogyprocess-isproposedasa
meansofprovidingexpertsystemswiththeabilitytoreasonfromcases, whetherthe
casesbepastexperienceorexternallyacquiredknowledge. However, humanexperts
can solveproblemsprogressively morequicklyandeffectively with repeatedexperience. Whereas case-based reasoning may reflect accurately a crucial intermediate
stageinthelearningprocessandmayaccountforproblem-solvingbehaviorininfrequently recurring situations, some knowledge is gradually compiled into more generalprocessesabstractedfromtheconcretecases. Thatistosay, forthemostroutine,
recurring problems, the derivational analogy process should produce general plans
thatcanbe instantiated directly. The following section explores learning techniques
in derivational analogy.
14.4.2 Automatic Acquisition of Plans and Strategies
Thestandardbehavioraldefinitionforlearningcanbeparaphrasedasfollows:
Definition:Asystem (biologicalormechanical) issaidtolearn ifitcan modify
itsbehaviorafterasetofexperiencessuchthatitcanperformataskmoreaccuratelyormoreefficientlythan beforeorperformanewtaskbeyonditsprevious
capabilities.
Whatcanbelearnedinthederivationalanalogyprocess,accordingtothisdefinition?
Learning can occur at many levels and in many forms.
14.4.2.1 Enrichment of Case-Based Memory
Asasystemsolvesproblemsorispresentedwith fullyannotatedderivationsof
solutions, its repertoire ofcases increases. Thus it will be able to derive analogical
solutions from these new experiences. This incremental, monotonic increase in its
--- PAGE 401 ---
CARBONELL 387
experientialknowledgebaseprovidesapowerfulargumentinfavorofamethodsuchas
derivational analogy, whichcanutilizetheexperiencedirectly to solve newproblems.
14.4.2.2 Generalized Plans
Ifonlytheresultantsolutionstoalargesetofanalogically relatedproblemsare
used (ratherthantheentirederivations), generalizedoperatorsequenceplanscanbe
abstracted. This process requires that solutions derived from a common analogical
parentformasetofpositiveexemplars, andunrelatedorfailedsolutionsformasetof
negative exemplars. These sets are given to a general inductive engine (Michalski,
1983) or to an incremental one such as Mitchell's version space method (Mitchell,
1978; Mitchell, Utgoff, and Banerji, 1983), which abstracts ageneralizedplan from
the recurring common aspects ofthese solutions. Later, the generalized plan can be
instantiated directly-or refined further if more instance solutions are derived.
Figure 14-5 summarizesthisprocess. (Foralongerdiscussion, seeCarbonell, 1983.)
14.4.2.3 Strategy Acquisition
The same method for inducing generalized plans from positive and negative
exemplars canbe applied to different parts ofthe full derivational trace.
Considerations ofalternate decision points in derivationally related solutions
can lead to the compilation ofdomain-specific heuristics for making future choices
Cluster of
I I
Solutionswith
aCommon
Derivational
Ancestor
5(Tj,Tj) < < 5(Tj, T k)
VT„ TjT ks.t. T
T k
Figure 14-5: Generalizing plans fromanalogically related solutions. Solutions derived fromcommon
transformational ancestors form aclusterofpositiveexemplars. Failedattempts and members ofother
clustersprovidethenegativeexemplarstoan inductionengine.
--- PAGE 402 ---
388 CHAPTER 14: DERIVATIONALANALOGY
of the same nature. If a particular decision was part of a successful derivation in
severalrelatedsolutionsbutledtoafalsepathunderotherproblemsolutions, therequisite grist for the induction engine again exists: a set ofpositive exemplars in the
justifications ofthe successful decision anda set ofnear-miss negative exemplars in
the cases where the same decision proved ineffective. In fact, the cause of failure
(propagated back to the causally related decision point and retained in the derivational trace, as discussed earlier) provides a set of necessary-but perhaps not
sufficient-conditionstodiscriminatebetweenthepositiveandnegativeinstancesof
the decision.
Consider, for instance, the selection of a means oftransportation in various
problem-solving situations that involve travel. If an automobile was successfully
selected threetimes totravel betweencities inthecontinental United States but was
erroneously suggested as a means of traveling between Boston and London, the
strategy for selecting a means oftransportation can be refined. The cause offailure
(no land route between the source and destination) serves to add a necessary conditiontothestrategy, andthefactthatautomobiletravelprovedsuccessful independent
of the exact compass orientation or identity of the cities within the United States
servestoassertthe independenceofthe strategy from suchconsiderations. In fact, a
trial implementation in the route-finding domain has yielded planning strategies
increasingly more appropriate to the taskdomain.
Applying the same technique to problem decomposition tasks, plan selection
tasks(whenmultiplegeneralizedplansexistforagivensubproblem), andmethodsof
avoiding the causes offailure under similarcircumstances can alsoyield automated
refinement ofthe system's behavior. In all cases, the internal and externaljustificationsprovidethemeansforthesystemtofocusonthefunctionallyrelevantaspectsof
thephase inthederivationfromwhich it isattemptingtolearn. However, incontrast
to the strategy selection task above, there is no empirical validation ofthe utility or
feasibility of attempting to produce better problem decomposition criteria, plan
selectionmethods, orgeneralizedavoidanceofrecurringpitfalls. Thisiscurrentlyan
active area ofexploration.
14.4.2.4 Fractioning Derivations into Rules
A process akin to "decomposition" is the formulation ofgenerally applicable
rules from more problem-specific derivational sequences. Contrary to Anderson
(1983) and others who view knowledge compilation as perhaps the most significant
learning strategy, this author considers the decompilation process to be at least as
important. Recall that incase-based reasoningone isgivencompiledbut fully annotated audit trails ofthe reasoning process-the derivational traces. The fractioning
taskisoneofaxiomatizingthelong, problem-specifictracesintoindividual rulesthat
areapplicabletoamuchwiderrangeofsituations, althougheachrulesolvesonlypart
ofthe new problem. The difficult aspect ofthe task is to bundle the derivationallv
--- PAGE 403 ---
, )
CARBONELL 389
related steps into useful rules, insuring that the necessary (and only the necessary)
preconditions areassociated witheach rule. But its utility lies in itsability topermit
the learning ofmore generally applicable knowledge from specific experiences.
Theknowledgeengineersmayyethavetheirprecious rule sets, butthey willberule
sets generated automatically after extended experience with derivational traces
ratherthanrulesproducedandgraduallyrefinedbyhandatthecostofmuchtimeand
frustration.
Let us see how rules would be fractioned offfrom longer derivational traces.
The process described below has been tested only in the route-finding domain thus
far, but there it has proven useful.
Findrelevantcandidates. Thefirststepintheformulationofrulesfromderivationaltracesistosearchforcandidatesubsequencesofactionsthatrecurindifferent, possibly unrelated, derivational traces inthedomain. For instance, the
sequence
LOCATE(bridge),
PLAN-ROUTE(here,bridge)
PLAN-ROUTE(bridge,destination)
occurred with high frequency and it was proposed as a rule kernel.
2. Tracejustifications. Why mustoneplanarouteorlocateabridge? Thejustificationforthe formercomes fromthe supergoalgoal PLAN-ROUTE (here, destination) , and the justification for the latter comes from the fact that the
presence ofa river between "here" and "destination" violates a precondition
for land travel.
3. Formulaterule. Computablepredicatesmustfirstbe foundtoestablishthejustifications. Thesebecometheconditionsideofthe rule. Thenthejustifiedsubsequences of actions, parameterized to the most general justifiable class of
actionsorobjects, becomestheaction sideofthe rule. Inthe presentexample,
the resultant rule is:
IF GOAL(x) is L0C(x,time-2) = destination
& LOC(x,time-l) = here
& BETWEEN (here,destination) = river
& TRANSPORTATION(x) = land-vehicle
THEN FIND(bridge,river)
PLAN-ROUTE here bridge
( ,
PLAN-ROUTE(bridge,destination)
Thus itcanbeseenthatfrommultipleplanningepisodesthefollowing rulecan
be induced: Ifone must cross a river, then one should first worry about finding a
bridge and plan the route according to this constraint. The rule-fractioning process
--- PAGE 404 ---
390 CHAPTER 14: DERIVATIONALANALOGY
truly requiresallthreephases: findingrelevantsequences, determiningthejustifications for these sequences, and actually formulating the rule from this information.
Withoutaderivationaltrace it would notbepossibletofraction rules reliably, because
thejustifications provided inthetrace are needed forsearching outthenecessaryand
usefulconditionsfortheleft-handsideoftherule. Otherwise, onewouldhavetopostulatethatthe recurrentsubsequencewaseithertotally independentofcontext(aterrible assumption-the system would be searching for bridges when there were no
riverstocross)orcompletelydependentoncontext, requiringthattheentiretraceup
to that point be included in the condition side, rather thanjust the causally relevant
conditions indicated by thejustifications.
CONCLUDING REMARK
14.5
Derivationalanalogyisapowerfulreasoningmechanism,onethatprovidesthe
informationnecessaryforlearningtooccurinmanydifferentforms, fromaccumulationofcasestoformulationofdomain-orientedstrategiesandsetsofdeductiverules.
Ithasbeen remarkedthatheuristicsare "compiledhindsight" andas suchcanprove
useful inguidingfuturebehavior. Buthowcanonetakeadvantageofhindsightunless
one recalls pastexperiences, including aspects ofone's state ofmind that are necessary inordertoreconstructpastproblem-solvingbehaviors innew situations? There
mustbe a retrospectiveprocesscapable ofexploiting pastexperience and a gradual,
incremental learning process that abstracts more generally applicable chunks of
knowledge from that experience. The derivational analogy process is one concrete
method for realizing the former, and the strategy and rule acquisition process are a
means of implementing the latter. Together they form a computational theory of
incrementalexpertiseacquisition, atheorythatis still intheprocessofbeing implemented, tested, refined, and reformulated.
ACKNOWLEDGMENTS
This research was supported in part by the Office of Naval Research (ONR)
undergrants Nos. N00014-79-C-0661 andN00014-82-C-50767andinpartbyagrant
fromIBM Theauthorthanksthefollowingcolleaguesfortheirenlighteningdiscussionsthathelpedtoclarifytheideaspresentedinthischapter: JonDoyle,Jill Larkin,
Steve Minton, and Allen Newell.
References
Anderson, J. A., "AcquisitionofProofSkills inGeometry," inMachineLearning: An ArtificialIntelligenceApproach, R. S. Miehalski, J. G. Carbonell, andT. M. Mitchell (Eds.). Tioga. Palo Alto.
Calif.. 1983.
Balzer, R.. "Imprecise ProgramSpecification," Technical Report RR-75-3o. USC Information Sciences
Institute. 1975.
--- PAGE 405 ---
CARBONELL 391
Barstow, D. R., "AutomaticConstructionofAlgorithmsandDataStructuresUsingaKnowledgeBaseof
ProgrammingRules," Ph.D. diss., StanfordUniversity, 1977.
Carbonell,J.G.
"Counterplanning:AStrategy-BasedModelofAdversaryPlanninginReal-WorldSituations."ArtificialIntelligence, Vol. 16, pp. 295-329, 1981a.
, "A Computational Model ofProblem Solving by Analogy," ProceedingsoftheSeventh IJCAI,
Vancouver, B.C., pp. 147-52, 1981b.
SubjectiveUnderstanding:ComputerModelsofBeliefSystems, UMIResearchPress,AnnArbor.
Mich., 1981c.
. "Experiential Learning in Analogical Problem Solving," ProceedingsofAAAI-82, Pittsburgh,
Pa., pp. 168-71, 1982.
"LearningbyAnalogy: FormulatingandGeneralizingPlansfromPastExperience," inMachine
Learning: An Artificial Intelligence Approach, R. S. Michalski. J. G. Carbonell, and T. M.
Mitchell (Eds.). Tioga, PaloAlto, Calif., 1983.
, "The SMOKEY Fire-Diagnosis System," Technical Report, Computer Science Department,
Carnegie-MellonUniversity, 1985.
Carbonell,J.G.,Larkin,LH.,andReif,F., "TowardsaGeneralScientificReasoningEngine."Technical
Report, CIPNo. 445, ComputerScience Department, Carnegie-Mellon University, 1983.
Clements, J., "Analogical Reasoning Patterns in Expert Problem Solving," Proceedings ofthe Fourth
AnnualConferenceoftheCognitiveScienceSociety, 1982.
Doyle,J., "ATruth MaintenanceSystem,"ArtificialIntelligence, Vol. 12, pp. 231-72, 1979.
. "ExpertSystemsWithoutComputers," AIMagazine, Vol. 5, No. 2, pp. 59-63, 1984.
Duda. R. O.; Hart, P. E.; Konolige, K.; and Reboh, R., "A Computer-Based Consultant for Mineral
Exploration," Technical Report6415, SRI, 1979.
Feigenbaum, E. A.. Buchanan, B. G., andLederberg, J., "OnGenerality and Problem Solving: ACase
Study Using the DENDRAL Program," in Machine Intelligence 6, D. Michie (Ed.), Edinburgh
University Press, Edinburgh, 1971.
Fikes, R. E.,andNilsson, N. J., "STRIPS: ANewApproachtotheApplicationofTheoremProvingto
ProblemSolving,"ArtificialIntelligence, Vol. 2, pp. 189-208. 1971.
Gentner, D., "TheStructureofAnalogical ModelsinScience."Technical Report4451, BoltBeranekand
Newman, 1980.
Hayes-Roth,F., "UsingProofsandRefutationstoLearnfromExperience,"inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski,J. G. Carbonell,andT M. Mitchell (Eds.). Tioga,
PaloAlto, Calif., 1983.
Kant, E., EfficiencyinProgramSynthesis, UMI Research Press, AnnArbor, Mich.. 1981.
Kling, R. E., "A ParadigmforReasoningby Analogy,"ArtificialIntelligence, Vol. 2, pp. 147-78. 1971.
Laird,J. E.,andNewell, A., "AUniversalWeakMethod," ProceedingsoftheEighthIJCAI, Karlsruhe,
W. Ger., pp. 771-73, 1983.
Larkin,J.,Reif,F,andCarbonell,J.G., "FERMI:AFlexibleExpertReasonerwithMulti-DomainInference," CognitiveScience, Vol. 9, 1985, submitted.
--- PAGE 406 ---
392 CHAPTER 14: DERIVATIONALANALOGY
McDermott, D. V., "PlanningandActing," CognitiveScience, Vol. 2, No. 2, pp. 71-109, 1967.
McDermott, D. V.,andDoyle,J., "Non-MonotonicLogicI,"ArtificialIntelligence, Vol. 13, pp. 41-72,
1980.
McDermott,J., "Rl: ARule-BasedConfigurerofComputerSystems,"TechnicalReport.ComputerScience Department, Carnegie-MellonUniversity, 1980.
, "XSEL: AComputerSalesperson'sAssistant," inMachineIntelligence10,J. Hayes, D. Michie,
and Y-H. Pao(Eds.), EllisHorwood Ltd., Chichester, U.K., 1982.
Michalski, R. S., "ATheory and Methodology ofLearning from Examples," inMachineLearning:An
ArtificialIntelligenceApproach, R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.),
Tioga, PaloAlto, Calif., 1983.
Minsky, M., "K-Lines: ATheoryofMemory," CognitiveScience, Vol. 4, No. 2, pp. 117-33, 1980.
Mitchell,T. M., "VersionSpaces: AnApproachtoConceptLearning," Ph.D.diss., StanfordUniversity,
1978.
Mitchell, T. M., Utgoff, P. E., and Banerji, R. B., "Learning by Experimentation: Acquiring and
RefiningProblem-SolvingHeuristics,"inMachineLearning:AnArtificialIntelligenceApproach,
R. S. Michalski,J. G. Carbonell,andT. M. Mitchell (Eds.),Tioga, PaloAlto, Calif., 1983.
Newell, A., andSimon, H. A., HumanProblemSolving, Prentice-Hall, EnglewoodCliffs, N. J., 1972.
Nilsson, N. J., PrinciplesofArtificialIntelligence, Tioga, PaloAlto, Calif., 1980.
Polya, G., HowtoSolveIt, PrincetonUniversity Press, Princeton, N. J., 1945.
Reif, J. H., and Scherlis, W. L., "Deriving EfficientGraph Algorithms," Technical Report. Computer
ScienceDepartment, Carnegie-MellonUniversity, 1982.
Sacerdoti, E. D., "PlanninginaHierarchyofAbstractionSpaces,"ArtificialIntelligence, Vol. 5, No. 2,
pp. 115-35, 1974.
Schank, R. C, "LanguageandMemory," CognitiveScience, Vol. 4, No. 3, pp. 243-84, 1980.
, DynamicMemory, CambridgeUniversity Press, Cambridge, 1982.
, "TheCurrentStateofAI: OneMan'sOpinion,"AIMagazine, Vol. 4, No. 1. pp. 1-8. 1983.
Shortliffe, E., ComputerBasedMedicalConsultations:MYCIN, American Elsevier, NewYork. 1976.
Swartout, W., and Balzer, R., "On the Inevitable Intertwining ofSpecification and Implementation."
CommunicationsoftheACM, Vol. 25, No. 7, pp. 438-40, 1982.
Waterman,D.,Hayes-Roth,F,andLenat,D.(Eds.),BuildingExpertSystems,Addison-Wesley.Reading.
Mass., 1983.
Wilensky. R., "UnderstandingGoal-BasedStories," Ph.D. diss., YaleUniversity. 1978.
, PlanningandUnderstanding, Addison-Wesley, Reading, Mass., 1983.
Winston,P., "LearningbyCreatingandJustifyingTransferFrames."TechnicalReportAIM-520, AILaboratory, MIT. 1978.
, "Learning and Reasoning by Analogy," Communications ofthe ACM. Vol. 23. No 12.
pp. 689-703, 1979.
--- PAGE 407 ---
PROGRAMMING BY ANALOGY
Nachum Dershowitz
UniversityofIllinois
at Urbana-Champaign
Abstract
Analogyisonetoolthatautomaticprogrammingsystemscanusetolearnfrom
experience,justasprogrammers do. Thischapter illustrateshow analogiesbetween
program specification (and derivations) can be used to debug incorrect programs,
modify existingprograms toperform differenttasks, deriveabstract schemata from
given sets ofcognate programs, and instantiate schemata to solve new problems.
An analogy between the specification of a given program and that of a new
problemisusedasthebasisformodifyingthegivenprogramtomeetthenewspecification. Debugging is a special case ofmodification: ifa program computes wrong
results, it mustbemodifiedtoachievetheintendedresults. Forprogramabstraction,
ananalogyissoughtbetweenthespecificationsofthegivenprograms;
maythenbe
used to transform an existing program into an abstract schema that embodies the
shared technique. Whenthe specification ofthe derived schema is compared with a
givenconcretespecificationandananalogybetweenthemisformulated, aninstantiation ofthe schema may be found that yields the desired concrete program.
Analogypervadesallourthinking, oureverydayspeechandour
trivialconclusionsas wellasartisticwaysofexpressionandthe
highestscientificachievements.
-GeorgePolya
15.1 INTRODUCTION
Programming begins with a specification of what the envisioned program
oughttodo. Itistheprogrammer'sjobtodevelopanexecutableprogramthatsatisfies
that specification. Yet only a small fraction of a programmer's time is typically
--- PAGE 408 ---
394 CHAPTER 15: PROGRAMMING BYANALOGY
devoted tothe creation oforiginal programs exnihilo. Rather, most oftheprogrammer's effort is normally directed at debugging incorrect programs, adapting known
techniques to specific problems at hand, modifying existing programs to meet
amended specifications, extending old programs to include expanded capabilities,
and abstracting ideas ofgeneral applicability into "subroutines."
The goal of research in automatic programming is to formalize the methods
and strategies used by programmers so that they may be incorporated in automatic
and interactive programming environments. In the author's view, program development systems should incorporate formal tools for transforming and manipulating
programs. This work investigates how analogies mightbe usedby such a system for
that purpose.
The importanceofanalogical reasoninghasbeen stressedby many, from Descartes to Polya. For a discussion ofthe role of analogy in the sciences, see Hesse
(1966); forareviewofpsychologicaltheoriesofanalogical reasoning, see Sternberg
(1977);otherworksonanalogyincludeRumelhartandNorman(1981), VanLehnand
Brown (1980), Gentner (1983), and Holyoak (1983). An early work on automating
analogical reasoning is Evans (1968). The use of analogy in automated problem
solvingingeneralandintheoremprovinginparticularwasproposedinKling (1971)
Otherworksemployinganalogy as an implement inproblem solving include Moore
and Newell (1973), Brown (1976), Chen and Findler (1976), Brown and Tarnlund
(1979)
McDermott (1979), Winston (1980)
Carbonell (1983b)
andBurstein (1983)
The useofanalogiestoguidethe modificationofprograms was suggested in Manna
and Waldinger (1975) and pursued further in Dershowitz and Manna (1977), Ulrich
andMoll (1977), andAmarel (chap. 18ofthisvolume). Recently, Carbonell (1983a)
has stressedthe importanceofconsideringthehistoryofaprogram'sderivation, not
just the "polished" final product.
Programmers improve with experience by assimilating programming techniquestheyencounterandjudiciouslyapplyingtheideastheylearntonewproblems.
One way a programmer can apply knowledge is by modifying a known program to
achieve some new goal. For example, a program that uses the binary-search technique tocompute square roots mightbe transformed intoone that searches an array.
Thischaptershowshowonecanmodifyprogramsbyfindingananalogybetweenthe
specification oftheexisting program and that ofthe desired program and then using
theanalogy asabasisfortransformingtheexistingprogramtomeetthe new specification. Program debugging is a special case ofmodification: ifa program computes
wrong results, it must be modified to achieve the intended results.
Theideathatprogramsshouldbeconstructedbyaseriesoftransformationshas
been widely promoted. The difference between such transformations and modification isthat in modificationcorrectnesswithrespecttotheoriginal specification isnot
preserved. Rather, it isnecessarythattheresultantprogrambecorrect with respect to
the transformed specification. Correctness-preserving transformations and
specification-changing modifications are thus complementary. A scenario of
--- PAGE 409 ---
DERSHOWITZ 395
computer-aided programming and debugging appeared in Floyd (1971). The
HACKER system (Sussman, 1975) constructed programs by trying out alternatives
andattemptingtodebugthemwhennecessary; otherknowledge-basedorplan-based
debugging systems have been designed as well.
All the programs described here are annotated with an output specification
(stating the desired relationship between the input and output variables upon termination ofthe program), an input specification (defining the set of legal inputs on
which the program is intended to operate), and invariant assertions (relations that
are known always to hold at specific points in the program forthe current values of
variables) demonstrating its correctness. The invariant assertions play an important
role in the derivation of analogies. Katz and Manna (1975) and Sagiv (1976)
describedebuggingtechniquesbased-likethemethoddescribedhere-on invariant
assertions.
Program modification is not the only way programmers use previously
acquired knowledge. After coming up with several modifications of their first
"wheel," they are likely to formulate for themselves (and perhaps for others) an
abstract notion ofthe underlying principle and reuse it in new, but related, applications. Programschemataareaconvenientformforrememberingsuchknowledge. A
schema may embody basic programming techniques and strategies (e.g., the
generate-and-test paradigm or the binary-search technique) and contains abstract,
uninstantiated symbols, in terms ofwhich its specification is stated.
The abstraction ofa setofconcrete programs to obtain aprogram schema and
theinstantiationofabstractschematatosolveconcreteproblemsmaybeviewedfrom
the perspective ofmodification methods. This perspective provides a methodology
for applying old knowledge to new problems. Beginning with a set ofprograms
sharing some basic strategy and their correctness proofs, one can derive a program
schema that represents their analogous elements. Preconditions for the schema's
applicabilityarederivedfromthecorrectnessproofs. Theresultantschema'sabstract
specification may be compared with a given concrete specification to suggest an
instantiationthatyieldsaconcreteprogramwhenappliedtotheschema. Iftheinstantiationsatisfiesthepreconditions, thecorrectnessofthenewprogramisguaranteed.
To date there has been a limited amount ofresearch on program abstraction.
TheSTRIPSsystem(Fikes, Hart, andNilsson, 1972)generalizedtheloop-freerobot
plansthatitgenerated; HACKER(Sussman, 1975) "subroutinized" andgeneralized
the "blocks-world" plansitsynthesized, executingtheplantodeterminewhichprogramconstants couldbe abstracted; Winston (1983) generalizes rules using analogical reasoning. Dershowitz and Manna (1975) suggested using the proofofcorrectness ofa program to guide the abstraction process; that idea was followed up in
Dershowitz (1985). Gerhart (1975) andothers haveadvocatedandillustratedtheuse
ofschemataas apowerful programming tool. A collection ofsuch schemata, along
with a catalog of correctness-preserving program transformations, could serve as
part ofan interactive program development system.
--- PAGE 410 ---
396 CHAPTER 15: PROGRAMMING BY ANALOGY
In the following sections the life-cycle ofan example program is traced. The
example illustrates someofthe kindsoftransformationsprograms undergoand how
analogy canbe used as a guide. First, an imperfectprogram forcomputing the quotientoftworealnumbersisanalyzed. Aftertheprogramisdebugged, itismodifiedto
approximate the cube root ofa real number and to search an ordered array. Underlying all three programs is the binary-search technique; a binary-search schema is
obtained by abstracting the programs. This schema is then instantiated to obtain
anotherprogram, onetocomputethesquarerootofan integer. The instantiatedprogramisthenmodifiedoncemoretoperformintegerdivisionwithremainderandthen
is subjectedtoa series oftransformationstotake advantage ofproperties specific to
division.
FurtherdetailsofthemethodologyadvocatedheremaybefoundinDershowitz
(1983), whereapartial implementationofsomeoftheanalogicalaspectsofprogrammingisalsodescribed, includingdebugging, modification,andinstantiation(butnot
abstraction).
AN EXAMPLE
15.2
Consider the problem of computing the quotient q of two nonnegative real
numbers c and d within a specified (positive) tolerance e. These requirements are
conveniently expressed in the form ofthe following skeleton program:
D] begin comment real-division specification
assert < c < d, e >
achieve c/d - q < e varying q
| \
end
The achieve statement,
achieve c/d - q < e varying q,
| \
containstheoutputspecificationgivingtherelationamongthevariablesq, c, d, ande
thatshouldbeattainedattheendofprogramexecution: the(absolutevalueofthe)difference between the exact quotient c/d and the result q should be less than e. The
clausevaryingqindicatesthatofthevariables inthespecification, onlyqmaybe set
by the program; the other variables, c, d, and e, contain input values that remain
fixed. Theinputspecificationdefinesthesetofinputvaluesonwhichtheprogram is
intendedtooperate. Assumingthatwewishtheprogramtohandlethecase whenthe
quotientisintherange to 1,thatis, whenthenumeratorcissmallerthanthedenominatord, the appropriate input specification is contained in the assert statement.
assert < c < d, e > 0,
attachedtothebeginningoftheprogram. Forthe problemat hand, it isassumed that
no general real-division operator / is available, though division by powers o\ 2
("shifts") is permissible.
--- PAGE 411 ---
DERSHOWITZ 397
Now let us imagine that a programmer went ahead and constructed the following program:
T] begin comment suggestedreal-divisionprogram
B assert < c < d, e >
pu x r \ pose c/d - q\ < e
purpose | q < c/d < q + s, s < e
(q,s) := (0,1)
loop L\\ suggest q < c/d < q + s
until s e
purpose q < c/d < q + s, < s < s^
ifd (q + s) < c then q : = q + s fi
s := s/2
repeat
suggest g < c/d < q + s, s < e
E\. suggest c/d - q < e
| \
end
The purpose statement,
purpose c/d - q < e,
| \
is acomment describing the intent ofthe code following it. The statement
suggest c/d - q < e
| \
contains the programmer's contention that the preceding code actually achieves the
desired relation; that is, the relation c/d - q < e holds for the value ofq when
| \
control reaches the end ofthe program. The comment
purpose q < c/d < q + s, s < e
indicates that the programmer's intention is to achieve the desired relation
c/d - q < e by achieving the subgoals q < c/d < q + s and s < e.
A | chievingth | ese relations is sufficientfor c/d - q < etohold. Forthispurpose
| \
the programmerconstructed an iterative loop intended to keep the first relation invariantly true while progress was being made towards the second. The suggested
invariant is contained inthe statement
suggest q < c/d < q + s
at label L\ . The goal ofthe loop body is
purpose q < c/d < q + s, < s < Slv
wheres^ denotesthevalueofthevariableswhencontrolwaslastatthelabelL
This
meansthatthevalueof5 istobelessthan itjustwasatthe headoftheloop. Thetwo
loop-body statements are accordingly repeated (zero or more times) until the test
s < ebecomes true, at which point the loop will be exited.
--- PAGE 412 ---
398 CHAPTER 15: PROGRAMMING BYANALOGY
We know what the above program was intended for, but in fact it does not
always fulfill those intentions. However, before we can debug it, we need to know
more about what it actually does. This can be accomplished by examining the code
andannotatingtheprogramwiththediscoveredinvariantrelations. (SeeDershowitz
and Manna, 1981, for one collection ofannotation rules.) Here a few necessary invariants are derived informally.
Theloopterminates whentheexittests < ebecomestrue. Itfollowsthatthe
relations < emustholdwhencontrolreachesthelabel E
Thiscanbeassertedinan
invariant
E <
x: assert s e.
Similarly, iftheexittestis nottakenandtheloopbody isexecuted, thentheexittest
must have been false, that is, s > e. Neither branch of the conditional statement
affects s, and therefore the relation s > e holds after the conditional statement as
well. Atthatpoints is dividedby 2. Ifbeforethedivision wehads > e, then at the
endoftheloopbodywehave2s > e. Thuswhenevertheloopbodyisexecuted, controlreturnstotheheadoftheloopwiththerelation2s > e. holding. Sincethatrelationdoesnotnecessarilyholdwhentheloopisfirstenteredwiths = 1, ititselfisnot
aloopinvariant. Nevertheless,thedisjunctionoftherelationss = land2 • s > e'\s
aloopinvariant, sinceonerelationholdswhentheloopis firstenteredandtheother
holds every time the loop is repeated; that is, we have
L\ : assert s = \ \J 2s > e.
Turning now tothebody ofthe loop, letusconsidertheconditional statement
ifd • (q + s) < c then q : = q + s fi.
The then-path ofthe conditional statement is taken when d • (q + s) < c; therefore, afterresettingqtoq -I- s, wehaved • q < c. Sinceconditionalstatementsare
often intendedto achievethe same purpose in differentcases, it is plausible that the
relationd • q < c-achieved by thethen-path ofthe conditional-is meant to hold
also when the then-path is nottaken. This suggests the "candidate" invariant
L\ : suggest d • q < c.
Indeed, sinced • q < cistrueinitially, whenq = Oandc > 0, anditisunaffected
when the conditional test is false (since the value ofq is not changed), it invariantly
holds when control reaches the head ofthe loop. We have derived the loop invariant
L, : assert d q < c.
Thethen-path is not taken when c < d • (q + s). In that case s is divided in
halfandqisleftunchanged, yieldingc < d • (q + 2s)attheendofthecurrent iter-
--- PAGE 413 ---
DERSHOWITZ 399
ation. Itturnsoutthatthisrelationholdsbeforetheloopisenteredandispreservedby
the then-path. Thus, we have the additional invariant
L\\ assert c < d • {q + 2s).
Theloopinvariantsd • q < candc < d - {q + 25)remaintruewhentheloopexit
istaken; along withtheexittests < e, they imply thatupontermination oftheprogram the output invariant
E\\ assert eld - q < 2 • e
| \
holds. Note that the desired relation eld - q < e is notimplied.
| \
The annotated program-with invariants that correctly express what the program does- is as follows:
T\ begin comment annotatedbuggy real-divisionprogram
B\\ assert < c < d, e >
purpose eld - q < e
purpose | q < c/d \ < q + s, s < e
(q,s) := (0,1)
loop L\. assert d - q < c, c < d • (q + 2 - s), 5=1 \/2
5 e
suggest c/d < q + s
until 5 e
purpose q < c/d, c/d < q + s, < s < 5^,
ifd - (q + 5) < c then q : = q + s fi
5 5/2
repeat
assert g < c/d <g + 2-5,5<£
suggest c/d < q + 5
£i: assert c/d - g < 2 • c
suggest c/ | d - q\ < | e
end
Now that something is known about what the program does, debugging can be
attempted.
DEBUGGING
15.3
Thetasknow istofindacorrectiontotheprogramthatwillachievethedesired
output invariant
suggest c/d - q < e.
| \
The first step is to look for a way to transform the actual invariant into the desired
one; then the same transformation is applied to the program, thereby, it is hoped,
--- PAGE 414 ---
400 CHAPTER 15: PROGRAMMING BYANALOGY
correcting the error. Accordingly, an analogy is sought between the actual output
invariant and the desired specification; we write
c/d - q < le => c/d - q < e
| | j \
Theobviousdifferencebetweenthetwoexpressions isthatwherethefirsthas2 • e,
the second hasjust e. So, the analogy can be reduced to simply
le => e.
The insufficient eld - q < 2e can therefore be transformed into the desired
c/d - q < e | by replacin \ g e with ell, that is, by applying the transformation
| \
e => ell.
So far ithas been determinedthat the transformation e => ell, appliedtothe
output invariant c/d - q < 2e, yields the desired output specification
c/d - q < e. T | hat same tr \ ansformation is now applied to the whole annotated
p | rogram. T \ he symbol e appears once in the program text: the exit clause s < e
accordingly becomes s < ell. It also appears four times in the invariants; for
example, the input assertion e > transforms into ell > 0, which is equivalent to
e > 0. The transformed program is
(q,s) := (0,1)
loop L 2: assert d • q < c < d • (q + Is)
until s e II
purpose q < c/d < q + Is, < s < s^
if d - (q + s) < c then q : = q + s fi
s s/1
repeat
assert q < c/d < q + Is, Is < e.
(The purpose statement has also been changed to reflect what the loop body does
in reality, as opposed to what was intended.) It can be shown that a transformation
suchase =£> ellpreservestherelationbetweentheprogramtextandinvariants; that
is, the transformed assertions are invariants of the transformed program (see
Dershowitz, 1983).
In this manner the program is modified to achieve the intended result
c/d - q < e. But notethat the loop invariant still differs from that suggested by
| |
the programmer. The difference between the two is that although the programmer
intendedc < d • (q + s)tobetrue, infactc < d • (q + Is)holds. Ifdesired, this
can be remedied by applying the transformation
s => 5/2.
Applyingthisglobaltransformationaffectsthefiveoccurrencesof5 inthe program code. The exit clause becomes
until v/2 ell,
--- PAGE 415 ---
DERSHOWITZ 401
orequivalently,
until s e;
the conditional statement becomes
ifd (q + 5/2) < c then q := q + s/2 fi.
The assignment statement
5 := 1
transforms into
5/2 := 1,
which, unfortunately, is not a legal assignment, since an expression appears on the
left-hand side. The intent ofthis illegal statement, however, is to
achieve 5/2 = 1 varyings
which is the same as
achieve 5 = 2 varying s,
and may be accomplished by the assignment
5 := 2.
Similarly, the assignment
5 := 5/2
gives rise tothe goal
achieve 5/2 = (s'12)12 varying s,
wheres' representsthevalueofthevariablespriortothisgoal
Thisisthesameasthe
goal 5 = 572, which is achieved by the original assignment
5 := 5/2
At this point, the loop body contains the two statements
ifd-(q + 5/2) < c then q : = q + 5/2 fi
5 := 5/2.
Since the expression 5/2 appears three times in these two statements, this program
may be slightly improved by transforming them into
5 := 5/2
If d • (q + s) < c then q : = q + s fi.
When all oftheabovechangesareincorporated, thefinal real-divisionprogram isas
follows:
begin comment real-divisionprogram
B 2: assert < c < d, e >
purpose eld - q < e
--- PAGE 416 ---
402 CHAPTER 15: PROGRAMMING BYANALOGY
purpose q < c/d < q + s, s < e
(q,s) := (0,2)
loop L 2: assert d - q < c < d • (q + s), s = 2 \j2s > e
until s e
purpose q < c/d, c/d < q + s, < s < si
= 2
s s 12
ifd • (q + s) < c then g : = q + 5 fi
repeat
assert q < c/J, c/d < q + s, s < e
E assert c/d - q\ < e
2: |
end
Notethatthisprogramisalmostthesameastheoriginal "buggy" program. Itdiffers
in two ways: the two loop-body assignments are interchanged (this presumably was
the programmer's error), and s is initialized to 2 rather than 1 (either initialization
works).
Now, considerthe following specification:
C begin comment cube-rootspecification
assert a > 0, e >
achieve ax/3 - r < e varying r
| \
end
Wewould like tousethecorrected real-divisionprogramas abasis forthe constructionofthespecifiedprogramforcomputingcuberoots. (Itisassumed, ofcourse,that
thecube-rootoperatorisnotprimitive.)Tothisend,thespecificationsofthetwoprograms are first compared. The output specification ofthe division program is
assert c/d - q\ < e,
and the output specification ofthe desired program is
achieve a]/3 - r < e varying r.
| |
The obvious analogy between the two is
q r
c/d => a
thatis, wherethefo
rmerspecificationhasq, theotherhasr, andwheretheformerhas
c/d, theotherhasa One waytoobtainacube-rootprogram fromthedivisionprogram is via the transformations
q r
u/v => u
c => a,
where by u/v => u is meant that every occurrence ofthe (general) division operator / is replaced by the cube-root operatorapplied to what was the numerator. (The
operator12 isnottransformedintoacuberoot, since-unlikegeneraldivision- itisa
--- PAGE 417 ---
DERSHOWITZ 403
primitive operation. If such a distinction were not made, the program derivation
would have to be examined more carefully.) Transformations that involve specific
functions, suchasdivision, arenot, however, guaranteedtoyieldacorrectprogram,
since the program may be based on some property thatholds fordivisionbutnotfor
extracting roots. Such transformations are heuristic in nature; they only suggest a
possible analogy between the two programs. Indeed, when this transformation is
appliedtothedivisionprogram
theresultisaprogramthatcomputesa/d, nota]/3
Whatmustbedoneinsuchcasesistoreviewthederivationoftheprogram,expressed
by the programmerin purpose statements, and see wheretheanalogy breaksdown.
Thepurposeofthedivisionprogramwas c/d - q < e, whichtransformed
into a1'3 - r < e as desired. The progra | mmer achi \ eved c/d - q < e by
| | | \
breaking it into the subgoals given in the statement
purpose q < c/d < q + s, s < e,
part ofwhich became theexittest forthe loop, and part ofwhich became a loop invariant. These subgoals transform into
purpose r < a < r + s, s < e,
which indeed imply the transformed goal a]/3 - r < e. The purpose ofthe loop
| \
body ofthe division program was
purpose q < c/d <q + s,ti<s< sir
In other words, the loop body reachieves the invariant while making progress
towardstheexittestbydecreasings. Theloop-body subgoal ofthetransformedprogram, then, is
purpose r < a m <r + s,0<s< siy
Atthispointthedivisionprogramhalvessand introducesaconditional withthe following purpose:
purpose q < c/d < q + s.
It is here that the analogy breaks down. The division program achieves the
above purpose in two cases, by testing ifd • (q + s) < c or not. For example, if
d • (q + s) < c does not hold, then c/d < q + s, as desired. On the other hand,
the fact that d • (r + s) < a does not hold in the cube-root program reveals
nothing about a]/3 < r + s. Therefore, a transformation is sought that makes
d • (r + s) > a implya1'3 < r + sortheequivalenta < (r + s)3 . Matchingthe
outcomeofthetestwiththedesiredinvarianttellsusthatthe implicationwouldhold
ifwe could transform d • (r + s) => (r -I- s)3 . Thus, where the division program
hasthe function u • v, thecube-rootprogram requires v3 . Theanalogy iscompleted
by adding the transformation
u - v => v3 ,
which is applied to the conditional test.
--- PAGE 418 ---
404 CHAPTER 15: PROGRAMMING BYANALOGY
There remains one problem: a transformed program can only be expected to
satisfy the output specification for those inputs that satisfy the transformed input
specification, which isunfortunately morerestrictivethanthegiveninputspecification. Inthecasepresentedhere, thiscanbesolvedif wefindanalternativemannerby
which to initialize the invariant r < a]/3 < r + s prior to entering the loop. To
achieve the subgoal r < av3 , we can let r = 0. Then to achievea]/3 < r + s = s,
we can let s = a + 1 (given the fact that a]l3 < a + 1). The complete cube-root
program is:
C begin comment cube-rootprogram
B 3: assert a > 0, e >
(r,s) := (0,a + 1)
loop L 3: assert r < a]/3 < r + s
until s e
s := s/2
if (r + s)3 < a then r : = r + s fi
repeat
£ assert a1/3 - r < £
3: | |
end
15.4 A MORE PROBLEMATIC EXAMPLE
Inthepreviousexamples, thetwogiven specifications were syntactically similar, and the transformations applied were relatively straightforward. This next
examplewill illustrate someofthedifficultiesthat maybeencountered when analogies are used to guide the modification ofprograms.
Inthisexample, itisshownhowthecorrectcube-rootprogram C
maybeused
as abasis forconstructing aprogram thatsearches forthe positionzofan element b
knowntooccurinanarray segment4[l:n]. Thearray isassumedtocontain integers
sorted in nondescending order. Our goal is
A 4: begin commentarray-search specification
assert it < v D A[u] < ^[v], A[u] E Z, b 6 A
achieve A[z] = b varyingz
end
whereb G A meansthattheelementboccurs inthearray segment/4[l:/i]. ThatA is
an array ofnondecreasing integers isexpressed inthe input conditionsA[u]G Z and
u < v D A[u] < A[v] (where the variables u and v are universally quantified).
Arrays may be indexed by any real number, and the convention is adopted that the
intended element may be found by truncating the index, that is,
A\u\ = A\[u\\
--- PAGE 419 ---
DERSHOWITZ 405
forallu. (Inasimilarmanner, onecoulddevelopaprogramfollowingthe ALGOL 60
convention ofrounding offthe index.) The desired goal
achieveA[z] = b varyingz
is not directly comparable with the output specification
assert a]l3 - r < e
| \
ofthe given program. Therefore, the goal must be decomposed somewhat.
As a firsttry, the desired goal is replaced with the equivalent goal
achieveA[z] < b < A[z] varying z.
This replacement is guided by the fact that an equality is desired and the given program achieves an inequality. Since only integers are involved, this is the same as
achieveA[z] < b < A[z] + 1 varying z.
Also, it is known that the cube-root program actually achieves the output invariants
assert r < #1/3 < r + e.
Accordingly, a transformation
r < a < r + e => A[z] < b < A[z] + 1
is sought and the subgoal r < a is compared with A[z] < b. Matching the two
sides ofthe inequality produces
r =*> A[z]
a => b.
To obtaina => b, we can let
a => b\
Applying these transformations to the whole goal leavesA[z\ < b < A[z] + e =$>
A[z] < b < A[z] + 1, suggesting the additional transformation
e => 1.
Applying the three transformations
r => A[z]
a => b3
e => 1
to the given cube-root program yields the following program:
X begin commentproposedarray-searchprogram
assert/?3 >
1, 1
(A[z],s) := (l,fc3 - 1)
loop L 4: assert A[z] < b < A[z] + s
until s 1
--- PAGE 420 ---
406 CHAPTER 15: PROGRAMMING BYANALOGY
s := sl2
if (A[z\ + s)3 < & then A[z]: = A[z] + s fi
repeat
£ 4: assert /l[z] < b < A[z] + 1
end
There are, however, a number ofproblems with this program, the insurmountable
one lying inthethen-branchassignmentA[z] : = A[z] + s. The original goal stated
thatonlyzisanoutputvariable; thearrayA isaninputvariablethatmaynotbemodified by an assignment. Furthermore, there is no way to
achieve A[z] = A[z'] + s varyingz
(wherez' denotesthepreviousvalueofz), sincethevalueA[z'] + smightnotappear
inA
at all.
So another alternative mustbe sought. SinceA[u] = A[[u\], the goal
achieve A[z] = b varyingz
is equivalent to
achieve A[[z\] = b varying z.
At this point it would be appropriate to extract z from the expression /l[|zj], as the
outputvariablerappearsbyitselfintheoutputinvariantsofthegivenprogram C
facilitate this, we need to make temporary use ofan inverse ofthe array-indexing
function. (The inverse function serves only as a formal mechanism for expressing
transformations; it will be eliminated from the final version ofthe program.) The
functionpos(U, u) gives the (integral) position ofthe (rightmost) occurrence ofthe
element u in the array U. Thus, A\pos(A,b)] = b, and, in orderto
achieve A[[z\] = b varyingz,
it suffices to
achieve pos(A,b) = [z\ varying z.
Applying now the definition of[u\, we obtain the conjunctive goal
achieve pos(A,b) ^ z < pos(A,b) + 1, pos(A,b) E Z varying -.
Sincetheconjunctpos(A,b) E Zfollowsfromthedefinitionofpos(andthefactthat
b E A), we are left with the goal
achieve pos(A,b) < z < pos(A,b) + 1 varying z.
The current goal is still not readily comparable with the output invariants
assert r ^ a < r + e
ofthecube-rootprogram. Whereasforthearray-searchprogramtheoutputvariable;
appears on the right-hand side ofthe < relation and on the left-hand side ofthe <
--- PAGE 421 ---
DERSHOWITZ 407
relation, for cube-root program the sides are reversed. One possible solution is to
transformthepredicates < and < To get
r < a =» z > pos{A,b),
thetransformations
r => z
a => pos(A,b)
may be applied. To obtainthe thirdtransformation, we let
a => pos(A,b)3
(This latter transformation hides the real analogy between ul/3 => pos(A,u). Its
advantage,however,isthatitisguaranteedtopreservecorrectnesswithrespecttothe
specifications.) Applyingthesetransformations tothe subgoal a113 < r + e leaves
pos(A,b) < z + e => pos(A,b) + 1 > z.
Transposingto isolatepos(A,b) onboth inequalities gives
pos(A,b) < z + e => pos(A,b) > z ~ 1,
sothetransformations < => > and e =» - 1 are added. Altogetherthis gives
r => z
a => pos{A,b)3
< => >
e => -1.
Applying these five transformations to the given cube-root program C and
simplifying yieldstheprogram
(Z,s) := (l,pos(A,b)3 + 1)
loop L 5: suggest z > pos{A,b) > z + s
> -
until 5
5 := 5/2
ifz + s > pos{A,b) then z : = z + 5 fi
repeat
E 5: suggest z > pos(A,b) > z - 1.
(Thetransformedinvariantsareonly suggestions, sincetheconstants < and < are
being transformed.) Before the nonprimitive function pos is eliminated from the
transformedprogram, anattemptis madetoverifythecorrectnessoftheprogramas
is. The suggested loop invariants
suggest z ^ pos(A,b) > z + s
along with theexitcondition
--- PAGE 422 ---
408 CHAPTER 15: PROGRAMMING BYANALOGY
> -
until s 1
clearly imply the desired output invariant
suggest pos{A,b) < z, z < pos(A,b) + 1.
Furthermore, the loop-body path preserves the loop invariants forboth cases ofthe
conditional.
The problem is with the verification condition for the initialization path: the
assignment (z,s) : = {\,pos{A y bf + 1) does not initialize the loop invariants. Since
thetransformed initializationdoes not work, it mustbe replacedwithanunachieved
subgoal
assert u < v D A[u] < A[v], A[u] G Z, b G A
achieve z > pos(A,b), pos(A,b) > z + s varying z,s.
The purpose of this goal is to set the variables z and s so that the loop invariants
z > pos(A,b)andpos(A,b) > z + 5holdwhentheloopisenteredforthefirsttime.
Since it is giventhatb appears withinthe segmenti4[l:n], the relationz > pos(A,b)
can be achieved by letting z = n. Nowpos(A,b) > z + s can be achieved by the
insistence thatz + s = 0, for which s is initialized to - z = -n.
Replacing the initialization requires rechecking the verification condition for
termination. It must be the case that when the initial value -n of s is repeatedly
halved, theexittests > - 1 becomestrueatsomepoint. Sincethisisinsured, allthe
verification conditions hold and the transformed program is correct.
Finally, the conditional test z + s > pos{A,b) containing the nonprimitive
functionpos may be replaced by'A [z + s + 1] > b. That the two tests are equivalentmaybededucedfromtheinputspecificationu < v D A[u] < A[v] andthedefinition ofpos. The program now is as follows:
A begin comment array-searchprogram
B 5: assert u < v D A[u] < A[v], A[u] G Z, b G A
(z,s) := (n,-n)
loop L 5: assert A[z + s + 1] < b < A[z + 1]
> -
until s 1
s : = s /2
ifA[z + s + 1] > b then z : z + sfi
repeat
E 5: assert A[z] = b
end
Theabovearray-searchprogramiscertainlynotthemostobviousone. But that
istobeexpected, since it wasderivedby analogy withadifferent program, not from
scratch. Ifdesired, it could be given a more conventional appearance by replacing
Z + s (the lower bound ofthe search) with a new variable y and following up with
additional program transformations.
--- PAGE 423 ---
DERSHOWITZ 409
15.5 ABSTRACTION
Atthispointtherearethreeprograms, D forfinding quotients, C forfinding
2 3
cube roots, andA 5 for searching ordered arrays, all utilizing thebinary-search technique. It would be nice ifone could extract an abstract version ofthe programs that
captures the essence of the technique but is not specific to any one problem. The
resultantabstract program schemacouldbe used as a model ofbinary search forthe
solution offuture problems.
For this purpose, consider the complete analogy that was found between the
specifications of D and C
2 3:
q r
u/v <$> u
c <£> a
u v <£> v3
Since bothu/vandu are functions, they can be generalizedtoan abstract function
y(u,v). Similarlythegeneralizationofu • vandv3 isanotherfunction6(w,v). Bothq
and r are output variables and are generalized to an abstract output variable z; the
input variables canda are generalizedtoan abstract input variablex. This gives the
following set oftransformations for generalizing the division program:
u/v y(u,v)
c =£> X
u • V => d(u,v).
Applying these transformations to the specification
achieve c/d - q < e varying q
| \
ofthe division program yields
achieve y(x,d) - z < e varying z.
| \
Thiswillbetheabstractoutputspecificationoftheschema. Substitutingtheabstract
functions7and6intotheirrespectivepositions inthedivisionprogram
does not,
however, result in a schema that will work for all instantiations of7 and 6. This is
because the original program relied upon facts specific to multiplication and division. Itmustthereforebedeterminedunderwhatconditionstheabstractschemadoes
achieve its specifications.
To begin with, the transformed initialization assignment does not achieve the
desired loop invariant. The loop initialization is therefore replaced with the subgoal
achieve d(d,z) < x < b{d,z + s) varying z,s,
leaving unspecified-for the time being at least-the details ofhow to initialize the
loop invariant. Forthe loop-body path to be correct, the truth ofthe invariant must
implythattheinvariantwill holdnexttimearound; thiscaneasilybeshowntobethe
--- PAGE 424 ---
410 CHAPTER 15: PROGRAMMING BYANALOGY
case forany function 5. Forthe loop-exitpathtobecorrect, theloop invariants, plus
exittest, mustimplythattheoutputinvariantholds. Forthistobethecase, itsuffices
to establish the condition
8(w,u) < v = u < y{v,w).
Inthismanner, ageneralprogramschemahasbeenderivedforabinary search
forthe value ofy(x,d) within a tolerance e:
S begincomment binary-search schema
B 6: assert e > 0, b(w,u) < v = u < y(v,w)
achieve b{d,z) < x < 5(d,z + s) varying z,s
loop L 6: assert b(d,z) < x < b{d,z + s)
until s e
s s/2
if b(d,z + s) < x then z : = z + s fi
repeat
£ 6: assert | y(x,d) - z \ < e
end
Ofcourse, for this schema to be executable, the function 5 appearing in it must be
primitive; otherwise, it should be replaced. Similarly, the unachieved subgoal
achieve b{d,z) < x < b{d,z + s) varying z,s
must be reduced to primitives. Ifthe analogy with A
had also been considered, a
somewhat more general schema would have resulted.
15.6 INSTANTIATION
The binary-search schema just derived from the division program may be
appliedtothe computationofthe square rootofan integer. Thegoal istoconstructa
program that finds the integer square rootz ofa nonnegative integera:
Rj: begin comment integersquare-rootspecification
assert a 6 N
achieve z = [\[a\ varying z
end
where the function [u\ yields the largest integer not greater than u.
This goal cannot be directly matched with the output specification of the
schema
assert y(x,d) - z < e varying z.
| \
However, ifthegoal z = \\fa\ isexpanded using thedefinitionof |if|, the equivalent
goal
--- PAGE 425 ---
DERSHOWITZ 411
z<\[a<z+l,z£Z
achieve varying z
(where Z is the set ofall integers) is obtained; that is, z shouldbe the largest integer
notgreaterthan yfa. Since it is knownthatthe schemaachievesthe output invariants
assert z < y(x,d) < z + e,
these invariantscanbecompared withthe abovegoal. This suggeststhetransformation
y(u,v) => yju
x => a
e => 1
to achieve z ^ \[d < z + 1. In addition, the program will have to be extended to
insure that the final value ofz is an integer.
The precondition for the schema's correctness is
assert e > 0, b{w,u) < v s u < y(v,w);
instantiating it yields
assert 1 > 0, <5(vv,w) < v = u < y/v.
Thisconditionmaybe satisfiedbytaking6(w,u)tobeu2 (andrequiringu > 0). This
completes the analogy and suggests the additional transformation
6(w,u) => u2
Applyingthe instantiation mappingtothe schema, we obtainthe following partially
written program:
P begin comment incomplete integersquare-rootprogram
B assert a G N
achieve z2 < a < (z + s)2 varying z,s
loop L 7: assert z2 < a < (z + s)2
until s 1
^ := s/2
if (z -I- 5)2 < a then z : = z + 5 fi
repeat
assert -Ja - z < 1
achieve | z G Z p \ rotecting z < V« < z + 1 varying z
end
This program still contains two unachieved subgoals. The first can be achieved by
assigning (z,s) := (0,a + 1). For the second, the current value of z may be perturbed just enough to make it an integer, leaving the relation z < \fa < z + 1,
already achieved by the instantiated schema, still "protected." This can be done by
assigning
if \z] 2 < a then z : = \z\ else z : = |zj fi.
--- PAGE 426 ---
412 CHAPTER 15: PROGRAMMING BY ANALOGY
15.7 EXTENSION
Analternativeapproachtocompletingtheaboveprogramistoinsistthatz E Z
hold throughout execution ofP 7. This is the avenue pursued, for example, in the
"structured programming" derivations in Dijkstra (1976) and Blikle (1978).
Note that actually z E N must be achieved, since the invariant yfa < z + 1
implies that z is nonnegative. To this end, the goal
achieve z E N in R 7 varying z
issetup, meaningthatz E N istobe "globally" invariantin P
Accordingly, zGN
must be established initially and then preserved throughout the loop computation.
Lettingz = initially givesz E N, as isdesired. Sincez is sometimes incremented
bys, the latter should alsobe anonnegative integer. Finally, in ordertopreserve the
invariants E N while it is repeatedly halved until it is no longergreaterthan 1, it is
necessary and sufficient that s E 2 N be invariant, where 2 N denotes the set ofnonnegative powers of2. Accordingly, theconjuncts E 2N is addedtothe initialization
subgoal a < s2
At this point, we have an unachieved subgoal
achieve a < s2 , s E 2 N varying s.
To achieve this conjunctive goal, we transform it into an iterative loop,
purpose a < s2 s E 2 N
achieve s E 2 N varying s
loop L' %: assert s E 2 N
until a < s2
achieve approach{a < s2 ) protecting s E 2 N varying s
repeat
assert a < s2 s E 2 N
choosing firstto achieve s E 2 N and then to keep ittrue while the loop isbeing executed until the remaining conjunct a < s2 is also satisfied.
To initialize 5 E 2N , we let 5 = 2° and assign
s := 1.
Within the loop we have the subgoal
achieve approach{a < s2 ) protecting s E 2N varying s:
that is, we wish to preserve the invariant s E 2N while progress is made towards the
exit test a < s2 . Since it is known that initially 5=1, and since ultimatel)
< s/a < s isdesired, it followsthats increases within the loop. Presuming that 5
increasesmonotonically, it followsthats must be multipliedby some positive power
of2, forexample.
--- PAGE 427 ---
DERSHOWITZ 413
When all the pieces are put together, the following program is obtained:
/?8: begin com
t integersquare-rootprogram
Z?8: assert
purpose a < s2 s G 2 N
s := 1
loop L assert s G 2N
until a < s1
s := 2 • s
repeat
purpose z2 < a, a < (z + s)2 , 5 < 1
loop L 8: assert z2 < a, a < (z + s)2 , z G N, s G 2 N
until s 1
5 5/2
if (z + s)2 < a then z : = z + 5 fi
repeat
Eg. assert z [yja\
end
In the above program, the exit test a < s2 and conditional test (z + s)2 < a
arethemostexpensiveexpressionscomputed. Theycanbereplacedbycheaperoperations by a series of correctness-preserving program transformations (see, e.g.,
Wensley, 1959; Dijkstra, 1976; Blikle, 1978; Dershowitz, 1983). In the following
example, something similar is done:
Supposenowthatwewishtoconstructaprogramtocomputethequotientqand
remainder roftwo integers c andd. The formal specification is
Q 9: begin comment integerdivision specification
assert c G N, d G N + 1
achieve q < c/d < q + 1, q E N, r = c - d • q varying q,r
end
Of course, the general-division operator is meant not to be a primitive; in fact it
wouldbepreferablethattheprogramnotusemultiplicationbyotherthanaconstant.
Inorderforthe abovegoal tobe achieved, programR mustbe modified. The
first step is to attempt to discover an analogy between the specifications ofthe two
programs. In this case the output specification
£ 8: assert z = [yfa\
ofthe square-root program bears little external resemblance to the goal
achieve q < c/d < q + 1, q : N, r = c - d • q varying q,r.
--- PAGE 428 ---
414 CHAPTER 15: PROGRAMMING BYANALOGY
But, ifit is also known that the output invariants
z<Va<z+l,zGZ
£ assert
hold, thenthe analogy is more readily apparent. One possible setoftransformations
Z => q
yfa => c/d.
The lattertransformation canbe accomplished by letting
a => (c/d)2
These transformations will achieve the desired conjuncts q < c/d and c/d <
q + 1; in addition the program will have to be extended to achieve r = c - d q.
Whenthesetransformationsareapplied, theexittestofthe firstloop, a < 52
becomes(c/d)2 < s2
Sincec, d, andsareallpositive, thisisthesameasc/d < sot
c < d s. Similarly, the conditional test (z + s)2 < a becomes d • (q + s) < c.
The transformed program accordingly is
(q,s) := (0,1)
loop LI,: assert s G 2 N
until c < d • s
s := 2 - s
repeat
loop L)\ assert q < c/d, c/d < q + s, q £ N, s E 2 N
until s 1
s := s/2
\i d • (q + s) < c then g : = q + 5 fi
repeat
Whenthetransformationa => (c/d)2isappliedtothegiveninputspecification
aGN
ofthe integer square-root program, the condition (c/d)2 G N on the inputs c
and d is obtained. The problem is that this is stronger than the specification c G N
andd G Z + 1 forintegerdivision. However, allthatisactually neededforthecorrectnessofthesquare-rootprogramisa > 0;theinputspecificationa G N isunnecessarilyrestrictive. Applyingthetransformationtoa > insteadyields(c/d)2 > 0.
Sincethis is impliedby c G N andd G N -I- 1, the above program is correct forall
legal inputs.
Atthispoint,theprogramachievesmostoftherequirementsintheoutputspecification of Q 9. The additional requirement r = c - d q can be achieved if the
aboveprogramisextendedby incorporatingtheoutputvariablerandmaintainingthe
desiredrelationthroughouttheprogram. Thisrequiresthatwheneverqisassignedto.
the new variable rbe updated in tandem. Thus, when q is initialized to 0. r is set to
c - d = c. When q is incremented by s, r is updated to c - d •
(q + s) = r - d • s.
--- PAGE 429 ---
DERSHOWITZ 415
With the new assignments to rincluded, the program is as follows:
(q,s,r)
(0,1,c)
loop Lg': assert s G 2N
until c < d - s
s : 2 • s
repeat
loop L 9: assert q < c/d < q + s, q G N, s G 2 N
5 := 5/2
ifJ (g + s) < c then (q,r) := {q + s,r - d - s) f\
repeat
Note that the conditional test d • (q + s) < c, which is equivalent to d • s <
c - d - q, can be replaced by J • s < r now that r is available. Furthermore, the
expressiond
sinvolvesmultiplicationandappearsseveraltimes. Theprogramcan
beimprovedbytheintroductionofanewvariableuandtheextensionoftheprogram
once moreto maintainthe new relation u = d • sglobally invariant. Substituting u
for all occurrences ofd • s and updating it wheneverthe value ofs is changed produces the following program:
Q begin comment integerdivisionprogram
B 9: assert c G N, d E N + 1
(q,s,r,u) := (0,\,c,d)
loop Lg': assert s G 2 N , u = d • s
until c < u
(s,u) := (2s,u)
repeat
loop Lg: assert q < c/d, c/d < q + s, q G N, s G 2 N
r = c - d • q, u = d • s
until s 1
(s,u) := (s/2, ull)
if u < r then (q,r) : = (q + s,r - u) fi
repeat
E 9: assert q < c/d < q + I, q E N, r = c - d • q.
end
This is the desired "hardware" integer-division program (see Wensley, 1959). Its
only operations are addition, subtraction, comparison, and shifting-hardware
instructions on binary computers.
15.8 DISCUSSION
Theexamplesinthischapterhaveillustratedhowanimaginaryautomatedprogrammingsystemmightuseanalogicalreasoningtoperformsomeofthevariedtasks
ofprogram construction. The author imagines thata futureprogrammer faced with
--- PAGE 430 ---
416 CHAPTER 15: PROGRAMMING BYANALOGY
thetaskofdevelopinganewprogram (orsubprogram)tomeetasetofspecifications
might first search for an applicable schema. After instantiating that schema appropriately, theprogrammermighthavetoperformvariousothertransformationstosatisfyremainingspecificationsortoincreaseefficiency. Ifnoapplicableschemacanbe
found, the programmer might still be able to find a program solving an analogous
problemandmodify
it.
Thosetwoprogramstogethercouldthenbeusedtoformulate
aschemaforfutureuse. (Usingaschemaispreferabletomodifyingarelatedprogram
because correctness is ensured ifthe preconditions are satisfied. The extraction of
appropriatepreconditions, however, is what makes abstraction more complex atask
thaneithermodificationorinstantiation.)Naturally, somestepstakenintheseexamples were more intuitivethanothers and moreeasily implementable. An attempthas
beenmadeheretohighlight, ratherthantohide, thedifficultiesthatmaybeencountered in finding and applying analogies.
The workpresented here lies atthe confluence ofmachine learning and automaticprogramming. Assuch, itillustratesthepotentialvalueofanalogicalreasoning
in the programming domain. This domain differs from many others in that (1) it is
welldefined, (2) reasoningis "monotonic," and (3) formaltoolsareavailable. Some
oftheproblems encountered in the examples, such asthe needtoanalyze andtransform programs, are typical for automatic programming. Their resolution requires
powerful tools, large libraries, and efficient search mechanisms.
The analogical methods described here are meant to complement other program transformation techniques and program synthesis and analysis tools. For
example, thedecisiontosplitasearchrangedownthemiddlecanonlybejustifiedby
theneatnessoftheresultantprogram, asinDijkstra(1976), bythedesiretominimize
the number ofvariables, as in Manna and Waldinger (1985), or by the efficiency of
thealgorithm(necessitatingexpostfactoanalysis). Butoncetheideaofbinarysearch
hasbeendevelopedinonecontext, itcanbeapplied-byanalogy-toothercontexts.
In general, an instantiated schema wouldbe transformed totake the specifics ofthe
problem domain into account. In the process, program statements may be moved,
changed, or deleted, to a point where the transformed program bears little resemblance to the schema. For example, Newton's faster method for calculating square
roots is in large measure analogous to the binary method, but splits the range at a
point other than the center.
Analogiesbetweendifferentobjectscanbesoughtonseveral levels-thelevels
ofexternal appearance, ofoutward performance, and ofinner workings. Generally,
onecandramaticallychangetheexternal appearanceofa program, that is, the code,
without affecting the underlying algorithm. At the opposite end of the spectrum,
input/outputspecifications, whichdefinetheoutwardperformanceofa programbut
not how it accomplishes what it does, can be identical for very disparate programs.
Between the two extremes, comments about the program's inner workings- its correctnessandefficiency-are perhapsabetterguide whenone is looking for similaritiesbetweenprograms. Therefore, it makessensetobeginby formulating an analog)
--- PAGE 431 ---
DERSHOWITZ 417
between program specifications and then to extend that analogy by examining how
the different programs achieve their analogous desiderata. The more one knows
aboutthe "rhymeandreason" ofaprogram, themorelikelyoneistobeabletoprofit
from comparing it with otherprograms.
The problems inherent in the use ofanalogies for program modification and
abstraction include hidden analogies, misleading analogies, incomplete analogies,
andoverzealousanalogies. Hidden analogiesarisewhengivenspecifications (ofthe
existing program and the desired problem in the case ofmodification; ofthe two or
more existing programs in the case ofabstraction) that are tobe compared with one
another have little in common syntactically. Since the pattern-matching ideas
employedhere are syntaxbased, the underlying analogy is hidden whenthe specifications are not syntactically similar. Then it is necessary to rephrase the specificationsinsomeequivalentmannerthatbringstheirsimilarityoutbeforeananalogycan
be found. Thisisclearlyadifficultprobleminits ownright; ingeneral some formof
means-end analysis seems appropriate.
At the opposite extreme, a syntactic analogy may be misleading. The same
symbolmayappearinthespecificationsoftwoprogramsbutplaynonanalogousroles
in them. Twoprograms mightevenhave exactly the same specifications butemploy
totally different methods ofsolution. This kind ofsituation wouldbe detected when
correctness conditions are analyzed. Other work on "repairing" analogies includes
Burstein (1983), Carbonell (1983a), and Winston (1983).
Knowing how a program was constructed can help one avoid overzealously
applying transformations to unrelated parts ofa program. Like Carbonell (1983a),
theauthorbelievesthatitisnecessaryingeneraltolookforanalogiesbetweenderivations. Anexamplehasbeengivenhereofhowcommentsonthepurposeofcodesegments help complete an analogy between two programs when only part of it was
found by a comparison ofspecifications.
In summary, programmingby analogy may be thoughtofas including the following (nonindependent) steps:
Findrelevantprograms. The question ofhow one knows which programs are
worthlookingatindetailhasnotbeenaddressed. Onewouldpresumablybegin
by looking for some kind ofbroad similarity before continuing with the next
step.
Analyze thegivenprogram. Ifnecessary, the given program can be annotated
with invariant assertions. In particular, the relation between input and output
variablesachievedbytheprogramshouldbedeterminedsoit maybecompared
with the desired relation. It would be better yet ifthe programmer (human or
machine) has provided an annotated history ofthe decisions made in deriving
the given program. Annotation is particularly essential for debugging, since
what the incorrect program actually does is unspecified.
--- PAGE 432 ---
418 CHAPTER 15: PROGRAMMING BYANALOGY
• Rephrasethespecifications tobring outtheirsimilarity. The specifications of
the given and desired programs may be given in a form that obscures any
analogy. Thus it maybenecessarytoexpressthespecificationsinsomeequivalentformthatmakestheirsimilaritymorepronounced. Ingeneralthismaybea
very difficult task.
• Discoverananalogybetweenthespecifications. Thisanalogy suggestsasetof
transformationsthatyieldthedesiredspecificationwhenappliedtothespecification ofthe given program.
• Checkthevalidityoftheproposedtransformations. Forthosetypesoftransformations that do not necessarily preserve correctness, the verification conditions are examined.
• Extend the analogy. When the program derivations are examined, it often
becomes clear that the analogy must be extended with additional transformations.
• Localize the transformations. Othertimes, transformations must be localized
to specific occurrencesofsymbols. One doesthisby looking atthedependencies ofsymbols in a program proof.
• Applythetransformations. Thetransformationsfoundinthepreviousstepsare
applied to the given program.
• Rewriteanyunexecutablestatements. Any nonprimitive statements orexpressions introduced intothe programby thetransformations mustbe reexpressed
in terms ofprimitives.
• Synthesizenewsegments. Ifnecessary, new program segments can be written
to replace parts ofthe program that cannot be modified.
• Extend the modifiedprogram. Code is integrated into the program for any
unachieved parts ofthe desired specification.
• Optimizethetransformedprogram. Atthispoint, domain-specifictransformations can be applied. One may be able to optimize the new program by taking
advantage ofproperties it has that may not have held forthe old program.
Theauthordoesnothaveafullimplementation,butheenvisionsthepossibility
ofsuchmethodsbeingembeddedinasemiautomatedprogramdevelopmentenvironment in which the system performs the more straightforward steps in a consistent
manner. The reasoningability neededby sucha system issimilartothat required by.
say, a program verification system; the program manipulation abilities are comparabletothose requiredoftransformationsystems. All requirethesame knowledgeof
the subject domains and need similar logical arithmetic capabilites. It may be that
logic-based programming languages (see Kowalski, 1974) will provide a relatively
convenient environment for this kind ofresearch. The identity ofspecification and
programming languages and the potential availability of suitable general-purpose
theorem provers should aid the design ofprogram manipulation systems. Ofcourse.
--- PAGE 433 ---
DERSHOWITZ 419
suchmethodsasthosedescribedbytheauthor(orbyothers)wouldnotsufficetoproduce large-scale, complex programs. Still, top-down programming methodology
favors small, easily comprehensible modules, each ofwhich should be amenable to
thekindofmanipulationspresentedhere. Thelargeraprogram,themoreconvoluted
itis, andthedeeperthe ideasthatwent intoit, themoredifficultit is-forhumansor
machine-to reason about it.
In conclusion, the author has endeavored to show how analogies may be
exploitedtoallowaprogramdevelopmentsystemtoprofitfrompastexperience. But
analogicalreasoningisonlyonemodeofthought, albeitanimportantone; it mustbe
combined with other techniques as well.
ACKNOWLEDGMENT
Thisresearchwas supportedinpartbythe National Science Foundationunder
Grants MCS 79-04897 and MCS 83-07755.
References
Amarel, S., "Program Synthesis as a Theory Formation Task: Problem Representations and Solution
Methods,"chap. 18ofthisvolume, 1985.
Blikle, A., "TowardsMathematical StructuredProgramming," inFormalDescriptionsofProgramming
Concepts, E. J. Neuhold(Ed.), North-Holland, Amsterdam, 1978.
Brown, R. H., "ReasoningbyAnalogy," WorkingPaper 132, AILab, MIT, October 1976.
Brown,F.M.,andTarnlund,S.A, "InductiveReasoningonRecursiveEquations,"ArtificialIntelligence,
Vol. 12, No. 3, pp. 207-29, November 1979.
Burstein, M. H., "Concept Formation by Incremental Reasoning and Debugging," Proceedingsofthe
InternationalMachineLearning Workshop, R. S. Michalski(Ed.),AllertonHouse, Universityof
Illinois at Urbana-Champaign, pp. 19-25, June 22-24, 1983. (An updated version ofthis paper
appearsaschap. 13ofthisvolume.)
Carbonell,J.G., "DerivationalAnalogyinProblemSolvingandKnowledgeAcquisition,"Proceedingsof
theInternationalMachineLearningWorkshop,R.S.Michalski(Ed.),AllertonHouse,University
ofIllinoisatUrbana-Champaign,pp. 12-18,June22-24, 1983a. (Anupdatedversionofthispaper
appearsaschap. 14ofthisvolume.)
"LearningbyAnalogy: FormulatingandGeneralizingPlansfromPastExperience,"inMachine
Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and
T M. Mitchell (Eds.),Tioga, PaloAlto,Calif., 1983b.
Chen, D. T W.,andFindler, N. V., "TowardAnalogicalReasoninginProblemSolvingbyComputers,"
Technical Report 115, DepartmentofComputerScience, State UniversityofNew York, Buffalo,
December 1976.
Dershowitz, N., "Program Abstraction and Instantiation," ACM Transactions on Programming LanguagesandSystems, Vol. 7, No. 3,July 1985.
--- PAGE 434 ---
420 CHAPTER 15: PROGRAMMING BY ANALOGY
JlieEvolutionofPrograms, Birkhauser, Boston, 1983.
Dershowitz, N., and Manna, Z., "On Automating Structured Programming," Proceedings ofthe ColloqueIRIAonProvingandImprovingPrograms, Arc-et-Senans, France, pp. 167-93,July. 1975.
, "The EvolutionofPrograms: Automatic Program Modification, " IEEETransactionsonSoftwareEngineering, Vol. SE-3, No. 6, pp. 377-85, November 1977.
, "Inference Rules forProgramAnnotation," IEEETransactionson SoftwareEngineering, Vol.
SE-7, No. 2, pp. 207-22, March 1981.
Dijkstra, E. W.,ADisciplineofProgramming, Prentice-Hall, EnglewoodCliffs, N.J., 1976.
Evans, T. G., "A Program for the Solution ofGeometric-Analogy Intelligence Test Questions," in
SemanticInformationProcessing, M. L. Minsky (Ed.), MITPress, Cambridge, 1968.
Fikes, R. E.,Hart,P. E.,andNilsson, N.J., "LearningandExecutingGeneralizedRobotPlans,"ArtificialIntelligence, Vol. 3, No. 4, pp. 251-88, Winter 1972.
Floyd, R. W. , "Toward Interactive Design ofCorrect Programs," Proceedings ofthe Information ProcessingCongress, Ljubljana, Yugoslavia, pp. 7-10, August 1971.
Gentner, D., "Structure-mapping: A Theoretical Framework for Analogy," Cognitive Science, Vol. 7,
pp. 155-70, 1983.
Gerhart,S. L.
"KnowledgeaboutPrograms:AModelandCaseStudy,"ProceedingsoftheInternational
ConferenceonReliableSoftware, LosAngeles, Calif., pp. 88-95, April 1975.
Hesse, M.,ModelsandAnalogiesinScience, UniversityofNotreDamePress, NotreDame, Ind., 1966.
Holyoak,K.J., "AnalogicalThinkingandHumanIntelligence,"inAdvancesinthePsychologyofHuman
Intelligence, Vol. 2, R. J. Sternberg(Ed.), Erlbaum, Hillsdale, N.J., 1983.
Katz,S. M.,andManna, Z., "TowardsAutomaticDebuggingofPrograms,"ProceedingsoftheInternationalConferenceonReliableSoftware, LosAngeles, Calif., pp. 143-55, April 1975.
Kling, R. E., "Reasoningby Analogy with ApplicationstoHeuristic Problem Solving: A Case Study."
Ph.D. diss., StanfordUniversity, 1971.
Kowalski, R. A., "Predicate Logic as Programming Language," Proceedings ofthe IFIP Congress,
Amsterdam, pp. 569-74, 1974.
Manna, Z., andWaldinger, R. J., "Knowledgeand Reasoning in ProgramSynthesis," ArtificialIntelligence, Vol. 6, No. 2, pp. 175-208, Summer 1975.
, "The origin ofthe binary-search paradigm," Proceedings ofthe Ninth IJCAI, Los Angeles.
Calif., August 1985.
McDermott,J., "LearningtoUseAnalogies,"ProceedingsoftheSixthIJCAI,Tokyo,pp.568-76. August
1979.
Moore, J. A., and Newell, A., "How Can MERLIN Understand?" in Knowledge and Cognition,
L. Gregg(Ed.), Erlbaum, Hillsdale. N.J., 1973.
Rumelhart. D E.,andNorman, D. A., "AnalogicalProcessesinLearning," inCognitiveSkillsandTlicir
Acquisition, J. R. Anderson(Ed.), Erlbaum. Hillsdale. N.J.. 1981.
--- PAGE 435 ---
DERSHOWITZ 421
Sagiv,Y., "AStudyoftheAutomaticDebuggingofPrograms,"Master'sthesis,WeizmannInstituteofScience, Rehovot. Israel, 1976.
Sternberg, R. J.. Intelligence, InformationProcessing, andAnalogicalReasoning, Erlbaum, Hillsdale,
N.J., 1977.
Sussman, G. J., A ComputerModelofSkillAcquisition, AmericanElsevier, NewYork, 1975.
Ulrich, J. W., and Moll, R., "ProgramSynthesisby Analogy," Proceedingsofthe ACMSymposiumon
ArtificialIntelligenceandProgrammingLanguages, Rochester, N.Y, pp. 22-28, August 1977.
VanLehn, K., and Brown, J. S., "Planning Nets: A Representation for Formalizing Analogies and
SemanticModelsofProceduralSkills," inAptitude,LearningandInstruction: CognitiveProcess
Analyses,R.E.Snow,P.A.Federico,andW.E.Montague(Eds.),Erlbaum,Hillsdale,N.J., 1980.
Wensley, J. H., "A Class ofNon-analytical Iterative Processes," ComputerJournal, Vol. 1, No. 4,
pp. 163-67,January 1959.
Winston, PH., "Learningand Reasoningby Analogy," CommunicationsoftheACM, Vol. 23, No. 12,
pp. 689-703, December 1980.
"LearningbyAugmentingRulesandAccumulatingCensors,"ProceedingsoftheInternational
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois at
Urbana-Champaign, pp. 2-11, June 22-24, 1983. (An updated version ofthis paper appears as
chap. 3ofthisvolume.)
--- PAGE 436 ---
--- PAGE 437 ---
PART
FIVE
LEARNING BY
OBSERVATION
AND DISCOVERY
--- PAGE 438 ---
--- PAGE 439 ---
THE SEARCH FOR REGULARITY:
Four Aspects of Scientific Discovery
Pat Langley
Jan M. Zytkow
HerbertA. Simon
Gary L. Bradshaw
Carnegie-Mellon University
Abstract
Scientific discovery is a complex activity involving many different components. Our interest in discovery has led us to construct fourAI systems that address
differentfacetsofthisprocess. BACON .6focusesonthediscoveryofempiricallaws
thatsummarizenumericaldata. Thisprogramsearchesaspaceofdataandaspaceof
numericallaws, andincludesmethodsforpostulatingintrinsicpropertiesandnoting
common divisors. GLAUBER is concerned with discovering laws of qualitative
structure, suchasthehypothesisthatacidsreactwithalkalistoformsalts. Itsearches
aspaceofqualitativelaws, usingevaluationfunctionstofocusattentiononlawscoveringthe greatestnumberofobservedfacts. STAHLattemptstodeterminethecomponentsofsubstancesinvolvedinreactions,andhasbeenusedtomodelthereasoning
that led to the phlogiston theory. This system searches through the space ofcomponential models, using heuristics to make plausible inferences. The final system,
DALTON, isconcernedwithformulatingstructuralmodelsofchemical reactions. It
searchesthe spaceofpossible models, considering simple modelsbeforemorecomplexonesandusingaconservationassumptiontoconstrainpossibilities. Whileeach
ofthesediscovery systems is interesting in its own right, we are alsoexploring ways
in which the systems can interact to helpdirect each other's search processes.
--- PAGE 440 ---
426 CHAPTER 16: FOUR ASPECTS OFSCIENTIFIC DISCOVERY
Key Terms: scientific discovery, empirical laws, structural models, explanation, qualitative laws, theoryofacidsandbases, theory ofphlogiston, atomic theory
16.1 EXPLORING THE SCIENTIFIC PROCESS
Science is a multifaceted process, concerned both with the collection ofdata
and with their explanation. Within these two basic components additional subdivisionsexist. Thefirstprocessranges fromexploratorydatagatheringtothedesignof
specific experiments to test explicit hypotheses. Similarly, the explanatory process
ranges from the induction of simple empirical laws to the formulation ofcomplex
structural and process models. These components are not independent, since the
relation between data and theory is all-important in science. Still, the relations
betweenthe variouscomponents arecomplex, and ifthe scientific process iseverto
be understood, powerful methods mustbe used.
Inthischapterweapplythe methodologyofAItoexploretheprocessesofscientific discovery. Our goal is not to explain the details ofhistorical science, though
thehistoryofscienceis fascinatingandwewillcertainlydrawupon itinourefforts.
Rather, we hope to understand the processes by which scientific discoveries could
havebeenmade; inotherwords, ourgoalistodevelopmethodsthataresufficientfor
making such discoveries. Tothis end, we will draw uponthe AItechnique ofimplementingone'stheory asarunningcomputerprogram. Thus, wewilldevote much of
this chapter to describing particular AI programs and their behavior in specific
domains.
One ofthe central insights ofAI is that intelligence involves the ability to
search-andtheabilitytodirectthat search inprofitable directions. Search involves
the exploration ofsome space ofpossibilities, which Newell and Simon (1972) have
called aproblem space. Aproblem space is defined by two components: (1) one or
more initialstates fromwhich searchbegins; and (2) oneormoreoperators forgeneratingnewstatesfromexistingones. Takentogether, thesecomponentsdeterminea
setofstatesthatcanbe systematically searched. Inordertosearch such a space, one
alsoneedssomesearchcontrolschemetodirectsearchdownonepathoranother, and
some testtodetermine whenthegoal state has been reached. The notion ofproblem
spaces is important for each of our discovery systems, and we will describe each
system in terms ofits search characteristics.
We have organized this chapter around four AI systems that address different
aspects ofthe discovery process. First we describe BACON, a system that is concerned with discovering empirical laws ofa quantitative nature. We will begin with
BACON since it isthe firstdiscovery system we constructed, and many readers may
have some familiarity with it. More important, our recent work has been largely
motivated by BACON's limitations, so a consideration of the system's capabilities
andlimitswill layasolid foundationforthe restofthechapter. Afterthiswedescribe
GLAUBER, asystemthat isalsoconcerned withempirical laws, but inthiscase laws
--- PAGE 441 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 427
havingaqualitative form. Nextweexamine STAHL, aprogramthat infersthecomponents of substances from reactions, followed by DALTON, a system that constructssimplestructuralmodels. Sincethesesystemsaddresscomplementaryaspects
ofthe discovery process, we close the chapter by discussing some possible interactionsamongtheprograms,andthepossibilityofconstructinganintegrateddiscovery
system.
16.2 DISCOVERING QUANTITATIVE EMPIRICAL LAWS
Atthe very end ofthe eighteenth century and the beginning ofthe nineteenth
century, three fundamental discoveries were made that shaped the directions of
chemical research for several generations thereafter. The first ofthese was Proust's
statementofthe law ofconstantproportions (1799). Proustconductedapainstaking
analysis ofchemical compounds, finding that the ratio ofthe combining weights of
theconstituentelementswasalwaysconstantforaparticularcompound. The second
fundamental advance was Dalton's introduction ofthe law ofmultiple proportions
(1804). This law asserts that when two elements combine to form several different
compounds, theratiosoftheircombiningweightsarealways small integermultiples
ofone another. The third advance was Gay-Lussac's discovery ofthe law ofcombining volumes for gaseous reactions (1809), which states that gases combine with
each other in very simple ratios by volume.
These three discoveries provided the foundation for a quantitative theory of
chemical reactions, and ultimately led to the determination of relative atomic
weights. To some extent, Dalton's and Gay-Lussac's laws were motivated by an
atomichypothesis, buttherewere strongempiricalcomponentstothediscoveriesas
well. Although Proust'slawmightbedealtwith usingtraditionalcurve-fittingtechniques, the other laws involve more complex relations. Thus, the history of early
chemistry provides a challenging domain for testing AI methods for empirical discovery. Belowwedescribeadiscovery systemthatfocusesonquantitativediscovery,
andexamine its approachto finding thesechemical laws, as well as otherlaws from
the history ofscience.
16.2.1 Searching the Space of Data
WehaveexploredtheprocessofquantitativediscoverythroughBACON. 6, the
sixthinalineofprogramsnamedafterSirFrancisBacon(1561-1626). Thesystemis
givenasetofindependentanddependentvariables, andbasedondata it gathers, the
programgeneratesempiricallawsthatrelatethesevariablestoeachother. Inorderto
achievethisgoal, BACON variesoneoftheterms, lookingforrelationsbetweenthat
term and the dependent variables. Once a functional relation has been found, the
parametersinthatfunctionaregiventhestatusofdependenttermsatahigherlevelof
description. Thesystemthenrepeatsthisprocesswithadifferentvalueforthesecond
--- PAGE 442 ---
428 CHAPTER 16: FOURASPECTSOFSCIENTIFIC DISCOVERY
independentterm, arrivingatanewsetofparameters. When allvaluesofthesecond
independenttermhavebeenconsidered, BACON hasasetofhigher-leveldependent
values(basedontheparameters)associatedwitheachoftheindependentvalues. The
system finds a numeric relation between these terms, and again the parameters
become dependent values at the next higher level ofdescription. This process continuesuntilall the independenttermshavebeenincorporatedintoacomplexquantitative relationship.
BACONcanbeviewedas searchingtwodistinctproblem spaces-the spaceof
data andthe space oflaws. These searches interact in acomplex manner, butbefore
we examine this interaction, let us examine each of the search schemes independently, starting with search through the data space (see table 16-1). As noted,
BACON is provided with a set ofindependentterms, along with possible values for
each term. Using these values, the system generates a complete factorial design
involvingall combinations ofindependentvalues, andthen it examines thevalues of
the known dependentterms foreachcombination. BACON's generation ofall independentcombinationscanbeviewedintermsofsearch, withstatescontainingpartiallyspecifiedexperimentalcombinations. Theinitialstatehasnoindependentvalues
specified, whilegoalstatesincludevaluesforalloftheindependentterms. Theoperator for moving through this space inputs apartially specified experimental combination and decides on the value for one ofthe unspecified terms. Search control is
depth first, but since many combinations mustbe generated, the system must backtrack and explore many different paths.
Forinstance, supposeBACON isgiventhree independentterms-thepressure
Fona gas, thetemperature TofthatgasindegreesCelsius, andthequantity N ofthe
gas-alongwiththesingledependentterm V, thevolumeofthegas. Furthersuppose
that BACON istoldtoexamineTVwithvalues 1, 2, and3, 7withvalues 10, 20, and30,
andPwithvalues 1000, 2000, and3000. Inordertogenerateanexperimentalcombination,thesystembeginswithaninitialstateinwhichnovalueshavebeenspecified,
which we may representas [ ]. Next, the SPECIFY-VALUEoperatorapplies, generatinganewstateinwhichthevalueofWisdetermined, say [TV = 1]. Upon itsnext
application, the operatorgenerates athird state in which the value ofP is given, say
|7V = 1, T = 10]. When BACON applies the operator a third time, the complete
Table 16-1: BACON'sdata-gatheringmethodviewedintermsofsearch.
Initialstate:Thenullcombination
Goalstate: Acompleteexperimentalcombinationofindependentvalues
Intermediatestate: A partialcombinationofindependentvalues
Operators:
Specify-value: Specifiesthevalueofanundetermined independentvalue
Heuristics/Evaluationfunctions: None; search isexhaustive
Searchcontrol: Exhaustivedepth-firstsearchwithbacktracking; generatesallgoal states
--- PAGE 443 ---
LANGLEY, ZYTKOW, SIMON, AND BRADSHAW 429
experimental combination [N = 1, T = 10, P = 1000] is generated, and the program can examine the volume associated with this combination.
However, if BACON is to gather sufficient data on which to base its laws, it
must continue the search. Accordingly, the system backs up to the previous state
[N = i,T= 10] and applies the operator with different arguments, generating the
second goal combination, [N = 1, T = 10, P = 2000]. This allows a second value
ofthe volume to be observed and associated with an experimental combination. At
thispoint,thesystemagainbacktracksto [N = \,T = 10] andthengeneratesathird
goal state, [N = 1, T = 10, P = 3000], thus gathering a third observation of the
volume. Having exhausted the potential values of T, BACON then proceeds to back
up two steps to [N = 1]. From here it generates the states [N = 1, T = 20] and
finally [N = 1, T = 20, P = 1000], another complete experimental combination.
BACON continues in this fashion until it has generated all experimental combinationsoftheindependentvaluesit wasgivenandobservesthevolumesassociatedwith
each combination. Figure 16-1 shows the tree that results from this search through
the space ofdata; the numbers oneach state representtheorderinwhichthatstate is
generated.1
16.2.2 Searching the Space of Laws
Now letusturntoBACON.6 's methodforsearchingthe spaceofnumericlaws
(seetable 16-2). Givenasetofindependentvaluesandacorresponding setofdependent values, the system attempts to find one or more laws that predict the observed
valuesasaccuratelyaspossible. Inordertoachievethisgoal, BACON requiressome
information abouttheform that plausible laws may take. For instance, forthe independent termxand the dependent term y, the user may tell the program to consider
laws having the form y = ax2 + bx + c, as well as those with the form
sinfj) = ax + b. These formsdefinethespaceofthelawsthat BACON willexplore
in its attempt to summarize the observed data.2
Given a set offorms, BACON generates a set of initial states from which to
beginthe search. This isdoneby insertingtheabstractparametersineachformwith
the values 1, 0, or - 1. For simplicity, letusconsideronly forformy = ax + b and
examine the resulting initial states. In this case, there are 32 = 9 possible initial
states: [a = \,b = 1], [a = l9 b = 0],[a = \,b = -\],[a = 0,b = I], [a = 0,
b = 0], [a = 0, b = -1], [a = -\, b = 1], [a = -1, b = 0], and
'An earlier version ofBACON (Langley, Bradshaw, and Simon, 1982) was capable ofmodifying this
searchbasedondiscoveriesithadmade. Thecurrentsystemdoesnotincludethisability.
2EarlierversionsofBACONwererestrictedtoparticularforms. Forinstance, BACON.5(Langley. Bradshaw,andSimon, 1982)onlyconsideredlawsoftheformy' = cvc2 + bx + c, where/tookonsmallintegral values, andthuswaslessflexiblethanthecurrentsystem.
--- PAGE 444 ---
430 CHAPTER 16: FOURASPECTSOFSCIENTIFIC DISCOVERY
A/ = 1; I = 10; P = 1000
N = 1; T = 10 AT = 1; T = 10; P = 2000
AT = 1; r = 10; P = 3000
N = 1; r = 20; P = 1000
AT = 1 A/ = 1; r = 20 Af = 1; T = 20; P = 2000
JV = 1; T = 20; P = 3000J
AT = 1; T = 30
W = 2
A/ = 3
Figure 16-1: BACON'ssearchthroughthespaceofdata.
Table 16-2: BACON'slaw-findingmethodviewedintermsofsearch.
Initialstates: Setsofparametersconsistingof1,0, and 1
Goalstate: A setofparametersthatmaximallypredictstheobserveddata
Intermediatestates: Setsofparametersthataccountforsomeofthedata
Operators:
Add/Subtract: Addsorsubtractsfromoneparametervalue
Evaluationfunction: Selectstatesthatleadtohighercorrelations, thusbetterpredictingthedata
Searchcontrol: Hillclimbingusingabeamsearch
--- PAGE 445 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 431
[a = -\,b = - 1]. These parameters are chosen because they are evenly distributedthroughoutthe spaceofparameters, sothatthebestsetofparameters shouldbe
near one of them. Starting from these idealized parameters, BACON attempts to
determine the optimum state through aprocess ofsuccessive approximation.
BACON.6employs asingleoperatorformovingthrough the spaceofparameters. This operator accepts one ofthe current sets as input, and generates a new
parameter combination by adding or subtracting some number from one of the
existing values. The amount that is added or subtracted decreases as the system's
search progresses. For instance, the system begins by adding/subtracting 0.5 from
the various values. On the second step, this amount is reduced to 0.25, and so on.
BACON's strategy for exploring the parameter space is best described as a beamsearchversionofhillclimbing. Attheoutset,the/Vbeststatesareselectedforfurther
attention, and the remainder are abandoned. The addition/subtraction operator is
thenappliedtothese N statesinallpossibleways, generatinganewsetof M states. Of
these N + M states, the N best are selected (some ofthe originals may be retained),
andtheprocessisrepeated. Whennoneofthenewstatesshowanyimprovementover
the preceding states, the search is terminated.
Inselectingsomestatesinfavorofothers, BACON considerstheabilityofeach
parameter set to predict the observed values. In order to estimate this ability, the
systemsubstitutestheparametervaluesintotheformofthelawandcomputesthecorrelationbetweentheobservedindependentanddependentvalues. Ahighcorrelation
meansthattheparameterspredictthedatawell, whilealowcorrelationimpliesthat
the state's predictive ability is poor. Since correlations are insensitive to absolute
values, only the relative values oftheparameters are important. It is forthis reason
thattheinitialvaluesof1, 0, and - 1 wereableto "cover" thespaceofparameters. In
any case, this evaluation function is used to direct search toward sets ofparameters
that account foras much ofthe data as possible.
Sincethis search strategy uses the data only to testhypotheses and notto generate them, it is robust with respect to numerical noise. BACON.6 is guaranteed to
find some law that summarizes regularity in the data, even ifthis regularity is only
partial. Ofcourse, whenthedataareverynoisy, theremay notbeone setofparameters(orevenoneformoflaw)thatisclearlysuperiortoitscompetitors. Insuchcases,
the program returns a number oflaws. One ofBACON's interesting features is that
the system carries out the same amount ofsearch regardless ofthe amount ofnoise
occurring in the data.
16.2.3 Relation Between the Search Methods
Now that we have examined BACON's two search schemes in isolation, it is
timetoconsidertheirrelationtooneanother. Basically,thesystem'ssearchforlawsis
embedded within its search for data. To understand this statement, let us return to
--- PAGE 446 ---
432 CHAPTER 16: FOUR ASPECTS OFSCIENTIFIC DISCOVERY
figure 16-1,
whichpresentstheorderinwhichBACON
gathersitsdata. Considerthe
three highest terminal nodes, [N = 1, T = 10, P = 1000], [N = 1, T = 10,
P = 2000], and [N = 1, T = 10, P = 3000]. For each ofthese combinations, the
system observes some value ofthe dependent volume V. When all three values have
been noted, BACON attempts to find a law relating them to the three values ofthe
pressureP, usingthesearchstrategyjustdescribed. Theresultofthissearch isoneor
moreparameters (let us assumethatone law is obviouslybetterthanall others), and
these are stored at the next higher state in the data search tree. For instance, for
P = 1000, 2000, and 3000, the observed values for Kwould be 2.35, 1.18, and 0.78.
Forthesedata, the form V~] = aP + bgivesthebestfit, withtheparametervalues
a = 0.000425and/? = 0. Thevalueforaisstoredwiththestate[N = l,T = 10] for
futureuse;however,thesystemtreats asaspecialvalue, sotheresultforbwouldnot
be stored.
Upon observingasecond setofvalues, BACON attemptsto find a second law.
For the experimental combinations [N = 1, T = 20, P = 1000], [N = 1, T = 20,
P = 2000],and[N = 1,7 = 20, P = 3000],thesystemfindsthevalues2.44, 1.22,
and0.81 forthevolume. Againtheform V~l = aP + bprovesuseful, thistimewith
the values a = 0.000410 and b = 0, and again these values are stored at a higher
state, inthiscase [N = 1, T = 20] . Very similareventsoccurwhenthevalueofTis
30, giving the parameter values a = 0.000396 and b = 0, which are stored with
[N = 1, T = 30]. Atthis point, BACON hasthree sets ofvalues forthehigher-level
dependenttermsaandb. Moreover, these valuesarestoredwiththeabstractedcombinations [N = 1, T = 10], [N = 1, T = 20], and [N = 1, T = 30]. Given the
values 10, 20, and 30 for T, along with the values 0.000425, 0.000410, and 0.000396
for a, the program attempts to find a law relating these two terms. In this case, it
finds the form a"1 = cT + d to best summarize the data, with c = 8.32 and
d = 2271.4. These values are stored with the next higher state in the data tree,
|7V 1], for future use.
Thisprocess iscontinuedas moredataaregathered. First BACON findsthree
additional laws relating the variables P and V. Based on the resulting parameter
values, the forma-1 = cT + disagain foundtobeuseful, thistime with c = 16.64
and d = 4542.7. These higher-level dependent values are stored with the state
[N = 2]. Similar steps leadtothree more laws ofthe form V~ ' = aP + b and then
to a third law ofthe form a~] = cT + d. This time BACON finds the best fit with
c = 24.96andd = 6814.1 andstoresthesevalueswith [/V = 3]. Now thesystemhas
threevaluesofN, alongwiththreeassociatedvaluesofbothcandd. Foreachofthese
dependent terms, BACON searches the space of laws, arriving at the two laws
c = eNandd = fN, withe = 8.32 and/ = 2271.4. Thesetwoparameter values are
stored at the initial data state and represent invariant parameters that are not
[ ]
conditional on any independent terms. By substituting these values into the forms
found at each level in BACON's search, we arrive at the relation
V ' = (8.32AT + 2271.4/V) 'P. This expression can be transformed into
PV = 8.32/VT + 2271.4/V if we divide through by P and invert the equation. If we
--- PAGE 447 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 433
then factor out 8.32 N on the right side of the relation, we arrive at
PV = $.32N(T + 273), which isthe standard form ofthe ideal gas law. Notethat in
some sense, BACON has determined that the Celsius temperature scale is insufficientfordescribingtherelationamongthefourterms, andhaseffectively introduced
the Kelvin scale by adding 273 to the observed Celsius values.
From this example, we see that BACON carries outas many searches through
the law space asthere are nonterminal states inthe data space. Figure 16-2 summarizestheparametervalues resultingfromeachofthese searches, along withthedata
statesatwhichtheyare stored. Thenumbernexttoeach state representstheorderin
which that law was discovered. Note that this order is different from the order in
/ V = 2.36
4 a = 0.000425 2 V = 1.18
y/ ^^•4
V = 0.78
^ 5 V = 2.44
c d = = 2 8 2 .3 7 2 1.4 8 a = 0.000410 ^^7 6 V = 1.22
V = 0.81
. .
a - 0.000396 \.
40 26
e - 8.32 c = 16.64
f = 2271.4 d = 4542.7
c - eN = cT + d aP observed
d = fN
Figure 16-2: BACON 'srediscoveryoftheidealgaslaw.
--- PAGE 448 ---
434 CHAPTER 16: FOURASPECTSOFSCIENTIFIC DISCOVERY
which the data space itselfwas searched. In an important sense the search for data
BACON
providesstructureto 'ssearchforlaws, sinceitprovidesbothdirectobservations and aplaceto storeparameters sothey canbeusedas dataatlater stages. This
process is somewhatsimilartoRosenbloom'smodelofthechunkingprocess(1983).
In this cognitive simulation, a goal hierarchy provides the top-down control that
determines the types of chunks that should be formed. However, a data-driven
learning mechanism determines the particular chunks that are acquired from the
bottomup. ThusBACON'ssearchthroughthedataspacecanbeviewedasproviding
top-down constraints on the types oflaws that will be discovered (e.g., which variables are related), whilethe system muststill searchthroughthe resulting law space
to determine the particular laws that best summarize the data.
We should mention in passing that once BACON discovers that a particular
form oflaw is useful in one context, it uses that information to constrain search in
similar contexts. For instance, when the system finds that only the form
V~x = aP + b is useful when [N = \, T = 10], it considers only this form when
[N = l,T = 20],[N = 1, T = 40], andsoforth. Inaddition, sinceitfoundb = 0,
this parameter was removed from the form, leaving the simplified expression
V~] = aP Inotherwords, BACON redefinesitsproblemspaceinthelightofitsprevious experience, so that considerably less search results. Now that we have examined BACON's basic methods for discovering empirical laws, let us examine some
additional methods that let it deal with the chemical domain.
16.2.4 Intrinsic Properties and Common Divisors
While BACON's basic methods are useful for discovering relations between
numerical terms, they cannot be used to relate nominal, or symbolic, independent
termstonumericdependentvariables, andthisispreciselythesituation inwhichthe
early chemists found themselves. For instance, the independent terms in Proust's,
Dalton's, and Gay-Lussac's chemical experiments were the elements or compounds
involved, while the dependent terms were numerical measures such as weight or
volume. In such cases, BACON defines intrinsic properties that take on numeric
values and then associates these properties with the nominal terms.
Letusconsiderthe role ofintrinsic properties in BACON's rediscovery ofthe
earlychemical laws. Givencontroloverthesubstancesenteringand resulting froma
reaction, as well as the weight ofthe first substance that is used, the system gathers
datalikethoseshown intable 16-3. Uponvaryingtheamountofoxygenusedtoform
nitric oxide (NO), the programdiscovers that the twoweights \\\ and us are linearly
related with a slope of 1.14 and an intercept ofzero. Upon varying the output oithe
reaction, BACON.6 then examines the weight relations for the compound nitrous
oxide (N 2 0). In thiscase, the law is also linear, but the slope has changed to0.57. A
similar result is obtained when the system examines the values for nitrogen dioxide,
and in this case the slope is 2.28.
--- PAGE 449 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 435
Table 16-3: Determiningthecombiningweightsforreactions.
Element, Elements Compound W, W W IW,
2 2
Nitrogen Oxygen NO 1.0. 1.14 1.14
Nitrogen Oxygen NO 2.0 2.28 1.14
Nitrogen Oxygen NO 3.0 3.42 1.14
Nitrogen Oxygen N 2 1.0 0.57 0.57
Nitrogen Oxygen N 2.0 1.14 0.57
Nitrogen Oxygen N 2 3.0 1.71 0.57
Nitrogen Oxygen N0 1.0 2.28 2.28
Nitrogen Oxygen N0 2.0 4.56 2.28
Nitrogen Oxygen N0 3.0 6.84 2.28
The slopesthat BACON .6finds intheseexperiments areclosely relatedtothe
weight ratios found by Proust. Having found these ratios, the program stores its
results at a higherlevel ofdescription, as shown intable 16-4, andtreats these summaries as data. The table also includes the results obtained for two reactions of
oxygen and carbon. In this case, the system finds three nominal independent terms
and a single numeric dependent variable, so it defines an intrinsic property (sayp)
whose values are associated with the three nominal values under which they occur.
Thus, the value of/? forthe triple nitrogen/oxygen/nitric oxide would be setto 1.14,
the value for nitrogen/oxygen/nitrous oxide would be 0.57, and the value for
nitrogen/oxygen/nitrogen dioxide would be 2.28. As stated, these intrinsic values
simplystoreanalready-knownfact, andinthissensetheyaretautological. However,
theycanberetrievedinfutureexperimentsinvolvingthesamechemicalsandusedto
make predictions orto discover new empirical laws.
As we have seen, Proust's insight about combining weights laid the groundworkforDalton'slawofmultipleproportions. Thislawstatedthatincaseswheretwo
elements combine to form different compounds, the ratios of their combining
weights were always small integer multiples of one another. BACON includes a
method that lets it discover just such a relation in the data from table 16-4. This
Table 16-4: Notingcommondivisorsforchemical reactions.
Element, Element Compound W IW, P W/WP
2 2 2 }
Nitrogen Oxygen NO 1.14 2 0.57
Nitrogen Oxygen N 2 0.57 1 0.57
Nitrogen Oxygen N0 2.28 4 0.57
Carbon Oxygen CO 1.33 1 1.33
Carbon Oxygen C0 2.66 2 1.33
--- PAGE 450 ---
436 CHAPTER 16: FOUR ASPECTSOFSCIENTIFIC DISCOVERY
methodoperateswheneverthesystemdefinesanewintrinsicproperty, andexamines
thevaluesofthenewpropertytoseeifthey(ortheirinverses)haveacommon
divisor.
This technique is especially useful when intrinsic values are associated with a conjunctionofnominalvalues, asoftenoccursinchemistry. Wehavedescribedboththe
intrinsic property and common divisor methods at length in earlier papers (Bradshaw, Langley, and Simon, 1980; Langley, Bradshaw, and Simon, 1983).
In this case, BACON notes that 1.14, 0.57, and 2.28 have the common divisor
0.57, and wouldreplacethese intrinsicvalues withtheircorresponding integers2, 1,
and4. Inaddition, theprogramdefinesahigher-level intrinsicpropertybasedonthe
divisors it finds in different situations, and associates the divisors with those cases.
Thus, the common divisor0.57 would be associated with the nitrogen/oxygen pair,
while the divisor 1.33 wouldbe associated with carbon and oxygen. These relations
areformallyequivalenttoDalton'slawofmultipleproportions. BACON takesasimilarpath in discovering Gay-Lussac's commondivisors forcombining volumes, and
has even arrived at the correct relative atomic weights for hydrogen, oxygen, and
nitrogen from data similartothose intable 16-3. Thus, BACON 's discovery mechanisms account for the major quantitative laws found by chemists in the early nineteenth century. Note that neither the intrinsic property method nor the common
divisor method involve any significant search themselves. Rather, their role is to
transform symbolic data into numeric data, so that BACON 's law-finding method
can be used todiscover relationships.
16.2.5 Comments on BACON.6
In the preceding pages, we have described BACON's methods for gathering
data, discovering numeric laws, andpostulating new properties. All in all, BACON
providesan interestingand useful accountofthe discovery ofquantitative empirical
laws. However, the system leaves some important questions unanswered. For
example, how do scientists decide which variables to employ in their experiments?
Similarly,howdotheyusetheirnewlydiscoveredlawsoncetheyhavefoundthem?In
BACON, the relevant variables are provided by the programmer, and the laws are
simply printed on a terminal screen. One can imagine a version of BACON with an
improveduserinterface, servingasascientist'saideinanalyzingdataandfulfillinga
usefulfunctionwhilestillrequiringitsusertodesignitsinputandinterpretitsoutput.
This isonedirection in whichthe system mightbeextended, and such an interactive
version could be very useful in some areas ofscience.
However, ifone'sgoal istounderstandthenatureofscientificdiscovery, thena
more seriousanswertothe above questions is required. For instance, we know from
the history of science that empirical laws eventually lead to theories and explanations, andBACON haslittletosayaboutsuchaspectsofdiscovery. Wealso know that
even vague theories can have important impacts on the data one gathers. This suggeststhat we will find answerstoboth questions only by studying other facets ofthe
--- PAGE 451 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 437
discovery process. Although constructing AI models of these components would
undoubtedly be interesting even in isolation, the true advantage will come from
exploring the interrelations among different forms of discovery. Our long-range
goal, then, shouldbetounderstandcomponents ofthediscoveryprocess whoseoutputs can be used as BACON's inputs and to uncover other components that can
employ BACON's outputs astheirinputs. Inthe remainder ofthis chapter, we focus
on three different models ofdiscovery that we have constructed to this end, and we
close with some speculations on possible interactions among the various systems.
16.3 DISCOVERING QUALITATIVE EMPIRICAL LAWS
Uponexaminingthehistory ofscience, one findsthatthediscovery ofquantitative laws is generally preceded by the discovery of qualitative relations. For
instance, early physicists noted that colliding objects tended to change velocities
before they determined the exact form of this relationship. Similarly, plant and
animal breeders knew that certain traits were passed on to offspring long before
Mendel formulated the quantitative principles ofinheritance. One ofthe best examplesofthistrendmaybefoundinthehistoryofchemistry, whereearly scientistsdiscovered qualitative laws of reaction decades before numeric relations were determined. In particular, the history of the theory of acids and bases provides useful
insights into the discovery ofqualitative empirical laws.
By the seventeenth andeighteenth centuries, chemists had made considerable
progress inclassifying substances on the basis ofqualitativeproperties. During this
period, researchersfocusedonfeaturessuchasthetasteandtextureofsubstances, as
well as the interactions between substances. Thus, they knew that the substance we
now call hydrochloric acid had a sour taste, and that it combined with ammonia to
formNH4CI (thoughthestructureofthiscompoundwasnotknown). Moreover, they
knewthat sulfuric acid alsotasted sour, and that it combined with ammoniato form
(NH 4)2 S0 4. From such facts as these, the early chemists defined classes such as
acids, alkalis, and salts and formulated laws involving these terms, such as "acids
tastesour" and "acidsreactwithalkalistoformsalts." Eventually, theycametoview
both alkalis and metals as special cases ofthe more abstract concept base, and they
arrivedatthemoregenerallaw "acidsreactwithbasestoformsalts." Althoughsome
exceptionstothesestatementswereknown, chemistsfoundthelawssufficientlygeneral to use in making predictions as well as in classifying new substances. We shall
seethatthetwoprocesses-definingclasseslikeacidandalkaliandformulatinglaws
involving these classes-play a central role in the qualitative discovery process.
--- PAGE 452 ---
438 CHAPTER 16: FOURASPECTSOFSCIENTIFIC DISCOVERY
16.3.1 The GLAUBER System
Inoureffortstounderstandtheprocessofscientificdiscovery, wehaveimplemented GLAUBER,3 an AI system that formulates qualitative empirical laws. The
program is named after Johann Rudolph Glauber (1607-1670), a seventeenthcentury German chemist who played an important role in developing the theory of
acidsandbases. Table 16-5 summarizesGLAUBER intermsofsearchconcepts. The
system accepts as inputa setofqualitative facts, which are represented interms ofa
simple framelike structure. Each fact contains apredicatethat specifies the type of
factitis, alongwithoneormoreattribute-valuepairs. Forexample, thefactthatHC1
reactswith NH 3 toformNH 4 C1wouldbestoredas(reactsinputs {HO NH 3} outputs
{NH 4C1}). Herethepredicateis reacts, theattributesareinputsandoutputs, andthe
sets {HC1NH 3} and {NH 4 C1} arethevaluesfortheseattributes.4Theknowledgethat
HC1 tastes sour would be stored as (has-quality object {HC1} taste {sour}). In this
casethe values are enclosed inbraces forconsistency with otherpredicates (such as
reacts), which may have multiple symbols as values.
GLAUBER'S goal is to transform these facts into a set of qualitative laws
havingthesameformastheoriginalfacts,butinwhichspecificsubstanceshavebeen
replacedbyabstractclasses, suchastheconceptsofacidandalkali. Inaddition,these
classesmusthaveanassociatedlistofmembers; forinstance, HC1andH 2 S0 4 would
be examples ofacids, while NaOH and KOH would be members ofthe alkali class.
Table 16-5: GLAUBERviewedintermsofsearchconcepts.
Initialstate: A listoffactscontainingonlyconstantterms
Goalstate: A listoflawsrelatingclasses, alongwithdefinitionsofthoseclasses
Intermediatestates:
A listoflawsrelatingsomeclasses, alongwithdefinitionsofclasses; somefactsremain
Operators:
Form-law: Definesaclassandsubstitutesitintofacts
Determine-quantifier: Specifiesexistentialoruniversalquantifiers
Heuristics:
ForForm-law: Selectthatobjectoccurringinthemostanalogousfacts
ForDetermine-quantifier: Quantifyuniversally ifthedatajustify it
Searchcontrol: Best-firstsearchwithnobacktracking
^ThecurrentversionofGLAUBER differsfromtheearlierversiondescribedbyLangley.Zytkow.Simon,
andBradshaw (1983). Althoughthestatedescriptionsareverysimilarinthetwosystems,boththeoperatorsandthesearchcontroldifferconsiderably.
4GLAUBERknowsthattheorderofsymbolscontainedinasetdoesnotmatter,sothat(reactsinputs{NFL
HO} outputs {NH4CI})wouldbeconsidered identical totheabovefact.
--- PAGE 453 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 439
Takentogether, thequalitativelawsrelatingclassesandtheextensionaldefinitionsof
theseclassesletonepredicttheoriginalfacts, alongwithotherfactsthathavenotyet
been observed.
GLAUBER'Stwooperatorsareconcernedwithtransformingtheoriginaldata
into such laws and classes. The first ofthese operators, FORM-LAW, inputs a set
of facts having the same predicate and at least one common argument; it replaces
thesewithasinglelawinwhichsomeargumentshavebeenreplacedbyaclassname,
and itdefineseachofthenewclasses intermsoftheirmembers. Forexample, given
the two facts (reacts inputs {HO NaOH} outputs {NaCl}) and (reacts inputs
{HN0 3 NaOH} outputs {NaN0 3}), the FORM-LAW operator would define two
classes, sayxand y, and replace the facts with the law (reacts inputs {x NaOH} outputs {>'}). The operator would also note that HC1 and HN0 3 are members of the
newlycreatedclassx, whileNaClandNaN0
aremembersoftheyclass. Finally,the
FORM-LAW operator iterates through the current set of facts and laws, replacing
occurrencesofthesesubstanceswiththeirclassnames. Forinstance, ifthefacts(hasquality object {HO} taste {sour}) and (has-quality object {HNO3} taste {sour})
were known, they would be replaced by the pattern (has-quality object {x} taste
{sour}).
When GLAUBER formulates a new set of laws, the system must decide the
appropriate level of generality for each law. To this end, the second operator
(DETERMINE-QUANTIFIER) iterates through the set of laws and determines
whethereach class mentioned in a law shouldbe existentially or universally quantified. Ifanexistentialquantifierissettledon,thenthelawisinterpretedasholdingfor
onlyasinglememberoftheclass. Ifauniversalquantifierisselected,thelawisinterpretedasholdingforallmembersoftheclass. Ifasingleclassisintroduced,thenthis
classisuniversallyquantifiedintheresultinglaw; inthiscase,thelevelofquantificationisnotanissue, sincethisistautologicallydeterminedbythemannerinwhichthe
classesweredefined. However, ifTVclassesareintroduced,thenAversionsofthelaw
result, each containing one universally quantified class and with the quantifiers for
the remaining classes undetermined. For instance, inthe above example, two variations onthe reactionlaw wouldbe formulated - Vjt?y(reacts inputs {x NaOH} outputs {v}) and Vjc?v (reacts inputs {x NaOH} outputs {y}). 5 The first ofthese states
that all members ofclassx react with at least one memberofthe classy; the second
states that all members ofthe class y can be formed by at least one member ofx in
reaction with NaOH. The first quantifier in each law follows from the class definition, but the second quantifier must be determined empirically.
5Inthischapter,expressionsoftheform V;tP{x)areintendedasshorthandforlongerexpressionsofthe
form \fy\y ExP{y),wherexisaclassnameandvisamemberofthatclass. Expressionsoftheform 3x
P(x) shouldbeinterpretedinasimilarfashion.
--- PAGE 454 ---
440 CHAPTER 16: FOUR ASPECTS OFSCIENTIFIC DISCOVERY
A similar issue arises when the FORM-LAW operator generates additional
laws by replacing substances with classes in other facts. In these cases, all of the
quantifiersmustbetestedagainstobservations. Forexample, thepattern(has-quality
object {x} taste {sour }) mightholdforallmembersofx, orforonlyafewmembersof
this class. Thus, the DETERMINE-QUANTIFIER operator examines the known
factsanddecidesontheappropriatequantifier. Ifmorethanoneclassisinvolved, the
possibility ofmultiple forms ofthe law must be considered. Thus ifa pattern were
formedby substitutingbothxandyformembers oftheseclasses, GLAUBER might
decide on a single law in which both were universally quantified, a single law in
which both were existentially quantified, ortwo laws involving both existential and
universal quantifiers.
Once GLAUBER has applied the FORM-LAW and DETERMINEQUANTIFIERoperators, ithas a revised setoffacts andlawstowhich these operatorscanbeappliedrecursively. The FORM-LAW operatormayapplytolawsaswell
astofacts, providedtheselawshaveidenticalquantifiers. Forexample, giventhetwo
laws Vjc3v (reacts inputs {x NaOH} outputs {>'}) and \fx3y (reacts inputs {xKOH}
outputs {y}), this operator would generate the more abstract pattern Vjc3yv (reacts
inputs {x u} outputs {w}). In addition, it would define the class u to have the members NaOH and KOH and define the class w with the classes y and z as subsets.
DETERMINE-QUANTIFIERwouldthenproceedtodecideonthegeneralityofthis
pattern, and the process would be repeated on the revised set of facts and laws.
GLAUBER continues this alternation between finding laws and determining their
generality until the goal state has been reached-this is a set ofmaximally general
laws that account for as many ofthe original facts as possible.
Using itstwooperators, GLAUBER carries outabest-firstsearch through the
spaceofpossiblelawsandclasses. Thesystem'ssearchcontroldoesnotincludebackupcapability, sinceitsevaluationfunctionsare sufficientlypowerfultodirectsearch
down acceptable paths. In determining which law to formulate (and thus which
classestodefine), GLAUBER considersall knownsubstancesandclassesandselects
that symbol occurring in the largest number ofanalogous facts. Thus, iftwo facts
having the reacts predicate were found to include NaOH in the inputs slot, then
NaOHwouldreceiveascoreoftwo, unlessitoccurredinsomeothersetoffactsmore
often. Inthecaseoflaws, GLAUBER usesthetotalnumberoffactscoveredbythose
laws. GLAUBER indexes its facts and laws in terms of their arguments, so these
scoresareeasilycomputedforeachsubstanceandclass. Oncethishasbeendone, the
system applies the FORM-LAW operator to those facts containing the highest
scoring symbol, with the constraint that existentially quantified classes are not
considered.
In determining the placement of universal and existential quantifiers.
GLAUBER examines the facts (or lower-level laws) on which the current law is
based. The system generates all ofthe laws/facts that would be produced b> a universal quantifier lor a given class, and if enough of these have been observed (or
--- PAGE 455 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 441
inferred),thentheuniversalquantifierisretainedforthatclass;otherwiseanexistential quantifier is used. Thus, the system can be viewed as looking ahead one step in
order to determine which move is most desirable. A certain percentage ofthe predicted facts must be observed before GLAUBER generalizes over a class; this percentageisspecifiedbytheuser. Theprograminterpretsmissingfactsasunobserved;
the current system cannot handle disconfirming evidence, such as ~3salt (reacts
inputs {HC1 HN0 3} outputs {salt}).
16.3.2 Rediscovering the Concepts of Acids and Alkalis
Now that we have described GLAUBER in the abstract, let us examine its
behaviorgivenaparticularsetoffactsasinput. Thesefactsarepresentedatthetopof
table 16-6 and are very similar to facts known by seventeenth-century chemists
Table 16-6: StatesgeneratedbyGLAUBER indiscoveringacidsandalkalis.
InitialstateSI:
(reactsinputs {HC1NaOH} outputs {NaCl}) (has-qualityobject {NaCl} taste {salty})
(reactsinputs {HC1KOH} outputs {KC1}) (has-qualityobject {KC1} taste {salty})
(reactsinputs {HN0 3 NaOH} outputs {NaN0 3}) (has-qualityobject {NaN0 3} taste {salty})
(reactsinputs {HN0 3 KOH} outputs {KN0 3}) (has-qualityobject {KN0 3} taste {salty})
(has-qualityobject {HC1} taste {sour}) (has-qualityobject {NaOH} taste {bitter})
(has-qualityobject {HN0 3} taste {sour}) (has-qualityobject {KOH} taste {bitter})
FIND-LAWandDETERMINE-QUANTIFIERleadtostateS3:
salts: {NaCl, KC1, NaN0 3, KN0 3}
3 salt(reactsinputs {HC1 NaOH} outputs {salt}) (has-qualityobject {HC1} taste {sour})
3salt(reactsinputs {HC1KOH} outputs {salt}) (has-qualityobject {HN0 3} taste {sour})
3salt(reactsinputs {HN0 3 NaOH} outputs {salt}) (has-qualityobject {NaOH} taste {bitter})
3salt(reactsinputs {HN0 3 KOH} outputs {salt}) (has-qualityobject {KOH} taste {bitter})
Vsalt(has-qualityobject {salt} taste {salty})
FIND-LAWandDETERMINE-QUANTIFIERleadtostateS5:
salts: {NaCl, KC1, NaN0 KN0
3, 3}
acids: {HC1, HN0
Vacid 3 salt(reactsinputs {acidNaOH} outputs {salt}) (has-qualityobject {NaOH} taste {bitter})
Vacid 3 salt(reactsinputs {acidKOH} outputs {salt}) (has-qualityobject {KOH} taste {bitter})
Vsalt(has-qualityobject {salt} taste {salty})
Vacid(has-qualityobject {acid} taste {sour})
FIND-LAWandDETERMINE-QUANTIFIERleadtostateS7:
salts: {NaCl, KC1, NaN0 KN0
3, 3}
acids: {HC1, HN0
alkalis: {NaOH, KOH}
Valkali Vacid 3 salt(reactsinputs {acidalkali} outputs {salt})
V salt(has-qualityobject {salt} taste {salty})
Vacid(has-qualityobject {acid} taste {sour})
Valkali (has-qualityobject {alkali} taste {bitter})
--- PAGE 456 ---
442 CHAPTER 16: FOUR ASPECTS OF SCIENTIFIC DISCOVERY
before they formulated the theory ofacids and bases. As we shall see, GLAUBER
arrivesata setoflawsandclasses very similartothose proposedby theearly chemists. Thedatainthetableareintentionallysimplifiedforthesakeofclarity. However,
wehavetestedthe system onlargersetsofdataas wellasonsetswithless regularity.
Giventhetwelvefacts as inputs, GLAUBER beginsby examiningthe symbols
used as arguments in the propositions and determining which ofthese occur in the
greatest number ofanalogous facts. It notes that the symbols HCL, HN0 3, NaOH,
andKOH areeachargumentsoftheinputsslotfortwofactsinvolvingthereactspredicate. Similarly, thesymbolssourandbittereachoccurasargumentsofthetasteslot
intwohas-qualityfacts. However, thehighest scoring symbol issalty, whichoccurs
infourhas-qualityfactsasthevaluefortaste. Asaresult,thesefourfactsarereplaced
by the pattern (has-quality object {salt} taste {salty}), which has the same form as
the original propositions, but in which the differing values ofthe object slot have
been replacedbythe class namesalt. Also, the four substances NaCl, KC1, NaN0
and KN0 are stored as members ofthe new class.
In addition to proposing this hypothesis, the FORM-LAW operator generates
fouradditionalpatternsby substitutingthesymbolsaltformembersofthisclassinto
otherfacts. Thus, the facts (reacts inputs {HC1 NaOH} outputs {NaCl}) and (reacts
inputs {HC1 KOH} outputs {KC1}) are replaced by the hypotheses (reacts inputs
{HC1 NaOH} outputs {salt}) and (reacts inputs {HC1 KOH} outputs {salt}). Similarly, the facts (reacts inputs {HN0 3 NaOH} outputs {NaN0 3}) and (reacts inputs
{HNO3 KOH} outputs {KNO3}) arereplacedby (reacts inputs {HNO3 NaOH} outputs {salt}) and (reacts inputs {HNO3 KOH} outputs {salt}). Although the first of
thesepatterns is guaranteedtobe universally quantifiedby the manner in which the
salt class was defined, the generality ofthe other laws must be empirically determined. For example, ifthe hypothesis (reacts inputs {HO NaOH} outputs {salt})
wereuniversallyquantifiedovertheclassofsalts,thenfourfactswouldbepredicted.
Sinceonly oneofthesepredictions hasbeenobserved, GLAUBER includes anexistentialquantifierratherthanauniversalone. Thesamedecisionis madefortheother
patterns formed by substitution, leading to the laws and facts shown in the second
section ofthe table.
Given this new state ofthe world, GLAUBER again determines which ofthe
known symbolsoccurinthe mostanalogous facts. Inthiscase, the setofalternatives
is slightly different from that on the earlier cycle, since the class name salt has
replacedthe individual membersofthatclass. Giventhecurrentsetoffactsandlaws,
six symbols tie for the honors-NaOH, KOH, HC1, HNO3, sour, and bitter. For
example, the first ofthese occurs in the laws3salt (reacts inputs {HO NaOH} outputs {salt}) and 3salt (reacts inputs {HNO3 NaOH} outputs {salt}), while the
second occurs in the laws3salt (reacts inputs {HO KOH} outputs {salt}) and 3salt
(reacts inputs {HNO, KOH} outputs {salt}). The salt symbol actually occurs in all
fourofthese laws, buttheclassname isnotconsidered, since it isexistentiallvquantified in the laws. Since all ofthe viable options involve two laws (each based on one
--- PAGE 457 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 443
fact apiece), GLAUBER selects one of them at random. Let us follow the course
events take when the system chooses the pairoffacts involving the symbol NaOH.
Based on these facts, the FORM-LAW operator generates the pattern (reacts
inputs {acidNaOH} outputs {salt}), anddefinesthenewclassacidascontainingthe
elements HC1 and HN0 Two additional hypotheses result from substitution-
(reacts inputs {acid KOH} outputs {salt}) and (has-quality object {acid} taste
{sour})-eachreplacingtwodirectly observedfacts. Aftersubstitution, GLAUBER
hasfourlawsandtwofacts inmemory. However, thesystemmuststill determinethe
generality of these laws. The DETERMINE-QUANTIFIER operator proceeds to
considerthepredictionsmadebyeach lawwhenuniversallyquantifiedoverthenew
classofacids. Sinceallofthepredictedfactshavebeenobserved,theuniversalquantifierisretainedforeachofthenewlaws, givingthesetoffactsandlawsshowninthe
third section ofthe table.
Atthispoint, onlyfivesymbolsremaintobeconsidered-NaOH, KOH, bitter,
andtheclassessaltandacid. The firsttwooccuronly in single laws, whilethethird
occursintwoanalogousfacts. Theclassnamesaltappearsintwoanalogouslawsbut
isignoredduetoitsexistentialquantifier. However,theclassnameacidoccursintwo
analogous laws that are based on two facts apiece, giving acida score offour. As a
result, the two laws are passed to the FORM-LAW operator and a higher-level
pattern-(reacts inputs {acid alkali} outputs {salt})-is formed on this basis. In
addition, the class alkali is defined as having the members NaOH and KOH. A
secondhypothesis-(has-qualityobject {alkali} taste {bitter})-isformedby substitution,andbothlawsareuniversallyquantifiedoverthenewclass,thefirstbydefinition and the second empirically. At this point, GLAUBER has reached its goal of
specifying a maximally general set oflaws that summarizes the original data. The
final laws (shown in the fourth section oftable 16-6) are very similar to those proposedbytheearlychemists. WhenGLAUBER isgivenreactionsinvolvingmetalsas
alkalis, itdefinesthebroaderclassbases(containingbothmetalsandalkalisasmembers) and arrives at the central tenet that acids combine with bases to form salts.
16.3.3 Comments on GLAUBER
In its present form, GLAUBER has some important limitations, and these
shouldbe remedied in future versionsofthe system. The firstdifficulty involvesthe
system's evaluation function for directing search through the space of classes and
laws. The current version iterates through the set ofknown symbols and selects the
symbolthatoccurs inthegreatest numberofanalogous facts. This leads GLAUBER
to prefer large classes to small ones, which in turn leads to laws with greatergeneralityinthesensethattheycovermoreoftheobservedfacts. However, recallthatonce
GLAUBER defines a new class on the basis ofsome law, it then creates additional
lawsby substitutingthe class forits members in otherfacts. This suggests abroader
definition ofgenerality, including all facts predicted by any law involving the new
--- PAGE 458 ---
444 CHAPTER 16: FOUR ASPECTS OF SCIENTIFIC DISCOVERY
class. This analysis leads totwo methods for preferring one class over another. The
most obvious approach involves computing the percentage of predictions that are
actually borne outby observations; we shall call this thepredictivepowerofaclass
and its associated laws. The second method involves computing the total number of
facts predicted by aclass and its related laws; we shall call this thepredictivepotentialofthe class.
Obviously, a set of laws that predicts a few observations but predicts many
unobservedonesisundesirable; thissuggeststhatpredictivepowershouldbeusedto
weed out grossly unacceptableclasses. However, given roughly equal scores on this
dimension, sets oflaws with greater predictive potential should be preferred, since
theseleadtomanypredictions, which, ifsatisfied, willleadtoan increase inpredictive power. One difficulty in implementing this scheme is that GLAUBER would
havetogeneratethepotential classesandtheirassociatedlaws in ordertodetermine
their predictive power and potential. Moreover, it would have to consider whether
theselawsshouldbeexistentiallyoruniversallyquantifiedinordertomaximizetheir
scores.
Inotherwords,thesystemwouldhavetoapplytheFIND-LAWoperatorinall
possibleways, andthenapplythe DETERMINE-QUANTIFIERoperatorinallpossibleways in ordertodeterminethebestpathto follow. This is equivalenttodoing a
two-steplook-aheadinthesearchtree, and it wouldinvolveconsiderably morecomputationtimethanthecurrentsimplestrategy. Thedetailsofthisschemeremaintobe
elaborated, butthebasicideaofdefiningclassesthataccountforthemostdataseems
a plausible approach.
A second limitation involves the possibility of alternative divisions of substancesintoclasses. Insomecases, twoormorebranches inthe searchtree may lead
toequally (oralmostequally) gooddescriptions ofthe data. These competing paths
mayultimatelyleadtothesamestate, ortheymayleadtocompletelydifferentorganizations ofknowledge. In the lattercase, one would like the system to discover both
frameworks. However, since the current version of GLAUBER carries out a depthfirstsearch withoutback-up, it must selectoneofthe paths at random, thus ignoring
what may be an equally useful summary ofthe data. Future versions ofthe system
shouldbeabletoconsidermultiplealternativeswhilestill usingevaluation functions
to keep search to a minimum.
In order to understand the last of GLAUBER'S limitations, we must review
some related work on machine learning. Wolff (1978) has explored an approach to
grammar learning that incorporates methods very similar to those used in
GLAUBER. Wolffs systembeginswithasequenceoflettersand, based on common
sequences of symbols, defines chunks in terms of these sequences. For example.
given the sequence "thedogchasedthecatthecatchasedthedog . . . ," the program
defineschunks likethe, dog, cat, andchased. Wheneverachunk iscreated, thecomponent symbols are replaced by the symbol forthat chunk. In this case, the sequence
"the-dog-chased-the-cat-the-cat-chased-the-dog*' would result. In addition, when a
number ofdifferent symbols (letters or chunks) are found to precede or follow a
--- PAGE 459 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 445
commonsymbol, adisjunctiveclass isdefinedintermsofthefirst set. Forinstance,
in the above sequence the sub-sequences "the-dog-chased" and "the-cat-chased"
occur. Based on this regularity, Wolff's program would define the disjunctive class
noun = {dog, cat} . The symbol for this new class is then substituted into the letter
sequence forthe membersymbols. Inthiscase, the sequence "the-noun-chased-thenoun-the noun-chased-the-noun" would be generated. These twobasic methods are
applied recursively, sothatchunkscanbedefined intermsofdisjunctiveclasses and
vice versa. Thus, given the last sequence, the chunk sentence = the-noun-chasedthe-noun would be defined, giving the final sequence "sentence-sentence."
From this description we see that Wolff's learning system employs two
operators-oneforformingdisjunctiveclassessuchasnounandanotherfordefining
chunks or conjunctive classes, such as dog. The first of these is identical to
GLAUBER'Soperatorforformingdisjunctiveclasseslikeacidandalkali.6Themain
difference betweenthetwo systems lies intheheuristics for forming such disjuncts.
Wolff employs adjacency criteria well suited to the language acquisition domain,
while GLAUBER usesthenotionofsharedarguments, whichismoreappropriatefor
relationaldomains. Incontrast, thesecondoperatorinWolff'smethodhasnoanalog
in GLAUBER'Srepertoire, andthissuggestsagapinourdiscoverysystem'scapabilities. Uponreflection, onewouldlike GLAUBER tonoterecurringrelationsbetween
conjunctionsoffactsaswellasthoseinvolvingisolatedpropositions. Letusconsider
anexamplefromthedomainofgeneticsthatrequiresthisformofreasoning. Suppose
the system observed (as did Mendel) that when certaingreen garden peas were selffertilized, they produced only green offspring, butthat when othergreen peas were
self-fertilized, theyproducedbothgreenandyellowchildren. Inthiscase, wewould
like GLAUBER to divide the green peas into two classes based not on their own
directlyobservablefeatures(sincetheseareidentical)butonthefeaturesoftheiroffspring. Thus, in looking for patterns, the system would have to examine not only
single facts, but pairs offacts, triples offacts, and so forth. Such a strategy, though
much more expensive than the current one, would enable the program to note that
somegreenpeashaveonlygreenoffspring, whileothershavemixedoffspring, andto
classifythemonthisbasis. Thiswouldbeequivalenttodefiningchunksbasedoncooccurring facts and could be viewed as a relational version ofthe chunking method
used in Wolff's system.
We should also briefly consider some other discovery systems with similar
concerns. First, MichalskiandStepp(1983)havestudiedthetaskofconceptualclustering, in which one forms a hierarchical taxonomy for classifying objects. Since
6RatherweshouldsaythatGLAUBER'SoperatorisidenticaltoWolffsoperator, sinceWolff'sworkprecededourownbymanyyears. AlthoughtheoriginalversionofGLAUBERwasdevelopedindependently
ofWolff'sapproach,thecurrentsystemborrowsconsiderablyfromhisresultsinthedomainofgrammar
learning.
--- PAGE 460 ---
446 CHAPTER 16: FOUR ASPECTS OF SCIENTIFIC DISCOVERY
GLAUBER alsodivides objects intoclasses, itcanbeviewed ascarrying outaform
of conceptual clustering, even though its methods differ significantly from those
used by Michalski and Stepp. GLAUBER also bears some resemblance to Brown's
discovery system (1973), which also generated abstract laws covering a set offacts.
However,thisearlysystem'ssearchmethodsalsodifferedconsiderablyfromthosein
GLAUBER, and it did notdefine new classes inthe process ofstating laws. Finally,
we should mention some recent work by Emde, Habel, and Rollinger (1983) that
involves the discovery ofqualitative laws. In this case, the focus is on determining
whether predicates obey certain relations, such as transitivity or inversivity.
Although this approach leads to laws very similar to those found by Brown, the
model-drivendiscovery method contrasts withthe data-driven approach used in the
other systems. To summarize, we find that GLAUBER bears some relation to other
systems for qualitative discovery but is most similar to Wolff's grammar-learning
system in both spirit and method.
16.4 DETERMINING THE COMPONENTS OF SUBSTANCES
Wehavealready seenthatearlychemistswereconcernedwithbothqualitative
and quantitative descriptions ofchemical reactions. However, another one oftheir
primary goals wastodeterminethe componentsofvarious substances, and information about chemical reactions proved quite useful in this regard. The goal ofdetermining such components assumed an important facet ofthe atomic theory in that it
postulated primitive building blocks for the observed substances, even though no
stance wastakenon whetherthesebuildingblocks wereparticulateorcontinuous in
nature. Thus the formulation ofcomponential models embodied a simple form of
explanation that is clearly distinct from the descriptive summaries generated by
BACON and GLAUBER.
During the eighteenth century, chemists developed models of many substances,buttheydevotedconsiderableattentiontoexplainingcombustionandrelated
phenomena. As a result, two different componential models were eventually proposed to account for this process. The first assumed that combustion involved the
decomposition oftwo substances, and was known as the theory ofphlogiston. The
second assumed that combustion involved the combination oftwo substances, and
wascalledtheoxygentheory. Althoughthephlogistontheorywaseventually rejected
in favorofitscompetitor, it providedaplausible accountofthe known reactions and
was well respected for decades. This suggests an important constraint on computational models ofscientific discovery: such models should be able to arrive at plausible laws or models even ifthey were ultimately rejected in favor o( others. This
makes the areaofcombustion reactions an ideal test forsystemsconcerned with formulatingcomponential models, sinceweknowtwomodels that can usefull\ account
for the observations.
--- PAGE 461 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 447
16.4.1 The STAHL System
Our interest in componential models has led us toconstruct athird AI system
that infers such models from a set of known reactions. The program is named
STAHL, afterG. E. Stahl (1660-1734), oneoftheprinciple formulators ofthephlogistontheory. LikeGLAUBER, thisprogramacceptsqualitativeinformationasinput
and generates qualitative statements as output (see table 16-7). However, since
STAHL's conclusions relate to the internal structure of substances, they can be
viewed as simple explanations rather than descriptive summaries. The system's initial stateconsistsofasetofreactions, representedinthesameframelikeformatused
by GLAUBER. For instance, the reaction of hydrogen and oxygen to form water
would be represented as (reacts inputs {hydrogen oxygen} outputs {water}).
STAHL's goal is to determine the components of all nonelemental substances
involved in the given reactions. This information is represented in the same formalism as the initial reactions. Thus, the conclusion that water is composed of
hydrogen and oxygen would be stated as (components of {water} are {hydrogen
oxygen}). Intermediate states consist of inferences about the components of some
substances, along with transformed versions ofthe initial reactions.
STAHL incorporates four operators for moving through the space ofpossible
componential models. These operators are closely linked to the heuristics that propose them, so they are best discussed together. The most basic ofthese operator/
heuristics deals with simple synthesis and decomposition reactions and lets the
system unambiguously infer the components of a compound. It can be stated as
follows:
INFER-COMPOSITION
If A and B react to form C
Or if C decomposes into A and B
Then inferthat C is composed of A and B.
Table 16-7: STAHLviewedintermsofsearchconcepts.
Initialstate: A listofreactionsrelatingsubstances
Goalstate: Thecomponentsofeachcompoundsubstance
Intermediatestates: Componentsofsomesubstances, modifiedreactions
Operators/Heuristics:
Infer-composition: Decidesonthecomponentsofsubstance
Reduce: Cancelssubstancesoccurringonbothsidesofareaction
Substitute: Replacesasubstancewith itscomponentsinareaction
Identify-components: Identifiestwocomponentsasthesame
Identify-compounds: Identifiestwocompoundsasthesame
Searchcontrol: Depth-firstsearchwithnobacktracking
--- PAGE 462 ---
448 CHAPTER 16: FOUR ASPECTS OF SCIENTIFIC DISCOVERY
An obviousexample ofthis rule's use involves determining the components of
water. Giventheinformationthathydrogenreactswithoxygentoformwater, STAHL
would inferthat the latter substance is composed ofthe first two. Note that STAHL
does not draw any conclusions about the amount ofhydrogen and oxygen contributingtowater, butonly infersthattheycontributesomething. Ofcourse, theINFERCOMPOSITION rule is not limited to reactions involving pairs ofelements; it can
also deal with cases in which three or more substances unite to form a single
compound.
Ifallchemical reactionswereoftheformshownabove, STAHL'staskwouldbe
simple indeed. However, more complex reactions are common in chemistry, so
STAHL includes additional operators for dealing with these more complex situations. The purpose ofthese operators istotransformcomplex reactions into simpler
formssotheycaneventuallybematchedbytheINFER-COMPOSITIONruleshown
above. Onesuchoperatorisresponsiblefor "canceling" outsubstancesoccurringon
both sides ofa reaction; the "reduction" heuristicthatproposesthis operatorcanbe
paraphrased as follows:
REDUCE
If A occurs on both sides ofa reaction
Then remove A from the reaction.
Thisheuristic leadsdirectlytoasimplifiedversionofareaction. Forinstance,
if STAHL istoldthat "A, B, andC reactto form D andC," the REDUCE rule would
apply, givingthe simplifiedreaction "AandB reacttoformD." Thisrevised relation
would then be used tothe INFER-COMPOSITION rule to inferthat D is composed
ofA and B. One can imagine cases in which this approach would lead to errors, as
whendifferentamountsofasubstanceareobservedbeforeandafterareaction. However, similarerrorswerecommonlymadebyearlychemists, andweare interestedin
explaining these reasoning errors rather than avoiding them.
STAHLincorporatesathirdoperatorthatinitiallyleadstomorecomplexstatementsofreactions, butmaymake itpossibleforthe REDUCE ruletoapply. Theheunstic for proposing this operator draws on information about the components ofa
substance that have been inferred earlier; it can be stated as follows:
SUBSTITUTE
[f A occurs in a reaction
And A is composed ofB and C
Then replace A with B and C.
For instance, thesystem may knowthat X is composedofY and Z. and that "x
reacts with W to form V and Z." In this case, the SUBSTITUTE rule would rewrite
the second relation as Y, Z, and reactto form V and Z." Given this formulation,
the REDUCE rule would lead to "Y and W react to form V," and the INFER-
( ( )\1P( >SI 1 K)N rule would lead to the conclusion that V is composed ofY and W.
--- PAGE 463 ---
LANGLEY, ZYTKOW, SIMON, AND BRADSHAW 449
As before, the SUBSTITUTE rule is not restricted to substances composed oftwo
elements but works equally well for more complex structures.
A final operator is responsible for postulating that two substances that were
originally thought to be different are in fact identical. Two separate heuristics propose when to apply this operator; the first ofthese rules may be stated as follows:
IDENTIFY-COMPONENTS
If A is composed ofB and C
And A is composed ofB and D
Then identify C with D.
This heuristic matches when STAHL learns that a compound can be decomposedintwodifferentways,butinwhichthosedecompositionsdifferbyonlyasingle
substance. Thesecondheuristicisverysimilar, exceptthatitapplieswhentwoapparently different compounds are found to have the same components. It can be paraphrased as follows:
IDENTIFY-COMPOUNDS
If A is composed ofC and D
And A is composed ofC and D
Then identify A with B.
Thehistoryofchemistryaboundswithcasesinwhichanewsubstancewasdiscovered in two different contexts, was originally thought to be two distinct substances, and waseventually combined intoasingleconcept. We will see anexample
ofsuch identification shortly.
STAHLcanbeviewedascarryingoutadepth-firstsearchthroughthe spaceof
componentialmodels, relyingentirelyonitsheuristicstoselecttheappropriatepath.
Ingeneral,theseheuristicsaresufficientlypowerfulthatthesystemneedneverbacktrack, though we will discuss some situations laterwherethiscapability is required.
In some cases, more than one heuristic (or more than one instantiation ofthe same
heuristic) can be applied to the current state. Different choices may lead STAHL to
quitedifferentcomponentialmodels,andthesystememploysasophisticatedstrategy
to resolve such conflicts, which we have described elsewhere (Zytkow and Simon,
1986).
One ofthe interesting features ofSTAHL isthe manner in which its heuristics
interact. Notethatthesubstitutionrulerequiresknowledgeofasubstance'scomposition, sothat some inferences aboutcomposition mustbe madebefore itcan be used.
However, we have also seen that complex reactions must be rewritten by the reduction and substitution rules before some composition inferences can be made. This
interdependenceleadstoa "bootstrapping" effect, inwhich inferences madebyone
ofthe rules enable further inferences to be made, these allow additional inferences,
and so forth, until as many conclusions as possible have been drawn. This process
--- PAGE 464 ---
450 CHAPTER 16: FOURASPECTS OFSCIENTIFIC DISCOVERY
generallybeginswith oneormore simple reactions, butafterthistheparticularpath
taken depends on the data available to the system.
LetusconsiderSTAHL'sheuristicsinoperationontherelativelysimpletaskof
inferring the composition of lime and magnesia. In order to formulate models of
thesetwosubstances, the system requirestwo initial reactions: (reacts inputs {lime}
outputs {quick-lime fixed-air}) and (reacts inputs {quick-lime magnesia} outputs
{lime calcined-magnesia}). Given this information, the INFER-COMPOSITION
rule applies first, leading to the inference that lime (CaC0 3) is composed ofquicklime (CaO) and fixed-air (C0 2). This result enables the SUBSTITUTION heuristic
to match, leading to a temporarily more complex version of the second reaction,
(reacts inputs {quick-lime magnesia} outputs {quick-lime fixed-air calcinedmagnesia}). However, since the substance quick-lime occurs in both sides of the
modified reaction, the REDUCTION rule applies, transforming it into the simpler
form(reactsinputs {magnesia} outputs {fixed-aircalcined-magnesia}). Finally,this
reducedformallowstheINFER-COMPOSITIONruletoinferthatmagnesiaiscomposedofthesubstancesfixed-air(C0
andcalcined-magnesia(MgO). Atthispoint,
since nomoreofits heuristics seemapplicable, STAHLconcludesthat ithas formulatedasmanycomponentialmodelsasthedataallowandhaltsitsoperation. Thesystem's behavior on this example is summarized in table 16-8. Now that we have presentedanoverviewofSTAHL'sinferencemethods, letusexaminetheirapplicationto
a historically more interesting example-discovering the phlogiston theory.
16.4.2 Discovering the Phlogiston Theory
The theory ofphlogiston originated early in the eighteenth century, and after
undergoing several transformations, it was widely accepted until the 1780s. This
Table 16-8: Inferringthecompositionoflimeandmagnesia.
InitialstateSI:
(reactsinputs {lime} outputs {quick-limefixed-air})
(reactsinputs {quick-limemagnesia} outputs {limecalcined-magnesia})
INFER-COMPOSITIONleadstostateS2:
(componentsof{lime} are {quick-lime fixed-air})
(reacts inputs {quick-limemagnesia} outputs {limecalcined-magnesia})
SUBSTITUTEleadstostateS3:
(componentsof{lime} are {quick-lime fixed-air})
(reacts inputs {quick-lime magnesia} outputs {quick-limefixed-aircalcined-magnesia})
REDUCEleadstostateS4:
(componentsof {lime} are {quick-lime fixed-air})
(reacts inputs (magnesia) outputs {fixed-aircalcined-magnesia})
INFER-COMPOSITION leadstofinalstateS5:
(componentsof {lime} are {quick-lime fixed-air})
(componentsol (magnesia] are (fixed aircalcined magnesia})
--- PAGE 465 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 451
theory adoptedtheancientviewthatfire, heat, andlightaredifferentmanifestations
ofacommonprinciplethatleavesabodyduringcombustion. Therefore, anyreaction
involvingcombustionwasviewedasadecomposition; forinstance, burningcoalwas
interpretedasdecomposing into the matteroffire (anotherterm forphlogiston) and
ash.7Earlyphlogistianswerenotabletoisolatephlogiston, butthedisengagementof
fire during combustion seemedtobe agood observational reason foradmitting that
somesubstance wasgivenoffbytheburningbody. Later, asthe notionofphlogiston
provedusefulinexplainingmanyadditionalreactions, theexistenceofthissubstance
was supported by a substantial body ofevidence.
Afterthey beganto study combustionwithinclosedvessels, chemists realized
thatairwasnecessaryforcombustiontooccur. However, theydidnotassumethatair
changeditschemicalidentityduringthisprocess. Rather,theydecidedthatairplayed
anauxiliary role, similartothatplayedbywaterinreactionsinvolvingacids, alkalis,
and salts. Thus, even starting with empirically more complete descriptions ofcombustion, such as "in the presence ofair, carbon burns to release phlogiston and to
formash," theyemployedthereductionheuristictoremoveairandsimplifytherelation. Given such data, STAHL makes similar "errors" in reasoning, so that it provides a simple explanation ofthe process by which chemists developed phlogistonbasedmodelsofcombustionreactions. Suchconfusionsarecommoninthehistoryof
chemistry, and a similar error led the followers of Lavoisier to believe (approximately 1810) that sodium was a compound ofsoda and hydrogen.
LetsusexaminethepathtakenbySTAHL inarrivingatoneversionofthephlogiston theory. The system is presented with two facts: (reacts inputs {coal air} outputs {matter-of-fireashair}) and (reactsinputs {calx-of-ironcoalair} outputs {iron
ashair}).8Onemayquestiontheexactrepresentationofthesefacts,butclearlysomething very much like them was believed during the period in which the phlogiston
theory was developed. Given this information, STAHL immediately applies its
REDUCE
operatortothe firstfact, givingthe revised reaction (reacts inputs {coal}
outputs {matter-of-fire ash}). The system then applies the same operator to the
second fact, giving the reduced reaction (reacts inputs {calx-of-iron coal} outputs
{iron ash}). After this, the INFER-COMPOSITION rule is applied to the first
revised reaction, giving the inference that coal is composed of matter-of-fire (or
phlogiston) and ash, one beliefofthe early phlogiston theorists. Having arrived at
this conclusion, STAHL applies the SUBSTITUTE rule, generating the expanded
relation (reacts inputs {calx-of-iron ash matter-of-fire} outputs {iron ash}). At this
7Severaldecadeslater, inthesecondhalfoftheeighteenthcentury, fixdair(carbondioxide)wasdiscoveredandrecognizedastheproductofburningcoal inplaceofash.
8Calx ofiron was the current name for iron oxide; we have used the original terminology because the
modernterm isbasedontheoxygentheorydevelopedby Lavoisier.
--- PAGE 466 ---
452 CHAPTER 16: FOUR ASPECTS OF SCIENTIFIC DISCOVERY
point, the REDUCE rule is used to remove ash from both sides of the equation,
giving (reacts inputs {calx-of-iron matter-of-fire} outputs {iron}). Finally, the
INFER-COMPOSITION operator leads STAHL to inferthat iron is a compound of
calx-of-iron and the matter offire. Table 16-9 summarizes the states visited by the
system in arriving at these conclusions, along with the operators used to generate
them.
Let us now consider how STAHL employs its identification heuristics with
respecttothephlogistontheory. Supposethesystemisgiventhefollowingadditional
data: (reacts inputs {iron sulfuric-acid water} outputs {vitriol-of-iron inflammableair water}) and (reacts inputs {calx-of-iron sulfuric-acid water} outputs {vitriol-ofironwater}).9Giventhesefacts, STAHLremovesthewaterfrombothreactionsusing
the REDUCE operator. Thissufficiently simplifiesthesecondreaction sothat itcan
applytheINFER-COMPOSITION rule, inferringthatvitriol-of-ironiscomposedof
calx-of-iron and sulfuric-acid. This fact is substituted into the first reaction, giving
(reacts inputs {iron sulfuric-acid} outputs {calx-of-iron sulfuric-acid inflammableair}). After using the REDUCE operatorto eliminate sulfuric-acid from both sides
Table 16-9: STAHL'sstepsinformulatingthephlogistonmodel.
InitialstateSI:
(reactsinputs {coalair} outputs {matter-of-fireashair})
(reactsinputs {calx-of-ironcoalair} outputs {ironashair})
REDUCEleadstostateS2:
(reactsinputs {coal} outputs {matter-of-fireash})
(reactsinputs {calx-of-ironcoalair} outputs {ironashair})
REDUCEleadstostateS3:
(reactsinputs {coal} outputs {matter-of-fireash})
(reactsinputs {calx-of-ironcoalair} outputs {ironash})
INFER-COMPOSITIONleadstostateS4:
(componentsof{coal} are {matter-of-fireash})
(reactsinputs {calx-of-ironcoal} outputs {ironash})
SUBSTITUTEleadstostateS5:
(componentsof{coal} are {matter-of-fireash})
(reactsinputs {calx-of-ironmatter-of-fireash} outputs {ironash})
REDUCEleadstostateS6:
(componentsof{coal} are {matter-of-fireash})
(reacts inputs {calx-of-ironmatter-of-fire} outputs {iron})
INFER-COMPOSITION leadstofinalstateS7:
(componentso\' {coal} are {matter-of-fireash})
(componentsof {iron} are {calx-of-iron matter-of-tnej
' The formula forVitriol o\ iron is F-eSO,. while the modern name tor inflammableair is hydrogen.
--- PAGE 467 ---
LANGLEY. ZYTKOW, SIMON, AND BRADSHAW 453
ofthis expression. STAHL infers that iron consists ofcalx-of-iron and inflammable
air. However, the system knows from the other reactions described earlier that iron
canalsobedecomposedintocalx-of-ironandphlogiston. Usingthefirstofitsidentification heuristics (IDENTIFY-COMPONENTS), the system infers that inflammableairandphlogistonareidentical. BoththereasoningandconclusionsofSTAHL
inthisexample are very similartothose ofCavendishand otherphlogistontheorists
during the 1760s.
16.4.3 Comments on STAHL
EarlierwementionedonecaseinwhichSTAHL'sheuristicsmightleadtoerroneousinferences,butwedidnotpursuethematter. Infact,thereareanumberofways
inwhichthesystem'sheuristicscanleaditastray. Onesituationinvolvesthenotionof
infinitely recursing componential models. For instance, given certain reactions
involving mercury, calx-of-mercury, and oxygen,10 STAHL eventually makes two
inferences: (componentsof{mercury} are {calx-of-mercuryphlogiston})and(components of {calx-of-mercury} are {mercury oxygen}). Taken together, these two
inferences imply that mercury is composed ofitself, and this seems an undesirable
characteristic foran explanatory model.
Ultimately, suchinfiniterecursionsmustbeduetothefaultydescriptionofone
or more reactions. Given trace information about which heuristics proposed which
inferences, STAHL isabletotrackdowntheresponsiblereactionandcallitintoquestion. Historically,chemistsintroducedconceptualdistinctionstoexplainsuchinconsistencies. Forinstance, toavoidthedifficulty mentionedabove, theyformulatedthe
conceptcalx-of-mercury-properasdistinctfromcalx-of-mercury, andSTAHL introduces ananalogous distinction. In some sense, this is similarto BACON 's introduction ofnew intrinsic properties when it encounters a situation in which its numeric
methods failtoapply. AswithBACON, suchconceptsmayappeartautologicalwhen
first introduced, but they become respectable to the extent that they prove useful in
dealing with other situations besides the one leading to their introduction.
STAHL's heuristics can lead to other forms of inconsistency as well. For
instance, the system may infer that A consists ofB and C and later inferthat A also
consistsofB, C, andD. Alternatively, theprogrammay reduceareactiontothe form
(reactsinputs {X} outputs { }), whichcontainsinputsbutnooutputs. Inbothcases,
STAHL isabletotracebackthroughitschainofinferencestodeterminethesourceof
the problem andeitherrejectthe offending observationorrestate itusing anewconcept. Thisprocesscanbeviewedasaformofbacktrackingthroughthe searchspace,
thoughnotinany simplesense. Itisbetterdescribedasrejectingthecurrentstateand
'"Laterversionsofthephlogistontheoryactually includedoxygenasanelement, butthey retainedphlogistonastheircentral feature.
--- PAGE 468 ---
454 CHAPTER 16: FOUR ASPECTS OF SCIENTIFIC DISCOVERY
moving sideways through the problem space to another state at approximately the
samedepth. Suchreformulationsoccurredmanytimesintheearlydaysofchemistry,
and STAHL's methods for error recovery parallel the historical developments in
many ways. Zytkow and Simon (1986) have described this aspect ofthe system in
more detail.
16.5 FORMULATING STRUCTURAL MODELS
Asanareaofsciencematures, researchersprogressfromdescriptionstoexplanations. Although the dividing line between these forms ofunderstanding is fuzzy,
some examples clearly lie at the explanatory end ofthe spectrum. For instance, the
kinetic theory ofheat provides an explanation ofboth Black's law and the ideal gas
law. A simpler example, though no less impressive at the time it was proposed, is
Dalton's atomic theory. Both examples involve some form of structural model in
which macroscopicphenomenaaredescribed intermsoftheirinferredcomponents.
Although this is not the only form ofscientific explanation, the notion ofstructural
modelsseemssignificantenoughtoexploreinsomedetail. Letusreviewthehistory
ofthe atomic theory as a prelude to our computational analysis ofthis aspect of
discovery.
Wehaveseenthataportionoftheatomichypothesiswasimplicitincomponential modelssuchasthephlogistontheory, butthefullversionoftheatomicmodelwas
first proposed by John Dalton in 1808. In his attempt to explain the law ofmultiple
proportions, Dalton assumed that substances were composed of particles called
atoms, and he focused on the numbers ofparticles making up each substance. Following his lead, chemists adopted the design ofsuch atomic models as one oftheir
centralconcerns. Daltonemployedhis ruleofgreatestsimplicitytoapplytheatomic
theorytospecificcases, andalthoughthisheuristicworkedinmanycases, itledhim
toincorrectconclusionsinothers. Forinstance, itledhimtoconcludethatwaterwas
composedofasinglehydrogenatomandasingleoxygenatom. Incontrast, Avogadro
(1811) employed Gay-Lussac's data on combining volumes, along with the assumption that equal volumes ofgases contained equal numbers of particles. Using this
information, he inferred diatomic models for hydrogen and oxygen and a different
structure forwater. Although we acceptAvogadro's hypothesistoday, it was rejected
by his contemporaries, since they believed that different atoms ofthe same element
wouldrepel, ratherthanattract,eachother. Thisisanothercaseinwhichtwohypotheses provided plausible accounts of phenomena, making the area an ideal one for
testing a discovery system concerned with formulating structural models.
16.5.1 The DALTON System
Our interest in structural models has led us to construct a fourth discovery
system concerned with this issue. Since John Dalton was one ofthe earliest propo-
--- PAGE 469 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 455
nentsofsuchmodels, wehavenamedthesystemDALTON. Thesystemacceptsaset
ofreactionsasinput, alongwithinformationaboutthecomponentsofthesubstances
involved in these reactions. For instance, DALTON would be told that hydrogen
reacts with oxygen to form water, and that hydrogen reacts with nitrogen to form
ammonia. Along with this information, the system would be told that water has
hydrogen and oxygen as its components, while ammonia has hydrogen and nitrogen
as its components.11 Finally, it would be informed that hydrogen, oxygen, and
nitrogenareelements, implyingthattheyhavenocomponentsotherthanthemselves.
DALTON knows thattwo quantities are important in a reaction-the number
ofmolecules ofeach substance that takes part (in the simplest form ofthe reaction)
andthenumberofparticlesofeachtypeinagivenmolecule.12Thesystem'sgoalisto
deviseamodelforeachreactionthatspecifiesthenumberofmoleculesandparticles
for each ofthe substances involved. Given this goal, the reactions from which
DALTON begins its search are best viewed as very abstract models in which these
numbers have not yet been specified. In its search through the space ofmodels, the
program generates intermediate states in which some amounts have been specified
butothershavenot. Table 16-10summarizestheprogramintermsofsearchconcepts.
Thesystemincorporatesthreeoperatorsforinstantiatingthesemodelsandthus
moving through the problem space. The first of these inputs a reaction in which
the number of molecules for a particular substance is unknown and outputs a
revised reaction in which this amount is specified. For instance, this routine must
Table 16-10: DALTONviewedintermsofsearchconcepts.
Initialstate: A listofreactionsandthecomponentsofthesubstancesinvolved
Goalstate: Amodelofeachreaction, specifyingthenumberofmoleculesandthenumberofparticles
ineachcompound
Intermediatestates: Partialmodelsofsomereactions
Operators:
Determine-molecules: Specifiesthenumberoftimesacompoundoccursinareaction
Determine-atoms: Specifiesthenumberofatomsofagiventypeinamolecule
Conserve-particles: Determinesremainingnumbersbasedonconservationprinciple
Heuristics:
ForDetermine-molecules: Consideronlymultiplesofthecombiningvolumes
ForDetermine-atoms: Selectsimplermodelsfirst
Searchcontrol: Depth-firstsearchwithbacktracking
1'Thus,DALTONacceptsasinputthetypeofinformationthatSTAHLgeneratesasoutput,suggestingthat
thesesystemscouldeasilybelinkedtogether. Wewilldiscussthispossibilityinalatersection.
12ThismeansthattheDALTONprogrambeginswithabetternotionofthetruesituationthandiditsnamesake, sinceJohn Daltondidnotmakethedistinctionbetweenatomsandmolecules.
--- PAGE 470 ---
456 CHAPTER 16: FOUR ASPECTS OF SCIENTIFIC DISCOVERY
hypothesize the number of oxygen molecules involved in the water reaction. A
second operator is responsible for specifying the number oftimes a given element
occurs in a particular molecule. For example, given the information that oxygen is
one ofthe components ofsulfuric acid, this operatorwould hypothesize the number
ofoxygenatomsinvolvedintheacid. A final operatoralsodeterminesthenumberof
atomsinasubstance, butinamuchmoreefficientmanner. Thisroutineassumesthat
foreachelementtakingpartinareaction, thetotal numberofparticles isconserved.
Theoperatorisgiventhenumberofmoleculesonbothsidesofareaction, alongwith
the numberofparticles onone side ofthat reaction. Fromthis information, itdetermineswhethertheconservationassumptioncanbesatisfied, andifso, itspecifiesthe
number ofparticles on the other side ofthe reaction necessary to balance the equation. Ifconservation cannot be satisfied under the existing assumptions, it returns
this information instead.
Usingthesethreeoperators, DALTON carriesoutadepth-firstsearchthrough
the space of possible models. The system focuses on one reaction at a time, first
determiningthenumberofmoleculesandthenthenumberofparticles ineachmolecule. Simpler models are considered before more complex ones. Thus models
involvingonemoleculeforsomesubstancewouldbeproposedbeforeonespecifying
two or three molecules. Similarly, models incorporating one occurrence ofan element (monatomic models) wouldbeconsideredbefore models involving two occurrencesofthatelement (diatomic models). Theconservationassumption isemployed
as soonasthe model forareactionis sufficiently constrainedforittobe used. Since
somepartial modelscannotbe instantiatedin any way thatwill satisfy theconservation constraint, DALTON must be able to backtrack and consider other paths to a
complete model.
One additional constraint makes the process of constructing models challenging. Consistency requiresthatthemodel forasubstancebethe same forall reactionsinwhich itoccurs. Forexample, ifhydrogenisassumedtobemonatomicforthe
water reaction, it must also be monatomic in the ammonia reaction. In general, this
assumption will simplify the search process, since models completed earlier will
constrain thosedealt with at laterpoints. However, it is possible that the model fora
substance results inaconservation-consistentmodel forone reactionbut leadstodifficulties for another reaction. In such cases, DALTON must revise its earlier model
inordertoconstructaconsistentexplanationforboth reactions. This involvesa form
of backtracking, though not the simple form discussed above, since some existing
models may be retained. We will presentanexampleofthis back-up method shortly.
16.5.2 A Monatomic Model of the Water Reaction
Now that we have examined DALTON's problem space and search control in
the abstract, let us consider their use in an example. Suppose the system is asked to
constructa model ot the water reaction, giventhe informationthat water is composed
--- PAGE 471 ---
LANGLEY. ZYTKOW. SIMON. AND BRADSHAW 457
ofhydrogen and oxygen and that hydrogen and oxygen are primitive elements (and
thuscomposedofthemselves). Inthiscase, the program mustdeterminethe number
ofhydrogen, oxygen, andwatermoleculesandthenumberofparticlesofeachtypein
thevariousmolecules. Aswehaveseen. DALTON beginswithaveryabstractmodel
in which no commitments are made and successively refines this model as it proceeds. Let us examine what happens at each state in the search through the space of
models.
Starting with an abstract model ofthe form (HO - W), the program first
considersthe numberofhydrogenmolecules involved. Lacking anytheoretical bias,
the system chooses the simplest hypothesis and assumes a single hydrogen molecule
is required. Ifthis choice later causes difficulty, the model builder can back up and
try another path. Similar initial choices are made for oxygen and water, so that the
partially specified model includes one molecule foreach. This is representedby the
proposition ((H) (O) (W)), in which each molecule is enclosed in parentheses.
NowDALTONmustdeterminetheinternal structureofeachtypeofmolecule,
and it decides to assume initially that both hydrogen and oxygen consist ofa single
elementary particle (say h ando), giving the model ((h) (o) (W)). Atthis point,
theprograminvokesitsconservation-basedoperator. Thisroutinecheckstoseeifthe
model can be finalized in such away thatconservation is obeyed. Ifthis is possible,
DALTON outputs the completed model and halts, but ifthe conservation principle
cannotbesatisfiedthesystembacksupandconsidersotherpossibilities. Inthiscase,
theconservation operatortells DALTON thatthe watermolecule mustbe composed
ofone h particleandone oparticle, andthatthe final model musthave the form ((h)
(o) (h o)). This model is equivalent to the one originally formulated by the
human chemistJohn Dalton. Figure 16-3 presents some ofthe paths available in the
space of molecular models. In arriving at the monatomic model just described,
DALTON takesthe leftpath, andsincethisleadstoanacceptable solution, nobacktracking is required.
16.5.3 A Diatomic Model of the Water Reaction
As we have seen, DALTON 'sbasic strategy istocarry outadepth-first search
through the space ofmodels, ordering the search so that simple models are considered first. However, when enough ofthe model has been specified, a theory-driven
heuristic (implementing the conservation assumption) takes over and finalizes the
model. DALTONcanalsoemploytheory-drivenmethodsatotherstagesinitssearch
process, andthesemethodscanalterthe system'sbehaviorin significantways. Thus,
in the above run, the system had notheoretical biases otherthan abeliefinconservation ofparticles and adesire to construct as simple a model as possible. However, if
wegive DALTONsomeadditional informationaboutthewaterreaction, itsbehavior
changes significantly. Avogadro was aware of Gay-Lussac's results, and he firmly
--- PAGE 472 ---
458 CHAPTER 16: FOUR ASPECTSOF SCIENTIFIC DISCOVERY
(/H -\V)
- -
((H) W) ((H) (H) W)
((/H) (0) - W) ((H) (H) \ (0) - V)
((H/) (0) - (W)) ((H) (H) (0) -\(W) (V))
((/h) (0) - (W)) /((h) (h) (0) - (W) (\W))
((h) (o) - (W)) ((h) (h) (o) - (W) (W)) ((h) (h) (o o) - (W) (W))
((h) (o) I- (h o)) Viol I ates ((h) (h) (o o) I - (h o) (h o))
Conservation
Figure 16-3: DALTON'ssearchforamodelofthewaterreaction.
believedthatthecombiningvolumesheobservedwererelatedtothenumberofmolecules involved in the reaction. To model this knowledge, we can add the heuristic:
INFER-MULTIPLES
Ifyou want to know the number ofmolecules ofX that are involved in a reaction
And the combining volume of X was V
Then consider only multiples ofV as possibilities.
Giventhis assumption (and knowledgeofthecombining volumes), the program (let
us call it DALTON') insteadpostulatestwo molecules ofhydrogenand water (and if
this was later found to be unsatisfactory, four and then six), while retaining the
assumption of one oxygen molecule. Thus, at the third level in the search tree,
DALTON' has the partially specified model ((H) (H) (O) - (W) (W)).
Atthis pointthe revised system moves toconsiderthe internal structure ofthe
hydrogenmolecule, assuming itiscomposedofasingleparticle (sayh), andmakesa
similar assumption for oxygen. However, for the resulting model, ((h) (h) (o)
(W)(W)), thereexistsnodecompositionofwaterintermsofhandothatsatisfies the
conservationassumption, sotheprogrambacksupandconsiderssomeotheralternative. DALTON' next hypothesizes that the oxygen molecule is composed of two
particles, and since this does allow conservation to be satisfied, a final model is
constructed in which oxygen is diatomic and hydrogen is monatomic: ((h) (h) (o o)
(h o) (h o)). (These twosearch pathsare shownonthe right side in figure 16-3.)
Whilethismodel differs fromthe modern-dayone, it isconsistentwithGa\-Lussac's
data and encounters difficulty only when other reactions are considered. For
example, the monatomic assumption for hydrogen does not work for the ammonia
reaction.
Like most ofthe programs we havedescribed, DALTON is stated as a production system. In default mode, the system usesa few simple rules to formulate simpler
--- PAGE 473 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 459
models first and more complicated ones as necessary. However, ifnew conditionaction rulesare addedtothe system, they takeprecedence overthedefault rulesand
candirect searchdownpaths that mightotherwise notbe considered. Thus, onecan
insertarulethat wouldmatch ifthecombiningvolumeofsome substance is known,
and use this information to determine the number of molecules used for that substance in the model. The conservation assumption is implemented in a similar
fashion, sothat itgenerates amolecularstructure fora reaction'soutputthatusesall
particles occurring in the input.
OnceDALTON hasgeneratedasuccessfulmodelforareaction, itconvertsthis
knowledgeintoproductions. Forinstance, havingarrivedatthediatomicexplanation
ofwatergivenabove, theprogramwould store one ruleconcerningthe moleculesof
hydrogen involved, another for oxygen molecules, and a third for the water molecules. Ifthesystemisaskedtoexplainthewaterreactionatalaterdate, itwillbeable
to recallthe numberofmolecules without search. DALTON alsoconstructsproductionsdescribingtheinternalstructureofvariousmolecules; thisknowledgeisuseful
notonly in reexplainingthewaterreactionbut inexplaining othercasesas well. For
instance, whenaskedtomodeltheammoniareaction, thesystemwouldimmediately
proposethathydrogenwasmonatomic, basedonthesuccessofthisassumptioninits
model ofwater. Noneofthe models incorporatingthisassumption satisfy conservation, sothesystemwouldbackup, hypothesizethathydrogenisinsteaddiatomic, and
eventually arrive at the correct model forammonia.
However, DALTONmustalsoupdateitsmodelofthewaterreaction. Sincethe
systemknowsthatitsmonatomichydrogenrulewasresponsibleforleading itastray,
removesthisrulefrommemoryandreplacesitwithadiatomicruleforhydrogen.
then focuses on the reaction that led it to construct the monatomic production and
checkstoseeifthereplacementruleworkshereaswell. Inthiscaseitdoes, buthad it
runintodifficulty, the
DALTONwouldhaverepeatedtheprocess,
consideringevermore-complicated molecular structures (up to a limit), until both the water and the
ammonia reaction had been successfully explained by a single rule.
16.5.4 Comments on DALTON
Although DALTON 's methodsareconcernedwith reactions, theyarenotlimitedtothechemicaldomain. Forexample, the fieldofelementaryparticlephysics is
also concerned with reactions and with the formulation of structural models to
explainthosereactions. Themostwidelyacceptedtheoryinthisdomainaccountsfor
the internal structure ofprotons, neutrons, and otherhadrons interms ofa small set
ofhypothesized particles called quarks. In its present form, DALTON cannot discoverthequarktheory, buttworelativelysimpleextensionsshouldenablethesystem
to arrive at the basic tenets ofthis framework.
--- PAGE 474 ---
460 CHAPTER 16: FOUR ASPECTS OFSCIENTIFIC DISCOVERY
First,thecurrentversionof DALTON requireseitherknowledgeofthecomponentsofasubstanceorknowledgethatasubstanceiselementary, suchashydrogenor
oxygen. However, therearenodirectlyobservable "elements" inthe fieldofparticle
physics, and inordertoexplainparticleinteractions, onemustpostulateentirelynew
substances that have never been seen. For example, the basic proton "molecule" is
viewed as composed not ofthree proton "atoms," but as composed oftwo u quarks
andonedquark. Inordertoregeneratethequarktheory, DALTONmustbemodified
to search the largerspace ofmodels in which such decompositionscan occur. Alternatively, one can imagine a modified version ofSTAHL capable ofdetermining the
unseen components ofhadrons, with DALTON retaining its focus on the numberof
particles involved. In any case, the issue ofinferred particles must be addressed in
one system orthe other.
An equally important aspect ofthe quantum theory involves the conservation
of mass and ofthe various quantum numbers, such as spin and electric charge. In
order to generate these features of the theory, DALTON must attempt to explain
quantitative attributes of directly observable substances (such as protons and neutrons) in terms of attributes associated with inferred substances (such as quarks).
Presumably, these constraintscanbe statedas theory-driven heuristics, much as the
conservationofparticlesassumptionisimplementedinthecurrentversion. Oncethe
system hasbeen given this capability, it may alsobe able to rediscoverthe basic version ofthe caloric theory, in which the conserved properties ofmass, heat, and heat
quantity are used to explain changes in the nonconserved quantity temperature.
A lessobviousapplicationof DALTON involvesthefieldofclassical genetics.
For example, the rules ofheredity for garden peas, first enumerated by Mendel in
1866, can be viewed as reactions in which characteristics ofthe parents are transformed into characteristics of the offspring. Given the first extension described
above, along witha suitable replacementforthe conservationassumption (sincethis
docs not apply in reproductive systems), DALTON should be able to arrive at the
two-trait model originally formulated by Mendel. For example, let us suppose that
the system is provided with genotypic statements ofthe result of inbreeding and
crossbreeding, which might be induced by another discovery system (like
GLAUBER) from phenotypic descriptions ofthese reactions.
Ifwc let G stand forgreen peas that produce only green offspring, Y stand for
yellowpeasthatproduceonlyyellowoffspring, andG' stand forgreenpeasthat producemixedoffspring, then fourbasicreactionssufficetodescribeMendel'sobservations: G G - G,YY - Y,GY - G',andG'G' - G G' Y. Giventhese
reactions, an extended versionof DALTON shouldbeableto inferthat two primitive
traits (say g and y) are requiredand todecide that thegenotype G can be modeled b\
the "molecular" pair(gg), that Ycanbemodeledbythepair(yy), and that G
canbe
modeled by the pair (g y). As we envision it, the system's explanation ofthese reactions would not involvethe notionofdominance, norwould it predict the proportions
--- PAGE 475 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 461
inwhichthevariousgenotypesareobserved,butit wouldaccountforthebasicqualitative relations between parents and offspring.
BeforeclosingourdiscussionofDALTON, weshouldbrieflyexamine itsrelationto DENDRAL(Feigenbaum,Buchanan, andLederberg, 1971),awell-knownAI
systemthat wasalsoconcernedwith formulating structural modelsofsubstances, in
thiscase, complexorganic molecules. Ratherthanusing reactions foritsbasic information, DENDRAL searched for models that would account for mass spectrogram
data. Inaddition, thesystememployedconsiderableknowledgeoforganicchemistry
to direct its search through the space ofpossible models. There is no doubt that this
early program could effectively search spaces in which DALTON would be quickly
overwhelmed, and could generate structural models more complex than our system
could begin to consider. However, this analysis misses an important point. DENDRAL was concerned primarily with imitating twentieth-century organic chemists
whodrawuponcenturiesofaccumulatedknowledgeaboutchemicalsandtheirreactions. Incontrast, DALTON isconcernedwith anearlierstage inthediscovery process, such as we find with the early chemists in their attempt to formulate atomic
modelswithverylittleavailableknowledge. Thus,
DALTONandDENDRAL
canbe
viewedaslyingattwoendsofaspectrum, withthe firststudying simplediscoveries
inaknowledge-poorenvironmentandthe secondfocusingonmorecomplexdiscoveries in a knowledge-rich environment. Ultimately, we may understand both
approachesasspecialcasesofamoregeneral methodforcreatingstructuralmodels,
but that remains a topic for future research.
16.6 TOWARD AN INTEGRATED DISCOVERY SYSTEM
Inthis chapter, we examined fourAI systems that address different aspects of
the discovery process. Although each of these programs is interesting in its own
right, theyshouldultimatelybecombinedintoasingle, integrateddiscovery system.
One advantage ofthis approach isthat it will increase ourunderstanding ofthe relationsamongthevariousformsofdiscovery. Thisunderstandingwillinturnconstrain
the component systems, since the outputs ofone program would have to conformto
the inputs ofanother. This will lead to revisions ofthe existing systems, and more
robust and plausible discovery programs will result. Another benefit is that the
resulting system would be more self-contained, relying less on the programmerand
more on its own devices. To the extent that this can be achieved, an integrated discoverysystemwouldbemuchlesssusceptibletothecriticismthatoneis "buildingin
discoveries" by providing the necessary inputs.
Sincethenotionofsearchiscentraltoallfourdiscoverysystems, letusexplore
the role ofsearch in the proposed integrated system. Clearly, the operators used by
each ofthe systems will remain the same, as will the heuristics for applying those
--- PAGE 476 ---
462 CHAPTER 16: FOUR ASPECTS OFSCIENTIFIC DISCOVERY
operators. Theinitial statesforeach systemwillbelargelythesame, butthey will no
longerbeprovidedby theprogrammer. Instead, they will be generatedby othersystems as output. Given a set ofoperators and rules for applying those operators, the
specificationofaninitialstateeffectivelydefinesaproblemspace. Thus,totheextent
thatdiscoverysystemA'sinitialstateiscreatedbyanothersystemB, wecanclaimthat
Bhasdefinedtheproblemspacethat A willsearch. Thismaylead A tospecifyanew
initial state for B, thus defining a new space for it to search. The dream ofthe AI
learning system that "pulls itselfup by its own bootstraps" is an old one, and we do
notexpectittobeachievedinthenearfuture. However, wedobelievethatitliesinthe
direction we proposetoexplore, in which individual learning systems are combined
to form a whole that becomes greaterthan the sum oftheir parts.
In this section, we examine some scenarios in which significant interactions
mighttakeplaceamongBACON, GLAUBER, STAHL, andDALTON. Ineachcase,
wewilltreatthe individualsystemsasblackboxes, andfocusontherelationbetween
their inputs and outputs. Although we are far from actually combining these programs into a unified system, we hope that these examples will convince the reader
thatsuchasystemisnotonlypossiblebutnecessaryifthecomplexprocesscalledscientific discovery is everto be understood.
16.6.1 Designing Experiments and Generalizing Laws
Earlierinthechapter, wenotedthatthediscoveryofqualitativelawsoftenprecedesthediscoveryofquantitativerelations. Thissuggeststhat GLAUBERshouldbe
able to contribute something to BACON's search for numeric laws. However, the
mostobviousconnectioninvolvesthesearchthroughthespaceofdataratherthanthe
space oflaws. The reader will recall that BACON relies on the programmer to provideasetofvariablesandvalues, leadingthesystemtorunparticularfactorialexperiments. Our hope is that GLAUBER will give BACON enough information to let it
design its ownexperiments. Forinstance, supposeBACONweretoldby GLAUBER
thatnitricoxide, nitrousoxide, andnitrogendioxidewereall substancesthat resulted
from reactions between nitrogen andoxygen. Given knowledge ofthis classofcompounds, an extended version of BACON might design an experiment in which the
substances entering a reaction (oxygen and nitrogen) were held constant, while the
output ofthe reaction was varied. Ifquantitative variables such as the weights ofthe
substances wereexamined, the resultingexperiment would lead BACON to Dalton's
law ofmultiple proportions, as described earlier.
The second use of GLAUBER's output relates to BACON's generalization
process. As it is currently implemented, BACON initially associates intrinsic
values with all potentially relevant symbolic conditionsand generalizes by remo\ ing
conditions whenever it finds that a set ofintrinsic values is useful in a new context.
--- PAGE 477 ---
LANGLEY, ZYTKOW, SIMON, AND BRADSHAW 463
However, theavailabilityoftheclassesgeneratedbyGLAUBER presentsanalternativegeneralizationmethod. Ratherthanremovingconditionsentirely, onecangeneralize by replacing the symbol in a condition with the class containing that symbol.
Forinstance, supposeBACON hasstoredasetofintrinsicvalues, withonecondition
forretrieval beingthatoneofthe substancesenteringthereaction isHC1. Next, suppose that the system finds the same intrinsic values useful when the substance is
HN0
insteadofHC1. Ratherthaninferringthatthiscondition isirrelevant, BACON
might decide that the intrinsic values should be retrieved whenever an acid is
involvedinthereaction(providedthat
GLAUBERhadalreadydefinedthisconcept).
Thisisamoreconservativeapproachtogeneralization,and it wouldallowBACON to
expressalargerclassofhypothesesthan itcurrentlycan. Ofcourse, thesystemcould
eventually decidetoremovethisconditionentirely, shouldthe intrinsic valuesprove
useful for nonacids as well.
This approach to generalization suggests that GLAUBER might findause for
BACON's output as well. Imagine an alternative scheme for generalizing intrinsic
values, in which BACON iterates through all symbolic values, collecting those for
whichasetofintrinsicvaluesisuseful. Supposetheconnectionbetweensymbolsand
values is stored in propositions, such as (intrinsics of {HO} are {1.23 2.76 4.35})
and (intrinsics of {HNO3} are {1.23 2.76 4.35}). Given such a set ofpropositions,
GLAUBER could define a class (say A) based on those substances for which the
valueswereusefulandformulatealawsummarizingthisknowledge, suchas (intrinsicsof{A} are {1.23 2.76 4.35}). Ifthisclasscorrespondedtoanotherclass, suchas
acids, somuchthebetter. Thus, onecanimagineGLAUBER aidingBACON'sgeneralization process, or BACON's generalization method providing data for
GLAUBER'S discoveries, depending on which system is allowed to operate first.
16.6.2 Determining the Components of Acids
ThefactthatbothSTAHLandGLAUBER
arecapableofdealingwithreactions
betweensubstancessuggeststhatthereis roomforinteractionbetweenthesesystems.
If GLAUBER isgivenreactionssuchas(reactsinputs {HO NaOH} outputs {Nad})
as inputs, it generates abstract reactions like (reacts inputs {acid alkali} outputs
{salt})asoutput. Ifsuchlawsarepassedto STAHL asdata, theprogramwill attempt
to determine the components ofthe "substances" involved. In this case, the system
wouldinferthatall saltsarecomposedofanacidandanalkali. Thisconclusionisnot
very surprising, though itisan inferenceonewouldlikeadiscovery systemtomake.
More complex interactions become possible when one realizes that concepts
suchas HO and NaOH arenotprimitiveatall, butarebaseduponlower-levelobservations much like the higher-level concepts of acid and alkali. For instance,
GLAUBER mightbegiven many factsaboutthetaste and colorofalarge setofsubstances (let us call them ol, o2, and so forth). Some ofthese substances would have
--- PAGE 478 ---
464 CHAPTER 16: FOUR ASPECTS OFSCIENTIFIC DISCOVERY
very similartastes, as well as very similarcolors. Based on such shared properties,
these chemicals would be grouped into the classes we know as hydrogen (H), chlorine (CI), andothers. Iftheprimitivesubstanceshadbeen involvedin reactions such
as (reacts inputs {ol o2} outputs {o3}), GLAUBER would rewritethese in terms of
the new classes, giving reaction "laws" like (reacts inputs {H CI} outputs {HC1}).
Such laws would then be processed by GLAUBER to determine still-higher-level
classesandlaws. However, theycouldalsobepassedasinputstotheSTAHLsystem.
Given inputs such as (reacts inputs {H CI} outputs {HO}), STAHL would
apply its rules to infer the components of the substances involved. In this case, it
wouldimmediatelyinferthat {HC1} iscomposedofhydrogenandchlorine. Byitself,
this inference is not very interesting. However, suppose STAHL then proceeded to
passthisresultbackto GLAUBER asadditionaldata. Inordertodothis, it mustrepresent the inference in GLAUBER'S terms, but the existing (components of {HO}
are {HO}) will serve quite well. Given this factand similar facts, such as (componentsof{HN0 3} are {H N0 3}), andgivenexamplesofreactionsinvolvingacidsand
alkalis, GLAUBER would formulate the class ofacids and generate by substitution
the laws (components of {acid} are {H CI}) and (components of {acid} are
{H N0 3}). Taken together, these laws would lead to a new class (let us say acidcomponents) with members like CI and N0 along with the law (components of
{acid} are {H acid-component}). This law (appropriately quantified) states that all
acids have hydrogen as one of their components. This conclusion can be reached
through acomplex interaction in whichGLAUBER affects STAHLs search through
the spaceofcomponential models, andSTAHL inturnaffectsthe GLAUBER search
through the space ofclasses and qualitative laws. A similar line ofreasoning would
leadthe
GLAUBER/STAHLcombinationtotheconclusionthatallmetalshavephlogiston as one oftheircomponents.
16.6.3 Building Structural Models
Aswe have seen, STAHL focusesondeterminingthe componentsofchemical
substances, while DALTON is concerned with thenumberofparticles involved in a
reaction. Thus, STAHLcanbeviewedaslayingthegroundworkforadetailed structural model, with DALTON being responsible for finalizing the model. Moreover.
DALTON requiresknowledgeaboutthecomponentsofasubstance intestingitsconservation assumption, and it is natural to assume that this information comes from
STAHL. Infact, thecouplingbetweentheseprogramsisalreadysufficientlycloseto
viewthemassuccessivestagesofasinglesystem, andweexpecttomergethem inour
future research. Let us explore the form such a combined system might take.
One can identify three distinct stages in the process of building structural
models. The first involves identifyingthecomponentsofsubstances, and isthe focus
ofthe STAHL system. We have discussed some potential extensions ofthis system,
such as providing theability topostulate unobservedcomponents, but this would not
--- PAGE 479 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 465
alterthebasicgoalofthe system. Thesecondstage involvesdeterminingthenumber
oftimes each component occurs in some substance, and is the focus ofDALTON.
Again, we have discussed some possible extensions, such as determining numeric
attributes ofthe components, but the basic task remains the same. The final stage,
which we have so far ignored, involves specifying the manner in which the various
componentsareconnectedtoeachother. Earlychemistswereabletoavoidthisissue,
but the discovery of organic molecules eventually forced them to deal with the
problem. Kekule's insight aboutthe structure ofthe benzene ring was essentially an
insight aboutthe connections between the components ofthat compound. Search in
this stage would involve selecting a pair ofcomponents to connect and selecting a
type ofbond to connect them.
We envision a single discovery system that searches the space of structural
models, first determining the components involved, then identifying the number of
particles takingpart, and finally modeling the connections betweentheseparticles.
Startingwithcompletelyabstractmodels, this systemwould successively instantiate
themuntiltheircompletestructurehadbeendetermined. Ateachstageinthisinstantiation process, the system would employ constraints, such as the conservation
assumption, to reject some models in favorofothers. Although the space ofmodels
would be quite large, search through this space would be relatively constrained.
Although considerable work would be involved in constructing such a program, it
would be an important step toward integrating the four discovery systems we have
described.
16.6.4 Discovering the Principles of Inheritance
Earlier in this chapter, we outlined an extended version of GLAUBER that
wouldbeabletonotepatternsamongconjunctionsoffacts. WediscussedtheapplicationofthissystemtoMendel'sdataonheredity, showinghow itcouldbeusedtoinfer
genotypicclasses(e.g.
pure-breedinggreenpeasG, mixedgreenpeasG'
andpurebreedingyellowpeasY)fromobservationsaboutphenotypes(e.g. greenandyellow
peas). In another section, weproposed an extended version of DALTON that, given
genotypicdescriptionsoftheoffspringresultingfromvariousmatings, wouldbeable
toinferMendel'stwo-traitmodeltoaccountforthosedescriptions.I3This suggestsa
straightforward relation between the two programs that should extend to other
domains besides genetics. We envision GLAUBER starting with directly observed
reactions and, based on regularities among those reactions, rewriting them at a
higherlevel ofdescription. DALTON wouldthentakethehigher-level reactionsand
devise structural models to account for them. According to this view, GLAUBER
,3In fact, this could best be accomplished by the integrated version ofSTAHL and DALTON just
described.
--- PAGE 480 ---
466 CHAPTER 16: FOUR ASPECTSOF SCIENTIFIC DISCOVERY
would serve mainly as a preprocessor for DALTON, transforming direct observations into an initial state upon which the structural modelercould operate.
However, information can flow in the opposite direction as well. Once
DALTON hasconstructedmodelsforasetofgenotypicclasses (suchas {gg} forG,
{yy} forY, and {gy} forG'), this informationcouldbepassedbacktoGLAUBER.
For instance, suppose GLAUBER begins with the following knowledge, some of
which would be provided by DALTON:
(components of {Y} are {y y})
(components of {G} are {g g})
(components of {G'} are {g y})
(has-property object {Y} color {yellow})
(has-property object {G} color {green})
(has-property object {G'} color {green})
Given this information, an extended version of GLAUBER would note two "facts"
involving green-colored classes and observe that both of these classes (G and G')
have the symbol g as one oftheir components. As a result, the following two laws
would be formulated:
VQ3P (components of {Q} are {g P})
VQ (has-property object {Q} color {green})
Inaddition, theclassQwouldbedefinedastheunionoftheclasses Gand G',
while P isdefined as having the members y andg. 14Takentogether, these laws state
thatall greenpeascontain atleastone instance ofg intheirlistofcomponents. This
example is similartotheearliercase inwhich GLAUBER noted hydrogen as acomponent of acids, but one can interpret it somewhat differently. In the context of
genetics, the above law is stating that g is a dominant trait, since it leads to green
plants whenever it occurs as a component. Again, we have seen that complex feedbackbetweentwodiscoverymethodscanleadtolawsthatcouldnotbediscoveredby
either method alone.
16.6.5 Constraining the Search for Structural Models
We have seen how DALTON's search through the space ofstructural models
canbealteredbyheuristics, suchasthecombiningvolumerulethatledtoAvogadro's
model ofthe waterreaction. However, we have notdiscussed the origin ofthe information used by such rules. For instance, Avogadro's heuristic must know the combining volumes for a reaction before it can be used to constrain search. Since this
l4Thecurrentversionof(hesystemcannothandlesituationsinwhichasubstancelikegistreatedasbotha
constantandamemberofaClass.ThiscapabilitywouldhavetobeaddedbeforeGLAUBERcould\sorkas
proposed.
--- PAGE 481 ---
LANGLEY,ZYTKOW, SIMON, AND BRADSHAW 467
information is numeric, it is natural to consider BACON as a possible source, and
upon reviewing BACON's chemical discoveries, we find that the system's common
divisor method generates the combining volumes required by DALTON. Thus
BACON's output can be used to direct DALTON's search through the space ofpossible models.
We have discussed an extended version of DALTON that would determine
numeric properties ofthecomponents in its models. Forexample, the system might
estimate the relative atomic weights of elements taking part in a set of reactions.
(This was a major concern of the early chemists.) Given such estimates, one can
imagine DALTON placing additional constraintson its models andusingthese constraints to reject some models in favor of others. For example, the system might
require that the estimated atomic weights be consistent across different reactions.
However, in order to estimate the relative weights ofthe components in a model,
DALTONwouldhavetoknowthecombiningweightsofthesubstancesinvolvedina
set ofreactions. Again, BACON is the obvious source for such knowledge, since it
generatescombiningweightsatthesametimethatitproducescombiningvolumes. In
summary, BACON has the potential to place significant constraints on DALTON's
search process. It is interesting to observethatdata-driven methods, like those used
in BACON, can be such an aid to theory-drive behavior of the type found in
DALTON.
16.6.6 Structure of the Proposed System
Theabovescenariosprovidesomeideaofthebehaviorweexpectfromtheintegrated discovery system, but we have not discussed the structure of the proposed
system. In particular, we should consider how closely linked the systems will be to
oneanother. InconsideringtherelationbetweenSTAHLandDALTON, wedecided
thatthecoupling shouldbeveryclose, sincethesesystemscanactuallybeviewedas
dealing withdifferentstages inthe samesearchprocess. However, itis notclearthat
the same conclusion holds for BACON, GLAUBER, and STAHL/DALTON, since
these systems seem to address genuinely different aspects ofdiscovery-the search
for quantitative laws, the search for qualitative laws, and the search for structural
models. More likely, the systems should be given access to a common blackboard,
and care should be taken to insure compatible representations.
Ifweassumethatthe systems shouldbeloosely coupled, we must still specify
whetherinteractionoccursoccasionallyorcontinuously. Thefirstapproachassumes
that one system would begin, run its course, and then deposit its results on the
common blackboard, tobe followedby anothersystemthattakesadvantageofthese
results to define its search space. This fits in well with the current version of
GLAUBER, which requires all facts at the outset ofa run. An alternative scheme
would have the systems running concurrently, with each depositing results on the
blackboard and with these results dynamically affecting the paths taken by other
--- PAGE 482 ---
468 CHAPTER 16: FOUR ASPECTSOFSCIENTIFIC DISCOVERY
systems. Thisapproach is well suitedtothe STAHLprogram, which already uses an
incremental approach to formulating componential models. Although an incremental systemlike STAHL(andtosomeextentBACON)canbeprovidedwithallthe
dataattheoutset, anall-at-oncesystemlike GLAUBERcannotberunin incremental
mode. Thus, if wedecidetopursueanincrementalversionoftheintegrateddiscovery
system, GLAUBER will have to be substantially revised in order to fit into this
framework.
16.6.7 Conclusions
Inthischapter, weexamined fouraspects ofthe diverse activity known as scientific discovery-finding quantitative laws, generating qualitative laws, inferring
the components of substances, and formulating structural models. Our approach
involvedconstructingAIsystemsthatfocusedonthesedifferentfacetsofscienceand
testingthemontheirabilitytoreplicatehistoricaldiscoveries. Wedrewoutexamples
mainly fromthehistoryofchemistry, sincethisareaprovidedusefultestsforeachof
the systems and since it allowed us to explore potential connections among the discovery programs. Wefoundthateachofthesystemscouldbe usefully viewedascarrying out search through a space oflaws or models, and we examined the operators
and heuristics usedtodirect searchthrough these spaces. We alsofoundthateach of
the systems has some important limitations, and we proposed some extensions that
should lead to improved future versions.
Although the four systems-BACON, GLAUBER, STAHL, and DALTONhave each contributed to our understanding of discovery, we believe that an even
greater understanding could result from exploring the relations among the systems.
Asaresult, weplantoconstructan integrateddiscovery systemthatwill incorporate
the individual systems as components. As we have noted many times, scientific discovery is a multifaceted process, and even within such an expanded framework, we
mustomitmanyofitsimportantaspects. Forinstance, wehavenotaddressedtheformulation ofmechanistic explanations such as the kinetic theory ofgases, the role of
structural analogies as studiedby Winston (1980) and Gentner (1983), orthe design
ofnew measurement devices. Thus, even ourgoal ofan integrated discover) system
is limited in some importantrespects. However, limitingone's focusofattention isa
venerableand useful tradition inthe history ofscience, and there will be ample time
toincorporatetheseadditional facetsofdiscoveryafterwebetterunderstandthe relations among the four existing systems.
ACKNOWLEDGMENTS
The research described in this chapter was supported b\ contract N00014-82
0168 1'rom the Office o\' Naval Research, Division o( Information Science.
--- PAGE 483 ---
LANGLEY,ZYTKOW, SIMON. AND BRADSHAW 469
References
Bradshaw; G. L., Langley, P.. and Simon, H. A.. "BACON.4: The Discovery ofIntrinsic Properties,"
ProceedingsoftheThirdBiennialConferenceoftheCanadianSocietyforComputationalStudies
ofIntelligence, Victoria, B.C., pp. 19-25, 1980.
Brown. J. S.. '"StepsTowardAutomatic Theory Formation," Proceedingsofthe Third1JCAI, Stanford,
Calif., pp. 20-23, 1973.
Emde,W.,Habel,C.H.,andRollinger,C, "TheDiscoveryoftheEquatororConceptDrivenLearning,"
ProceedingsoftheEighthIJCAI, Karlsruhe, W. Ger., pp. 455-58, 1983.
Feigenbaum, E. A., Buchanan, B. G. , andLederberg, J.. "OnGenerality andProblemSolving: ACase
Study Using the DENDRAL Program." inMachineIntelligence 6, Edinburgh University Press,
Edinburgh, 1971.
Gentner. D., "StructureMapping:ATheoreticalFrameworkforAnalogy," CognitiveScience,Vol.7,pp.
155-70, 1983.
Langley. P.. Bradshaw, G. L., and Simon, H. A., "Data-driven and Expectation-driven Discovery of
EmpiricalLaws,"ProceedingsoftheFourthBiennialConferenceoftheCanadianSocietyforComputationalStudiesofIntelligence, Sasketoon, Saskatchewan, pp. 137-43, 1982.
"RediscoveringChemistrywiththeBACONSystem,"inMachineLearning:AnArtificialIntelligenceApproach , R. S. Michalski, J. G. Carbonell, andT M. Mitchell (Eds.), Tioga, PaloAlto,
Calif.. 1983.
Langley,P., Zytkow,J., Bradshaw,G. L..andSimon,H. A., "MechanismsforQualitativeandQuantitative Discovery," ProceedingsoftheInternationalMachineLearning Workshop, R. S. Michalski
(Ed.),AllertonHouse,UniversityofIllinoisatUrbana-Champaign,pp. 12-32,June22-24, 1983.
(AnupdatedversionofthispaperappearsasChap. 16ofthisvolume.)
Michalski. R. S., and Stepp, R. E., "Learning from Observation: Conceptual Clustering," inMachine
Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
Newell, A., andSimon, H. A., HumanProblemSolving, Prentice-Hall, EnglewoodCliffs, N.J., 1972.
Rosenbloom,P., "TheChunkingModelofGoalHierarchies:AModelofPracticeandStimulus-Response
Compatibility," Ph.D. diss., Carnegie-MellonUniversity, 1983.
Winston, P. H., "Learning and Reasoning by Analogy," Communications ofthe ACM, Vol. 23, pp.
689-703, 1980.
Wolff, J. G., "Grammar Discovery as DataCompression," ProceedingsoftheAISB/GIConferenceon
ArtificialIntelligence, Hamburg, W. Ger., pp. 375-79, 1978.
Zytkow,J.M.,andSimon,H.A., "ATheoryofHistoricalDiscovery:TheConstructionofComponential
Models." ToappearinMachineLearning, Vol. I, 1986.
--- PAGE 484 ---
--- PAGE 485 ---
CONCEPTUAL CLUSTERING:
Inventing Goal-Oriented Classifications of
Structured Objects
Robert E. Stepp III
UniversityofIllinois at Urbana-Champaign
Ryszard S. Michalski*
Massachusetts Institute ofTechnology
Abstract
An important form ofinductive learning is inventing a meaningful classification ofgiven objects or events. This chapter extends the authors' previous work on
this problem that was based on conceptualclustering, that is, grouping objects into
conceptually simpleclasses. Incontrasttothepastwork, thenewmethoddealswith
classifyingobjectsrepresentedbystructuraldescriptionsratherthanby sequencesof
attribute values. These descriptions are expressed inAnnotatedPredicate Calculus
(APC), which is atypedpredicate logic calculus with additional operators.
Itisshownthatinordertocreateameaningfulclassification, asystemmustbe
equippedwithbackgroundknowledge, whichincludesgoalsofclassification, classification evaluation criteria, and deductive and inductive inference rules. The goals
and goal-relevant descriptive concepts are organized into a Goal Dependency Network(GDN). Inferencerulespermitthe systemtoderivehigh-leveldescriptiveconcepts such as functional and causal attributes from lower-level descriptive concepts
provided initially. ExampleclassificationscreatedbytheprogramCLUSTER/S and
by people are presented.
*OnleaveofabsencefromtheUniversityofIllinoisatUrbana-Champaign.
--- PAGE 486 ---
472 CHAPTER 17: CONCEPTUALCLUSTERING
INTRODUCTION
17.1
Creatingaclassification1 istypicallythefirststepindevelopingatheoryabout
a collection ofobservations or phenomena. This process is a form oflearning from
observation (learning without a teacher), and its goal is to structure given observations into a hierarchy ofmeaningful categories. The problem ofautomatically creatingsuchahierarchyhassofarreceivedlittleattentioninAI. Yetcreatingclassifications is a very basic and widely practiced intellectual process.
Past work on this problem was done mostly outside AI under the headings of
numerical taxonomy and cluster analysis (Anderberg, 1973). Those methods are
based on the application ofa mathematical measure ofsimilarity between objects,
defined over a finite, a priori given set ofobject attributes. Classes of objects are
taken ascollectionsofobjectswithhigh intraclass andlow interclass similarity. The
methods assumethat objects are characterized by sequences ofattribute/value pairs
and that this information is sufficient for creating a classification. The methods do
not take into consideration any backgroundknowledge about the semantic relationshipsamongobjectattributesorglobalconceptsthatcouldbeusedforcharacterizing
objectconfigurations. Nordotheytake intoconsiderationpossible goalsofclassification that might be indicated by background knowledge.
Asaresult,classificationsobtainedbytraditionalmethodsareoftendifficultto
interpret conceptually. The problem ofinterpreting the results has remained a challenging task for the data analyst. In addition, traditional classification-building
methods describe objects by attribute value sequences and therefore are inadequate
for creating classifications of structured objects. The description of such objects
must involve not only attributes ofobjects as a whole but also attributes of object
components and relationships among these components.
Thischapterdescribesamethod forautomatedgeneration ofclassificationsof
structuredobjectsthroughaprocessofconceptualclustering. Thisprocessgenerates
classes (clustersofobjects)by firstgeneratingconceptual descriptions oftheclasses
andthenclassifyingtheobjectsaccordingtothesedescriptions. The method is illustrated by a sample problem, and classifications produced by machine are compared
to those produced by people.
17.2 THE GOAL OF THIS RESEARCH
The idea of conceptual clustering leads to an entirely new approach to the
problem ofcreating classifications (Michalski, 1980a; Michalski and Stepp. 1983a,
1983b; Stepp, 1984). This ideastatesthatobjectsshouldbearranged intoclasses that
'Creatingorbuildingaclassificationinvolvestwosubprocesses:
generatinganappropriatesotofcategories and (2)classifyingall givenentitiesaccording tothe generatedcategories.
--- PAGE 487 ---
STEPPANDMICHALSKI 473
representsimpleconceptsratherthanclassesdefinedsolelybyapredefinedmeasure
ofsimilarity among their members.
Inthe earlierwork on conceptual clustering, objects orevents were described
by attribute-value sequences. The method arranged the objects into a hierarchy of
classes described by conjunctive concepts. These concepts are expressed as logical
products ofrelations on selected object attributes. The generated sibling classes of
anynodeinthehierarchyrepresentedthemostpreferred(sub)classificationfromthis
nodeaccordingtoagivenpreferencecriterion. Thebackgroundknowledgeincluded
thedefinitions ofthe attributesused inobjectdescriptions, theirdomains andtypes,
and the classification preference criterion.
This research extends the previous work in three ways:
• Objectsandclassesaredescribedbystructuraldescriptions, whichareexpressed
in AnnotatedPredicate Calculus (APC), a typed predicate calculus with additional operators.
• The background knowledge includes inference rules for deriving high-level
descriptive concepts from the low-level concepts initially provided.2
• Thesystemissuppliedwithageneralgoaloftheclassification, whichprovides
the means for identifying relevantdescriptors and inference rules forderiving
newdescriptors. Thisavoidsthenecessityofdefiningthemexplicitly, asinthe
previous method.
An important aspect of this approach is the emphasis placed on the role ofbackground knowledge for constructing meaningful and useful classifications. In this
method the background knowledge consists ofa network ofgoals ofthe classification, inference rules and heuristics for deriving new descriptors, definitions of
attributedomainsandtypes, andtheclassificationpreferencecriterion. Thenetwork
ofgoals, calledthe GoalDependencyNetwork(GDN), isusedforguidingthe search
for relevant descriptors and inference rules.
The necessity of using background knowledge in any form of inductive
learningisindicatedinthetheoryofinductivelearning(Michalski, 1983). Important
workinvolvingbackgroundknowledgehasbeendonebyWinston(seechap. 3 ofthis
volume), who describes an incremental learning process in which the background
knowledgecontainsrelevantprecedents, exercises, andunlessconditions. Inchapter
19DeJongpresentsamethodofusingbackgroundknowledgetoacquireexplanatory
schematathatdescribe sequencesofeventspresentedas stories. Background knowledge hasalsobeenusedby Mitchell andKeller(1983)toguide an inductive learning
program for acquiring aproblem-solving heuristics in integral calculus. In learning
2Thedescriptiveconceptsarecalleddescriptorsandincludeattributes,rt-aryfunctions,andrelationsused
tocharacterizeobjectsorevents.
--- PAGE 488 ---
474 CHAPTER 17: CONCEPTUALCLUSTERING
by analogy, describedby Burstein inchapter 13, alargebody ofcausal knowledge is
usedto "fill out" incompletedescriptionsandguideanalogical inference. Carbonell
(1983, and chap. 14 of this volume) developed a method for acquiring problemsolvingstrategiesbyanalogytosolutionstosimilarproblems. Rendell'sProbabilistic
Learning System demonstrated the usefulness of clustering points in the solution
spaceforreducingthesearchrequiredinproblemsolving(Rendell, 1983). Asforthe
problem of learning structural descriptions from examples, various aspects ofthis
problem are discussed in Winston (1984) and Dietterich and Michalski (1983).
Toprovidethenecessarybackground, section 17.3 presentsabriefoverviewof
the authors' earlier method of attribute-based conjunctive conceptual clustering.
Section 17.4 focusesonthe roleofbackgroundknowledgeandgoals inbuildingclassifications. Following that, section 17.5 presents a sample problem involving
building a classification of structured objects. Finally, section 17.6 presents two
methods for constructing classifications of structured objects that employ background knowledge.
17.3 ATTRIBUTE-BASED CONJUNCTIVE CONCEPTUAL CLUSTERING
(PREVIOUS WORK)
This section briefly describes the authors' previous work on classifications
usingthe methodofattribute-basedconjunctiveconceptualclustering (AC3
which
isthe sortingbasisofthe methodpresentedhere. Themain ideabehind AC3 isthata
configurationofobjectsformsaclassonlyifitcanbedescribedbyaconjunctiveconcept involving relationsonobjectattributes. AC3 isa special case ofgeneral conceptual clustering that generates a network ofconcepts to characterize a collection of
objects. The problem posed in the framework ofAC3 is defined as follows:
Given: A set ofobjects (physical or abstract),
A set ofattributes to be used to characterize the objects, and
A body ofbackground knowledge, which includes the problem constraints, properties ofattributes, inference rules for generating new
attributes, and a criterion for evaluating the quality of candidate
classifications;
Find: A hierarchy of object classes, and their descriptions in the form of
conjunctive statements. Subclassesthataredescendantsofany parent
class should have logically disjointdescriptionsand maximize a clusteringpreference criterion.
As mentioned before, in conventional data analysis classes ofobjects arc formulated solely on the basis ofa measure ofobject similarity. The similarity between
anytwoobjects ischaracterized b\ a single number: the valueofa similarit) function
--- PAGE 489 ---
STEPPAND MICHALSKI 475
appliedtosymbolicdescriptionsofobjects. Thesesymbolicdescriptionsarevectors,
whose components are scores on selected object attributes. Such measures ofsimilarityarecontextfree;thatis,thesimilaritybetweenanytwoobjectsAandBdepends
solely onthepropertiesoftheobjects and is notinfluenced by any context (theenvironmentsurroundingtheobjects). Consequently, methodsthatusesuchmeasuresare
fundamentally unable to capture the gestalt properties of object clusters, that is,
propertiesthatcharacterizeaclusterasawholeandarenotderivablefromproperties
ofindividualentities. Inordertodetectsuchproperties,thesystemmustbeequipped
with the ability to recognize configurations of objects representing certain global
concepts.
This idea is the basis ofconceptual clustering. Instead ofsimilarity between
objects, say, A and B, the method uses conceptual cohesiveness ofA and B, which
depends not only on those objects and surrounding objects E (the environment) but
alsoonasetofconceptsCthatareavailablefordescribingAandBtogether. Thus,the
conceptual cohesiveness between two objects A and B is a four-argument function
f(A,B,E,C) in contrast to an ordinary two-argument similarity functionf(A,B).
The conjunctive conceptual clustering method consists oftwo phases: a clustering phase and a hierarchy-building phase. The clustering phase arranges objects
intoclassesusingconceptualcohesiveness, sothattheobtainedclusteringmaximizes
the given context-based clustering preference criterion. The hierarchy-building
phase starts with building first-level conceptual classifications ofall objects (at the
root of the hierarchy). Then it recursively builds a classification for each sibling
group of objects from the previous classification until the stop growth criterion
is met.
The clustering phase algorithm works by alternately selecting a set of seed
objects(oneperclass) andusingtheseedstoguideinductiveinferenceoverpositiveonly events to produce generalized, but mutually disjoint, descriptions of object
classes. This process insures that each seed object is placed into a separate class.
Eachclusterdescriptionisasgeneralaspossible (variousgeneralizationtransformationsareexhaustivelyapplied)sothatitcoversthegivenseedbutnootherseeds. Differentseedsareusedoverseveral iterationswhiletheclusteringpreferencecriterion
is monitored. The algorithm halts when the clustering preference criterion does not
improve for a dynamically determined number of iterations. The algorithm is
described in detail in Michalski and Stepp (1983b).
17.4 THE USE OF BACKGROUND KNOWLEDGE AND GOALS
Suppose that we are observing a typical restaurant table on which there are
suchobjectsasfoodonaplate, asalad, utensils, saltandpepper, napkins, avasewith
flowers, a coffee cup, and so on, as illustrated in figure 17-1. Suppose a person is
--- PAGE 490 ---
476 CHAPTER 17: CONCEPTUALCLUSTERING
Figure 17-1: Atypical restauranttable.
askedtobuildameaningfulclassificationofobjectsonthetable. Onewaytocreatea
classification is to perform the following chain ofinferences:
• Salt and pepper are seasonings
Seasonings are used to add zest to food
Seasoned food is something to be eaten
Things that are to be eaten are edible
Salt and pepper are edible
• Salad is a vegetable
Vegetables are food
Food is something to be eaten
Things that are to be eaten are edible
Salad is edible
A similarchain ofinferences applied tomeat on aplate and cake on a dessertplate
will also lead tothe concept is edible. On the otherhand, a napkin is not food and is
therefore notedible. A vasecontaining flowersisnot foodand istherefore not edible.
Consequently, one meaningful classification ofobjects on the table is simply edible
versus inedible.
--- PAGE 491 ---
STEPPANDMICHALSKI 477
One may observe that when the background knowledge contains many such
rulesofinference, alargenumberofdifferentbutequally meaningfulclassifications
canbecreated. Theproblemisthenhowtodecidewhichoftheclassificationsisbest
ormostappropriate. Forexample, ifinferencerulesaboutfoodtypes, suppliers,processing, andpackagingwerecontainedintheknowledgebase, theycouldbeusedfor
generating otherclassifications. Some new classifications mightproduce categories
such asdomesticversus importedorperishable versus nonperishable. The problem
ofwhich classification to select can be resolved by assuming a general goal or purposetobeservedbytheclassification. Assume, forexample, thattheclassificationis
tobe useful to anagent who wants tosurvive. Such abehavioral goal to survivedictatesthataperson hasto ingestfoodandliquids, andbe safe. Furthermore, the subgoal ingestcanbelinkedtothetwo modes ofingestion, thatis, consuming foodand
drinking liquids. In the context ofthe subordinate goals reached by links from the
most general goal node, the relevant attributes are, for example, is_edible, and
is_potable, tastes_good. The attribute tastes_goodis linked by the implication relation to is_edible or is_potable (if something tastes good then it is either edible or
potable).
Thusthereisageneralgoal leadingto subgoalsandthentooneormore attributesthatarerelevantinthecontextofthegoal. Suchrelationshipsarecapturedinthe
GoalDependencyNetwork(GDN)mentionedearlier. Thisnetworklinksgoals, subGDN
goals, andrelevantattributestogether. Partofahypothetical headedbythesurvivegoalisshowninfigure 17-2. Intheillustration, maingoalsaredenotedbydouble
is_potable ^••.
' drink
. liquid ^
ingest tastes_good l>
j^-
consume
C^ v food
Ageneralgoal
C3 Asubordinategoal TI
- - A A r g e o l a e l va s n u t bo a r t d tr i i n b a u t t i e o o n r r p el r a e t d i i o c n ate is_edible Jk"
--*• Anattributerelevancyrelation
*- Animplicationrelationbetweenattributes
Figure 17-2: AGDNheadedbythegoalsurvive.
--- PAGE 492 ---
478 CHAPTER 17: CONCEPTUALCLUSTERING
ellipses, and subgoals and relevant descriptors are denoted by regular ellipses and
rectangles, respectively. The solid arcs between nodes are directed from goal nodes
towards subordinate goal nodes. The dashed arcs between nodes and attributes are
directed from goal nodes to relevant attribute nodes, and the dotted arcs link an
attribute with an implied attribute.
Supposethatthegoalsoftheagentincludenotonlysurvivebutalsobehealthy
andbeautiful. Whenbothgoalsareinvolved, a GDN suchastheoneinfigure 17-3 is
used. Here, the links from the two top-level goals converge at the consume dietary
foodsubgoal which links to the subordinate goals consume leanfoods and consume
balanceddiet. Attached tothese latter nodes are the relevant descriptorsfat content
safe
is_edible
fatcontent islean
consume
lean
foods
consume
dietary
foods
/ consume \
<^^> Ageneral goal 1 N. bal d a ie n t ced J, / \ vege e t a a t bles J 1
Asubordinategoal
I i
-I 1 A relevantattributeorpredicate t i
- *• Agoal subordination relation nutrient is_vegetabk
•» Anattributerclevanc) relation content
Figure 17-3: A (il)N forthe goalssurviveandbehealthyandbeautiful.
--- PAGE 493 ---
STEPPAND MICHALSKI 479
and isJean, and nutrient content, respectively. The two subgoal nodes mentioned
abovehave subordinategoalnodesoftheirown. These includeeatleanmeatandeat
vegetables. The relevant descriptors attached to these nodes include the predicates
is_lean, is_meat, and is_vegetable. Thus, by the addition of the top-level goal be
healthyandbeautiful, five additional relevant attributes are proposedby the GDN.
Addingatop-levelgoalmay reducethenumberofattributesthoughttoberelevant. Supposeweaddavegetarianlife-stylegoal. Linkpathsfromthethreetop-level
goalsconvergeatthesubordinategoaleatvegetables. This increasestherelevancyof
the isvegetable predicate which now dominates in relevancy over the other attributes. The GDN forthis last situation is illustrated in figure 17-4.
Letus now considera specific problem: the system isgiven symbolic descriptionsofobjectsonthetable intermsoftheirphysical attributes (including structure)
alongwithsurviveasageneralgoalofclassification, andwewant ittocreatetheclassificationintoedibleversusinedibleobjects. Noticefirstthatcreatingsuchaclassification solely on the basis of original attributes is practically impossible, because
objectsthatareinthesamefunctionalclass(edibleorinedible)canbevastlydifferent
in terms oftheir physical properties (see Winston, 1984, for a discussion of this
problem). Aprogramthatcouldclassifyobjectsonatableasedibleorinediblewould
have to be equipped with background knowledge consisting of the previously
described inference rules and with the ability to use them in a goal-directed way.
Backgroundknowledgebuiltintotheprogramcanbedividedintogeneralpurposeanddomainspecific. General-purposeknowledgeconsistsoffundamentalconstraints and criteria specifying general properties ofclassifications. This includes a
specification ofthe domain ofeach descriptor, the type ofthe domain (unordered,
linearlyordered, ortree-structureordered), andasequenceofelementarycriteriato
be appliedlexicographically with tolerancestoevaluate classifications. TheLexicographicalEvaluationFunctionalwithtolerances, orLEF(seesection 17.6.2), isused
to select from among candidate classification schemes the one that is the most preferred viewpoint ofthe given goal.
Domain-specific background knowledge consists of inference rules for
deriving values fornew descriptors and GDN to infer which descriptors (attributes,
functions, or predicates) are relevantto the goal ofclassification.
Event descriptors can be divided into initialdescriptors and deriveddescriptors. Both kinds ofdescriptors can appear attached to goal nodes in the GDN. The
initialdescriptorscanbedividedintothosethatare relevantwith respecttothegoals
andthosethatareirrelevant. Insomeproblems, therelevantdescriptorsareunknown
andnotnecessarilyprovidedasinitialdescriptors. A solutioncanstillbeobtainedin
such cases ifbackground knowledge canbeusedtoderive relevantdescriptors from
thosethatare initially given. Inference rules inthe knowledge baseare usedto infer
GDN
thevaluesofthederiveddescriptors. Domain-specificknowledgeinthe isused
toguidetheapplicationofinferencerulestowarddescriptorsthatarelikelytoberelevant and thus worth the computational cost oftheir derivation.
--- PAGE 494 ---
480 CHAPTER 17: CONCEPTUALCLUSTERING
safe
is_edible
fatcontent is_lean is_meat
dietary
foods
is_vegetable
(vegetarian \\_
life-style
Ageneralgoal
CZ) Asubordinategoal
-CZH A relevantattributeorpredicate
»• Agoalsubordinationrelation
--* Anattribute relevancy relation
Figure 17-4: A (il)N forthegoalssurvive, behealthyandbeautiful, and vegetarian Hfe-stvU-
--- PAGE 495 ---
STEPPAND MICHALSKI 481
Derived descriptors can be divided into two categories:
• Descriptorsderivedbylogicalinference. These descriptors are predicatesand
functions obtained by the application ofgeneral and problem-specific inferencerulestotheinitialdescriptionsoftheobjects. Inthiswork, inferencerules
consistofaconditionpartandaconsequencepart. Wheneveranobjectdescriptionmatchestheconditionportionofarule, theconsequenceportionisapplied
to the object description. The consequence may be composed either of new
predicates and functions to be asserted or of arithmetic expressions that are
evaluated. In either case, the new descriptors (unless already present) are
appended to the object description and become available as attributes that are
potentially relevant forbuilding classifications ofthe objects.
• Descriptors derivedby special computations, experiments, ordevices. These
descriptors are obtained from the initial descriptors by the application ofspecializeddescriptorgenerationprocedures, by runningexperiments, orbyactivatingsomeexternaldevice,thatis,anyprocedureotherthantheapplicationof
condition-consequence rules. Examples ofsuch descriptors generated by the
INDUCE/2 program (Hoff, Michalski, and Stepp, 1983) are "the number of
object subparts," "the numberofsubparts with some specific property," "the
numberofdifferentvaluesobservedforanattribute," and"propertiescommon
toallsubparts."Theprogramcanalsoautomaticallygeneratemultiplacepredicates to assert "same function value" for several parts-for example,
samecolor(/?l,/?2)-andsingle-placepredicatestoassertheadandtailpositions
in a chain of properties-for example, to assert most-ontop(pl) and leastontop(/?l) when given ontop(/?l,/?2) and ontop(/?2,/?3).
17.5 BUILDING CLASSIFICATIONS OF STRUCTURED OBJECTS
Let us turn now to the problem of classifying structured objects. Consider,
forexample, theproblem ofclassifying trains,3 shown in figure 17-5. The trains are
structuredobjects,eachconsistingofasequenceofcarsofdifferentshapesandsizes.
Theindividualcarscarryavariablenumberofitemsofdifferentshapes. Theproblem
presented is in a class of learning problems known as learningfrom observation,
or conceptformation. It is interesting to both AI researchers and cognitive
psychologists.
3Thisexample isareformulationofaproblem knownas "East-andWestboundTrains" (Michalskiand
Larson, 1977).Intheoriginalformulation,twocollectionsoftrainsweregiven,thosethatwereeastbound
(AtoE)andthosethatwerewestbound(FtoJ);theproblemwastolearnasimplerulefordistinguishing
between the eastbound and the westbound trains. Thus the original problem was that oflearningfrom
examples, orconceptacquisition.
--- PAGE 496 ---
482 CHAPTER 17: CONCEPTUALCLUSTERING
^^e^AHQ
hqoqt JL
f. i ts .-a. cr ^J -d? # - •-
t^KAMaj-tKi
Figure 17-5: Howwouldyouclassifythesetrains?
Humanclassificationsofthetrainsshowninfigure 17-5havebeeninvestigated
by Medin, Wattenmaker, and Michalski (1985). Thetentrains were placed on separate index cards sothey could be arranged into groups by the subjects in the experiment. Each subject was instructed topartitionthe trainsaccording tothree methods
and to state the rationale used:
1. Arrange the trains into any number ofgroups.
2. Arrange the trains into two equal groups.
3. Arrange the trains into any number ofgroups ofconceptually similar objects
plus an "other" group to hold any unusual or hard-to-classify trains.
The experiment was completed by thirty-one subjects who made a total o( ninetythree classification schemes forpartitioning the objects. The most popular basis for
classification (seventeen repetitions) was the number ofcars in the trains (a simple
attributethat characterizeseachtrainasawhole). Thethreeclusters formedwerethe
following: "trains containing two cars," "trains containing three cars," and "trains
--- PAGE 497 ---
STEPPAND MICHALSKI 483
containing four cars." The second most frequent classification (seven repetitions)
was based on the engine wheel color. These two classifications are shown in figure
17-6. Ofthe ninety-three classifications produced, forty ofthemwere unique. Thus,
althoughtherewasnoexplicitgoal forclassificationgiven, therewasapatternofuniformity among the subjects. Thepattern was notavery strong one, however, aswitnessed by a wide spectrum ofsingleton solutions.
Thisproblem isanexampleofaclassofproblemsforwhichtheimplicitclassificationgoal istogenerateclassesthatareconceptually simpleandbasedoneasy-todeterminevisualattributes. Whenpeopleareaskedtobuildsuchclassifications, they
typically form classes with disjoint descriptions, as in the study by Medin. People
typically do not suggest intersecting classifications, and it is for this reason that we
focus on methods that produce disjoint descriptions.
Classificationproblemssuchasthisoneoccurwhenonewantstoorganizeand
classify observations that require structural descriptions-for example, when one
wants to classify physical or chemical structures, analyze genetic sequences, build
taxonomies ofplants or animals, characterize visual scenes, or split a sequence of
temporal events into episodes with simple meanings. As an example of the latter
problem,considersplittingakidnappingstoryintoepisodessuchaskidnapping,bargaining, and exchange (DeJong, 1981).
Oneproblemofconcernhere istodevelopageneral methodthatwhenapplied
tothe collection ofstructured objects, such as trains, could potentially generate the
conjunctive concepts occurring in human classifications or invent new concepts
having similar appeal. We first assume that there is only a very general goal for a
classification, suchassimplicityofdescriptionsofcategoriesorgoodfitofthe categoriestotheexamples. Themethod shouldbeabletogenerateconceptual categories
thatcanbedescribedby aconjunctionofpredicates. Theseconjunctions should represent a minimal overgeneralization ofobserved events in the class soas to insure a
good "fit" between each class description and the events.
Figure 17-7showsahypothetical GDN foraclassificationforwhichthegeneral
goalistofindsimplevisualpatterns. Asubordinategoalistolookforsimplegeometrical regularities in objectdescriptions. Forthe trains problem, this goal node leads
tothe relevant variables such as numberofcars, colorofwheels, numberofwheels,
numberofitemscarried, andsoon. Thesimplegeometricalregularitiesgoallinksto
the two subordinate goals shape ofcomponents and similarity ofcomponents. The
first ofthese subgoals leads to relevant attributes involving shape {cargo shape,
engineshape, carshape). Thesecondsubgoal leadstoavarietyofrelevantattributes
relating one component of a train to other components. The number ofdifferent
shapes attribute gives the countofthe differentcar shapes in a train. A count ofthe
numberofdifferentcargoshapesinacarwouldbeanotherattributeofthissametype.
Thesamecarshapeorsamecolorofwheelsattributes are predicates oftwoor more
variables that denote the equality of feature values across several components in
the train. If all components have the same value for some attribute, then aforall
--- PAGE 498 ---
J5 511 e
g C I Z] e U g
FT U
1 \
5o3
'8)
0C) C
'c5b
. o n
d U
i-i ft,
uu u
--- PAGE 499 ---
STEPPANDMICHALSKI 485
number
ofcars
CIl2> Ageneralgoal
CZ^> Asubordinategoal
-I I Arelevantattributeorpredicate
*• Agoalsubordinationrelation
Anattributerelevancyrelation
Figure 17-7: AGDN forthegoaloffindingsimplevisual patterns.
predicate, such as all wheels in the train are black, is a relevant attribute for
describing the situation.
AsexamplesofsolutionsobtainedbytheprogramCLUSTER/S implementing
themethod, figure 17-8 showstwoclassificationscreatedforthetrainsproblem. For
this problem, the structured descriptions ofeach train involve the descriptors contains, infront, carshape, numberofwheels, wheelcolor, cargoshape, andnumberof
items carried. The program determined several new descriptors that were not in the
initial descriptions, such as number ofdifferent shapes, same-shape predicates,
same-color-of-wheels predicates and so on.
The generated attribute vectors were processed using a classification evaluation criterion that attempts to minimize the number ofattributes used in a description, maximize the number ofattributes that singly discriminate among all classes,
and maximizethe numberofattributesthattakedifferentvalues indifferentclasses.
Minimizing the number ofattributes used tends to conflict with the other two elementarycriteria. Thiswashandledbyspecifyingahightolerance(90percent)forthe
firstelementarycriterionandzerotolerancesforthesecondandthirdelementarycriteria in the LEF evaluation criterion described in section 17.6.2.
--- PAGE 500 ---
•!->
j^s
I o
C£ 'I eofl
C C
c G
09 09
o <D
I £ rc
O ~ < ; N o o
12c )5
U O U S
woc
-UJ
1) £
s 2
t3 £a
oil <ED y
O ^
I E i
Ion "
C 3
<u -c
I - < t c D - . > c. o Cy.
2-5 s
T jFe« c L>
- o E
o (• ^N•
Q&. s
« -%
ajp
--- PAGE 501 ---
STEPPAND MICHALSKI 487
Classification A infigure 17-8wasgeneratedbytheprogramwithtwodifferent
sets ofclass descriptions. The top class ("There are two different car shapes in the
train") was also described as "The third car from the engine (ifit exists) has black
wheels." The bottom class ("There are three or more different car shapes in the
train") was also described as "The third car from the engine exists and has white
wheels." ClassificationB infigure 17-8 isbasedonthederivedpredicatesamecolor.
Bothclassificationsreceivedthesameevaluationcriterionscoreandwereconsidered
to be alternative classifications. Solutions of the kind shown in figure 17-8 are
appealing because the differences between classes is striking yet not obvious from
casual inspection.
17.6 TWO METHODS FOR BUILDING CLASSIFICATIONS
This section describes two methods for solving problems ofthe kindposed in
the preceding section, that is, building a classification ofa collection ofstructured
objects. OnemethodiscalledRD, whichstandsforrepeateddiscrimination, andthe
other is called CA, which stands forclassifyingattributes. The RD method is based
on the authors' previous work and reduces the problem ofbuilding a classification
into a sequence of concept acquisition problems, specifically, problems of determining discriminant descriptions ofobjects with given class labels (Michalski and
Stepp, 1983b). The CA method is based on generating candidate classifying attributeseitherfromtheinitiallygivenpoolofattributesorfromderivedattributesgenerated with the aid ofinference rules and the Goal Dependency Network.
Thetwomethodsaresimilarinthattheybothusethe same representationlanguage (APC) for describing objects, classes of objects, and general and problemspecific background knowledge. Both methods use the LEF as the general-purpose
criterion for measuring the quality ofgenerated candidate solutions. The APC and
LEF are described in the nexttwo sections, respectively.
17.6.1 The Description Language: Annotated Predicate Calculus
The AnnotatedPredicateCalculus (APC) isanextensionofpredicatecalculus
that uses several novel forms and attaches anannotation toeach predicate, variable,
and function (Michalski, 1983). The annotation is a store ofinformation about the
given predicate or atomic function, such as the type and structure ofits legal value
set, related (more general or more specific) descriptors in descriptorhierarchy, and
other information. In addition to all the forms found in predicate calculus, the languagealsousesaspecialkindofpredicatecalledaselector. Asimpleselectorisinthe
form
[atomic-function REL value-of-atomic-function]
--- PAGE 502 ---
488 CHAPTER 17: CONCEPTUALCLUSTERING
whereREL (relation) standsforoneofthesymbols = =£ < > < >. Anexampleof
such a selector is
[weight(box) > 2kg]
which means "the weight ofthe box is greaterthan 2 kg." A more complex selector
may involve internal disjunction or internal conjunction. These two operators apply to terms rather than to predicates and are illustrated by the two corresponding
examples:
[color(box) = red & purple] "The colorofthe box is either red or
purple."
[color(boxl & box2) = red] "Thecolorofbox 1 andbox2 isred."
The meaning ofthe internal disjunction operator is defined by
= a&b]^ = & =
[f(x) [f(x) a] [f(x) b]
and the meaning ofthe internal conjunction operator is definedby
&y)-a]* = & =
[/(* [f(x) a] [fiy) a].
Selectors can be combined by standard logical operators to form more complex
expressions. Background knowledge is expressed as a set ofAPC implicative rules:
CONDITION => CONSEQUENCE
where CONDITION and CONSEQUENCE are conjunctions of selectors. Thus a
rulein APC ismoregeneralthantheHornclauseusedin PROLOG. If CONDITION
is satisfied, the CONSEQUENCE is asserted. To understand the implicative statement, considertheassertion "vegetablesarefood" fromtheexample insection 17.4.
Itcanbeexpressed in APCbythe following statement, which says, "ifanobject isa
vegetable then it is also a food":
[is_vegetable(object)] => [is_food(object)]
An alternative way to express this idea in APC is
[object-type(object) = vegetable] => [object-type(object) = food]
which says, "ifthe type category ofan object is vegetable than the type category is
alsofood." In this latter statement vegetable andfoodare treated as elements o\ the
structured domain ofthe attribute object-type. This implication expresses a generalizing inference rule called climbing the generalization tree. Further details on the
APC language are given in Miehalski (1983).
--- PAGE 503 ---
STEPPANDMICHALSKI 489
17.6.2 Directing the Process by Measuring Classification Quality
Creatingaclassification isadifficultproblem becausethere are usually many
potential solutionswithnoclearlycorrectorincorrectanswers. Thisproliferationof
answers was seen in the experiment with humanclassification building presented in
section 17.5. Thedecisionaboutwhichclassificationtochoosecanbebasedon some
perceivedsetofgoals (Medin, Wattenmaker, andMichalski, 1985), agoal-oriented,
statistic-basedutilityfunction(Rendell, 1983), orsomemeasureofthequalityofthe
classification.
One way to measure classification quality that has been successful in both
INDUCE/2 and CLUSTER/2 is to define various elementary, easy-to-measure criteriaspecifyingdesirablepropertiesofaclassificationandtoassemblethemtogether
intoonegeneralcriterion,calledtheLexicographicalEvaluationFunctionalwithtolerances (LEF) (Michalski, 1980b). Each elementary criterion measures a certain
aspectofthegeneratedclassifications. Examples ofelementary criteriaarethe relevanceofdescriptorsusedintheclassdescriptionstothegeneralgoal, thefitbetween
theclassificationandtheobjects, thesimplicityoftheclassdescriptions, thenumber
ofattributes thatsingly discriminateamongall classes, andthe numberofattributes
necessary to classify the objects into the proposed classes (Michalski and Stepp,
1983b).
TheLEFconsistsofanorderedsequenceofelementarycriteriaalongwithtolerances that control to what extent different solutions are considered equivalent.
First, all classifications are evaluated according to the first elementary criterion.
Those that score best or within the given tolerance range from the bestare retained.
Those retainedarethenevaluatedaccordingtothe nextelementary criterion, and so
on, until eitherasingleclassification remainsorthelistofelementarycriteriainthe
LEF is exhausted. In the lattercase, all classifications that remain arejudged equal
andthealgorithmpicksonearbitrarily. Tocontrolcombinatorialexplosion, theLEF
isalsoappliedduringthesearchprocessthatgeneratesclassifications. TheLEFprovidesapowerfulheuristicforsearchingthehugespaceofhypotheticalclassifications
to find aclassification that optimizes several criteria at once.
17.6.3 Using Background Knowledge
Building a meaningful classification relies on finding good classifying attributes(high-levelattributesusedtodefineclasses). Forexample, theattributeis_edible
discussed in section 17.4 is such a high-level classifying attribute. The repeated discrimination and classifying attributes methods, described in sections 17.6.4 and
17.6.5, both use background knowledge in the search for such attributes. The Goal
Dependency Network istraversedto findthe interactionsbetweentheclassification
goal(s)andthepotentialdescriptors. Backgroundknowledgerulesenablethesystem
toperform achain ofinferencestoderive values fornewdescriptors forinclusion in
object descriptions. The new descriptors are tested to determine ifthey make good
--- PAGE 504 ---
490 CHAPTER 17: CONCEPTUALCLUSTERING
classifyingattributesby applyingthe LEFtotheclassificationdefinedbythe classifying attribute.
As described in section 17.4, the background knowledge rules can represent
both the built-in general-purpose knowledge and the domain-specific knowledge
provided by the data analyst. In the latter case, knowledge for generating inferentially deriveddescriptors is supplied in the form ofan inference rule (called abackground rule, or b-rule). Special types ofb-rules include expressions of arithmetic
relationships (a-rules), such as
Vobject, girth(object) = length(object) -I- width(object)
and implicative rules that specify logical relationships (/-rules), such as:
V/?l,/?2,/?3, [above(/?l,/?2)][above(/?2,/?3)] => [above(/? 1,/?3)]
Vp\,P2,Pi, [mother (pi,p2) & ([mother(/? 2,P3)] V O[father(/?2,/?3)])
[grandmother(/7 ,,p3)]
Each rule is associated with a condition defining the situations to which it is
applicable.
17.6.4 Concept Formation by Repeated Discrimination: Method RD
This section explains how a problem of concept formation (here, building a
classification)canbesolvedviaasequenceofcontrolledstepsofconceptacquisition
(learningconcepts fromexamples). We startwith abriefdescription ofthe program
INDUCE/2, which solves concept acquisition tasks involving structured objects.
Givenasetofevents(bywhichwemeansymbolicdescriptionsofobjectsorsituations)arrangedintotwoormoreclasses,INDUCE/2inducesageneraldescription
of each class in the form of an annotated predicate calculus expression. First, all
events are divided into two sets: set Fl ofevents belonging to the class currently
beingconsideredand setFOofeventsbelongingtoany otherclass (counterexamples
tosetFl ). OneeventatatimeisselectedfromsetFl (theseedevent)andastarisbuilt
thatcoverstheseedeventagainstalleventsinsetF0. Thestaristhesetofall alternativemostgeneral descriptionsthatdescribetheseedevent(andpossiblyotherevents
from F\) and no events from F0 (Michalski, 1983; Michalski and Stepp, 1983b).
To control combinatorial explosion, INDUCE/2 determines bounded stars
rather than complete stars. A bounded starcontains only a fixed numberofdescriptions selected as most promising according to LEF. The highest-rank description in
the bounded star is chosen as a part of the solution. The events covered by the
resulting description are removed from set Fl. Ifany events remain in Fl. another
--- PAGE 505 ---
STEPPANDMICHALSKI 491
seed event (from among those not yet covered) is selected and the whole process is
repeated. When alleventsinthesetF\ havebeencovered, thesolutioniscomplete; it
is the disjunction ofthe descriptions selected in each iteration.
Thisalgorithmforconceptacquisitioncanbeadaptedforsolvingclassification
constructionproblems. Givenasetofunclassifiedobjects,kseedobjectsareselected
randomly andtreatedas individual representativesof&imaginary classes. The algorithmthengeneratesdescriptionsofeach seedthataremaximallygeneralanddonot
coverany other seed. Thesedescriptions arethen used todetermine the most representative object in each newly formed class (defined as the set ofobjects satisfying
the class description). The representative objects are used as new seeds forthe next
iteration. The process stops either when consecutive iterations converge to some
stable solution or when a specific number ofiterations pass without improving the
classification (from the viewpoint ofthe criterion LEF).
This approach requires the selection of a defined number of representative
objects (correspondingtothenumberofclasses). Sincethebestnumberofclassesto
formisusuallyunknown, twotechniquesareused: (1)varyingthenumberofclasses
and (2) composing the classes hierarchically.
Sincetheclassificationtobe formed shouldbe simple andeasytounderstand,
the number ofclasses that stem from any node ofthe classification hierarchy was
assumed tobe in the range oftwo to seven. Since this range is small, it is computationallyfeasibletorepeatthewholeprocessforeverynumberinthisrange. ThesolutionthatoptimizesthescoreontheLEF(withappropriateadjustmentfortheeffectof
thenumberofclassesonthescore)indicatesthebestnumberofclassestoformatthis
level ofthe hierarchy.
The above ideaofrepeateddiscrimination forperforming concept acquisition
has been implemented in the program CLUSTER/2 fora subset ofannotated predicatecalculusinvolvingonlyattributes(zero-argumentfunctions). Besidesitsrelative
computational simplicity, this approach has other advantages stemming from
descriptions (for both objects and classes) that are quantifier free. Specifically, it
should be notedthat classifications normally have the property that they can unambiguously classify any object into its corresponding class. To have this property, the
class descriptions must be mutually disjoint.
For conjunctive descriptions involving relations on attribute/value pairs, the
disjointnesspropertyiseasytotestandeasytomaintain. ForthelargersubsetofAPC
involving existentially quantified variables, predicates on these variables, and
function/value relationships over quantified variables, the test for mutual disjointness ofdescriptions and the maintenance ofdisjointness are difficult. As a result of
this difficulty, the approach taken for concept acquisition from structured objects
involvestwoprocessing steps. The firststep, using algorithms ofINDUCE/2, finds
an optimizedcharacteristic generalization oftheentire collection ofevents and then
appliesittogenerateaquantifier-freedescriptionofeachobject(avectorofattribute
values). The second step processes the quantifier-free object descriptions with the
--- PAGE 506 ---
492 CHAPTER 17: CONCEPTUALCLUSTERING
CLUSTER/2 algorithm to form optimized classifications. These two processes are
combined in the program CLUSTER/S.
A characteristic generalization expresses a common substructure in all structuredobjectsthatfacilitatesthebindingofasubsetofthefreevariables (representing
object parts) to specific parts. That portion of the structure of each object that is
describedby thecharacteristic generalization is calledthe coreofeach object. With
corresponding parts identified in all objects, thecores may bedescribedby avector
ofattributevalues. Thusthedescriptionsofobjectcoresneedneitherquantifiedvariables nor multiplace predicates in their descriptions (i.e., such descriptions can be
handled by the CLUSTER/2 program).
Itisrecognizedthatstructuraldifferencesbetweenobjectswouldbelostbythe
above approach since it focuses on the common substructure found in all given
objects. Toretainsomeuniquestructuralfeaturesofindividualobjects, aninspection
is made ofthe connections between object subparts within the core and object subparts outside the core. New predicates are automatically generated and added to
object descriptions to denote the attachment ofdifferent kinds ofadditional structures to the core ofeach object.
Thedescriptionsofeachsubstructureconnectedtothecoresofobjectsarecollectedandclassifiedbyrecursiveapplicationoftheconjunctiveconceptualclustering
procedure. Theresultingtypesofsubstructuresaregivenlabels (e.g., auniqueclass
number)whichareusedinthegeneratedpredicatesthatshowwhatkindofadditional
structureisattachedwheretothecorestructure. Thefinalobjectdescriptionscontain
attributesforcorepartsandpredicatesdenotingthekindofattachedsubstructures,as
wellasderiveddescriptorsforbothcoresubpartsandtheobjectasawhole. Afterthis
transformation, objects are describable (with reduced detail) by attribute vectors.
Thefollowingextensionofthetrainsproblemwillfurtherillustratetheuseofa
GDN and problem-specific background knowledge. Suppose that the knowledge
base includes an inference rule that can identify trains carrying toxic chemicals.
Suppose also that the general goal survive has a subordinate goal to monitor dangerousshipments. Theadditionalbackgroundknowledgecanbeusedtohelpbuild a
classification.
Intheillustrationsofthetrainsatoxicchemicalcontainerwillbeidentifiedasa
single sphere (circle) riding in an open-top car. The logical inference rule (/-rule)
supplied to CLUSTER/S is
[contains(trains,car)][car-shape(car) = opentop]
lcargo-shape(car) = circle][items-carried(car) = l]
4S> [has_toxic_chemicals(train)]
In the above rule, equivalence is used to indicate that the negation ofthe condition
part is sufficient to assert the negative of the consequence part. After this rule
is applied, all trains will have descriptions containing either the toxic-chemical
--- PAGE 507 ---
STEPPANDMICHALSKI 493
predicate or its negation. The characteristic description generated by CLUSTER/
will nowcontainthe additionalpredicatehasjoxic_chemicals(train) orits negation.
In the GDN we find the main goal survive and a chain of subordinate goals
beginning with be safe and monitordangerous shipments. Two additional subgoals
aremonitorchemicalsshipmentsandmonitortoxicchemicalsshipments. Attachedto
these nodes are relevant attributes such as isjexplosive, isjradioactive, is_flammable, is_corrosive,hasjoxic_chemicals, andsoon.
TheGDN
isillustratedinfigure
17.9. The GDN signals the relevancy of these descriptors to the goal survive.
Assuming that this goal takes precedence overthe goalfindsimple visualpatterns
classifications that make use ofthe has_toxic_chemicals descriptor in formulating
conceptual classes score higher than those that use descriptors. The classification
produced in this case is shown in figure 17-10.
17.6.5 Concept Formation by Finding Classifying Attributes:
Method CA
This section describes another approach for building classifications called
classifyingattributes(briefly, CA). Thisapproachattemptstofindoneormoreclassifying attributes whose value sets can be split into ranges that define individual
classes. The importantaspectofthisapproach isthattheclassifyingattributecanbe
derived through a goal-directed chain ofinferences from the initial attributes. The
is_radioactive is_corrosive
/T monitor S monitor >>
dangerous chemicals
V shipments J v shipments
is_explosive is_flammable
has_toxic_
chemicals
CZ^ Ageneralgoal
C3> Asubordinategoal
- Arelevantattributeorpredicate
»- Agoalsubordinationrelation
--* Anattributerelevancyrelation
Figure 17-9: Ahypothetical GDN fordangeroustrainshipments.
--- PAGE 508 ---
494 CHAPTER 17: CONCEPTUALCLUSTERING
A io h, .i -pj- ° D ° -D
. i l
A^HLS^J^
"o u
"These trains are carrying toxic chemicals."
T3 cr
w-(£H&-S&/-^=t
CQj-^^Ir-M^Dr
HoooT
I.-A -ID?
TJ cr ^T7
L^i^a^
"These trains are not carrying toxic chemicals."
Figure 17-10: Aclassificationproducedusingthetoxicchemicalsinferencerule.
classifying attributes sought are the ones that lead to classes ofobjects that are best
according to the classification goal.
Thepromiseofadescriptortoserveasaclassifyingattribute isdetermined by
consulting the GDN and by considering how many otherdescriptors it implies. For
example, ifthegoaloftheclassificationdescribed in section 17.4 isfindingfood, the
attributeedibilitymightbeaclassifyingattribute. Thesecondwayofdeterminingthe
promise ofan attribute can be illustrated by the problem ofclassifying birds. The
questionofwhethercolorisamoreimportantclassifyingattributethan isjwaterbird
is answered in favor ofis_waterbird, because the latter implicatively leads to more
implied attributes than does the attribute color in a given GDN network (e.g.,
--- PAGE 509 ---
STEPPAND MICHALSKI 495
is_waterbird implies can_swim, has_webbed_feet, eats_fish, and so on) (Medin,
1982).
There are two fundamental processes that operate alternately to generate the
classification. The firstprocess search searches fortheclassifying attribute whose
value set can be partitioned to form classes such that the produced classification
scores best according to the LEF. The second process generate generates new
descriptorsbyachainofinferencesusingtwoformsofbackgroundknowledgerules:
logical implicative rules (1-rules) and arithmetic rules (a-rules). Descriptors that
GDN
can be inferred are ordered by relevancy indicated by the and the goals ofthe
classification.
searchcanbeperformedintwoways. Whenthenumberofclassestoform(k)
is known in advance, the process searches for attributes having k or more different
values in the descriptions ofthe objects tobe classified. These values are called the
observed values of the attribute. Attributes with the number of observed values
smallerthankarenotconsidered. Forattributeswithobservedvaluesetslargerthan
thechoiceofthemappingofvaluesubsetstoclassesdependsontheresultingLEF
score fortheclassificationproducedandthetypeofthevalue set. Whenthe number
ofclasses to form is not known, the above technique is performed for a range of
values ofk. The best number ofclasses is indicated by the classification that is best
according to the LEF.
generate constructs new attributes from combinations ofexisting attributes.
Certain heuristics of attribute construction are used to guide the process. For
example, twoattributes thathave linearly ordered value sets canbecombined using
arithmeticoperators. Whentheattributeshavenumericalvalues (asopposedtosymbolic values such as small, medium, and large), atrend analysis can be usedto suggestappropriatearithmeticoperators, asinthe
BACONsystem(seechap.
16). Predicates can be combined by logical operators to form new attributes through 1-rules.
Forexample, arulethat says ananimal is areptile ifitis cold-bloodedand lays eggs
can be written in APC as
[cold-blooded(fl1)][offspringbirth(al) = egg] => [animal-type(al)
reptile].
The applicationofthis ruletothe given animal descriptions yieldsthe new attribute
animal-type with the specified value reptile. Using this rule and similar ones, one
mightclassify someanimals intoreptiles, mammals, andbirdseventhoughthetype
ofeach animal is not stated in the original data.
SUMMARY
17.7
Thischapterhasdiscussedtheproblemofbuildingclassificationsofstructured
objects using goal-directed inferences from background knowledge. Two methods
--- PAGE 510 ---
496 CHAPTER 17: CONCEPTUALCLUSTERING
forperformingthistaskweredescribed. Thefirstmethod, RD (repeateddiscrimination),transformsconceptformationintoasequenceofconceptacquisitiontasks. The
second method, CA (classifying attributes), forms classes by generating new
descriptors using achain ofinferences andtestingthem ascandidateclassifying criteria. Thecriterionselectedistheonethatpartitionsthesetofeventsinthewaymost
preferred according to a Lexicographical Evaluation Functional (LEF).
TheclassifyingattributesaregeneratedwiththeaidofaGoalDependencyNetwork, which relates goalsto subgoalsandtorelevantattributes. Theability toincorporate domain-specific background knowledge in the form of inference rules and
Goal Dependency Networks adds anew dimensiontothe process ofconcept formation and data analysis.
Thisworkcouldbefurtherextendedthroughtheinvestigationofalternativerepresentations for describing classes in a classification. These could include the use of
logicaloperatorssuchasimplication,equivalence,andexceptioninaclassdescription.
Theexceptionoperatorappearstobeespecially interestingbecauseofits frequentuse
bypeople. Exceptionclauses inlogical rulescanbe introducedby using unlessconditions to handle the cases that are not frequent orordinary (seechap. 3).
Anotherextensionofthisworkcouldbethedevelopmentofasystemcapableof
characterizingacollectionofobservations(facts,events, andsoon)notjustbyahierarchy of concepts but by a concept network, in which nodes represent conceptual
classes and links represent various relations among them. In the kind ofhierarchy
consideredhere, any twogenerated concepts are relatedby the relation isageneralization oforisaspecialization oforisadisjointof In aconcept network (a form of
semantic network) a much larger set ofrelations would be allowed.
ACKNOWLEDGMENTS
This research wasdone inpartatthe DepartmentofComputerScience Artificial Intelligence Laboratory atthe University ofIllinois and in part at the Artificial
Intelligence LaboratoryattheMassachusettsInstituteofTechnology. Supportforthe
UniversityofIllinoisLaboratory isprovidedinpartbygrants fromthe National Science Foundation under grant No. NSF DCR 84-06801 and the Office of Naval
Research under grant No. N00014-82-K-0185. Support for the Massachusetts InstituteofTechnology Laboratory isprovided inpartbytheAdvanced Research Projects
Agency ofthe U.S. Department ofDefense underthe Office ofNaval Research contract number N00014-80-C-0505.
The authors wish to thank Tom Mitchell at Rutgers University and Larry
Rendell at the University ofIllinois forremarksandcriticismsonearlier versions o\
the manuscript. They also thank Peter Andreae at the Massachusetts Institute o\
Technology Artificial Intelligence Laboratory and Doug Medin at the University of
Illinois Department of Psychology for useful discussions and valuable comments.
--- PAGE 511 ---
STEPPAND MICHALSKI 497
References
Anderberg, M. R., ClusterAnalysisforApplications, Academic Press, NewYork, 1973.
Burstein,M.H.. "ConceptFormationbyIncrementalAnalogicalReasoningandDebugging,,,chap. 13of
thisvolume, 1985.
Carbonell,J.G.
'"DerivationalAnalogyinProblemSolvingandKnowledgeAcquisition,"Proceedingsof
theInternationalMachineLearningWorkshop,R.S. Michalski(Ed.),AllertonHouse,University
ofIllinoisatUrbana-Champaign, pp. 12-18,June22-24, 1983. (Anupdatedversionofthispaper
appearsaschap. 14ofthisvolume.)
DeJong, G., "Generalizations Based on Explanations," Proceedings ofthe Seventh IJCAI, Vancouver,
B.C.. pp. 67-69, 1981.
, "AnApproachtoLearningfromObservation"chap. 19ofthisvolume, 1986.
Dietterich,T. G..andMichalski, R. S., "AComparativeReviewofSelectedMethodsforLearningfrom
Examples,"inMachineLearning:AnArtificialIntelligenceApproach,R. S. Michalski,J.G.Carbonell, andT. M. Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
Hoff,W.,Michalski,R.S.,andStepp,R., "INDUCE/2:AProgramforLearningStructuralDescriptions
fromExamples," Technical ReportNo. UIUCDCS-F-83-904, DepartmentofComputerScience,
UniversityofIllinoisatUrbana-Champaign, 1983.
Langley,P.;Zytkow,J.;Simon,H.A.;andBradshaw,G. L.. "TheSearchforRegularity: FourAspectsof
Scientific Discovery," chap. 16ofthisvolume, 1986.
Lingle,J.H.,Altom, M.W. , andMedin, D. L., "OfCabbagesandKings: AssessingtheExtendibilityof
Natural Object Concept Models toSocial Things," inHandbookon Social Cognition, R. Wyer,
T Srull, andJ. Hortwick(Eds.), Erlbaum, Hillsdale, N.J., 1983.
Medin. D. L., "Structural PrinciplesinCategorization," in TheDevelopmentsofPerceptionandCognition, T. Tighe, B. Shepp, H. Pick(Eds.), Erlbaum, Hillsdale, N.J., 1982.
Medin.D.L.,Wattenmaker,W.S.,andMichalski.R.S., "ConstraintsinInductiveLearning:AnExperimental Study Comparing Human and Machine Performance," submitted to Cognitive Science,
1985.
Michalski, R. S.. "Knowledge Acquisition Through Conceptual Clustering: A Theoretical Framework
andanAlgorithmforPartitioningDataintoConjunctiveConcepts,"PolicyAnalysisandInformationSystems, Vol. 4, No. 3, pp. 219-44, 1980a.
"PatternRecognitionasRule-GuidedInductiveInference,"IEEETransactionsonPatternAnalysisandMachineIntelligence, Vol. PAMI-2, No. 4., pp. 349-61,July 1980b.
"ATheoryandMethodologyofInductiveLearning," inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski, J. G. Carbonell, andT M. Mitchell (Eds.), Tioga, PaloAlto,
Calif., 1983.
Michalski, R. S., and Larson, J. B., "Inductive Inference ofVL Decision Rules," Paper presented at
WorkshopinPattern-DirectedInferenceSystems,Hawaii,May 1977.(PublishedinSIGARTNewsletter, ACM, No. 63, pp. 38-44,June 1977.)
--- PAGE 512 ---
498 CHAPTER 17: CONCEPTUALCLUSTERING
Michalski, R. S., andStepp, R. E., "'AutomatedConstructionofClassifications: ConceptualClustering
versusNumerical Taxonomy," IEEETransactionsonPatternAnalysisandMachineIntelligence,
Vol. PAMI-5, No. 4, pp. 396-410,July 1983a.
"LearningfromObservation:ConceptualClustering,"inMachineLearning:AnArtificialIntelligenceApproach, R. S. Michalski,J. G. Carbonell, andT. M. Mitchell(Eds.),Tioga, PaloAlto,
Calif., 1983b.
Mitchell, T. M., and Keller, R. M., "Goal Directed Learning," Proceedings ofthe 2ndInternational
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois at
Urbana-Champaign,June22-24, 1983.
Rendell, L. A., "Toward a Unified Approach for Conceptual Knowledge Acquisition," AIMagazine,
Winter 1983.
Stepp, R. E., "ConjunctiveConceptual Clustering: AMethodologyandExperimentation," Ph.D. diss.,
DepartmentofComputerScience, UniversityofIllinoisatUrbana-Champaign, 1984.
Winston, P. H.,ArtificialIntelligence, Addison-Wesley, Reading, Mass., 1984.
, "Learningby AugmentingRulesandAccumulatingCensors," chap. 3ofthisvolume, 1985.
--- PAGE 513 ---
PROGRAM SYNTHESIS AS A THEORY
FORMATION TASK:
Problem Representations and Solution Methods
Saul Amarel
Rutgers University
Abstract
This chapter is concerned with theory formation processes in the context ofa
programsynthesistask. Theproblemistosynthesizeaprogram, inagivenprogramminglanguage, thatsatisfiesagivensetofinput-outputdataassociations. Theinputoutputassociationsaredrawnfrom somedataspace, whereasthepossible solutions/
programs(i.e., theprogramminglanguage)defineaprogramspace. Severaldifferent
formulationsofthetheoryformationproblemarepresentedthatimpactthedifficulty
of solving the problem. The movement to more effective problem formulations
involves structuring the program space in a way that facilitates the establishment of
linkstodataspaceandanincreaseintheamountofreasoningthattakesplaceindata
space. The use ofmodels (algebraic and geometric) was found to be essential in the
reasoning that takes place in the more effective problem formulations.
Three main procedural formulations are discussed, two ofwhich were developedinpreviouswork. One isbasedonheuristichillclimbinginprogramspace. The
second is moregoal oriented and involves "navigating" in program space underthe
guidance ofan algebraic model. The third formulation, a more recent one, is based
on detailed reasoning in data space. Two methods are presented in connection with
thisthird formulation: acombination methodandanelimination method. Whenthe
reasoning is shifted toward the data space, several interesting domain-dependent
problems of concept specialization and generalization are encountered. Basic AI
issues that are identified and on which more work is needed include formation of
--- PAGE 514 ---
500 CHAPTER 18: PROGRAM SYNTHESISAS ATHEORY FORMATION TASK
macromoves in "appropriate" representations of program space and data space;
developmentofmethods forcombining "partially correct" programs and for modifying "almostcorrect" programs; andexplorationofthe interplaybetweenchoosing
adomainforwhichtoconstructatheoryandtheformationofan "interesting" theory
forthat domain.
18.1 INTRODUCTION: MOTIVATION AND ASSUMPTIONS
Themainobjectiveofthisresearchistounderstandtheoryformationprocesses
and to develop effective methods ofmechanizing them. A secondary objective is to
developapproachesforthedesignofcertainautomaticprogramsynthesissystems. In
particular, situations are considered in which functional properties of a computer
program are specified in the form of explicit input-output associations, and the
problem isto synthesizeaprogram, inagivenprogramming language, that satisfies
the given associations. Although this is a rather uncommon formulation ofrealistic
program synthesis problems, itprovides an excellenttaskenvironment forthe study
ofcertain types oftheory formation processes.
Thereisanother, morefundamentalobjectivethatmotivatesthiswork. Theory
formationprocessesarecentraltoanunderstandingoftheproblemofrepresentation
inproblemsolving;thisproblemhasbeenrecognizedbynowasbeingofbasicimportancetoAI(Newell, 1969; Amarel, 1970). Thequestionsofhowtochooseanappropriateproblem formulationandhowtochange ittofitthe special characteristicsofa
taskareattheheartofthisproblem. Oneofthewaysinwhichthestudyoftheoryformation processes contributes to ourunderstanding ofthese questions ofrepresentational choice issimply bythe factthatitaddstoourcorpusofknowledge inthisarea
anotherimportantcaseofaproblemsolvingsituationinwhichthenatureofrelationships between alternative problem formulations and problem solving efficiency has
beenexamined insomedepth. Inpreviouswork, theauthoranalyzedseveralcasesof
problem solving activity in specific domains in an attempt to elucidate issues of
choiceofformulationand mechanismsforshifting fromone formulationtoanother.
The focus of most ofthis work has been on derivation problems (Amarel, 1968.
1981). This chapterconcentrates on a similaranalysis ofalternative representations
ofa formation problem. Theemphasis isonthevariousbodiesofknowledge that arc
associated with different formulations of the problem and on the various ways in
which this knowledge can be used in problem solving. Of special interest in the
present case istheuseofmodelsinguidingtheformationprocess. Itisalsoimportant
to know whether there are any substantial differences between the mechanisms that
arc needed for shifting between formulations oftheory formation problems and the
mechanisms that mediate shifts between formulations ofother types ofproblems.
The latterquestion is ofspecial significance in light ofrecent studies ofexpertiseacquisition via shifts in problem representation (Amarel. 1982) that suggest the
--- PAGE 515 ---
AMAREL 501
following: Theabilitytoshiftproblem representationsinan "appropriate"direction
requires theoryformation capabilitiesfordiscovering "interesting" regularities in
some body ofproblem solving experience andprogram synthesis capabilitiesfor
exploiting these regularities in the development ofspecialized, high-performance
procedures.
But theory formation and program synthesis problems face representation
problems themselves as questions of improving their performance arise. Shifts in
representation may be required to improve theory formation processes, and theory
formation processes may be required in turn to effect these shifts, and so on. It is
importanttorecognizeandtoclarifytheserelationships. Thefirststepistorecognize
the centrality of theory formation processes to the task of mechanizing shifts
betweenproblemformulationsandtofocusnotonly onthedevelopmentofeffective
schemes fortheory formationbutalsoonthe study of"first-order" mechanisms for
improving theory formation schemes via shifts in representation.
It has been clear for some time that progress is needed in various aspects of
theory formation processes-conceptual approaches as well as specific design
techniques ifAI applications are to be developed that will have serious impact on
the "doingofscience." However, progressinthisareahasbeenslow. Themaindevelopmentduringtheseventies, whichwasmostpromisingintermsofexplorationofAI
ideas in the context ofan empirical science application, was the Meta-DENDRAL
p ( r L o a j n e g c l t ey at , S B t r a a n d f s or h d aw ( , Li a n n d d sa S y i e m t o a n l. , , 1 1 9 9 8 80 3 ) ) . a T n h d e L w e o na r t k 's on w B or A k C o O n N A at M Ca a r n n d eg E i U e- R M I e S ll C o O n
(Davis and Lenat, 1982) are more recent contributions to theory formation. These
projectsareintroducingpromisingnewideasintothisareaofstudy, andtheyareproviding a certain momentum in this field that is important to maintain.
In general, theory formation involves both the choice ofa domain ofphenomenathataretobe expressed within some conceptual frameworkandthefinding
ofanexpressionwithintheframeworkforexplaining/definingthedomain. Theinterplay between domain choice and the formulation of a theory for the domain is an
interestingandcomplexprocessthatisdifficulttocaptureatpresent. Atthisstageof
knowledge oftheory formation, it is useful to focus attention on each ofthese parts
separately and to gain some insight into the nature ofdependencies between them.
This is the approach taken in the work presented here.
More specifically, it is assumed that the initial statement ofthe theory formation problem includes a fundamental supposition about the domain of the theory,
namely, thattheelementsofthedomainaretobeconsideredasbelongingtoadistinct
classforwhichatheory, withinthegivenconceptual framework, canbeconstructed.
The supposition can be regarded as an exploratory one-tobe confirmed ordenied.
Theefforttofindacoherenttheoryforthegivendomaincanbe seenasanattemptto
validatethe supposition. Only inthecourseofthisattempt may itbepossibletofind
out whether changes should be made to the supposition and what they may be. The
process ofsetting and resetting the domain ofa theory is closely related to concept
--- PAGE 516 ---
502 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
discoveryprocesses. These processes are discussed below in the context ofcertain
knowledge-rich formulations ofthe theory formation problem.
Atheoryinanempiricalscienceisaboutadomainofphenomena, andithasthe
functionofanintellectualmechanismforexplainingandpredictinginthatdomainof
thescience. Theformofthetheorythatcanemergeinascientificculturedependson
the basic concepts, the languages, and the schemas/paradigms that are available in
theculture. Thereisastronganalogybetweenthissituationandtheprocessoftrying
tosynthesizeacomputerprogramthatcanembodyagiven setofdataassociations in
termsofaprogramminglanguagethatthecomputercaneffectivelyuse. Ifasaresult
ofsuch an effort a program is obtained that "satisfies" all the data associations that
are given up to acertain point in time, then the program is considered to be acceptable. Ifanewsetofassociationsispresented, thentheprogrammightstillbeacceptableorit mightnotbe. In the lattercase, theproblem is how to modify the previous
program in order to attain a new program that is acceptable over the new extended
domain. It maybeextremelydifficulttoaccommodatethedomainextensionswithin
a single program/theory. In such a case it would be appropriate to consider partitioning the problem into subdomains foreach ofwhich acceptable programs can be
obtained within certain bounds ofavailable effort.
Thequestionofwhetheragivenprogramthatis knowntosatisfy asubdomain
ofsomedomainwillbeabletosatisfyextensionsofthesubdomainisofimportanceto
inductive logicbut nottoAI. What matters from the point ofview ofAI is to create
andstudy schemeswhosebehavioristhatofatheoreticianwhoseekstointegrateall
currentevidenceinacertaindomainwithinatheorythatheinvented, andwhosedistinguishing characteristics is that he never "leaves the game" despite evidence
refuting his previous theory, but he attempts to construct new theories that would
accommodate the new evidence.
Inaccordancewiththisview, theproblemofprogramformationcanbecharacterized as a constraintsatisfaction problem. A finite set offunctional conditions is
imposedonadesiredstructure, andtheproblem istosynthesizethe structureusinga
specified repertoire ofconstruction material and assembly techniques. In this case,
functional conditions are in the form ofdata associations that the desired theory/
programmustsatisfy, andthesetofpossiblesolutionstructuresisdefinedintermsof
a language ofprograms.
Theproblemfacedhereis howtosearchinthegivenlanguageofprograms fora
program that satisfies the constraints imposed by the given set ofdata associations.
The constraints are presented in a data space, and the possible solutions are presented in aprogram space; furthermore, the initial body ofavailable knowledge is
not in a form that permits the establishment ofuseful links between the two spaces.
Thusthis isacharacteristic/?/W?/r///offormation (Amarel, 1970). Inproblemsofthis
type, the solution process cannot proceed through the use of reasoning from the
problemconstraintstospecificpartsofpossiblesolutions. Thebulkofsearch isbeing
done in program space, where candidate solutionsare generated, and thedata is used
--- PAGE 517 ---
AMAREL 503
mainlytotestthesolutioncandidates. Ifthepoweroftheprocessistobeincreased, it
is essential that the a priori control that problem constraints-that is, the data
associations-can have on the generation ofcandidate programs also be increased.
Thisrequiresthedevelopmentofstronglinksbetweendataspaceandprogramspace,
aswellasastrategyforgoodcoordinationbetweenthe search/reasoningactivitiesin
thetwospaces. Thesignificanceofthetwo-spacemodel forthestudyingofproblems
oftheory formation is discussed in detail by Simon and Lea (Simon and Lea, 1974).
In this chapter several formulations ofa specific program formation problem
are presented. The movement to formulations of higher effectiveness involves the
structuring ofprogram space in suchaway thatlinks with data spaceare facilitated,
andthe amount ofreasoning that takes place in data space is increased. The role of
models(algebraic, geometric)isextremelyimportantinthestrongerproblemformulations. Three main approaches to the problem are discussed. The first approach,
developed abouttwenty years ago (Amarel, 1962a, 1962b) is based on heuristic hill
climbing in program space. The second approach, developed in the early seventies
(Amarel, 1971), is more goal oriented and involves searching the program space
undertheguidanceofanalgebraicmodel. Workonthethirdapproachismorerecent
andisbasedondetailedreasoningindataspace. Twomethodsarepresentedforusing
the resultsofthedetailed analysis ofindividual dataassociations inthe construction
ofa program/solution for the entire domain. The first is a combination method in
which it is crucial to have effective techniques ofprogram modification in order to
proceedtoasuccessfulassemblyofasolution. Thesecondisamethodofelimination
that can be seen as a variant ofthe "version space" approach to learning (Mitchell,
1978). Ifageometricrepresentationofdataspaceisintroduced, itispossibletomove
toavery strongformulationofthe formationproblemin whichasmall setofmacromovesandofwell-chosen intermediate concepts provides the basis forhighly effective ways of constructing solutions. In the following sections, the first two
approaches to the formation problem will be briefly described, and the third
approach and the relationships among the approaches will be more fully discussed.
Beforethedetaileddiscussionisbegun, awordaboutthechoiceofdomainisin
order. The tasks considered are in the relatively simple mathematical area offinite,
partially ordered structures. This area has proventobe an excellent environment in
whichcertainessentialaspectsofprogramformationproblemscanbestudied. Since
an important objective ofthis work is to analyze various approaches to the same
problem andto identify the effects ofrepresentational changes onperformance, it is
necessarytokeeptheproblemdomainas stableaspossible. Thusnoeffortwasmade
to "diversify" the domain over the various stages of work in this area. The initial
choice ofdomain was much influencedby Piaget's work (Piaget, 1936, 1952) on the
mental development ofchildren. As suggested by Piaget's theory, the next stage of
development after that involving the classification ofconcrete objects into sets and
the formation of relational concepts is that involving the formation of algebraic
structures-in particular, those relatedtothe propositional calculus. Thusthe initial
--- PAGE 518 ---
504 CHAPTER 18: PROGRAM SYNTHESIS AS ATHEORY FORMATION TASK
choice ofdomain and the specific assumptions about the task environment were
intended to provide a setting foran exploration ofthe mechanization ofthis stage of
development. Fromthepresentperspectiveitis nowclearthatthedifficultyofmechanizing this stage of development was greatly underestimated twenty years ago,
althoughitisprobablyeasiertoapproachthistasknow. Inanyevent,eventhoughthe
original reason for choosing the domain is largely irrelevant now, this domain can
still be used fruitfully in the exploration oftheory formation processes in program
synthesis.
18.2 A CLASS OF PROGRAM FORMATION PROBLEMS: INITIAL
STAGES OF PROBLEM FORMULATION
Letussupposethataproblemsolver1 withagiven(limited) knowledgeofsets,
elementary relations, andprogramming ispresentedwiththe following statementof
aproblem tt\ (see figure 18-1):
Given(1)adatabaseintheformofafinite,partiallyorderedstructureL = (a,
/,E),whereai isasetoffiveelements,/isaproperinclusionrelationdefinedfor
Finite, partiallyorderedstructure E, = (a,. /, E)
Finiteset<j\\ {a, b, c, d, e}
Inclusion relation/: {ae, ba, be, ca, ce, da, db, dc, de)
Graphic representationofstructure E
(Chainofoneormoredescendingbranches
denotes inclusion;e.g., "dincludesa")
Specifying Set <rllllmuini ,: {(aa. a), (ab, a), (ac, a), (ad, a). (Ja
\bb\ b). (be a), (bd. b). (/'<
(cd, c). (ce, c). (dd. d). {de, f), (ee,e))
Figure 18-1: Data forprogram formation problem 7r,.
Ihetermproblem solverwillbeusedinterchangeably forhumanorAI systems, unlessaspecificdistinctionisneeded. Also, thepronounIk willheusedtorefertotheproblemsoberandshouldbereadas ""he.'"
"she." 01 "it
--- PAGE 519 ---
AMAREL 505
alltheelementsofo
andE istheequalityrelation); (2)alanguageforspecifying
programs, inwhich inputvalues forthe programscanbetaken from o\. andthe
languagecanhandleatthebasiclevelsetoperationsandtherelations/,E, andthe
converseof/(called "is-included-by*4 and denoted by /); and (3) a finite set of
associations(Calledthespecifyingset C Infimum i) betweenpairsofelementsofthe
set o\ andelements ofthe same set.
Findasimpleprograminthegivenlanguageofprogramsthatcomputestheright
side ofan association from its left side foreach ofthe associations in the given
specifying set.
In essence, the problem solver is asked to discover an effective procedure for
assigningtoeachleftsideofagivenassociationitsspecifiedrighthandandtoexpress
thisprocedureasaprograminthegivenprogramlanguage. Anotherviewisthatheis
askedtoforma theorythatexplainsthe given set ofassociations in terms oftheconcept in the program language.
The set ofassociations is a specification ofthe Infimum function, explicitly
defined forthe partially ordered structure. Thus the task ofthe problem solver is to
discoveraprogramforcomputingtheInfimuminthegivenfinitestructure. Theprogram sought can also be seen as atheory forthe given set ofassociations.
Therequirementthatthesolutiontotheproblembe simplecanbeeasilyappreciatedinthetheoryformationcontext. Themoresimpleatheory is, theeasieritisto
testit anduseit. Moreimportantly, asimpletheoryismoremanageablewhen ithasto
be analyzed and modified, that is, when it has to be "debugged." And it should be
recalled that the bulk ofthe process oftheory formation consists ofdebugging.
In ordertoproceed with the solution ofthe problem, the problem solver must
recognize it as a member ofaproblem class for which he has available one or more
problemsolvingschemas. Suchrecognitionleadstoaninternal formulationthatrepresents an assimilation oftheproblem by the problem solver; that is. the problem
solver "understands" the problem and knows whattodo with it.
Theformulationof
problemclassindeclarativeformcanbepresentedingeneral as a4-tuple (Amarel, 1982)
(D, X, C, A),
where D is the domain specification (i.e., a body ofknowledge within which problemsintheclassareconceptualizedandhandled); Xisthesetofpossiblesolutions; C
specifies the type ofproblem conditions that are characteristic of problems in the
class; and A is a problem-data domain. Each element in A represents specific
problemdatathatidentifyanindividualproblemintheclass. Thusaspecificproblem
in the class can be formulated by providing (1) the class formulation, that is, the
above4-tuple, and (2) the specific problem-data, that is, anelement in Athat identifiestheproblem. A solution totheproblem mustbe anelementof X thatsatisfiesthe
class conditions Cas specialized by the individual problem-data.
--- PAGE 520 ---
506 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
LetusconsidernowaninitialdeclarativeformulationF
foraclassofprogram
formation problems in which it is assumed that the problem 7r, is embedded.
18.2.1 Initial Declarative Problem Formulation
18.2.1.1 Initial Domain Specification D
The specificationofadomain includesdescriptions ofconcepts in the domain
and of relationships among them. It is assumed that D\ has at its basic level the
following concepts: sets, relations, functions; predicates of set membership, set
inclusion, and setequality, aswell aspredicates fortesting whethera setisempty or
isasingleton; setoperationsofunion, intersection, andcomplementation; relational
operations of disjunction, conjunction, product, converse-of, and negation; and
properties ofthese concepts.
In addition to its basic concepts, it is assumed that D has some knowledge of
the concept of afinite, partially ordered structure L. In particular a definition of
finite, partially ordered structures is assumed to exist in terms ofbasic concepts in
x, and the problem solver is assumed to be currently engaged in an exploration of
key properties of these structures (Infimum, Supremum, and their relationships).
Actually,theproblem tt\ canbeseenaspartofthisexploration;andeventuallyasolutionto7Ti andtorelatedproblemscanbeexpectedtoaddproceduraldefinitionsofthe
"Infimum" concepttothedomain. Morespecifically, theproblemsolverisassumed
to be now at a stage oftrying to define operationally the concepts ofInfimum and
Supremum intermsoftheinclusionrelationanditsproperties. This isataskthathas
both mathematical significance (Birkhoff and MacLane, 1953) and psychological
significance (Piaget, 1952).
A finite, partiallyorderedstructureisatuple E = (a, /, E), whereaisafinite
set ofn elements, /is aproper inclusion relation (dyadic, antisymmetric, transitive)
definedover a, andE istheequality relation forelementsofa. Partial orderings are
extremely importantmathematicalconcepts. Booleanalgebras,thesystemofall subgroupsofafinitegroup, andfinitesetsofpositiveintegersunderthedivisibility relation are examples ofsuch structures. Many information structures handled in computers can also be seen as partially ordered structures.
For any inclusion relation /, a new relation I ( (for local inclusion) may be
defined, suchthatbl
a(read, bincludes-locallya) ifbla(i.e., bincludesa)andthere
is nox such that bh and xla. The term used commonly for the concept of "localincludes" is covers. Now, finite, partially ordered structures may be represented in
graphic form by diagrams ofthe type shown in figure 18-1. In such diagrams, each
clement ofo is represented by a node soplaced that the node forb isabove the node
fora i\ bla, and there is a descending branch from b toa in case that bla. It can be
established that the relationbla holds fromthediagram ifit ispossible toclimb from
a to h along an ascending chain in the diagram. The diagram shown in figure 18-2
--- PAGE 521 ---
AMAREL 507
Figure 18-2: Exampleofafive-element, partiallyorderedstructure.
representsasecondexampleofafinite,partiallyorderedstructurewithfiveelements
that defines the structure ofall subgroups ofthe four-group.
Thegeometricrepresentationis shownhereforconvenienceofexposition.
Itis
not assumed that the concept oflocal inclusion or the geometric representation of
finite partially ordered structures is part ofthe that is available to the problem
solver in the initial stages ofhis solution-finding activity. However, as will be seen
later, thegeometric representationofpartiallyorderedstructuresplaysanimportant
roleintheformulationofcertainstrongmethodsofprogramformationinthepresent
domain.
Relative to the given finite, partially ordered structure, there are in D\ three
types ofdata objects:
w-type: These are elements ofo.
(They are implementable as LISP atoms.)
g-type: These are elements of2°, that is, they are subsets ofa.
(They are implementable as lists ofatoms without repetition.)
g-type: These are bags whose elements are ofg-type.
(They are implementable as lists oflists.)
A computationalactioncanbedefinedasatransitionbetweenoneormoredata
objects(theinputoftheaction)andsomeotherdataobject (theoutputoftheaction)
Such an action can be seen as the application ofa basic function on specified input
data. As will be seenbelow, computationalactionsprovidetheatomicelements (the
vocabulary) for the language ofconstructible programs. The computational actions
inD] can be conveniently represented in the form ofspecial simple graphs.
Thedataobjects are represented as nodes ofvarious types. Dataofw-type are
represented byP; ofg-type by and ofg-type by
\\ | |.
Theavailablecomputational actionsare shown in figures 18-3, 18-4, and 18-5.
Each ofthese special graphs represents a process that when executed, transformsthedataattheleftofthearrowtotherightinaccordancewiththefunctionthat
labels the graph.
--- PAGE 522 ---
, |
508 -CHAPTER 18: PROGRA-M SYNT - HESISASATHEO-RY FOR - MATION TASK
* *
(o \d \d
Figure18-3: Theleftmostgraphstandsforthefunctionalexpressionr\(u) = q, where 17denotesafunctionthatcorrespondstotherelation/inthefollowingway: Ifaisthevalueoftheargumentu,thenqisthe
setofallelementsxinoforwhichalxholds. ItsLISPrepresentationis
(lambda (u) (I-function u))
whereI-functionistheLISPfunctioncorrespondingto77.Similarly,theLISPrepresentationoftherjcomputational action is (lambda (u) (leap-function u)), where leap-function is the LISP function correspondingtorj,andrjcorrespondsto/inamannersimilartothecorrespondencebetween
and/. TheLISP
representationoftheecomputationalactionis(lambda(u)(E-functionu)),whereE-functioncorresponds
toe,anditreturnsforan inpututhelist(u)thatrepresentsthesingleton {«}.
Figure 18-4: The uppertwographs denote the union and intersection functions overtwo sets. In the
bottomgraphs,unionandintersectionaredefinedasiterativeoperationsoveranynumberofsetsthatare
containedinaninputbag.
Figure18-5: Thesegraphsstandforlist-processingfunctions, inparticular,forselectingelementsfrom
aset(leftgraph)andforcollectingelementsintoabag(rightgraph).Theywillbefurtherdefinedbelow in
thecontextofconstructibleactionsequencesorprograms.
For a given finite, partially ordered structure, the set ofall data objects in D,
andthesetofallcomputationalactionsdefineadataspace. Asinall problemsolving
situations, this is an extremely important concept. Given a set ofinput data objects
and an output data object, the output is directly obtainable from the inputs ifthere
existsacomputationalactionthatcaneffectthetransitionfromthegiveninputstothe
output. Acomputationalpath fromasetofinputdataobjectstoanoutput data object
is defined as a finite aggregate ofcomputational actions that can effect a transition
from the inputs to the output. The notion ofa "computational path" is equivalent to
the notion ofa "computational trace" ofa program for given input data.
It is importanttostressthatthedefinitionofthedata space isgivenimplicitlyin
Dj- via the specification of the set ofall data objects and the set ofcomputational
actions.
--- PAGE 523 ---
AMAREL 509
18.2.1.2 Specification of the Set X, of Problem Solutions-That Is, of
the Constructible Programs
In the problem class described here, the possible solutions are constructible
programs. ThesetX! isthelanguageofconstructibleprograms. Theinitialdefinition
ofthelanguage, whichisassumedtobepartoftheinitialdeclarativeproblemformulation, ispresented indetail inprevious work(Amarel, 1971). The mainelementsof
this definition are summarized below.
The language of constructible programs is given in terms of available (permissible) program statements and ofconditions for combining such statements into
programs.
The set ofprogram statements is identical with the set ofbasic computational
actionsdefinedinD\ EachprogramstatementcanbeimplementedasasimpleLISP
function. In this implementation, data ofw-type are atoms, data ofg-type are lists
without repetition, and data ofg-type are lists oflists.
Programscanberepresentedasgraphsthatareconstructedbyaggregatingprogram statementgraphs in certain ways. This representation isespecially suitable for
describing (and for reasoning about) the dataflow in the program. There are two
kinds of branches in program graphs: solid branches -> for the flow of data and
dashedbranches --> fortheflowofcontrol. Inmostcases, theflowofcontrolcanbe
determinedina straightforwardway fromthe flow ofdata. Explicitspecificationof
controlisneededonlywhenloopsinvolvinglist-processingstatements(selectionand
collection statements) mustbe specified. However, the loop structure ofconstructible programs for our problem class is assumed to be severely constrained. Only
simplenestingofiterativeloopsispermitted. A loophasthecharacteristic structure
shown in figure 18-6.
In this structure, p x denotes a. program variable that can take as values programsthathaveaw-type inputandag-typeoutput. The inputtotheoverall structure
is ofg-type, and its output is also ofg-type. The structure can be regarded as a loop
macrostatementthatdoesthefollowing: eachelementoftheinputq isoperatedupon
by/?!, andtheresultsarecollectedinabagg;theoutputq„isobtainedbyapplyingthe
specifiedsetoperation( U or Pi) ontheelementsofg. Toclarifyfurtherthenotionof
aloopmacrostatement, letusrepresentitin LISPforaspecificassignmentofvalues
loopentry . . . loopexit
> / \ /
s U or n
q, . u q« 8 Ho
Figure 18-6: Loopstructureofconstructibleprograms.
--- PAGE 524 ---
510 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
to/?i andtothe set operation. Specifically, lettheprogram P, bethe valueof/?,, and
let the set operation be an iterated intersection operation, which will be denoted by
Intersection-*. TheLISPfunctionIntersection-*takesasinputalistoflists in which
thelistsstandforsetstobeintersectedandtheresultistobereturnedasanoutputlist.
The convention used here for this function is that ifthe input list is empty, then the
outputlist is alsoempty; and ifthe input listhas asingle listas itselement, thenthis
lististhefunction'soutput. Withtheseassumptions,aLISPrepresentationoftheloop
macrostatement is a follows:
(lambda (q) (Intersection-* (Mapcar '^ q)))
Nowconsiderasetofprogramstatementsthatis madeofloopmacrostatements
and of the previously specified basic program statements except s-statements and
c-statements; letuscallthisset T
Supposealsothattheinputandoutputstructureof
constructibleprogramsisspecified;thatis,thenumberandtypeofinputvariableand
thetypeoftheoutputvariablearegiven.
Aprograminputcanbemadetocorrespond
tothe inputofastatement from T
iftheirdatatypes match; the same istrue foroutputs. Further, two statements from T
can be concatenated ifthe output type ofthe
first matches the input type ofthe second.
A constructibleprogramcanbedefinedrecursivelyasadirectedgraphmadeof
aggregations of statements from T that establish some kind ofconnective tissue
between the program input and the output; and furthermore, any loop macrostatement in such a graph can have as values ofits program variable only graphs representingconstructible programs. Itcanbe seenthat agraph representing aconstructibleprogram isadataflowgraphwithoutloops. Thistypeofgraphhasbeencalleda
riverlike graph (Amarel, 1971), because it has the general form ofwater flow in a
riversystem. Insuchasystem, riverchannelsmerge, diverge, and runsideby side in
a pattern of motions whose general direction is from river sources (the program
inputs) to the river mouth (the program output).
It can be shown that riverlike graphs can be partitioned into equivalence
classes,eachrepresentedbyadecoupledgraphasitsnormalformrepresentative. Ina
constructibleprogramthatisrepresentedbyadecoupledgraph, theoutputvariableof
acomponentstatement(unlessitcorrespondstotheprogramoutput)isusedprecisely
once as an input variable in another statement ofthe program. Under these conditions, it ispossiblefortheprogramtospecifythesamecomputationmorethenonceif
theresultofthecomputationistobeusedmorethanonceasinputofsubsequentcomputations. Clearlyitispossibleinthiscasetosimplifytheprogrambyeliminatingthe
redundantcomputationsandbyusing intermediateresultsmorethenonce. However,
forpurposesofprogramsynthesisit ismoredesirabletoworkwith representations in
the formofdecoupledgraphs. It isassumedtherefore, that the specificationofthe set
X, ofconstructible programs amounts to a characterization ofthe set ofdecoupled
riverlikegraphsthat arc made fromelements(programstatements)taken fromthe sot
7j. Notethat the specificationofA', is not in agenerative form. Membership in A can
--- PAGE 525 ---
AMAREL 511
bedeterminedby testingwhetheragivencandidategraph satisfiesthe requiredconditions. Further, associated with X, are procedures for expanding a riverlike graph
that is not decoupled into a functionally equivalent decoupled form and for simplifying decoupled graphs.
Clearlythereisaconnectionbetweenthenotionofaconstructibleprogramand
that ofacomputational path, which was introduced as partofthe domain specificationD\ Asindicatedpreviously,eachcomputationaltraceofaconstructibleprogram
for any input data is equivalent to a computational path in data space. However,
becauseofsomeoftheconstraints imposedonthelanguageofprograms, itisnotthe
casethatforeverycomputationalpathindataspacethereexistsacomputationaltrace
ofa constructible program inX\
Now, a memberof X is acceptable as a solution to aproblem in ourclass ifit
satisfies in addition the problem class conditions as specified by the individual
problem-data.
18.2.1.3 General Conditions d Imposed on the Problem Class
The conditions C are as follows:
1. Givena "specifying set" intheformofafinite setofassociations, whereeach
a s s o s m oc e ia p t a i r o t n ia h l a l s y t o h r e d f e o r r e m d { s u t x r u u 2 c , t w ur 3) e a E n . d u A u s u o 2 l , ut w i 3 o a n re pr e o l g em r e a n m ts P of mu t s he t s s e a t ti o sf o y f
P{u x u 2) = w 3 foreach association in the specifying set.
2. The structure of the program should be the simplest possible. (This will be
made clearerbelow.)
18.2.1.4 Problem-Data Domain A-i: Initial Internal Formulation of the
Specific Problem ^
The problem-data domain consists of all the finite, partially ordered structures;andforeachstructureLwithasetaofnelements, itconsistsofasetofpossible
associationsintheformofpairs(u\U 2, ut). Thesetofpossibleassociationsischaracterized by a domain ofassociations, which is the set ofall the possible left sides in
associations, and a range ofassociations, which is the set ofall the possible right
sides. Intheproblemclassnowunderconsideration, thedomainofassociationsconsists ofall unorderedpairs taken from a. Let us call this domain d\. The range of
associationsistheseto. Thusthesizeofthedomainsetd\ isn(n + l)/2, andthesize
ofthe range is n.
The problem-data for an individual problem is given by defining a specific
structure £ and a specific "specifying set" ofassociations. Thus the initial internal
formulation ofthe problem ir\ in declarative form consists ofthe problem class formulation (Dj, X u C\, AO, augmented by the specific problem-data that is shown in
figure 18-1.
--- PAGE 526 ---
512 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
18.2.2 Transforming the Initial Formulation of the Formation Problem
into Procedural Form
Fora problem solverto proceed with a solution finding/constructing activity,
he needstotransform (partsof) thedeclarative formulation oftheproblem intoproceduralform; that is, he needs to define a solution construction process that is controlled by the problem formulation. Basically, the problem solver must reformulate
knowledgeabouttheprobleminaformsuitablefordefiningagenerativeprocessfor
producing candidate solutions. Thus it is of central importance to find ways of
describing the language ofprograms in terms of some generative grammar. The
choiceofagrammarforspecifyingthelanguageofprogramsandthedevelopmentof
methods for using such a grammar in theory formation processes have been major
components ofprevious work in this area.
Ifthereisagenerativegrammarforthelanguageofprograms, thenastructural
description can be assigned to each program in the language with respect to the
grammar. Thestructuraldescriptionwouldarticulatethestructureoftheprogramin
termsoftypesofcombinationsofprogramstatementsthataredefinedinthegrammar.
Thebasicapproachusedherefromtheearlystagesofexplorationoftheprogramformationproblemwastorepresentandmanipulateprogramsintermsoftheirstructural
descriptions. Thisapproachisguidedbytheexpectationthatcertainregularitiesexist
betweenfeaturesofastructuraldescriptionofaprogramanditsfunctionalcharacteristics and also that such regularities can be exploited in the formulation ofefficient
procedures for search overthe program language.
Sincethereisnouniquewayofdescribingthelanguageofprogramsintermsof
a grammar, then the question must be faced of how to choose an "appropriate"
grammar, that is, a grammar that assigns "appropriate" structural descriptions to
programs. Appropriatenesshereis meantinthesenseofleadingtotheestablishment
ofstructure-function regularitiesthatcanbe fruitfully used in problem solving. The
problem ofchoosing among possible grammars ofthe language ofprograms proved
to be at the crux oftheproblem ofrepresentation in the present formation problem
(Amarel, 1970, 1971).
Previous work on the program formation problem resulted in two major
approaches to solving the problem. During work with thefirst approach (Amarel.
1962a, 1962b), several grammars were tried and used with varying degrees ofsuccess as parts of heuristic "generate-and-test" procedures for solution finding. It
became clear very soon that performance was highly sensitive to the choice ofprogram grammar. Small changes in the way of specifying the grammar had strong
effects on overall system performance-over a range ofparameters ofthe heuristic
search strategy. This experience provided convincing evidence of how central the
problem of representation was in the program formation process; and it led to a
secondapproach (Amarel, 1971), which concentrated on conditions for choosing a
program grammarand-moregenerally-onwaysofformulating a useful concept o\
program space.
--- PAGE 527 ---
AMAREL 513
A usefuldefinitionofprogramspacerequiresadefinitionofthesetofprograms
(i.e., of the program language) and, in addition, a definition of some relations
between programs that can provide addedstructure that can be used for reasoning
aboutprogram behaviors. Whenthe additional relational structureonthe setofprograms is defined, additional conditions on how to represent this set are introduced,
which amounts toaddedguidance onthe choice ofprogram grammar. Inthe second
approach to the program formation problem, the following condition was used as a
guide to the choice ofprogram grammar:
Thegrammarshouldbesuchthateachstructuraldescriptionconstructedinthe
grammarmusthave an interpretation ina mathematical systemin which there
existsastructureofrelationshipsthatcanbeusedforreasoningaboutprograms
in terms oftheir functional properties.
This condition was satisfied by the construction ofan algebraic model ofprogram
space in which for each structural description of a program there corresponds an
expression inthe algebra. The algebraic model willbediscussed later. Firsttheprogram grammar G\ that was obtained under guidance ofthe model will be described
briefly. (A more detailed description is given in Amarel, 1971.)
Although this is one of several grammars that were used in the past in this
research, it illustrates the main characteristics ofall the grammars used in the first
approachtoprogram formation, and itcanbeusedtodescribe the strategy ofsearch
inthatapproach. Itcanalsobeusedtodescribethesecondapproachtotheproblemas
well as more recent work with a third approach.
18.2.2.1 The Program Grammar G^: Structural Descriptions of
Programs and Program Schemes
Aprogramgrammarcaneasilyembodythepartofthedeclarativeproblemformulation that specifies the set X, ofconstructible programs, that is, the possible
(legal) solutions to the formation problem. In the case described here, a grammar
may also be constrained to some (small) extent by the problem class conditions C\
andbythecharacteristicsofthedatadomainA\. Specifically, thegrammarG\ islimitedtotwo-inputproblems whereeach inputacceptsdataofw-type; it isalsolimited
to programs that treat each input symmetrically; and it assigns to each generated
program a measure ofstructural cost, a weight w, that can be used in handling the
condition ofprogram simplicity.
As indicated previously, programs are represented as graphs that obey certain
local rules ofarticulation and that havecertain boundary (input-output) characteristics. Thesegraphscanbeviewedasanalogsofstringsinordinarylanguages, andthey
are called aggregates. An aggregate is a combination of graph elements each of
--- PAGE 528 ---
514 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
whichcanbeeitheraterminaloranonterminalintheprogramgrammar: itisassembledinaccordancewiththerulesofaggregationinthegrammar. Theserulesareanalogsofrulesofreplacementingrammarsofordinary languages. Thegrammarhasa
startingnonterminalelement, and aggregates can be generated by repeated applications ofrules ofaggregation starting from the starting element.
Anaggregatethatconsistsexclusivelyofterminalelementsiscalledaterminal
aggregate; and itrepresentsacompletely specifiedprogram. Theterminal elements
oftheprogramgrammarG arethesetofcomputationalactions, representedingraph
form, thatweredefinedpreviouslyinsection 18.2.1.1 aspartofthedomainspecification D\.
Thenonterminalelementsof G aretheelementarygraphsshowninfigure 18-7.
Thegraphatleftiscalleda(u
^-program, andthegraphatrightiscalleda
(u, g)-program. These nonterminal elements play the usual syntactic role of representatives for program types. In the present grammar, nonterminals classify
programs (orpartofprograms) bytheirinput-output structure. This is a roughcharacterization that provides necessary conditions for possible connectivities, or compositions, between program parts.
Therulesofaggregationof G areshowninfigure 18-8. Eachrulehasaname, a
transition part, and an associated tree, which is called a unit ofstructural description. The transition part ofa rule represents a possible replacement ofits left side,
that is, ofanonterminal element, by its right side, which is in the form ofan aggregate that contains one or more terminal elements. The application ofa rule ofaggregation amounts to specifying a part ofthe structure ofa graph that represents a
program. Thestructuraldescription ofa (graph representing a) programconsists of
the record of rule applications that collectively specify the program (graph). Thus
every rule application can be seen as building a piece ofstructural description, and
this is representedby theelementary tree shown inthe right side ofa rule. In such a
tree, the upper node corresponds to the nonterminal element in the left side ofthe
rule's transition; the dark node below it corresponds to the rule name; and nodes
belowthedarknode(iftheyexist)correspondtothenonterminalsinthe right sideof
the rule's transition.
Each rulerepresentsaspecific modeofcombining nonterminalsorthe instantiationofa nonterminal by adefiniteprogram statement. The nameofa rule reflects
the specific rule action. For example, R 2t ® is a rule that specifies the structure of
a two-input program (of a {u { u 2, <?)-type) as being in the form ofa cascade ofa
Figure 18-7: Nonterminal elementsofG\
--- PAGE 529 ---
Rulename
(Least-expectedWeight u*) Transition UnitofStructural Description
Pi Q
2iV
orR2A
P: U or D R2.V 0r^2.A
(7)
P2.2
PiQ
.©orR
2, P: *• .\-* 1- H- U »-or n R2.GJorR 2
J P2.\ S ' Pi ' C H 4
P2Q
Pl,2
, E orR 2 P: \> U or n R 2, orR 2.ra
Pl.l Pl,2
P\.\ = P\2
Rlv orR,,A
R-i.v orR-i,a
(3)
Pi.i P1.2
Pi Q
RiaorRi
(3) t> |Io P1 » .1 1 I t » | 1o p, » 2 1 I »' I,|- U *^ o - r | n Ri.©orR,,
Pl.l Pl,2
P\ O
R,j
v >
17or77ore
Ri.i
(1)
Notes: Forsimplicity, severalrulesarecombinedinonerow. Theassignmentofnames,P2,P2,i, . . . ,P\,
Pi.i, . . . ,tothenonterminalshasbeen introducedinordertoindicatethecorrespondencebetween
partsofprogramgraphsinthetransitionpartofaruleandpartsoftheassociatedstructuraldescription. Thesenotationsarenotnecessaryfordescribingthetransitions(exceptforrulesR 2,ig, R2,ia>
wheresomenamingmechanismisneededtoexpresstheconditionthattheaggregatescorresponding
tothetwo "arms" ofthegraphareidentical).
Figure 18-8: RulesofaggregationofgrammarC,
--- PAGE 530 ---
516 CHAPTER 18: PROGRAM SYNTHESIS ASATHEORY FORMATION TASK
two-inputprogram and aone-inputprogram followed by a set-union operation. The
symbol isusedtorepresentthistypeofcascade + unioncombination. Thechoice
ofthis symbol and ofthe other symbols for rules derives from the algebraic model
interpretation, as will be seen later.
Togetherwith each rule name in figure 18-8 there is anumberthatdenotes the
least-expected weight, w*, ofaterminal aggregate that would result ifthe rule were
applied. This is the weightofthe simplestprogramthatcanbe structured in accordance with the rule. This notion will be discussed furtherbelow.
NotethatinRules/?2,0 and/? 2,^, aconditionpu = p]2 isassociatedwiththe
transition. This reflects the requirement that a two-input program should treat its
inputs symmetrically.
Therules Ru Rij, R\ ^arecalledterminalrules, sincetheirapplicationresults
in theproduction ofterminal aggregates. They arerepresentedby special nodes, _•
in structural descriptions.
For two-input programs, where each input is of w-type, the starting nonterminal element is a (U]U 2, q)-program graph (see figure 18-9).
The notion ofderivability in thegrammarcan be readily transferred from the
conventional linguistic context into the present situation.
The language of constructible programs for the problem class can now be
x f
g {
U 2
rminal
The structural description ofa program P in G\ is a tree that can be obtained
from the derivation (construction) ofP in G\ by putting together in an appropriate
mannerthe units ofstructural description thatcorrespond tothe rule applications in
the derivation. The structural description, call it d(P), contains all the information
neededtoconstructtheprogramP inaccordancewiththe rulesofaggregation in G\
Basically, the grammar determines how a candidate solution (a program) can
be constructed from component substructures and fromelementary parts. Since the
grammar rules are nondeterministic, each time a rule is applied a specific choice
mustbemadeamongalternativeapplicablerules. Fromthepointofviewofaformationprocess, acandidateprogram iscompletelydeterminedbythepatternofchoices
of aggregation rules that collectively specify the program's structural description.
Now, the white nodes inastructural descriptioncorrespondtothechoice pointsthat
determinetheprogram. Thusthenumberofwhitenodesinthe structuraldescription
ofaprogram isasignificantmeasureofformationcomplexityfortheprogram,called
Figure lS-*>: Starting nonterminal clement fortwo-input programs
--- PAGE 531 ---
AMAREL 517
theprogram's weight, vv. The weightofaprogram isalsoclosely relatedtothe count
ofstatements in the program; it equals the count ofall program statements with the
exception ofthe list-processing statements (i.e., the s-statements and c-statements).
Notethat vvcanalsobeseenasthecountofblacknodes (ruleapplications) inastructural description.
These concepts are illustrated in figure 18-10, which shows a program for the
Infimum function-both its structural description in G] and its graph representation
as a terminal aggregate in the language L x. This program is the solution of the
problem ir\ specified in figure 18-1. Note that the weight ofthis program is 19.
Anexaminationofthe structuraldescriptionofP Infimumorofitscorresponding
graph representation shows that it has a redundant substructure marked as P F in
figure 18-10b. Clearly a simplificationcanbe obtainedby eliminatingthe redundant
P F and by reorganizing the data paths in an appropriate way. Although this would
result in a program that is more efficient for execution purposes, it would be an
inferior representation for purposes of program formation, where reasoning with
programs in the form ofdecoupled riverlike graphs is more desirable.
It ispossibletoabbreviatethe structural description ofaprogramby retaining
onlythenodesforrulenamesandbyeliminatingallbuttheessential informationina
rule name. Thisyields anabstractdescription, dA(P) ofP. Such an abstractdescriptioncaptures theessence ofaplan forconstructing aprogram. The abstractdescription ofthe program Pinf, mum is shown in figure 18-11.
The following is a LISP representation ofthe program P[nfjmum:
^mfimum: (lambda (u ± u 2)
(Intersection
[Intersection-* (Mapcar
1 (lambda (x) (Union
(leap-function x)
(E-function x)))
(Intersection
(Union (I-function u E-function uj
(Union (I-function u 2) (E-function u 2)))]
[Intersection (Union (I-function uj (E-function uj
(Union (I-function u2) (E-function u 2))]))
The functions I-function, leap-function, E-function, and Intersection-* are
specific to the domain language; they were defined previously.
Theconceptsofprogramvariableandprogramschemeareusefulfortherepresentation and manipulation ofprograms during the formation process. A program
scheme is an incompletely specified program that has a given structure, parts of
whichare well definedandotherparts ofwhichare identifiedby programvariables.
--- PAGE 532 ---
518 CHAPTER 18: PROGRAM SYNTHESIS ASATHEORY FORMATION TASK
jR.v
ill
a. StructuraldescriptionoftheInfimum program in G|.
b Representation ofthe [nfimum program in thegraph language /
Figure 1S-10: Representationsofsolution toproblem n: the Infimumprogram
--- PAGE 533 ---
AMAREL 519
E E
I I
Figure 18-11: AbstractdescriptionoftheInfimumprograminG\.
Thevaluesofaprogramvariablecanbeeithercompletelyspecifiedprogramsorprogram schemes that are themselves expressed in terms of(other) program variables.
Thesetofconstructibletwo-inputprogramschemescaneasilybedefinedasan
extension ofthe languageL oftwo-input programs viathe introduction ofprogram
variables as terminal elements in an extension ofthe grammar G\
18.2.3 Initial Procedural Formulation of the Formation Problem:
Heuristic Search in Program Space
The formulationofaprogramgrammar suchas G] provides akey element for
the specifications ofproblem solving procedures forprogram formation.
Letusreviewthewayinwhichtheprogramgrammar, togetherwithanassociated body ofcontrol knowledge, was used in the first approach to a solution ofthe
formation problem (Amarel, 1962b), which was based on heuristic "hill climbing"
in program space.
The program grammarprovides the basis for specifying a generator ofcandidate solutions in a typical "generate-and-test" loop. The behavior ofthe generator
can be conveniently represented by an AND-OR search tree. The tree has decision
(choice)nodesandruleapplicationsnodes. Thedecisionnodesare OR nodes,andthe
ruleapplicationnodesare AND nodes. A solutioncandidate(aprogram)isgenerated
as a structural description, which is a special kind ofsubtree ofthe search tree. A
solution tree has a single descendant at each decision node and a characteristic
number ofdescendants (two or one, depending on the rule) at each rule application
node; itsterminalnodesareallruleapplicationnodesoftypeR { h R xj, orR UE. Itcan
be seen in figure 18-10a that the structural description ofthe Infimum program is a
tree ofthis type. The white nodes in the structural description tree correspond to
decision nodes in the search tree.
Afterthegenerationofacandidatesolutionintheformofastructuraldescriptionofaprogram, the formationproceduretranslates it intoan executable program,
possibly simplifies it, and then tests the program by running it overasample ofthe
given table ofdata associations. The test provides an estimate ofthe proportion of
dataassociationsthatare satisfiedby thecandidateprogram. Thisestimate is called
--- PAGE 534 ---
520 CHAPTER 18: PROGRAM SYNTHESIS ASATHEORY FORMATION TASK
the program value v. The v ofa program is intended as a measure ofthe functional
worth ofthe program relative to the given set ofproblem conditions. This measure
providesanevaluationofthesetofdecisionsthatcodeterminethe structuraldescriptionofthecandidate solution/program fromthepointofview ofthe problem conditionsthatthesolutionisaskedtosatisfy. Therefore, theprocedureassociatesthevofa
programtoeachdecision nodeinthesearchtreethatparticipates inthe specification
ofitsstructuraldescription. Thisassignmentofvaluestodecisionpointsinthesearch
tree is then used in subsequent decisions ofthe formation process.
Intheformulationofthehill-climbingheuristics, thefollowingadditionalconcepts are being used: the least expected weight ofa program that would result from
choosingacertaingrammarruleatadecision nodeofthe searchtree (this is the w*,
discussedpreviously);ameasureofaccumulatedsearcheffortassociatedwithadecisionnode(call it e)-this isacountofthechoicesmadeatthedecisionnodeuptothe
present as the system is trying to build candidate programs; and a value decrement
associated with adecision node, which provides ameasure ofsensitivityofprogram
value to decision changes at the node (call it Sv).
The generation process proceeds by experimenting with simple program
(small w) first and by moving gradually to more complex programs. The system
attempts to build a locally stableprogram at a given level ofprogram complexity:
it focuses then on a decision node of such a program, and it "grows" a program
substructure of increased complexity below that node; it then attempts to find a
locally stable programatthe increasedlevelofcomplexity, andsoonuntil asolution
is obtained oracertain predetermined level ofavailable effort is reached.
A program is considered to be locally stable if, for all its decision nodes,
changing a decision one at a time does not yield a program ofhigher v. Using this
notion saves considerable search effort and storage capacity, as search proceeds
mainlyina"bestfirst" mannerbymodificationsofthemostpromisinglocallystable
programs.
The most important heuristic choice in this process is the choice ofwhere to
focus next on the structure ofa "high-v" program for the generation ofnew (more
complex) program substructures and then what initial assignments ofstructure and
programelementstomakeatthatpoint. Theheuristicsthatcontrol focusofattention
ondecision nodesare guidedby largest6v, smallest vv*, and smallest e. in that order
ofpriority. After attention has focused on a decision node, the system must decide
what structure to "grow" below that node. The initial assignment of program elements to the new structure is made by an 'associative transfer"ofnearby structures
inthe searchtree, with priority giventostructuresthat participated in relatively successful "locally stable" programs. (Details ofthese heuristics are given in Amarel.
1962b.)
Application of the heuristic "generate-and-test" approach to the formation
problem 7r, resulted inaprocess that can be summarized by the "learning curve" (or
"formation curve'*) o\ figure 18-12. The abscissa represents numbers ofprograms
--- PAGE 535 ---
AMAREL 521
Max. ValueAttainable
Best
Program
Value
Attained
No. ofProgramsTried
Figure 18-12: FormationcurveforInfimumprograms(first "top-down" approach).
tried so far. Clearly the improvements in vcome in suddenjumps. At each point of
improvement, the weight wofthe "best" program attained is shown in the figure.
In experiments with the heuristic hill-climbing search process, some of the
heuristicspostulated/usedappearedtobe successful. However, it isvery difficultto
understandtheir strengths andlimitations, andthus itis difficultto have any consistentexpectationsaboutsystemperformancefordifferentproblemsinthegivenclass.
18.2.4 Comments on the Heuristic Search Approach
Finding a solution to the Infimum problem by a simple exhaustive search
strategy requirestrying roughly 109programs. This is aratherlarge search space. In
the heuristic search approachjust outlined, a solution is found after about 102 program tries (see figure 18-12). This clearly represents a strong improvement. However, thesolutionisquiteunstable. Iftheproblem-dataisgivenintermsofadifferent
partiallyorderedstructure(notEj asinourproblem7T\),orifsmallchangesaremade
inthedefinitionofprogramgrammar, thenthesearcheffortneededtofindasolution
may grow appreciably. What is more disturbing is that it is difficult to see how is it
possibletoimproveperformancewithinthepresentapproach. Theseissuesandconcerns ledtothe formulation ofasecondapproach, in which new knowledge is introduced in the formofamodel ofprogram spacethathelps indeveloping search plans
thatare "betterinformed." Thisapproachresults inanimproved-andmorestablemethod ofsolution.
--- PAGE 536 ---
522 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
18.3 IMPROVED PROCEDURAL FORMULATION OF THE FORMATION
PROBLEM: MODEL-GUIDED REASONING IN PROGRAM SPACE
18.3.1 Algebraic Model of Program Space
This sectionwill describethemathematical model ofprogram spacethatplays
akey role inthe secondapproachtotheprogram formationproblem. The model and
its formal properties are presented in detail in previous work (Amarel, 1971).
Themodelisamodifiedalgebraofrelations. Itconsistsofasystem di forhandling one-input programs and an extended system ^kd\ for handling two-input
programs.2
18.3.1.1 Introduction to the Algebraic Model
Givenafinitesetaofnelements, letuscalladyadicrelation Ronaaone-input
relation. Let us representRby an n x n binary matrix and denote the matrix by R.
TobuildthematrixR,letusassociatewitheachelementofaanumericalindex
from 1 ton; andletthe /-row and/-columnofthe matrixcorrespondtothe /-element
of o. The value of the r component ofthe matrix R is 1 if R holds between the
/-element and they-element ofa: otherwise, it is 0. For example, the matrix correspondingtotheproperinclusionrelation/ontheset0\ oftheproblemshown infigure
18-1 is as shown in figure 18-13.
In the specification ofthis matrix, the assignment ofindices is
- - - -
L e, 2 -~ a, 3 b, 4 c, 5 d.
Elementsofthesetsaand2°canberepresentedby^-dimensional rowvectors,
as follows: An elementofoto which there corresponds anumerical index / is representedbyavectorwitha 1 atits/-componentanda elsewhere; asubsetofelements
ofais representedbyavectorwitha
ateachcomponentthatcorrespondstoanelement ofthe subset and elsewhere. Now, it can easily be seen that program statements inthe languageofconstructibleprogramscanbe modeledby ordinary vectormatrix multiplications. For example, the program statement shown in figure 18-14
00000 _
oooo
ooo
1 i
I11110
Figure 18-13: Relation matrix forproperinclusion relation/.
Thesubscriptd\ comesfromthedefinitionofthedomainotassociationsintheproblemclass(secsection
is:, i.4).
--- PAGE 537 ---
AMAREL 523
-IdFigure18-14: Aone-inputprogramstatement.
canbemodeledbytheexpressionq = uI, whereu, qarethevectorrepresentations
ofuandqrespectively, I isthematrixcorrespondingto/, andthemultiplicationcan
be defined as
q = row, (I) ifcomp, (u) = 1 and comp, (u) = for all) =£ i
row, (0) ifall the components ofu are0.
In this definition, comp, (u) denotes the /-component ofthe vector u, and
denotes the matrix ofzeros thatcorresponds to the nullrelation.
In addition to the usual operations ofthe algebra of relations, a newproduct
operation, denoted by ©, is introduced here. The ordinary product ofrelations is
© ©
denotedby
Thenewoperation,
isneededinordertoobtainacompletemodel
forthesetofconstructibleprograms; its introductionis responsibleforthedeviation
between the system described here and the ordinary algebra of relations. The two
product operations can be defined as follows:
Forany relationsX, Y, Z, and for all indices i from 1 to n,
(Z = X © Y) = (row, (Z) = V row k (Y)
k E t [row, (X)]
(Z = X © Y) m (row, (Z) = & row*(Y)
k E r[row,(X)],
where r[row,(X)] denotesthe setofcoordinates in the /-row of X whosevalue is 1
and V and & are used for iterated disjunction and conjunction, respectively.
The logical interpretation ofthese operations is as follows:
©-product: uZv holds ifthere is a w in a such that (uXw) and (wYv) both
hold. (This is the ordinary product ofrelations.)
©-product: uZv holds iffor all w in o such that (uXw) holds, the relations
(wYv)holdalso. (Thisisthenew, morerestrictive, productintroduced in the present modifiedalgebra ofrelations.)
Itcaneasilybeverifiedthatthereexistsasimplecorrespondencebetweenany
constructibleprogramof(u,q)-typeandaone-inputrelationalexpressionmadeofthe
relations /, /, E, the products ©, ©, and the Boolean operations V » &• Consider
--- PAGE 538 ---
524 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
in particular the aggregations of(u,q)-type statements that are defined by the rules
R, v, R| A and Ri,©, Ri,© ofthe program grammar G\ (see figure 18-8):
To the aggregates3 shown in figure 18-15 there corresponds the relational
expressions (X V Y), (X & Y), respectively, where X, Fare one-input relations;
and to the aggregates shown in figure 18-16 there correspond the expressions
© ©
(X F), (X Y), respectively.
18.3.1.2 The Algebra of One-Input Programs 2ft
Thealgebraofone-inputprogramshasasatomicelementsthe relations/, 7, E,
and 0, and as operations the Boolean operations V and & and the relation products
© ©
and . The terms of 2ft are relational expressions made from these atomic elements, relationvariables,andthegivenoperations(usedhereininfixform). Foreach
term in this algebra there is a corresponding relation matrix as well as a corresponding one-input program in our language.
The terms of Sft are partially ordered under an implication relation, denoted
by - . ForanytwotermsX, Y, theimplication X - Y holdswhentherelationmatrix
correspondingto X isincludedinthe relationmatrixcorrespondingto Y The logical
interpretationof X - Y isthatforanypairofelementsuand vina, ifuXvholdsthen
uYvalsoholds. Now, if X - Fand Y - X, then Xand yareequivalentand we write
X -
Fromourpointofview,theusefulnessofthemodelisdeterminedbythedegree
towhich itprovidessomestructureinprogramspacethatcanhelponetoreasonabout
programs in functional terms. The implication relation induces a rich structure in
?A which proves to be useful in developing a more goal-oriented strategy for program formation.
U or n
Figure 18-15: Aone-i-nput "p - arallel" - programaggre - gate. - -
|o- - |>-~ It--
I I
X s Y c U or H
Figure 18-16: Aone-input "cascade" programaggregate.
Toi simplicity ofexposition, nodistinction ismadeherebetweenarelation Yanddie function (call it \)
whosevalue foi an) elemeniwofoisthesetofall vinasuchthatuXvholds. Inastrictsense,thelabel.Yin
thegraph representation ofthe program shouldbechanged to \
--- PAGE 539 ---
AMAREL 525
Key features ofthe structure of r Si are summarized below:
1. PropertiesofBoolean Operations: Theusualpropertiesofidempotency, commutativity, distributivity, and associativity
2. GeneralProperties ofProduct Operations
© - ©
(X Y) (X F), forany termsX, Y
(X © 0) - (0 © X) - (X © 0) ~ (0 © X) -
(X © £) - (£ © X) ~ (£ © X) - X
(X© - X
if X - £ then (Z © X) - (Z © F)
(X© Z) - (F© Z)
© - ©
(Z X) (Z F)
(X©Z) - (F©
Z),
foranyX, F Z.
3. AssociativeProperties ofProduct Operations
a. General: Forany one-input relationsX,Y,Z,
((X © F) © Z) - (X © (F© Z))
((X © F) © Z) - (X © (F© Z))
((X© F) © Z) - (X© (F© Z)).
b. Special case: IfF = E \J JK forany W (including W = 0)
then ((X © F) © Z) - (X © (F© Z)).
4. DistributivePropertiesofProducts withBoolean Operations
a. Rightdistributivities: Forany one-input relationsX,Y,Z,
((X V Y)Q)Z) - ((X © Z) V (F© Z))
((X <£ F) © Z) - ((X © Z) & (F© Z))
((X V F) © Z) - ((X © Z) & (F© Z))
b. Left Distributivities: Forany one-input relationsX,Y,Z,
(X © (F V Z)) - ((X © F) V (X © Z))
(X © (F V Z)) - ((X © F) V (X © Z))
(X © (Fc£ Z)) - ((X © F) & (X © Z))
(X © (Y& Z)) - ((X © F) & (X © Z))
--- PAGE 540 ---
526 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
5. SpecificProperties ofAtomicRelation I, I, E
(I & I) - (I & E) - (7 & E) -
(/©/)- /;(/©/) -
© - © -
(7 7) I; (7 7)
(£©£)-(£©£)-£
((/ V E)&(I V E)) ~ E
((/ V E) © (/ V E)) - (I V E)
((/ v £) © (/ V £)) - (7 V E)
((/ V E) © (7 V E) - (7 v £)
((7 V £) © (/ V E)) ~ (/ V E)
18.3.1.3 The Algebra of Two-Input Programs 2ftd1
Considernowtriadic relationsonafinite setooinelements. A triadic relation
is a rule that specifies for each pair taken from the set o x a ifthe relation holds
betweenthispairandanyelementofa. Ingeneral, thedomain sizeofsucharelation
is n2 . In accordance with the specification ofour problem-data domain A, (see section 18.2.1.4), the domain oftriadic relations in the model is restricted to the set of
n(n + 1)12unorderedpairsofelementstakenfromo,thatis,tothedomainofassociations d\. Letus call the triadic relations in our model two-inputrelations and mark
them with a subscript d\, for example, R d\.
A two-input relation R d] can be represented by a binary matrix ofn(n -I- 1)12
rowsandncolumns. LetusdenotethematrixbyR^, The//-rowofthematrixcorrespondstoaninputpairmadeofthe/-elementofaandthey'-elementofa(inaccordance
with some indexingofelementsofa); andthe/:-columnofthe matrixcorrespondsto
the /:-element ofa. The value ofthe r
ljk
componentofthe matrix R
(/|
holds between
the //"-pair ofoand the ^-element ofa; otherwise it is a 0.
Fora given indexing ofthe elements ofa, let us arrange input pairs in accordancewiththeordering <(1, 1),(1, 2), . . . ,(1,k),(2,2), (2,/?),(/2 - In - 1).
(n - 1, n), (n, n)> . Now, let us order the rows oftwo-input relation matrices in
accordance with this ordering, and let us establish a correspondence between the
components of row vectors (with n(n + l)/2 dimensions) and this ordering. In
analogy with the one-input case, an element ofthe input domain d\ to which there
correspondsapairofelementsofawith indexesijcanbe representedby a row vector
with a 1 at its //'-component and elsewhere.
If //,, u 2 are variables that take as values elements of a, then let u,u : denote
the vector representation ofthe input pair. The expression q = u,u R denotes an
: (/,
--- PAGE 541 ---
AMAREL 527
ordinary multiplication of the n(n + l)/2-dimensional row vector U)ii2 and the
matrix R^, which produces the /^-dimensional row vector q, underthe rule:
q = (row, 7 (R^)), ifij is the component ofthe vectoru t u 2 whose value is 1.
Twooperations, and El are introduced forcomposingtwo-inputrelations
from one-input relations. They are called crossproducts, and they are defined as
follows:
Foranypairofone-inputrelationsX, Y, anytwo-inputrelation Z d], andanypair
ofindices i,j,
(Z =XEHY) = (row,On) = row,(X) V row,- (Y)),
(Zdl ,=X0y)E (row (Z = row (X) & row,(Y)).
iy rfl) f
Cross-productoperationsbehaveinthesamewayasBooleanoperations. Their
compositionscanbeshowntobeequivalenttoBooleanoperationsbetweenmatrices
that are special extensions ofthe matrices that represent the composing relations.
© ©
Thetwoproductoperations canbeextended inanatural way fromoneinput relationstotwo-inputrelations. The Boolean operations between relationscan
alsobe used heie in the usual way.
Itcaneasilybeverifiedthatthereexists asimplecorrespondencebetween any
constructibleprogramof(u\u ,q)-typeandatwo-inputrelationalexpressionmadeof
© ©
therelation/, I, E, theproducts , , theBooleanoperations V > &» andthecross
products El , El . Consider inparticularthe aggregates involving (w 1 M 2 ,^)-type prouce
2)0, R 2 ,E], R2,©, R2,©, R2,v and R 2,a °ftne
To the aggregates shown in figure 18-17 there correspond the relational expressions (X EH Y),(X El Y), respectively, whereX,Fareanyone-inputrelations;
to the aggregates shown in figure 18-18 there correspond the relational expressions
{X dx © Y), (X dl © Y), whereX d] is any two-input relation and Y is any one-input
t> -U or n
Figure 18-17: Atwo-input "parallel" programaggregate.
^> U or n
Figure 18-18: Atwo-input "cascade" programaggregate.
--- PAGE 542 ---
528 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
relation; andtothe aggregates shown in figure 18-19 there correspondthe relational
expressions (X dx V Ydi), (^i & Y dx), whereX du Y dx are any two-input relations.
Consider as an example the relational expression that corresponds to the
Infimum program shown in figures 18-10 and 18-11. The expression is
[[(/ V E) (/ V E)] © (7 V E)] & [(/ V E) H (/ V £)]•
Note that this expression is the infix representation ofthe "abstract description" oftheInfimumprogramthatderivesfromthestructuraldescriptionoftheprogram in G\ (given in figure 18-11).
In general, for any two-input constructible program there is a corresponding
relational expression that can be obtained in a straightforward way from the structural description ofthe program obtained in the grammar G\ . The program and its
corresponding relational expression are functionally equivalent-in the sense that
their input-output behaviors are the same. This correspondence is the basis for the
modeling relationshipbetweenthe setofconstructibleprograms described here and
the relational expressions ofthe modified algebra ofrelations 2ftdi.
Thealgebraoftwo-i E nputprograms *3td i isanextensionofthealgebra 2ft , with
the two operations M, added. The terms of 2ftd\ are the two-input relational
expressionsdescribedpreviously. Thesetermsarepartiallyorderedundertheimplication relation, , as described above in connection © with 2ft .
The properties ofthe operations V » &, ©, carry over from 2ft to 2ftdx
with appropriate replacements ofone-input relation variables by two-input relation
variables. The associativities of El are the same as the associativities ofthe
Boolean operations V &, respecti , vely; the operations E2 E distribute with
, ,
Booleanoperationsinthe sameway as V or&, respectively; andthe rightdistribu-
© © M
tivitiesoftheproducts withrespecttothecrossproducts El arethe same
© , ,
as those between ©, and the Boolean operations V &» respectively.
18.3.1.4 The Program Space Induced by the Algebraic Model:
Distances in the Space
Inviewofthemodelingcorrespondencethatwasestablishedbetweenthesetof
constructible programs and the algebraic system 2ftdu the algebraic model induces
Figure 1S-1">: Another formofa two input "parallel" programaggregate
--- PAGE 543 ---
AMAREL 529
animplicational structureonthesetofprograms-thusprovidingamoreusefuldefinition ofprogram space than hadbeen availablebefore. Forany two-input programs
/ P ?, u - P 2 R w 2 i h t o h ld p s r , oj t e h c e t n io w ns e R c \ a , n R a 2 l , so re s s a p y ec t t h i a v t el P y, 1 - in t P h 2 e . m T o h d e e r l e n 2 o ft w rfI e , x i i f st t s he a i c m a p l l c i u c l a u t s io o n f
programs in which one can reason about program equivalence and inclusion. Furthermore these behavioral concepts are related to structural features ofprograms.
The numberoftwo-inputprograms inL\-andcorrespondingly the numberof
termsin rJl^, isnotfinite. However, thereisafinitenumberoffunctionsthatthese
programscancompute, sincethereisafinitenumberofrelationmatricesthatcorrespond to terms of 'SIm. Forourproblem class, this number is certainly smallerthan
2«2(n+i)/2 w herenisthenumberofelementsintheseto. Wecanvisualizeallthetwoinputconstructibleprogramssortedintoafinitenumberofequivalenceclasses, each
definedbyarelationmatrixthatexpressesthefunctional behaviorofanyprogram in
theclass. Toeachoftheseequivalenceclasses,thatis,toeachrelationmatrixthatcorresponds to some constructible program, there corresponds a point in our new program space. Let us call this space M\{n).
ThepointsofM\(n)arepartiallyorderedundertheinclusionrelation, They
are asubsetofthe full lattice ofrelationsthat is definedby the setofall the2n (n+^12
relation matrices. Let us call this Boolean lattice B {n).
The set ofassociationsgiven inthe statementofthe formation problemcanbe
seen as a specification ofthe "goal program" in terms ofits relation matrix. (Each
associationcorrespondstoarowoftherelationmatrixofthegoalprogram.)Thisisa
functional specification, and it can always be represented as a point in the Boolean
latticeB](n). Ifthedesiredprogram is representable inthe language ofconstructible
programs,thenitcanalsoberepresentedasapointintheprogramspaceM\{n). Ifthe
desiredprogram is not representable inthe program language, which meansthatthe
problem has no solution, then it cannot be represented as a point in {n). The
problem solverhasnowayofknowingthattheproblemhasnosolutionbyanalyzing
theproblemstatement. Hecanonlytrytocomeascloseaspossibletothepointinthe
Boolean lattice that represents the desired function-by constructing candidate program structures whose functional behavior is "close" to the desired function. Ifan
endlesssearch intheunsolvablecases istobeavoided, apredeterminedceilingmust
besetonavailableproblemsolvingeffortsothatthesystemcanstopwhentheceiling
is reached.
In orderthat a strategy may be developed for moving from points in program
spacethatrepresentknownprogramstructurestothedesiredpoint, thatis,tothegoal
program, itis importanttointroduceanotionofdistancebetweenprogramsthatcan
be used to guidedistance-reducing moves.
The notion ofdistance is defined here as follows: Forany program P, letm(P)
denote the number of l's in the relation matrix that represents P. Then, the distance
D(P|, P 2) between two programs P u P 2 in the lattice B\{n) is
D(PU P 2) = m(P l V Pi) ~ m(P l & P 2).
--- PAGE 544 ---
530 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
This distance is a measure of the symmetric set difference between the relation
matrices that correspond to the programs P u P 2.
Sincethealgebra ^kd\ hasa rich implicational structure (i.e., the structureof
inclusions between relation matrices) and this structure can provide considerable
guidance on how to move "vertically" in the lattice B (ri), it is useful to define the
distance D intermsofa"vertical" componentD
anda "horizontal" componentD
Thus we have
D(PU P
* D
(PU P
+ D
(Pu P
2),
where
(PU P
= m(P,) - m(P
2),
D h (Pu P 2) = 2(m(P 2) - m{P x & P 2)),
underthe assumption that m{P x) > m(P 2).
The notion ofverticality is induced from a commonly used geometric representationofthelatticeB\(ri). Inthisrepresentation, thenullprogram4 is shownatthe
bottom, the unitprogram5 is atthetop, and forany pairofprogramsP u P 2, if P x -
thenPi isportrayedbelowP 2inthelattice. Aswemoveupinthelattice, thevalue
of m increases. The distance relationships between two programs P u P 2 in a lattice
B\(ri)are schematically showninfigure 18-20. The relationshipbetweenthe relation
matricesthatcorrespondtotheprogramsP
offigure 18-20are shown schematically in figure 18-21.
Notethatthesmallerthehorizontaldistancebetweentwoprograms,themorea
situation is approached in which one program includes the other.
Thedistancesasdefinedheredonotprovidedetailedinformationaboutthedifferencesetsintheoutputsofthetwoprograms; theyprovideonlymeasuresofsizeof
thedifferencesets. Thisisadequateforthepresentapproach,butithascertaindisadvantages and limitations, which will be discussed later.
18.3.2 Goal-Oriented Method of Solution That Uses Properties of the
Model
In view of the structure of program space that is induced by the algebraic
model, theprocessofsolutionfindingcannowtakeamoregoal-orientedform. The
goal to be achieved can be seen as a point in the lattice B\(n) of programs that is
defined by its specifying set ofinput-output associations. Clearly this set represents
the relation matrix ofthe desired goal program P
4Thcnull program hasa relation matrix filledwithzeros.
sTheunit program hasa relation matrix filled withones.
--- PAGE 545 ---
AMAREL 531
m(P.)
m(p
2) v.
m(/>, Pi)
Theminimal path </>,, />, f\ P 2, P 2> inthelatticeB {D (n),
thatis, the "latticedistance," representsthe distance
betweentheprogramsPu P
Figure 18-20: DistancebetweenprogramsinlatticeofprogramsB\{n).
The solution process is again based on a generate-and-test scheme, but the
sequenceofcandidateprogramsthatarebeinggeneratedismorestronglyguided(relative to the previous approach, described in section 18.2.3) by the problem-data.
Morespecifically, theproblem-dataareseenasthespecificationofthegoalprogram
P inthelattice B (n) andthedistancefromacandidateprogramPandtheprogramP
g x ; g
canbeusedtoreasoninamoreinformedwayaboutthegenerationofsubsequentprogram candidates whose distance to P should be (one hopes) smaller.
Atanystageoftheproblemsolvingprocess, theproblemstateisdefinedasthe
set ofpairs
<e(P),(D
(P,P
),D
(P,P
g))>
for all candidate programs P that have been generated and tested so far. Here, q(P)
denotes the relational expression that represents the structure ofP in the algebraic
model.
The problem solver must use the information in the current problem state,
together with general knowledge about program space, in deciding what move to
--- PAGE 546 ---
532 CHAPTER 18: PROGRAM SYNTHESISAS ATHEORY FORMATION TASK
Range
n Elementsfrom0\
Domain P\ Pi
n(n + l)/2
Input
Pairsfrom a. D D„ V { ( P P\ U , P P 2 2 ) ) = = m 2 ( ( P m i ( ) P0 - ~ m(P m - ( > > ) , A P 2))
Theshadedareas representthe l's inthe relation
matrices. Thesizeoftheshadedareafora
program represents itsm.
Figure 18-21: RelationmatricescorrespondingtoprogramsP\, P
shown infigure 18-20.
take, that is, what nextprogramtogenerate, in orderto reachthe desired pointP^by
generating (and testing) as small a number ofcandidate programs as possible. The
problem can be regarded as navigating in problem space. Appropriate navigating
actions can be taken on the basis ofknowledge in the algebraic model. A dominant
direction in the program lattice is the vertical, determined by inclusion relations in
the algebra. Thusa reasonableplan ofnavigation istomove first intoa roughly vertical alignment with the desired goal point P by trying to reduce the horizontal distancetothegoalandthentoslide(toascendortodescend) into Pbyusingtherich set
of inclusion relations in the algebraic model. This plan has to be further refined,
especially at the terminal stage of the formation process, where more subtle and
"local" reasoning is needed in orderto reach the goal. At the terminal stage, something akin to a delicate maneuver is needed: a pair ofprogram structures must be
found fromamongthosethatevolve inaccordancewiththevertical alignment planeach at zero horizontal distance to but at a substantial horizontal distance relative
/J,
to each other-whose intersection "falls on" P r
The entire generation process is under the overall control ofthe grammar G\.
w ill) initial preference given tolow-weight program structures. Asthe process moves
--- PAGE 547 ---
AMAREL 533
to the vertical alignment stage and to the preparation ofcandidates forthe terminal
intersection maneuver the algebraic model provides much ofthe guidance for the
specific choices ofgeneration.
18.3.3 The Model-Guided Approach Applied to the Formation
Problem ^
Let us illustrate the model-guided approach to formation viaan outline ofthe
solution process for ourproblem -k\ (which is described in figure 18-1).
A candidate program P is specified here in terms ofits structure q(P) and its
distanceD(P,P g) fromthe goal programP g = Pinfimum. Thedistance is givenasapair
ofcomponents(D
h).
Thevaluev' ofaprogram(whichisamodifiednotionofthe
valuevofaprogram, definedhereasthecountofassociationsthataresatisfiedbythe
program) isalsogiven, forpurposesofcomparison. The relationmatricesofthekey
programs during the present process are shown in figure 18-22.
GoalProgram Candidateprograms
F, P'u Pl2 P 2J Pll Pl3 P3.1
((/ V £) Q (P2.2 ©
Inputs rp Infimum EME 70/ /0 / Px V />,, (/ V E)) (/ V £)) P2.2 A P23
© © © ©
ee e abed eabed
© © ©
ea e ea abed ea eabed
© ©
eb e eb ea abed eab eabed
© ©
ec e ec ea abed eac eabed
© ©
ed e ed eabc abed eabed eabed
© ©
aa a e bed ea ea abed
ab a ab ea bed eab ea abed
ac a ac ea bed eac ea abed
ad a ad eabc bed eabed ea abed
® ©
bb b ea d eab eab bd
be a be ea d eabc ea abed
bd b bd eabc d eabed eab bd
© ©
cc c ea d eac eac cd
cd c cd eabc d eabed eac cd
® - ©
dd d eabc eabed eabed d
m 15 25 36 37 52 32 54 15
D 12 31 52 37 17 39
10 21 22 37 17 39
D h 2 10 30
v' 5 1 1 5 15
Note: Theentriesincirclesshowpartsofcandidateprogramsthatmatchthegoal program.
Figure 18-22: Relationmatricesofmainprogramsgeneratedinmodel-guidedapproach.
--- PAGE 548 ---
534 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
The following are the main steps in the process offormation:
1. A set ofsimpleprograms (with w = 3) is generated andtested. This is an initialexploration stage.
= (EM =
Pi E), (10,2); v' 5
= =
/>,,, (/ /), (21,10); v' 1
P = =
l2 (7 7), (22,30); v'
The programs P\,P\\ have the smallest horizontal distance from P and they
are chosen forthe next step.
2. Combinationsofpromisingprograms fromtheprevious steparetriedinorder
to bring the horizontal distance to zero. This is a stage whose objective is to
reachverticalalignmentwiththegoalprogram. By usingP\ , P xA, azerohorizontal distance is obtained.
P 2A = ((£ ME) y (IM /)), (37,0); v' = 1, w = 7
Fromthealgebra, thestructureofP 2,i isequivalentto((/ V E) El (/ V E)).
3. Algebraic properties are used to find programs with zero horizontal distance
thathaveasmallerverticaldistancefromP bymanipulationofpreviouspromising programs. This represents the stage of "sliding vertically." Here, a
change ofthe cross product El in P 2,i> to \N , takes us in the right direction.
^2,2 = ((/ V E) (/ V £)), (17,0); v' = 5, w = 1
4. Inpreparationfor the terminal intersection maneuver, programs are formed
that have zero horizontal distance to P and large horizontal distances among
themselves.
P23 = [((/ V E) El (/ V E))] © (7 V £)], (39,0);
= w =
v' 0, 11
Note that while the horizontal distance ofP 2,3 to P g is zero, its horizontal distanceto P 2,2 is large: D h (P 2i, Pij) = 34
5. A terminal intersection maneuver is tried among previously developed programs. ThedesiredgoalprogramisreachedbytheintersectionofP
2.2
andP
2.3.
/\, = ([(/ V E) (/ V E)] & [((/ V E) El (/ V £))
(7 V £)]), (0,0); v' = 15, w = 19
Thus /\ 1 = /J = PinfimumAgraphic representationofthemainstagesinthemodel-guided formationprocess is
shown in figure 18-23.
The mostdifficultpart inthepresentprocess isthechoice ofcandidates forthe
terminal intersection maneuver. Agoodheuristic, whichwasfoundempirically, isas
follows: Suppose that at step 3 a good program P a isobtained and we wish to use it as
--- PAGE 549 ---
AMAREL 535
Figure18-23: Graphicrepresentationofstagesinmodel-guidedformationprocess-showninlatticeof
programs.
oneofthepairofprogramstointersectintheterminalmaneuver; constructacascade
structureP a P a] for someP al and use itas the secondprogram inthe intersection.
Such a structure has a good chance ofsuccess. Even with the guidance ofthis heuristic, considerable search may be needed to find "appropriate" structures P a, P a\.
Thenumberofmajordecisionsmadeduringthisformationprocessisapproximatelytwenty. Thesituationisclearlymore selectiveandmoregoal orientedthan in
thepreviousheuristichill-climbingapproach. However, thepresentdistance-guided
approachdoes notprovidethedetailedtypeof"local" informationthat is needed in
theterminal stageofreasoningforachievingthegoal. Furthermore, withoutabetter
--- PAGE 550 ---
536 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
understanding ofthe grounds for the heuristics that guide the terminal intersection
maneuver, itisverydifficulttorestrictthesearchinvolvedandtoseehowtheprocess
can be improved.
It is interesting to examine the relation matrices that correspond to the main
programs formed in the present path to solution (see figure 18-22). There is much
detailed structure in the pattern ofdeviations between a candidate program and the
goalprogramthatisnotcapturedbythecoarseconceptofdistanceinprogramspace.
It makes sense to consider using this structure more directly in orderto understand
thedomain-specificnatureoftheterminalphaseinthepresentformationprocessand
to develop improved approaches to formation. These considerations led to a third
approach to the formation problem, which relies on more detailed reasoning with
individual items ofproblem-data.
Beforeadescriptionofcurrentworkwiththethirdapproachispresented, some
general comments about the model-guided approach will be made.
18.3.4 Comments on the Model-Guided Approach: Relationship to the
Previous Approach and Open Problems
The model-guided method ofconstructing candidate solutions differs appreciably fromtheheuristic hill-climbing method. Inthe hill-climbing method, a solution candidate (a program) is built from top to bottom-first the global structure is
specified, andgraduallychoicesofsubstructuresandofterminalelementsaremade.
Onlyinthefirst,exploratory, stageofthemodel-guidedapproachissuchamethodof
solutionconstruction used. In the subsequent stages, candidate programs are graduallybuiltfromthebottomup- firstsomeoftheterminal substructuresarebuilt, and
thentheyarecombinedintobiggerstructures. Thegrammaroftheprogramlanguage
determines the program units and their possible mode ofaggregation, but it is the
overall plan offormation-which is based on the algebraic model-that determines
the specific manner in which candidate programs are put together.
Itshouldbeemphasizedthatthecentralrepresentationalchoiceinthisproblem
hasbeenthechoiceofprogramgrammar. Thischoicewasguidedbythe introduction
ofthe algebraic model. However, the model made a more direct contribution to the
formulationofamethodofsolution. Propertiesofthemodel ledtoanewoverallplan
forsolutionandtothe introductionofnew problem solving moves foreffectingtransitions in program space.
The key steps that are required for a transition from the pre\ lous formulation
(described in section 18.2.3) to the present model-guided formulation are the
following:
1. Find a model ofprogram space.
2. Find "useful" properties ofthe model.
--- PAGE 551 ---
AMAREL 537
3. Use the model properties to formulate a method of solution (overall plan,
detailed moves, control heuristics).
These steps were made "by hand" in the present work. One ofthe important
objectives offuture work inthis area istoexaminethese steps in considerable detail
inordertoclarifytheproblemsthatonemustface inmechanizingthem. The mechanizationofstep 3 isapproachableatpresent. TheworkofMostow (1981) on "operationalizing" problemsolvingknowledgeisrelevanthere. Similarproblemsarefound
in otherproblem reformulation studies (Amarel, 1981, 1982). The mechanization of
step 2 is more difficult, but Lenat's work on AM and EURISCO (Davis and Lenat,
1982; Lenat, 1983a) hasresultedinseveral ideasandmethodsthatarerelevanttothis
task.
Mechanizationofthe model-findingtask (step 1) continuestoappearverydifficult. The problem offinding, adapting, and extending existing mathematical systems, to be used as models ofa domain, was discussed in previous work (Amarel,
1971). Nomajorprogresshasbeenmadeinthisareaoverthelastdecade. Ifadvances
are to be made in the area of model-finding processes, a better understanding of
model-utilizing processes must be reached. This is an area of AI in which more
research is needed. It is also an area in which good progress is possible in the near
future.
18.4 A THIRD APPROACH TO PROCEDURAL FORMULATIONS OF
THE FORMATION PROBLEM: REASONING IN DATA SPACE
In the third approach to the program formation problem, the solution-finding
process is stronglyguidedbythedetailedanalysis ofindividual dataassociations. In
contrasttotheprevioustwoapproaches, inwhichthemainthrustofreasoningisfrom
candidate programs to the entiresetofdata associations, the present approach goes
fromindividualassociationstocandidateprograms. Althoughthesearchforsolution
istightlyconstrainedbytheprogramgrammarandbythealgebraicproperties ofthe
modelofprogramspace, anincreasedamountofreasoningtakesplaceindataspace.
Given a data association, a derivation problem is solved to obtain a "computational
path" thatlinksthetwoelementsoftheassociation. Thederivationisconstrainedby
the program grammar. Only computational paths that can be generated by a constructive program are considered. The goal program is obtained by reasoning with
thesetofprogramsthataregeneratedascandidates forproducingthecomputational
paths forall the data associations.
Two alternative methods have been explored within the present data-driven
approach, called the combination method and the elimination method. These
methodsdiffermainly inthe way in which they obtain theoverall solution from partial solutions. A partial solution isaprogram thatsatisfies oneormoredataassociationsbutnotalltheassociationsinthedomain. Forexample, intheproblem Xj apartialsolutionmaybeaprogramthatsatisfiesthedataassociation(ab, a)intheproblem's
--- PAGE 552 ---
538 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
specifyingset(i.e., itreturns {a} fortheinputpair(ab))butdoesnot "satisfy" some
otherdata association (be, a), as in this case the program returns {ae} forthe input
pair(be) insteadofthedesired {a}. (Seefigure 18-1 wherethe specifying setofdata
association forthe problem 7Tj is shown.)
18.4.1 Knowledge Available in the Data-Driven Approach
Beforethedescriptionofthe solutionmethods isgiven, itis importantto summarize the bodies of knowledge that are available in the new formulation and the
form in whichthisknowledge isavailable. Theprogram space isdefined interms of
the program grammar G
and the algebraic model <ftrfI. For our present purposes,
the structureofinclusion relationsinthealgebrais notimportant. However, equivalence properties between relational expressions are very important, and so is the
matrix interpretationofalgebraicproperties, whichprovidesthebasis forreasoning
about "partial equivalences," that is, equivalences that hold only for parts of the
domain ofinput pairs d\.
As indicated previously in section 18.2.1.1, the domain specification D ofthe
problemclassincludesadefinitionofdataspace. Foragivenfinite,partiallyordered
structure,thisdefinitionisintermsofallthedataobjectsinZ), andthesetofallavailable computational actions. A computational path in data space from a set ofinput
data objects to an output data object is an aggregate ofcomputational actions that
effectsthetransitionfrominputtooutput. Inthepresentdata-drivenapproachtoformation, effective ways are needed of finding/constructing a computational path
betweenagivenpairofinputdataandoutputdata. Thisrequiresthesolutionofaderivationproblem, forwhichtherearewell-knownmethods inAI. To solve derivation
problems effectively, it is important to have a good representation ofthe available
actions; this includes notonlyadescriptionofeffects ofactionsbutalsoappropriate
applicabilityconditionsforactions. Inourproblem,theseconditionscanbeobtained
from knowledgeabouttheavailablecomputational actions, thatis, knowledgeabout
the relations /, 7, E, about the set operations V » &» ana" about the list-processing
operations s, c. In the previous two procedural formulations of the formation
problem,theavailabilityofthisknowledgewasnotnecessary. Inthepresentformulation it is.
Thefollowingisapartiallistofthepropertiesofcomputationalactionsthatare
needed by the formation process in its present approach.
18.4.1.1 Properties of Computational Actions
l. /: x $ Ix, for allx in a.
ifIx = {y, a] , thenx E Iy;x, yare in o.
(Forconvenience, the notation Ix is used here instead ofthe proper functional
notation t/.y which was introduced in section 18.2.1.1.. where the value oi >/.v
--- PAGE 553 ---
AMAREL 539
(hereIx) is the setofall elements in athat stand in relation/tox; the notation
{y,
a} denotesasetwhereoneoftheelementsisyandtherestoftheelementsis
representedby a).
2. I: x $ Ix,forallxin a.
ifIx = {y, a} , thenx £ Iy;x,yare in o.
3. E:x E Ex, forallx in o.
The set Ex = {x} is a singleton.
4. fl : if C\x x x 2 . . . x n = y, wherexux 2, . . . , x n are in2aandyis a singleton set
{w}, then the input sets xux 2, . . . , x n must have the form jci = {u, c^},
x 2 = {u, a 2}, . . . ,x n = {u, a n}, where ct\t a 2, . . . , ctn representdisjointsets
(andoneormoreofthemmaybeempty). Inthespecialcase Ox, thenxmustbe
{»}
5. U : if \Jx\,x2 . . . x n, where jci, x 2, . . . jcw arein2°andyis asingleton set {«}
thentheinputsetsx x ,x2, . . . , x„musthavetheform {u} , ortheymustbeempty;
but at least one ofthe input sets mustbe nonempty.
Itisassumedthatthis knowledge aboutpropertiesofcomputational actions is availabletotheformationprocessandfurthermorethattheknowledgeisinaformthatcan
beconveniently applieddepending onthe direction ofreasoning-forward from the
inputs orbackwards from the output.
18.4.2 The Data-Driven Combination Method
Thefollowingisageneraloutlineofthemethod. Foragivendataassociation,
theprocessrequestsfromthegrammarG candidateprogramschemes, startingfrom
the lowest weight schemes and gradually increasing the weight. Given a program
scheme,aneffortis madetoinstantiateitinawaythatwouldsolvethecomputational
path problem forthe given data association. When aprogram is found that satisfies
thedataassociation, itistestedovertheremainingassociations. Ifalltheassociations
are satisfied, thentheprocessterminateswithsuccess. Ifnot, thenattention focuses
onanassociationthatisnotsatisfiedbythisprogram, andaprocessoffindingaprogramthat solves the computational path problem forthis association is carried out.
Given a set ofprograms, each partially satisfying the given set ofdata associations,
an effort is madeto combine them so as to achieve a solution forthe entire set.
18.4.2.1 Application of the Data-Driven Combination Method to the
Formation Problem
A good way ofdescribing the formation method and its properties is to show
how it applies to a specific problem. The following is an outline ofthe process of
applying the methodtothe formation problem tt\.
--- PAGE 554 ---
540 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
1. Considerthedata association (ee, e).
Assume Scheme S\ = (p El p) with w = 3.
Theprogramvariable/?cantakevalues/, /, orE. Optionscanbedeterminedby
(1)trialassignmentandevaluation(i.e., runningacandidateprogramwiththe
left side ofthe data association as input and checking whether the computed
output matches the right side ofthe data association) or (2) reasoning backwards from the right side of the data association and propagating data constraintsinthedirectionoftheinput. Inthepresentcase, trialassignmentsyield
a program6
P =(E El £); w = 3
lj3
that satisfies the given data association.
In a LISP paraphrase, P\$ corresponds to
(lambda (u x u 2) (intersection (E-function u x) (E-function u 2))),
where E-function is the functional form ofE (see section 18.2.1.1 above).
The program Pi 3 is tried overthe entire data domain d\. It satisfies a total of
five from among the fifteen associations; these are (ee, e), (aa, a), (bb, b),
(cc, c), (dd, d). Furthermore, the program behaviorhas an ''interesting"pattern: it has null values outside the subdomain of d\ in which its behavior
matches the desired program behavior. As will be seen later, this behavior is
considered interesting because it facilitates the combination ofpartially successful programs-which is a highly valued property in the present method.
Now attentionisfocusedononeofthedataassociationsthatarenotsatisfiedby
Pl.3Consider thedata association (ab, a).
A. Assume scheme Si = (p K p) with w = 3.
Bytheuseoftrialassignmentsandevaluationsitcanbedeterminedthatthereis
nosolutionoftypeSu that is, there is noinstantiationofthis schemethat satisfies the given data association.
Retain this failure ofS\.
B. Assume scheme S 2 = (p M p) with w = 3.
By similar method, S 2fails. Retain this fact.
C. Move now to schemes with w = 5. Assume scheme Si = ((p\ \N p }) ©
Pi).
In the graph language ofprograms, the structure is as shown in figure 18-24.
'Toi purposesolcomparisonacrossmethods,asinglenameforaprogramstructureisusedthroughoutthis
chapter, Thisexplainstheratheroddchoiceofsubscripts.TherelationmatricesoftheprogramP aswell
asol other Ice) programsgenerated via this methodare shown In Figure 18-27.
--- PAGE 555 ---
) )
AMAREL 541
"P i
"P <?2 q\ g q = \a\
Figure 18-24: Graphrepresentationofscheme5
Data constraints are propagated from the output backwards. Since the output
setqshouldbe {a} , theninviewofthepropertiesof C\, theelementsofthebag
g shouldbeoftheform {a, a {}, {a, a 2}, . . . , {a, a n}, wheren > 1, andthe
sets«!, a 2, . . . , ct„ shouldbedisjoint. Inthe special case inwhichghas only
oneelement, thenitshouldbe {a}
Thusthevaluesofq
shouldbeintheform
{a, a:}, where a may be empty.
Considerthepossible value assignments top
= E?
(Outputofp
canbe {a}
Then, inputtop
mustbea. Inputtosmustbeoneor
morea. Butsinceq2isasetit mustbeasingleton, {a}
ButweknowfrompreviousworkwithS\ thatthereisnosolutionforthefrontblock(p x \N p x), when
the input-output ofthe block is (ab, a). Thus/? 2 = E fails. This fact and the
reason for failure-that is, failure of S\ for the given data association-are
retained.
Pi = I?
(Inputstop 2 mustbe b orcord(fromgeneral properties of/andthe specific
definition of/in the present problem (see figure 18-1)). But in each ofthese
cases, outputofp 2 includesaand e. Thisviolatesthedataconstraintonthebag
g. Thus/? 2 = /fails. This factandthe reasonforfailure-thatis, propertiesof
/-are retained.
Pi = I?
(By similarreasoning, thisassignment/a//5also. Thus, scheme5 3fails. Retain
this factand its reasons. Notethatthisconclusion is obtainedwithoutlooking
into assignments for x.)
D. TheschemeS\ = {{p\ El p\) © p 2) isalsofoundtofail. Heremuchofthe
reasoning for5 canbe "borrowed."
E. Assume now scheme S 4 = {{p x \N p x) © p 2).
ByreasoningsimilartothatusedintheS 3 case, schemeS 4fails. Itisinteresting
tonotehowanalgebraicpropertycanbeusedinthepresentanalysis: Checking
a fo l r ge t b h r e a a t s h s a i t gn X me © ntp E 2 - = X E, (s w e e e h s a e v ct e io S n 4 1 = 8.3 S .1 x .2, 2 E ) ; ; b t u h t us it S i 4 s i k s n r o e w du n ce f d ro t m o t S h i, e
which has already failed.
--- PAGE 556 ---
542 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
F. Move now to schemes with w = 1.
By analysis, thefourfollowing schemesfail:
E ©
5 5 = ((/?, Pl) (p2 V pi))
5 6 = ((Pi E E />,) Q © (P2&P3))
5 s 7 s = = ( ( ( (P P ] l E P Px x) ) © ( (P p 2 2 & v p p ii i ) ) \ )
E M
and the schemes obtained by changing (p x p x) into (p x p x) in these
schemes alsofail. It is interesting to discuss one part ofthis analysis which
illustrates the use ofthe algebra and the possibility ofobtaining "serendipitous"solutions. ConsidertheschemeS
Bytheleftdistributivitypropertiesof
the algebra (see section 18.3.1.2, 4b) the scheme S 6 is equivalent to
E © & ^ ©
((Pi Pi) Pi) ((Pi Pi) Ps)-
Propagationoftheoutputconstraintrequiresthatthetwopartialoutputsofthis
decompositionbeintheform {a, a x}, {a, a 2}, wherea.\,ct2 representdisjoint
sets. Pursuing an analysis ofthe subschemes in a manner similar to S 3 shows
thattherearenopossibleassignments forp 2, p^,Pi thatsatisfythe outputconstrai M nts. However, if one of the front blocks (p x p x) is changed to
(p[ p[), then a solution exists. The solution is
p 4 = ((/ H /) © 7) & ((/ El 7) © /); w = 11.
This program cannot be "simplified" into a form of type S 6 (with w = 7),
because changing the connective inthe front block has spoiled the possibility
ofusingtheleftdistributivityproperty ofthealgebrainreverse. Evaluationof
overtheentiredatadomainshowsthatitsatisfiesfourofthefifteenassociations (see figure 18-27). This happens nottobe an especially strong program.
However, the program is retained.
ItshouldbenotedthatinthepresentcaseP
wasobtainedbydeviatingslightly
froma specific goal-orientedtaskandby exploring opportunistically a "local
neighborhood" when the goal-directed analysis returned failure. This may be
an interesting source of serendipitous findings. More work in this area is
needed.
G. Exploration continues now with other schemes that have vv = 7.
e W v o er r , k w o o n rk th o e n s th c e he s m c e he S m 9 e = 5 10 (( = /?, (( E p l \ p E x l ) p & x) ( & p2 ( E p ) 2 p M2)) p r 2 e )) tu y r i n e s ld f s ai a lu s r o e l . uti H o o n w i - n
the form
P 2 = ((E 0£)&(/0 /)); w = 1.
Evaluation oiP 2 overthe entire data domain shows that it satisfies nine ofthe
titteenassociations(seefigure 18-27). Furthermore, theprogrambehaviorhas
the same interesting pattern that wasobserved previously in P\y. that is. it has
null values outside the subdomain ofd\ in which its behavior matches the
desired program behavior. This is considered a strong program.
--- PAGE 557 ---
AMAREL 543
Byalgebraicmanipulation, theprogramP
canbeshowntobeequivalenttothe
following program, P 2 with w = 9.
P' 2 = [(E M E) & ((£ M E) © /)].
This derives from the following:
(/ /)- ((£" © /) El (£ © /)) - ((£ £) © /).
Establishing such an equivalence is motivatedby (1) the possibility ofsimplifyingtheprogramand(2)theusefulnessoftheformationprocess'shavingseveral structures that are functionally equivalent. Program simplification is
helpedby identifyingcommonsubblocksthathandlethesamedata. Functionallyequivalentstructuresprovidedifferent "startingpoints" inprogram space
aroundwhich it maybefruitfultoexploreforpromisingsolutioncandidates. It
shouldbenotedthatthegenerationofcandidatestakesplaceinthespaceofprogram structures.
Theproblemsolveris nowatapointatwhichhemusthandletheissueofhowto
combine partially successful programs. This is a key problem in the present
process, and itwillbegivenspecialattentioninthe nextsection-attheriskof
introducing some discontinuity in the description ofthe problem solving process for TT\.
3. A keypartoftheprocess: How to combineprograms.
Examination ofthe programs obtained so far shows that the two interesting
programsP! and/^jointlycoveraverylargepartofthedatadomaind\. Actually, theirunion covers fourteen out ofthe fifteen associations. The only data
association notcovered is (be, a). In this situation, it is natural topropose the
following program as a new candidate:
^5 = P\,3 V Pi = [(E El E) V ((£ E) & (/ /))]; w = 11.
Byalgebraicmanipulation,theprogramP
canbeshowntobeequivalenttothe
following program, whose weight is also 11:
P 3 = [(E M E) & ((E £")©(/ v £))]; w = 11.
Theuseofaunionoperatorforcombiningtwoprogramsinthewayjustdoneis
a very powerful and simple method of solving the program combination
problem. Itwaspossibleinthiscasebecauseofthe "interestingness" property
ofthetwocomponentprograms;thatis, foreveryinput, suchaprogramreturns
the correct output orthe null set.
However, it is not always possible to find a way ofcombining partly correct
programsinthissimpleway. Actually,themostdifficultandcentralproblemin
this area is how to combine/merge programs that provide partial solutions to
the formationproblemby satisfying subdomainsofthegiven function. This is
aninstanceofthedifficultAIproblemofhowtohandleconjunctivegoals. The
formationproblemcaneasilybeseenasaproblemoffindingaplantosatisfya
--- PAGE 558 ---
544 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
setofconjunctive goals. Thisproblem is especially difficult ifthere is no way
ofdesigningaplanthatisdecomposable-thatis, thatcanbedividedintoparts
eachofwhichcanbeseenasdevotedtosatisfyingoneofthegoals. Thisiscertainly the situation in the program formation program, and it is common in
many other theory formation or design tasks. The nature ofthis problem and
theimportanceofworkingwith "nearlydecomposablesystems," arediscussed
in depth by Simon in his bookSciences oftheArtificial (Simon, 1969).
There has been a certain amount ofprogress in AI on methods for problem
solvingandplangenerationinconjunctivegoal situations. Theearliestworkin
this area was by Ernst and Newell (1969) on GPS, which was pursued further
by Ernst and Goldstein (1982) and by Korf (1982) in connection with
approaches for handling interacting goals. The work described by Sacerdoti
(1977), Stefik (1981), Waldinger (1977), and Warren (1974, 1976) has further
advanced ourunderstanding ofthe problem.
Intheareaofprogramsynthesis, theproblemofconjunctivegoalshasreceived
some attention from MannaandWaldinger (1975, 1977). This was recognized
by them as an especially difficult problem, one in which methods ofsolution
seem to be very domain specific. However, they suggest two very general
approaches in their work that are relevant to the present task. The first
approachinvolvesthenotionofprogrammodification. Thisapproachwasalso
proposedby Sussman(1975)andMostow (1981). Themainideaisthatinorder
toachieve goals C
andC
simultaneously, one should firstwriteaprogramto
achieve C and then modify that program to achieve C as well. Ofcourse, in
x 2
the course ofmodification one must keep in mind that the achievement ofC\
must be protected while an effort is being made to achieve the goal C 2.
The second approach involves some variant of the notion ofgeneralization.
Themain ideahereistomakethesolutionofthe firstgoalas "general" aspossible, sothat some "special case" ofthe solutionmight satisfy the secondgoal
as well. The key step in this approach is to develop an appropriate notion ofa
"generalization hierarchy." Mitchell's approach to learning, in which the
notion of "version space" is used (Mitchell, 1977, 1978) is closely related to
this general approach.
In the present data-driven combination method of program formation, a
strategy ofprogram modification has been explored. A key decision here is
choosingaspecificprogramonwhichmodificationsaretobemade. Goodcandidates for modification are any ofthe interesting programs obtained so far.
that is, eithertheelementary interestingprogramssuchasP { } orP 2 orthecompoundinterestingprogramP thatwasobtainedbycombiningother interesting
programs via a union operator.
The generalization approach to the handling ofconjunctive goals will be discussed further below in connection with the second data-driven method offormation, which is based on a process ofelimination.
--- PAGE 559 ---
AMAREL 545
Let us return now to the solution process, which we left after obtaining the
strong interesting program
P 3 = [(E M E) & ((£ M E) © (/ v £))]; w = 11,
whichsatisfiesallthedataassociationsbutone, namely, the{be, a)association
(see figure 18-27). A LISP representation ofthe program P 3 is as follows:
P = (lambda(u u (Intersection)
3 1 2)
[Union (E-function uj (E-function u 2)]
[Intersection-* (Mapcar
'(lambda (x)
(Union (I-function x
E-function x)))
(Union (E-function uj
(E-function u 2)))])) .
18.4.2.2 Continuation of the Problem-Solving Process for in: Program
Modification Stage
Itis possible thata statistical approach to formation, in which much weight is
ordinarilygiventothenumberofsatisfiedassociations, may decideto stophere and
to neglect the "troublemaking" association (be, a). However, such process would
completely miss an important part ofthe concept of Infimum that the program/
theory must capture. In particular, in the partially ordered structure £ l9 this is the
only association that illustrates the case of Infimum for noncomparable elements
(i.e., elements not on the same chain). The number ofsituations that exemplify an
importantcasewithinthedomainofatheorycaneasilybechanged. Forexample, the
structure L 2 in figure 18-25 has the same number ofelements as L u but it presents
more associations than in £, that illustrate the case ofInfimum for noncomparable
elements, namely, (be, a) (eb, a), (ec, a). The main point ofthis comment is that an
effectivetheory formationprocessshouldbeabletoapproacheach itemofdatain its
domain and try to "understand" it. A lone item, as exemplified by the (be, a) data
associationinthisproblem, mayberepresentativeofanimportantfacetofaphenomenon, and it should receive attention unless there is good reason for it not toincluding, forexample, the lack ofresources to pursue the investigation.
It happens that considerable "modification effort" is needed in order to go
fromthe strongprogramP toagoal programthat satisfiesthe association (be, a) as
well astheotherfourteen associations. Let us now outline the reasoning involvedas
we pursue the solution ofir\
--- PAGE 560 ---
546 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
d d
Figure18-25: Twopartiallyorderedstructureswiththesamenumberofelementsbutwithdifferentproportionsofnoncomparableelements.
The structure ofthe program P 3 is shown in figure 18-26a in the graph language. Iftheinputpair(b, c)isapplied, theoutputistheemptyset, andthedistribution ofdataat various parts ofthe program is as shown inthe figure. We
wouldliketheoutputtobe {a}
The inputstothelastintersectionoperationin
theprogram shouldbe {a, aj, {a, a 2}, fora 1? a 2, representing disjoint sets.
One ofthebranchesofP (thelowerone inthe figure) produces an output {a,
e} thatmatchesthedesired form. Letustry tochange the otherbranch sothat
itsoutputwillbe (a, a 2), suchthata 2 and {e} haveanull intersection. We now
have the following subproblem: Find a computational path from the set {b,
c}-that is, from the present output ofthe "delinquent" program part-to the
desired output {a, a 2}. An attempt can be made to obtain a solution to this
problemviaasystematic searchofthetypepresentedearlier. However, amore
goal-directed approach based on reasoning in data space is alsopossible. The
reasoningcanbeseenbestinthecontextofthegeometricrepresentationofthe
partially ordered structure L,, which is shown in figure 18-26b.
The reasoning isas follows: Toobtain fromtheelementsb,casetthat includes
theelementa, oneshouldapplythe/operationonbothbandcandcombinethe
results by union or intersection; this will produce a set {a, e}. The e must be
"filtered out" ofthat set; this can be obtained by applying the / operation on
theset {a, e] andbyproducingasoutputtheunionoftheresults. Alternatively,
the (/ V E) operationmaybeappliedonthe set {a, e] , andtheoutput may be
obtained fromthe intersectionoftheresults. Thisargument resultsinfourpossible programs forpatchingthe delinquent part of/Y shown in figure 18-26c.
Unfortunately, although a patched program satisfies the association (be, u). it
losesground inseveraloftheassociationsthatwerepreviously satisfiedbyP
--- PAGE 561 ---
AMAREL 547
E ' i- E { \ b ca a e e \ } {ae)
a. Initial strongprogram/Y which istobemodified.
b. Thepartiallyorderedstructure L\
^ DesiredOutput
AssumedInput
{bc}\- J{a,a :}- wherea 2 doesnot
includee
{bc}\-+ b-£ U or n
s 1 s ' c {a£c</}
W| i -- |o- 7 i | * ,. U » or n
{abed}
c. Developmentofpossible "patches" formodifyingP 3.
Figure 18-26: Approachtoprogrammodification.
Actually, thebestpatchedprogram, call it P 5, satisfies only five ofthe fifteen
associations inthedomain (see figure 18-27). The structureofthisprogram is
P 5 = [(((£ E) © I) © 7) & ((£ E) © (/ V £))]; w = 15.
Now, additionalmodificationofP 5isneeded. Attentionremainsfocused
onthepartthatwasrecentlypatched. Afteracertainamountofsearch, thepart
£)©/)©
((£ 7
--- PAGE 562 ---
,' •
548 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
© ©
Q, ,,*,
- >
C ^S§ ©©©©©®®®®®®©©©@
©<>
of k©T^ £>
<C £?
© >
<C©@ |®®@®"§ W^ -ft U >J ~5 v S i
I I I I
k? k?
a a
kaj^ kw^
< >
/-V, 000000000® ®00©
<Ss
k-i ka
kwj^
< ^
*£/-vav». ©©©© ®®®
I® i©
B sw I i i I
k-j
f^ ^
,sl i®®®®.£
I i I i - v O j - V G J - V O J - V O J I I q..a
H.2 E fe
* i_ • _ , i > ° 1
ki CJ C
a i© i® I© I©
cc i i i i i I
E g C
Z E 3 v\>vUv>v\>vX>v3v3v3v3«0 v3 £> vw Vj ^3
-O -O vj o ~v3
§ 3
5X 2
--- PAGE 563 ---
AMAREL 549
is modified into
© ©
[((£ E) (/ V £)) (/ V £)],
and this results in the following program, which satisfies all the data
associations:
P 6 = [((£ M £) © (/ v E)) © (7 V £)) & ((£ M £)
© V w =
(/ £))]; 19.
ItcaneasilybeseenthattheprogramP 6isequivalenttotheprogramP
3 ,,
which
was the solution obtained in previous approaches.
A LISP representation ofthe program P 6 is as follows:
P = (lambda (u u
6 1 2)
(Intersection
[Intersection-* (Mapcar
'(lambda (x) (Union
(leap-function x)
(E-function x)))
(Intersection-* (Mapcar
'(lambda (v) (Union
(I-function y) (E-function y))
(Union (E-function u
(E-function u 2)))))]
[Intersection-* (Mapcar
'(lambda (z) (Union
(I-function z) (E-function z)))
(Union (E-function u±) (E-function u )))])).
Note that while P 6 is functionally equivalent to the program Pinfimum
which was shown in section 18.2.1.1, their structures (and LISP representations)aredifferent. The functionalequivalenceofthe twoprogramscaneasily
be established within the algebraic model.
18.4.2.3 Comments on the Data-Driven Combination Method:
Discovery of New Concepts Defined over Subdomains
In general, although the data-driven program combination/modification
approach is more controlled and better guided than the previous approaches, it still
involves a considerable amount ofsearch. The problem ofcombining partial solutionsandofmodifyingprograms ineffective ways remainsaformidableone. This is
an area where more research is needed.
The notion of "interesting" programs that was introduced here induces the
idea that these programs may be used to define new concepts in the problem under
investigation. Theseareconceptsofinterestingsubdomainsofthedomaind\ ofinput
--- PAGE 564 ---
550 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
data that was originally given. The concepts are identified by subdomains ofd\ for
which interesting special solutions tothe Infimum problem were found. These subdomains can be defined as follows:
d ]A: Pairs from d\ for which application of the program ^3 = (E El E)
returns a nonnull value (In ourcase, the pairs are ee, aa, bb, cc, dd).
d\2- Pairs from d\ for which application of the program P = ((E El
£")&(/ /))returnsanonnull value(Inourcase, thepairsareea, eb, ec, ed,
ab, ac, ad, bd, cd).
d va x l y. ue U o n f io th n e o p f r d o \ g A r a a n m d P d 3 \ = 2- E [ l ( e £ ments £) of & th ( is (E setar E e ) ch © ara ( c / te V riz £ e ) d )] b . y a nonnull
Thus the present approach to theory formation can provide the basis for conceptdiscoveryprocesses. Intheseprocesses, adomainofaphenomenon is redefined
so that it can be captured by a program with certain special characteristics, such as
simplicity inagivenlanguage. Thisisapromisingareaofinvestigation, anditshould
be explored further.
Thereisaninterestingconnectionbetweenthisnotionofconceptdiscoveryand
thenotionofperformanceimprovementinexpertproblemsolvingsystems. Improvementinproblemsolvingperformanceisoftenobtainedby specializingamethodtoa
subdomainofaclassofproblems. Thisisanimportantapproachtoexpertiseacquisition; it involvestheformationoftheconceptofan interesting subdomainofspecialization andofan efficient solution method for handling problems in the subdomain
(Amarel, 1982). The issues ofdefining a subdomain interms ofan interesting solution methodare similartothoseencountered inthe presentprogram formation task.
In view ofthe subdomain conceptsjust discussed, it is possible to express the
program forP/,0mum in the general form ofa conditional statement,7 as follows:
infimum = It NOT NULL P 1>3 = (E E) , then P M1>3
If NOT NULL P 2 = ((£ E) & (I I)) , then P 2
Else P 6 = ( [ ( ( E ((£ E) E) © © (I (I V V £)) £ . ) ] )©(! V E ) ) &
Ofcourse, asuitableaugmentation intheprogram language is needed in order
to obtain statements in this form.
18.4.3 The Data-Driven Elimination Method
18.4.3.1 Geometric Representation of Data Space
Beforeproceedingwithadiscussionofthedata-drivenelimination method, let
usbriefly introduce ageometric representationofdataspace that isextremely useful
Itie \\ntax used here is informal, but thestatement is meant tobe road .is.1 LISPconditional
--- PAGE 565 ---
AMAREL
inexpressingandunderstanding theconcepts involvedinthemethod. The representation is basedonthe well-known graphic representation offinite, partially ordered
structures, that is, the structure diagram discussed in section 18.2.1.1 above. Now,
eachpointindataspacecanberepresentedasageometricregionthatcoverspartsof
the structurediagram. A computationalactionthattakesoneormoredataobjectsto
anothercan be seen as atransformationbetween regions thatcorrespondtothedata
objects. The idea is very similartothat ofVenn diagrams, except that the geometry
helps in the handling ofthe inclusion structure ofthe sets involved. As an example,
figure 18-28 showsonthegeometric representationofdata space fortheproblem tt\
thedatapoint {ea} obtainedfromthe intersectionofalltheelementsincludedbythe
elementbandallthoseincludedbyc. Severalotherexamplesofusesofthegeometric
representation ofdata space will be seen below.
18.4.3.2 General Description of the Elimination Method: Assumptions
Made on the Basis of Specialized Knowledge About
Problems in the Class
For a given data association, the process finds all the constructible programs
up to a specified weight ceiling that satisfy the association. Ifno program is found,
thentheconditionontheweightceilingisrelaxedandsearchcontinuesforprograms
ofhigherweight,andsoon, untilaspecifiedceilingofavailablecomputationaleffort
isreached;thentheprocessstops. Supposenowthatasetofprogramsisobtainedthat
satisfies the first association. This set is checked overa secondassociation; and the
programsthatdonot satisfy the new associationare eliminated. The remainingprogramsarecheckedoverathirdassociation, andsoon, until allthedataassociations
areconsidered. Ifoneormoreprogramsremainafterthiseliminationprocess, thena
solutiontotheproblemhasbeen found. Ifnot, thentheprocess is repeated with further relaxation ofthe program weight ceiling until all the computational effort that
was allocated to this problem has been spent.
Structure
Region RepresentingSetofElements
Included bybandc.
Figure 18-28: Exampleofuseofgeometricrepresentationofdataspace.
--- PAGE 566 ---
552 CHAPTER 18: PROGRAM SYNTHESIS ASATHEORY FORMATION TASK
Inaddition, inthepresentformulationitisassumedthatcertainrestrictionsare
imposed on the process offinding a program that satisfies a given data association.
Let us assume that an association in the form {x\X 2, y) is underconsideration.
Exceptincases inwhichasolutionhasthe formofanelementary scheme with
w = 3 (e.g., (E El £)), the structureofcandidate solutionsis restrictedtothe form
P a V Pb- Thismeansthatforadesiredoutputofthesolution/programintheformofa
singleton {y}, the outputs ofP a and b are sets thathave the forms {y, a\}, {y, a 2}
respectively, and the parts ct\, a 2 ofthese sets are disjoint (or one or both may be
empty).
Furthermore, an assumption is made that there is a very restricted number of
points in data space that can instantiate the sets {y, c^}, {y, a 2} whose intersection
produces the desired output. These are (1) the set {x\, x 2) made ofthe inputs to the
program, providedthatatleastoneofthex'sis identicalwithy(letuscall itthe input
set)', (2)thesetofallelementswina, suchthatu = yorylu; (3)thesetofallelements
u inasuchthatu = yorylu. Itisusefultoseeaninterpretationofthelasttwosetsin
thegeometricrepresentationofdataspace(seefigure 18-29). Thesecondsetis made
ofalltheelementsthatcanbereachedfromybydescendingalongthechainsinEthat
gothroughy; it is suggestivetocall this setthe coneofy. The third set is made ofall
theelementsthatcanbereachedfromybyascendingalongallthechainsin £ thatgo
throughy; it is suggestive to call it the invertedcone ofy.
Now, the process offinding a program that satisfies a given data association
can proceedas a search in data space movingforwardfromthe left side ofthe given
association(theinputpairoftheprogram)towardthesetsthatarepossiblecandidates
SetofElements Equal toorIncluding v
=InvertedConeofv)
Set ofElements Equal toor
Included-bv v( =Coneofv)
Figure 18-29: Thetwo"conesets"' ofanelement vofapartialk orderedstructure E, shown inthegeo
metric representation ol data space.
--- PAGE 567 ---
AMAREL 553
foraterminal intersection. Alternatively, thesearch indataspacecanproceedbackward from the sets that are candidates for terminal intersection toward the given
input pair.
Clearlythe assumptionon restrictingthepoints in dataspace thatarepossible
candidates fortheterminal intersection representsan importantintroductionofspecializedknowledgeoftheproblemdomainintothesolutionmethod. Thisknowledge
can be derived from analysis ofproperties ofcomputational actions and oftheir
aggregates in the problem domain and/or by empirical exploration ofproperties of
solutions inthedomain. Thegeometric representationofdata space facilitatesenormously our understanding of the properties of the domain. Mechanizing the discovery ofthe geometric representation and ofuses ofthe representation in finding
propertiesofthedomainthatareusefulforsolvingproblemsinagivenclassremains
an important open problem in AI.
Letusproceednowwith anoutlineofanexample ofapplication ofthepresent
method to the problem tc\.
18.4.3.3 Application of the Data-Driven Elimination Method to the
Formation Problem
7r1
1. Assume the weight ceiling is set at w = 7.
A. Considerthe data association (ab, a).
Inthepresentcase, thecandidatesetsforterminalintersectionaretheinputset
{ab} andthe "cone sets" {ae}, {abed} (see figure 18-30a).
The input set {ab} can be produced from the input pair by the program
(E E) (vv = 3).
The cone set {ae} can be produced from the input pair by the following programs:
w = 3: (/ El I)
w = 7)©/)
5: ((/
w = 7: ((/ V E) \N (I V E))
The inverted cone set {abed} can be produced from the input pair by the following programs:
w = 5: ((/ /) © /), ((/ El /) © /), ((/ E /) © 7)
w = 7: ((/ V E) El (7 V £)), ((/ /) © (7 V E))
In this listing ofthe programs, ifa new program is found that is known to be
functionally equivalent to a program that is already in the list (i.e., the previouslygenerated/listedprogramhas wlowerthanorequaltothenewprogram),
then a new program is not added to the list. For example, the program
(E E) © (/ V E)) with w = 1 is not added to the list of "producers" of
the set {ae} after the program ((/ V E) El (/ V E)), which is its equivalent, has been listed there. The test for equivalence is based on the known
--- PAGE 568 ---
554 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
InvertedCone InvertedCone InputSet; also,
ofa ofe Inverted Coneofb
Coneof« Coneofb
Input Set
Input Set Coneofe
a. (ah,a)-c'dse b. (ea.e)-case c. (bd,b)-case
Inverted Cone
ofc
Inverted
Coneofa
Coneof
d. (rr,c)-case e. (bc\«)-case
Note: Thedesiredoutput ineaehease isshownasadarkenednode.
Figure18-30: Importantsetsusedforfindingsolutionsinindividualcases,showninthegeometricrepresentationofdataspace.
propertiesofthealgebraicmodel. Notethatthepruningoffunctionallyequivalent programs is not necessary, but it may help with efficiency.
The objective of these program listings is to produce a sufficient amount of
functional variety inthe formofasetofprograms, allofwhich satisfy the data
associationunderconsideration. Thehopeisthatwithinthis functional variety
a program will be found that satisfies all the otherassociations in the domain.
This isa special instance oftheapproach tosolvingconjunctive goal problems
--- PAGE 569 ---
AMAREL 555
bygeneralization. Inthisapproach, anattemptis madetofindasetofsolutions
for the first goal which is as general as possible, so that a subset ofthis set
might satisfy the second goal, and then the third goal, and so on.
Onthebasisofthepreviousanalysis, threesolutionsareobtainedthatarebased
onaterminal intersectionofthe sets {ab} {ae} andfifteen solutionsthatare
, ,
based on a terminal intersection ofthe cone sets {ae}, {abed}. For example,
the first three solutions are as follows:8
p P 2 7 = = [ [ ( ( £ ( " £ El E E ) ) & & ( ( / / M El /) 7 ] ) © /)]
P 3 = [(£ E) & ((/ V E) \N (I V E ))]
B. Consider now the data association (ea, e).
In the present case, the candidate sets for terminal intersection consist ofthe
input set {ea} and the cone sets {e}, {eabed} (see figure 18-30b).
Asinthepreviouscase, the inputset {ea} canbeproducedfromthe inputpair
by the program (E E).
The cone set {e} can be produced from the input pair by the same three programs that produced the set {ae} in the previous case.
The inverted cone set {eabed} can be produced from the input pair by using
only two ofthe five programs that were used to produce the set {ae} in the
previous case. The three programs that are eliminated are ((/ El /) /),
© ©
((/ /) 7), ((/ El /) /).
Thusatotalofninesolutionsareobtained, threeby intersectionwiththeinput
setand six by intersection between the two cone sets.
C. Consider next the data association (bd, b).
Thecandidatesforterminalintersectionaretheinputset {bd} andtheconesets
{eab} and {bd} (seefigure 18-30c). Notethatinthiscasethe input setandthe
invertedconesetoftheoutputareequal. Despitethis, theprocesscontinuesto
consider alternate ways ofconstructing the set {bd}.
The first way of constructing {bd} from the input pair is via the program
(E E) which, as before, produces/characterizes the input set.
The second way ofconstructing the set {bd} is by viewing it as the inverted
cone set ofthe output b and checking whether any ofthe remaining programs
8Ifaprogramreceivedanamepreviouslyinthischapter,itisgiventhesamenamehere.ThusP
Pycanbe
recognizedasprogramsthatwereencounteredpreviously.
--- PAGE 570 ---
556 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
forproducing inverted cone sets can produce it. Fromthe two remaining programs, only onecan do it; itis ((/ V E) (/ V E)). The otherprogram-
((/ El /) (7 V E))- is eliminated. Thus one out ofthe five initial programs forproducing inverted cone sets is left.
The cone set {eab} can be produced from the input pair by using only one of
the three remaining programs for producing cone sets. This program is
((/ V E) El (/ V E)). The othertwoprograms are eliminated. Thus atthis
pointonlytwocompleteprogramsremainaspossiblesolutionstoourprogram.
They are
p 3 = [(E El E) & ((/ v E) El (/ V £))]; w = n
P 8 = [((/ V E) (I V E)) & ((/ V E) (7 v £))];
w =
15.
The programP 3 isbasedonan intersection ofthe input set and the cone set of
theoutput. F 3 wasencounteredpreviously inworkwithotherapproaches. Itis
a very strong program; it satisfies all but one ofthe fifteen data associations
(see figure 18-27). TheprogramP 8 isnew. It isbasedonan intersection ofthe
two cone sets ofthe output. Examination ofthis program shows that it is as
strong as P 3; actually, it is functionally equivalentto P 3.
Theprocesscancontinuenowbygoingfromotherindividualdataassociations
towardthesetofremainingprogramsandbyeliminatingprograms as needed.
Alternatively, as in the present case in which a small number of programs
remain, the remainingprograms are runovertheentire datadomain, anddata
associationsthatarenotsatisfiedbyanyoftheprogramsarenoted. Proceeding
with the latter approach shows that the only data association not satisfied by
either P 3 orP s is (be, a). Attention focuses now onthis association.
2. Assume now that the weightceiling is raised to w = 9.
Inthedataassociation(be, a), theinputsetdoesnotincludetheoutputa. Thus
theonlycandidatesforterminal intersectionaretheconesetsofa, thatis, {ae}
and {abed} (see figure 18-30e).
Letusfirstconsiderprogramsthatcanproducetheconeset {ae} fromtheinput
pair (be). The remaining program ((/ V E) \N (I V E)) with w = 7 can do
it. No additional programs with w = 8, 9 can be found that can satisfy this
goal.
Considernextprogramsthatcanproducetheinvertedconeset {abed} fromthe
input pair (be). The program ((7 \f E) M (ly E)) with w = 1 that remained
frompreviousworkfails; thusitiseliminated. Nonewprogramswith w = 8, 9
can be found that can satisfy the present goal.
Thus at this point noprogram exists as a possible solution to our problem.
--- PAGE 571 ---
AMAREL 557
3. Assume finally thatthe weight ceiling is raised tow = 11.
Therearenowtwoprogramsthatcanproducetheconeset {ae} fromtheinput
pair (be):
w = 7: ((/ V E) E (/ V E)) (This is the "old" program.)
w = 11: ((E M E) © ((/ V E) © (/ V E))).
Note that the second program is functionally equivalent to the first.9 Thus it
does notcontributeextrafunctionalvariety. Theformationprocesscandevote
sometimeto such recognitionofequivalenceanddropthe new candidateprogram, or it can ignore this issue. This does not have a fundamental effect on
whetherasolutioncanbefound. However, theissueshouldbeconsideredfrom
the point ofview ofsolution-finding efficiency. Since efficiency is important
fortheformationprocess, someheuristicguidanceisneededonthequestionof
pruningfunctionallyequivalentprogramcandidates. Ingeneral, ifthenumber
ofcandidates is large, itpays todevote some effort tothis pruning process.
Letus move now tothe search forprograms with weights w = 10, 11 thatcan
produce the inverted cone set {abed} from the input pair (be). The only programs that can be found are
(((/ V E) E (/ V E)) © (/ V E)); w = 11, and
(((£ El E) © (/ v E)) © (7 v £)); w = 11.
Thesetwoprograms are functionally equivalent.
From the component programs for the two cone sets, several functionally
equivalentsolutionstotheproblemir cannowbeobtained. Twoofthesesolutions are as follows:
P 3,i = [((/ V E) El (/ V E)) & ((/ V E) El (/ V E) ©
(7 V E))]; w = 19,
P 6 = [((E E) © (/ V E)) & ((E E) © (/ V E) ©
v w =
(7 £))]; 19.
(The firstprogramcanbe recognized as P
3>i,
which was obtained as the solutiontotheproblemtt inthecourseofworkwiththefirsttwoapproachestoformation. The secondprogram canbe recognized as P 6, which was the solution
obtained in the data-driven combination method.)
In work so far with the present method, no special attention was given to the
orderin which input associations are considered. Clearly orderhas an effect on the
efficiency of the process, and it should be studied in the context of the present
9Fromthealgebraic model itcanbe seenthat((/ v E) (/ V £)) - ((£ E) £) © (/ V E))\ and in
view ofthe property ((/ v E) © (/ v E))~ (I v E) (see section 18.3.1.2, 5) we obtain that
((E M E) © (/ V E)) - ((E E) © ((/ v £) © (/ V E ))).
--- PAGE 572 ---
558 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
approach. There has been a considerable amount of work on the ordering of constraints in constraint satisfaction problems (Nudel, 1983). Much ofthis work will
probably be relevantto the present problem.
18.4.3.4 Discussion of the Data-Driven Elimination Method
The viewpointfrom relation matrices. Letus examine the process ofelimination in
theframeworkoftherelationmatricesofprograms. Thesetofdataassociationhasa
domain ofn(n + l)/2 possible inputs and a range ofn possible outputs. Thus there
areatmostnn(n+1)/2possiblefunctions; andifitisassumedthatafunctionisspecified
for k point of its domain, then the functional variety drops to flM»+iV2]-* Only a
small part ofthe functional variety can be realized in the language ofconstructible
programs.
Now, given the first data association, a set ofprograms-up to a given complexityceiling-isgeneratedthatsatisfiestheassociation. Thissetcanbe seenasthe
"version space" (Mitchell, 1977, 1978) ofall possible solutions that are consistent
withthegivendataassociation. Whenthesecondassociationisprocessed, onlythose
programsfromtheprevioussetthatalsosatisfythenewassociationareretained. This
processmay resultintheeliminationofsomepossiblesolutionsandthus inthetrimming down ofthe version space ofpossible solutions. In general, at some intermediatestageintheprocess, afterseveraldataassociationshavebeenhandledthatcover
asubdomaind
ofthetotaldomain, aversionspaceofpossiblesolutions isobtained
that representsallprograms with weightuptoacertain maximum that arefunctionallyequivalentwith respecttothesubdomaind
Xa.
Unfortunately, convenient ways of
characterizing these version spaces and of reasoning with them do not at present
exist. An extension ofthe algebraic model that would handlepartialfunctions and
their relationships would be very helpful here.
Theprocessofsolutioneliminationisillustratedschematically infigure 18-31.
After the first association is processed, the version space ofsolutions includes the
programs P u, P v, P w. These programs are represented by their graphs, that is, by the
distributionofl'sintheirrelationmatrixrepresentation. Afterthesecondassociation
is processed, one ofthe programs, P w, is eliminated because it does not satisfy the
new association; and the remaining version space is made ofP N and P v.
The viewpointfromthegeometricalrepresentationofdataspace. Itisvery revealing
tointerpretthesolutionobtainedfortheproblem ir\ andsomeofthe "almostcorrect"
solutions from the viewpoint ofthe geometric representation ofdata space.
Letusconsiderfirstthecorrectsolutiontothe Infimumproblem. Givenadata
association (jc,jc2, v), the solution program can be seen to produce the output y from
the input pair (Jt,jc2) in three major steps (see figure 18-32).
1. The cone set of the output y is formed from the input pair (V|.v :). (This is
achieved by first forming the cone sets of v, and ot.\ : and then intersecting
--- PAGE 573 ---
AMAREL 559
nOutputs
P u P v P„
X\A x
2,\
n(n + l)/2
Inputs
Then
x x
\.2 l,2
Functionalvarietyafter Functionalvarietyafter
firstassociation secondassociation
(*i,i*2,i>y\)isprocessed C*i,2*2,2>^2)isprocessed
Figure 18-31: Illustration ofthe functional elimination process on the matrix representation of
programs.
them. The subprogram ((/ \/ E) \N (I \/ E)) appliedtothe inputpair {x x x 2)
implements this step. It should be noted that forany elementx, application of
the program (/ V E) onxproduces the cone ofx.)
2. The invertedcone setoftheoutputyis formed fromthecone setofv. (This is
achievedbyfirstformingtheinvertedconesetforeachelementintheconeset
ofv and then intersecting the results. This step is implemented by the second
part ofthe cascade subprogram [((/ V E) \N (I V E)) (7 V E)]. This
partappliestheprogram(I V E)toeachelementofthesetthatisproducedby
((/ V E) El (/ V E)) in the first step ofthis process, and it intersects the
results. It should be noted that for any elementx, application ofthe program
(/ V E) produces the inverted cone ofx.)
3. The cone set and the inverted cone set ofv are intersected to obtainy. (It can
easilybeseenthatthisisimplementedbythemainintersectionoperationofthe
solution program /$,,.)
--- PAGE 574 ---
560 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
Cone Set / \ Tone Set r~ InvertedCone
of.*, / \ ofX / v Setofv
/^~\ \ / /r-Xi~ X-,
Aux\\ \y / r-, \\ / \\
^>\
Cone Sc' \ / t one Set\
/ ofA' \
Step 1. Inputs (V|.v2) - cone Step2. Conesetofv - inverted
setofv conesetofv
InvertedCone
V.KV
Setofv
Output v
ConeSetof
Step3. Conesetofv, invertedcone setofv - y
Figure 18-32: InterpretationofsolutiontotheInfimumproblemasathree-stepprocessonthegeometrical representationofdataspace.
Letusreviewnowthepreviousmodel-guidedprocessofformation(whichwas
summarized in figure 18-23 above) in light ofthe present interpretation in the geometric representation. Steps 1, 2, and 3 ofthe previous process correspond to the
search ofa program for achieving present step 1. Step4 ofthe previous process correspondstothebuildingofaprogramforachievingpresentstep2. Previousstep5, in
whichaterminalmaneuverwasusedtoobtainthedesiredgoalprogram, corresponds
to building a program that implements present step 3, that is, the intersection ofthe
conesetsoftheoutput. Althoughthemodel-guidedapproachtonavigating inastructuredprogramspacewasuseful forcomingclosetothedesiredsolution, it was inadequate For guiding the terminal maneuver in that process. The terminal maneuver
--- PAGE 575 ---
AMAREL 561
mustbe seen from a completely different viewpoint in orderto understand it and to
devise strong ways ofimplementing it. Thepresentgeometric representationofdata
space, inwhichdataandtheirrelationshipsarepresentedinanespeciallyappropriate
way, provides the right framework.
Evaluationfunctionsthatmeasuretheproportionofinputssatisfiedorthatestimatedistancefromthegoalprograminsomeprogramspacehavelimitedvalueinthe
formation process. They can help the process in the task ofproducing some promising fragments ofa solution, butthey leave openthe extremely difficultproblem of
howtocombinethese fragments intoasuccessful solutionassembly. A considerable
amountofspecificdomainknowledgeandstrongmethodsofreasoningareneededin
order to approach the latter problem. Above all, it is clearly essential to handle the
problemwithinanappropriateconceptual framework. Thedataspacerepresentation
thatwasusedtoguideandinterpretthepresentformationmethodappearsverypromising in this respect.
18.5 FURTHER IMPROVEMENT IN PROCEDURAL FORMULATION:
FORMATION OF MACROMOVES AND OF NEW RELATED
CONCEPTS; DOMAIN EXTENSION
An improved formulation ofthe program formation problem can be obtained
byexplicitlyformingtheconceptsofmacromovesforcomputingtheconesetandthe
inverted cone set of an element in the finite set a. The compound computational
action (7 V E) will compute the former, and {I \J E) will compute the latter. The
automatic formation of these macromoves is an approachable task at present. It
involvesthe identification ofuseful regularities in records ofproblem solving activity
in the problem class.
A stronger macromove would consist ofthe computation that takes as input a
setofelementsqandproducesasoutputtheintersectionofalltheconesetsoftheelements in q. Inthegraphic language ofconstructiveprograms, suchamacromoveis
represented as shown in figure 18-33.
In the algebraic model for two-input programs, the operator sequence
© (/ V E) in an expression ofthe form (P © (/ V E)) for any P represents the
macromove. Letusdenotethemacromovebyitsoperatorsequence. Inthegeometric
representation ofdata space, the macromove can be seen as in figure 18-34.
Figure 18-33: Macromove© (/ v E).
--- PAGE 576 ---
562 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
InputSetq
OutputSetResulting from
ApplicationofMacromove©
V E)onq
Structure
Figure 18-34: Effectofmacromove© (/ v £)showningeometricrepresentationofdataspace.
Thus ifasetqcanberepresentedasaregion withaminimumpoint in thediagramofastructureL thenthemacromoveproducesaconewhoseapextouchesfrom
below the minimum point ofthe region representing the set q.
Actually, in terms of conventional mathematical concepts, the macromove
V E)whenappliedtoasetqproducesthesetofalllowerboundsofq. Alower
bound of the set q is an element u in a satisfying xlu or xEu for all elements x
inq.
A LISP representation ofthis macromove, call it Lower-bounds-of-set, is as
follows:
Lower-bounds-of-set: (lambda (q) (Intersection-* (Mapcar
'(lambda (x) (Union (I-function x)
(E-function x)))q))
Amacromove © (I V E) forcomputingthesetofallupperboundsofasetq
canbedefinedandinterpretedinamannersimilartothe (/ V E)macromove. Its
LISP representation is:
Upper-bounds-of-set: (lambda (q) (Intersection-* (Mapcar
'(lambda (x) (Union (leap-function x)
(E-function x)))q))
Theautomatic formationofthestrongmacromovesandoftherelatedconcepts
ofthe set ofall lowerbounds ofa set and ofthe set ofall upperbounds ofa set is an
interesting, but approachable, AI task at present. Mechanizing these concept discovery and macromove formation processes is a prerequisite for moving automatically to strong formulationsofthe program formulationproblem. It should be noted
that, as in the case ofthe interesting programs introduced above in section 18.4.2.1
(and their related subdomain concepts), discovery processes are being encountered
herethatareaimedattwomutually interdependentnotions: theconceptofan "interestingset," whichdependsontheexistenceofa"goodmethod" forcomputing it. and
--- PAGE 577 ---
AMAREL 563
a "good method" worthy ofbeing singled out and given a separate identity which
dependsontheabilitytofocusontheinterestingsetasagoodcandidateofanewconcept. This is the type of concept discovery problems that were identified in other
studies of problem reformulation (Amarel, 1982). Preliminary work in this area
shows that it would be promising to pursue some of Lenat's approaches (Lenat,
1983b) in the present context.
© ©
Assuming that the above macromoves (/ V E), (/ V E) and their
related concepts are available, then it is possible to define the following simple but
powerfulmaneuverinourdataspace: Givenasetqin2°witha"smallest"elementu,
thenucanbeextractedbycomputing fromqthesetofalllowerboundsofqandthen
intersecting the latter set with q.
Thismaneuvercanbeeasilyseeninthegeometric representationofdataspace
(see figure 18-35). The smallest element in q is obtained by pointing from below a
cone that touches it and by intersecting q and the cone.
This maneuvercan be represented by the following LISP function:
Smallest-of-set: (lambda (q) (Intersection
(Lower-bounds-of-set q) q))
A similarmaneuvercanbedefinedforobtainingthe "greatest" elementofaset
in the partially ordered structure.
Its LISP representation is
Greatest-element-of-set: (lambda (q) (Intersection
(Upper-bounds-of-set q) q)) .
Clearly these two maneuvers can themselves be seen as macromoves that are
defined in terms ofpreviously defined macromoves.
*- "Smallest" Elementofq
ersection
Figure 18-35: The "minimumextractionmaneuver" (q a <7i) = {«}
--- PAGE 578 ---
564 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
Nowathree-stepprocessforcomputingtheInfimumcanbeexpressedinterms
ofour macromoves as follows:
1. Given an input pair {x x x 2), form the input set q = {x\X 2}.
2. Applythemacromove"Lower-bounds-of-set"onq. Thisproducesthesetq of
lowerbounds ofq.
3. Apply the macromove "Greatest-element-of-set" on q x. This produces the
desired output.
Notethatthisprocess is identical withthethree-stepprocess described in section 18.4.3.4 (in which a geometric interpretation of the solution process is presented), exceptthattheindividual stepsaredefineddifferently; presentsteps 1 and2
correspondtothesinglestep 1 inthatprocess, andpresentstep3 correspondstoprevious steps 2 and 3.
A LISPrepresentationofaprogramthatexpressesthepresentthree steps isas
follows:
P' (lambda (x x (Greatest-element-of-set
6: 1 2)
(Lower-bounds-of-set (Union (E-function x
(E-function x 2)))))
Note that P' 6 is a summary representation ofthe program P 6 for the Infimum
(which was shown previously in section 18.4.2.2) with several program operations
chunked together into biggeraggregates defined by the macromoves.
Clearlythechunkingofstructureembodiedinthemacromovesandtheformationofrelatedintermediateconceptsoflower-boundandupper-boundsetsresultina
most appropriate representation for handling the Infimum formation problem. One
canreasoneffectivelywiththeelementsoftherepresentation, andtheseelementsare
ofthe rightgrain forthe formation task.
Experienceofproblemsolvingwithintheimprovedformulationinducesattention on the definition ofprograms whose inputs are sets (i.e., subsets of o). For
example, the macro-operations (/ V £),©(/ V £) nave as me ^r domain the
set2°. Althoughtheinitialproblemformulationwastofindadefinitionofamapping
(inagiven languageofprograms) that was specified overadomain d\ ofunordered
pairs from a, the solution that was obtained is made mostly ofcomponents that arc
specifiedoverthemoreextendeddomainof2°. Actually,theonlypartoftheprogram
P\ that directly handles elements ofa is the "front part"
(Union (E-function xj
(E-function x
2))
that transforms an input pair (x^) into an input set. Under these conditions, it is
possible to define an extension ofthe Infimum concept by redefining the domain of
the function; the new domain is 2°. The new definition ofthe extended concept of
Infimum can be seen as the first step in the discovery ofa new concept, where a
--- PAGE 579 ---
AMAREL 565
plausibledomainextensionofanexistingconceptisproposedasaconceptcandidate.
It turns out that the proposed new concept is an important and useful mathematical
concept; itisthegreatestlowerboundofasetinapartiallyorderedstructure. ALISP
representation ofthis concept is as follows:
Greatest-lower-bound-of-set: (lambda (q)
(Greatest-element-of-set (Lower-bounds-of-set <?))).
Themethodsdevelopedby Lenatarerelevanttothemechanizationofthisconceptextension. Representationofthenewconceptrequiresthatseveralmodifications
be introduced intheprogram language and in its relatedalgebraic model. Theseare
fairly straightforward, and they will notbe described in detail here.
It should be noted that the algebraic model is still available (with only minor
changesrelativetothemodelthatisusedforhandlingprogramsoverthedomaind\)
for handling programs overthe extended domain 2°.
CONCLUDING COMMENTS
18.6
The evolution of methods described in this chapter shows that an effective
theory formation process needs large amounts ofknowledge ofvarious kinds and
careful choice of representations. Although a considerable amount of progress
toward a solution can take place by intelligent searches in program space, it seems
essentialtopursuetheterminal stagesofsolutionviadetaileddata-driven reasoning
supportedbydomain-specificknowledge. Morework isneededonmethodsforeffective coordination ofreasoning in program space and in data space.
Theexplorationofdata-drivenmethodsofprogram formationhasbroughtout
forcefullythefundamentalimportanceofthedegreeofdependencebetweenproblem
conditions onthechoice ofmethodofproblem solving. The problems ofcombining
several "partially correct" programs into a desired program and of modifying
"almost correct" programs into a correct one are closely related to the problems of
constructingplansforsatisfyingseveralinteractinggoals. Thisisabasicproblem in
AI that needs much more research.
Itisbecomingevidentthatthe notionofproblemformulation intheory formation is somehow fluid and open-ended. More specifically, although attention may
focus initially on some domain of phenomena to be "covered" by a theory, the
attempttoconstructthetheorymayleadtoaredefinitionofthedomainofthetheory
(partitions of the initial domain or extensions) that creates new theory formation
tasks. Theprocessesofredefining adomain areclosely relatedtoconceptdiscovery
processes ofthe type studied by Lenat; they are precipitated/guided by knowledge
gatheredinthecourseofattemptingtosolvethetheoryformationproblemfortheinitialdomain. Apromisingapproach, whichisalsoofdirectrelevancetotheautomatic
acquisition ofexpertise in other types ofproblems, is to define a domain as "interesting" ifa specialized method is discovered that is especially effective for solving
--- PAGE 580 ---
566 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
problems inthatdomain. The interplaybetweendomainchoiceandtheformationof
a theory forthe domain is an interesting and difficult problem. Much more work is
needed in this area.
The discovery and use ofappropriate models plays an important role in the
theory formation processes presented in this chapter. The algebraic model ofprogram spaceprovides sufficientstructuretothe spacetoenableamajorchange inthe
method of solution-from a heuristic hill-climbing method to a goal-oriented
method in which it is possible to develop aplan in program space for moving in the
direction ofthe goal. An interesting, and approachable, problem in this area is to
mechanizetheprocessofdevelopingasolutionmethodonthebasisofknownproperties ofa relevant model. The problem offinding a relevant model (or ofadapting a
givenmodeltotheneedsofatask)andoffinding interestingpropertiesofthe model
on the basis of which a good solution method can be formulated, continues to be
beyondthepresentcapabilitiesofAI. However, furtherprogressintheoryformation
processeswillbringusclosertoapointwherethemechanizationofthesecapabilities
can be contemplated.
Inordertodiscoverthestrong formulationoftheprogramformationproblem,
where macromoves and high-level intermediate concepts could be defined, it was
essential to work within an 'appropriate" representation ofdata space. The geometric model, which is based on the diagrammatic representation of partially
orderedstructures,hasprovedtobeextremelyfruitfulinthisrespect. Therearesimilaritiesbetweenthepresentsituationandtheimpactofgeometric representations on
the solution ofother problems studied in AI, for example, in the Missionaries and
Cannibals problem (Amarel, 1968). There are also similar open questions. Is the
appropriateness ofthe geometric representation, where several properties ofacomplex situation are conveyed in a single structure, due solely to certain properties of
theperceptual and reasoningprocessesofpeople? Inwhatway shouldthegeometric
representationbeencodedinamachine, andwhatprocessesofanalysisandinterpretation shouldbe used to makethe representation as appropriate formachines as it is
forpeople?Theseareimportantquestionsfortheory formation problemsasthey are
for other areas of problem solving, and they require much more study. Given an
appropriate representationofdataspace, theproblemofdiscoveringusefulregularities in solutions that lead to a problem reformulation in terms of higher-level concepts, suchasmacromoves, isapproachableatpresent. Processesofmacromove formation are ofgeneral relevance to problems of improving performance in problem
solving (expertise) via problem reformulations. Progress in this area requires the
applications oftheory formation techniques ofthe type described here.
Afewfinalcommentsabout research methodology. Inthecourse ofthis work.
several computer-based experiments were made. Most ofthe experiments were concerned with properties of searches in program space and used the heuristic
hill-climbing method andthe model-guided method. Workon a system fordoing experiments with data-driven methods has begun recently. Because o\' the relative
--- PAGE 581 ---
AMAREL 567
simplicity ofourdomain (and its formal properties) ithasbeen possibletocarry out
several detailed hand simulations in order to probe possible methods of formation
and the effects of various assumptions about representations and models on these
methods. Inthisareamajoremphasisstillneedstobeplaceontheconceptualclarificationofapproaches;thecomputerprovidesthenecessarymeansforselectiveexperimentation in support ofthe conceptual work.
ACKNOWLEDGMENTS
Theresearchpresentedinthischapterwas supportedinpartbythe Divisionof
Research Resources, NIH, Grant RR00643 to the Rutgers Research Resource on
Computers in Biomedicine.
An early version ofthis chapter was written while the author was a Visiting
ResearchFellowattheArtificialIntelligenceCenter, SRIInternational, MenloPark,
Calif., during the spring of 1983. The support of SRI International is gratefully
acknowledged.
References
Amarel, S., "AnApproachtoAutomaticTheoryFormation," inPrinciplesofSelf-Organization, H. Von
FoersterandG. Zopf(Eds.), PergamonPress, NewYork, 1962a.
, "On the Automatic Formation ofa Computer Program Which Represents a Theory," in SelfOrganizingSystems-1962, M. Yovits, G. Jacobi, andG. Goldstein (Eds.), Spartan Books. Washington, D.C., 1962b.
, "On Representation of Problems of Reasoning About Action." In Machine Intelligence 3,
D. Michie(Ed.), UniversityofEdinburghPress, Edinburgh, 1968.
"OntheRepresentationofProblemsandGoal-DirectedProceduresforComputers,"inTheoreticalApproaches to Non-Numerical Problem Solving, R. B. Banerji and M. Mesarovic (Eds.),
SpringerVerlag, Heidelberg,W. Ger. , 1970.
"RepresentationsandModelinginProblemsofProgramFormation," inMachineIntelligence6,
B. MeltzerandD. Michie(Eds.), UniversityofEdinburghPress, Edinburgh, 1971.
"ProblemsofRepresentationinHeuristicProblemSolving:RelatedIssuesintheDevelopmentof
Expert Systems," Technical Report CBM-TR-118, Rutgers University, 1981. (Also published in
MethodsofHeuristics,R.Groner,M.Groner,andW. F. Bischof(Eds.),Erlbaum.Hillsdale,N.J.,
1983.
"ExpertBehaviorand Problem Representation," Technical Report CBM-TR-126. Rutgers University. 1982. (AlsopublishedinArtificialandHumanIntelligence,A. ElithornandR. B. Banerji
(Eds.), North-Holland, Amsterdam, 1984.
Birkhoff, G., andMacLane, S.,ASurveyofModernAlgebra, Macmillan, NewYork, 1953.
Davis,R.,andLenat,D
"AM:DiscoveryinMathematicsasHeuristicSearch,"inKnowledge-BasedSystemsinArtificialIntelligence, PartOne, McGraw-Hill, New York, 1982.
--- PAGE 582 ---
568 CHAPTER 18: PROGRAM SYNTHESISASATHEORY FORMATION TASK
Ernst,G.
andGoldstein,M.
"MechanicalDiscoveryofClassesofProblem-SolvingStrategies,"Journal
oftheACM, Vol. 29, No. 1, pp. 1-23,January 1982.
Ernst,G. , andNewell,A. , GPS:ACaseStudyinGeneralityandProblemSolving, AcademicPress,New
York, 1969.
Korf. R., "A ProgramThat LearnstoSolveRubik'sCube," inProceedingsofAAAI-82, Pittsburgh. Pa.
pp. 164-67, 1982.
Langley,P.,Bradshaw,G.L.,andSimon,H.A., "RediscoveringChemistrywiththeBACONSystem,"in
Machine Learning: An Artificial IntelligenceApproach, R. S. Michalski, J. G. Carbonell, and
T. M. Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
Lenat, D., "EURISCO: A ProgramThat Learns New Heuristicsand Domain Concepts. The Natureof
HeuristicsIII: ProgramDesignandResults,"ArtificialIntelligence,Vol. 21,Nos. 1,2,pp. 61-98.
March 1983a.
, "Theory Formation by Heuristic Search. The Nature ofHeuristics II: Backgroundand Examples,"ArtificialIntelligence, Vol. 21, Nos. 1, 2, pp. 31-59, March 1983b.
Lindsay,K.;Buchanan,B. G.; Feignebaum,E. A.;andLederberg,J., TheDendralProject:Applications
ofArtificialIntelligenceforOrganicChemistry, McGraw-Hill, NewYork, 1980.
Manna, Z., andWaldinger, R., "KnowledgeandReasoninginProgramSynthesis,"AIJournal, 1975.
Manna, S., andWaldinger, R., Synthesis: Dreams-Programs, Technical Report No. 156, SRI International, 1977.
Mitchell,T. M.
"VersionSpaces:ACandidateEliminationApproachtoRuleLearning."Proceedingsof
theFifthIJCAI, pp. 305-10, Cambridge, Mass., 1977.
, "Version Spaces: An Approach to Concept Learning," Ph.D. diss., Stanford University. 1978.
(AlsopublishedasStanfordCSReportSTAN-CS-78-711, HPP-79-2, StanfordUniversity. 1978.)
Mostow,D.J.,"MechanicalTransformationofTaskHeuristicsintoOperationalProcedures,"Ph.D.diss.,
Carnegie-MellonUniversity, 1981.
Newell, A., "Heuristic Programming: III Structured Problems," in Progress in Operations Research,
Vol. 3,J. S. Aronofsky (Ed.), Wiley, NewYork, 1969.
Nudel, B., "Consistent-Labeling Problemsand TheirAlgorithms: Expected-ComplexitiesandTheor\-
Based Heuristics,"ArtificialIntelligence, Vol. 21, Nos. 1, 2, pp. 135-78, March 1983.
Piaget,J., TlteOriginsofIntelligenceinChildren, M.Cook(Trans.),InternationalUniversit> Press.New
York, 1936.
"LaLogistiqueAxiomatiqueoupureLaLogistiqueOperatoireouPsychologiqueel lesRealties
Auxquelles EllesCorrespondent," MethodosIV, 1952.
Sacerdoti. E. D,AStructureforPlansandBehavior, American Elsevier, Now York. 1977. (Ph.D. diss.,
StanfordUniversity, 1975.)
Simon. H. A.. TheSciencesoftheArtificial, MIT Press. Cambridge. 1969.
Simon. H. A., and Lea. G., "ProblemSolvingand Rule Induction: A Unified View" in Knowledgeand
Cognition, Erlbaum, Hillsdale. N.J.. 1974.
Stefik, M.. "Planning With Constraints," ArtificialIntelligence, 1981.
--- PAGE 583 ---
AMAREL 569
Sussman, G.,A ComputerModelofSkillAcquisition, AmericanElsevier, NewYork, 1975.
Waldinger, R. J., "Achieving Several Goals Simultaneously," in MachineIntelligence8, E. Elcockand
D. Michie(Eds.), EllisHorwood, Chichester, England, 1977.
Warren, D. H. D., "WARPLAN: A System for Generating Plans,1' Technical Report 76, University of
Edinburgh, Edinburgh, 1974.
, "GeneratingConditional PlansandPrograms," ProceedingsoftheAISB, Edinburgh, 1976.
--- PAGE 584 ---
--- PAGE 585 ---
AN APPROACH TO LEARNING FROM
OBSERVATION
Gerald DeJong
UniversityofIllinois
at Urbana-Champaign
Abstract
Learningfromobservationrequiresthatasystemappreciatethesignificanceof
anevent(orsetofevents)thatiseitherfortuitousorpartofanother'splanningprocess
andthengeneralizethenewevents intoanew concept. A frameworkispresented in
whichtheseprocessescanoccur. Theapproachiscalledexplanatoryschemaacquisition. It involves knowledge-based generalization that can construct a first-pass
generalized concept from just one input example. A natural language system that
acquires new schemata has been implemented. When presented with a story that
illustrates new problem-solving behavior in a character, the system generalizes its
understandingofthetechniqueandremembersthegeneral formtoaidinprocessing
later stories.
19.1 INTRODUCTION
Thischapterdiscussesanapproachtolearningconceptsfromobservation. The
concepts learned are problem-solving schemata. The approach taken is not the
standardcorrelationalone, inwhichoneexaminesanumberofeventsandconstructs
a new concept by noting the commonalities and differences that emerge among the
events. In the correlational approach the certainty that a new concept is correct and
useful increases with the number ofevents; with only a few events some observed
commonalities may be chance similarities.
By contrast, in explanatory schema acquisition (ESA) feature significance is
judgedthroughtheuseofbackgroundknowledgeratherthancorrelationalsimilarity.
--- PAGE 586 ---
572 CHAPTER 19: LEARNING FROM OBSERVATION
Thus a new concept can be constructed fromjust one event. New concepts can be
incompleteandrequirelaterrefinementbasedonotherevents. However, theyarefull
schemata and can be used immediately to aid in processing. The approach requires
muchbackgroundknowledge; thedomainknowledgeisusedbothtojudgethesignificance ofan event and to generalize it into a schema.
As mentioned above, the concepts learned are problem-solving schemata.
Therefore much ofthe knowledge must be problem-solving knowledge: knowledge
oftheoperatorsinthedomainandknowledgeaboutgoalsandhowthey interact. The
motivation for this work has thus far been entirely computational. This kind of
learning is driven by the system's explanation ofan observed event (DeJong, 1981).
The author's intuition is that real-world human adult learning is largely explanation
driven, but no psychological experiments have as yet been performed to test this
hypothesis; therefore, the author is not in a position to defend it.
19.1.1 Problem Solving with Schemata
In this section the task ofproblem solving will be cast in terms necessary for
ESA learning to take place. Problem solving can be viewed as the process oftransforming some initial state of the world into a goal state by the application of a
sequence of known operators. The operators define a search space that must be
explored in order for one to discover how the goal state can be achieved. The complexityofablindsearch is 0(m"), wherem isthenumberofoperatorsavailableandn
is the number ofsteps in the solution path. Figure 19.1 shows such a search tree for
m = 3 and n = 4. This complexity prohibits the use ofblind search in all but the
most trivial of tasks. Instead methods such as means-ends analysis (Newell and
Simon, 1972), heuristic searches like A* (Nilsson, 1980), and schematic problem
solving are used. This chapter will concentrate on the last alternative.
Schematic problem solving involvesthe use ofschemata to augment the set of
operators the system can use. Schemata are prestored, canned solutions to general
problems. These solutions are sequences ofthe original operators. The hope is to
simplify greatly the search space by the use ofonly a few (oftenjust one or two) of
these schemata or macro-operators to solve the problem.
Consider figures 19-2 and 19-3. Figure 19-2 represents the same problem as
figure 19-1, buthere it issimplifiedtoshowonlythesolutionpaththrough the search
tree. Suppose the system has a schema Sch 1 composed of a sequence of original
operators Oph Op 2, Op h Op 3. This schema solves the problem immediately. As
figure 19-3 shows, the depth ofthe search tree isjust one.
Schematicproblemsolverscanberelativelyuntroubledby richdomains. However, twonewdifficultiesarc raisedthatstem fromthegreat numberofschematathat
must be apart ofthe system. The first isthe schemaselection problem. Little will be
said about this here; the problem has been discussed elsewhere (Minsky, 1974:
Charniak, 1977; Schank and Abelson, 1977; DeJong. 1979; LebowitZ, L980). The
--- PAGE 587 ---
--- PAGE 588 ---
574 CHAPTER 19: LEARNING FROM OBSERVATION
Op,
Figure 19-2: Solutionusingoperator.
Schl
Figure 19-3: Solutionusingschemata.
seconddifficultyis howtogetanextensiveandusefulsetofschemataintothesystem.
This is the problem addressed in the remainderofthis chapter.
19.1.2 Constraint-Based Learning
Suppose we have some initial state /that must be transformed intoa goal state
The system must select a schema that is capable ofthe desired transformation. If
no single schema transforms I to G then the system must combine a few schemata
sequentially to do the job. However, the system must not be permitted to look for
sequences o! arbitrary length. Unconstrained searching must be avoided since this
would reducetheschemasystemtoasearchproblemsolverwith itsconcomitant inefficiencies in rich domains.
--- PAGE 589 ---
DEJONG 575
Now assume that G can be achieved from / only through the application of
manyschematainsequence; thatis, nosingleschemanoranyshortsequenceofschemata can transform /into G. In this case the problem is intractable for the system
eventhoughthere is, inprinciple, a solution. In rich domains such intractableproblemsare, forallpracticalpurposes,justasinsolubleasimpossibleproblems. Without
theappropriateschemata, searchingforasolution isabitlikethemonkey recreating
Shakespeare's Hamletfrom random keystrokes on atypewriter.
All is not lost, however. True, the system cannot solve the problem oftransforming /to G, but it can verify (and in some sense "understand") a solution presentedtoit. Thesystemhasknowledgeofalltheoperatorsnecessaryforthesolution.
It can, when given a solution, verify that all the preconditions for each operator
are satisfied and that each operator application yields the desired effects. The time
complexity of this verification is polynomial in n (linear in n with a reasonable
simplifying assumption), where n is the number of operator steps in the solution
transformation.
Verification ofa proposed solution involves two parts. First, the sequence of
operatorsmustbeviable; thatis, atthetimeofapplicationofeachoperatorallofthe
preconditionsforthatoperatormustbesatisfied. Second, theresultingstateafterthe
application ofthe last operatorofthe sequence mustbedemonstratedtocontain the
goal state.
A state is madeupofacollectionoffeatures (orpropositions). Eachpreconditioncanalsobeviewedasarequiredfeature (orproposition). Itwillbeassumedthat
thecostofcheckingwhetherapreconditionissatisfiedinastateisproportionaltothe
number offeatures in that state.
LetPj be the numberofpreconditions forthe7th operator,
a be the number offeatures added by the7th operator,
Tjbe the numberoffeatures removed by they'th operator,
Ebe the numberoffeatures in the initial state.
The cost Cofverifying the viability ofthe sequence is
C-k1%Pj(e + *2 to " n)) (l)
whereATisaconstantofproportionality. Up isthelargestp andaisthelargestaj, we
have
c<*5>(W5«)
(2)
;-1 \ /-l /
Simplifying, the cost ofdemonstrating the viability ofa solution is
C < KpEn + Kpa "(n " 1} (3)
--- PAGE 590 ---
576 CHAPTER 19: LEARNING FROM OBSERVATION
Testing that the goal state is satisfied by the final state is at worst linear in n.
The numberoffeatures in the final state is no greater than
E + J£ a or E + na (4)
Each feature of the goal must be checked against this state. However, since the
numberoffeaturescomposingthegoal stateisindependentofn, thecostis atmosta
constant multiplied by expression (4).
Ifitisassumedthateachoperatordeletesontheaverageaboutasmanyfeatures
froma state as itadds, thenthe inner summation in (1) approacheszeroandthe cost
becomes 0(n); thecostoftestingthegoal state againstthe final statebecomes independentofn. 0(n) isalsoatrivial lowerbound sinceone mustatleastconsidereach
operatorto verify the sequence.
Thus, understandingasolution isveryefficienteven iftheoriginal problem is
intractable forthe system.
Once a solution is verified it can be used by the system to solve the same
problemshoulditoccuragain. Todothisthesystemmustfileawaytheverifiedsolution indexed by the initial and final states. When the system is presented with the
same problem it can simply retrieve and execute the stored solution as illustrated in
figure 19-3. Such a stored structure mightbeconsideredthebeginning ofa schema.
However, more things are possible than just solving very similar problems.
Thenewverifiedsolutioncanbegeneralizedtoapplytoabroadclassofsimilarproblems. Generalizationcanbedrivenbyinspectionoftheverifiedexampleusingworld
knowledge about the component operators. This process is called constraint-based
generalization because it isdrivenby thepreservationoftheunderlying verification
steps composing the explanation of why the solution worked in the first place. A
schema(orsetofschemata)isthenconstructedtorememberthegeneralizationandto
serve as an organization point for retrieval in laterproblem solving.
Briefly, the process ofexplanatory schema acquisition can be summarized as
(1) verifying or "understanding" the input, (2) deciding whether this example warrantsgeneralization,and(3)ifso, generalizingtheinputtoanewschema. Theverificationphasecanbe more involvedthanpreviously indicated. Inparticular, complete
solutionsare seldomapparentfromtheobservationofthe problem-solving behavior
ofothers. Generally only a few overt actions are apparent. To complete the solution
the system must infer many missing steps and fill in causal inferences. The domain
largelydetermineshowdifficultthe inferenceprocess is. Inasimplified robot world
nearly all important state changes can be observed, and therefore, very few inferencesneedtobeadded. However, inanatural languagetaskdomainthesystemmight
have to hypothesize missing but implied actions and infer mental and other internal
statesofcharacters. Itisessentialthattherepresentationofthesolutionbeaugmented
with these missing inferences; without them the explanation would be incomplete
and no generalization would be possible. Although the inference problem is indeed
--- PAGE 591 ---
DEJONG 577
difficult, it is common to all natural language systems and has been addressed elsewhere (Schmidt and Sridharan, 1977; Cullingford, 1978; DeJong, 1979; Kolodner,
1980; Lebowitz, 1980; Granger, 1980).
Theunderstandingphaseisnotsignificantlydifferentfrommostplanning-type
understanders, withoneexception: theunderstandermustmaintaindatadependency
supports (Fikes, 1975; Doyle, 1979) foralltheinferencesnecessarytomakesenseof
theinput. Thisdependencynetworkdrivesthegeneralizationprocess. Theparticular
input representation is generalized as far as possible without violating the dependency support structure. Violating the dependency support structure would undermine the solution's verification (i.e., the reasons the system has for believing the
input makes sense).
Generalizationcanbe done ina numberofdifferentways. The particulargeneralizationconstructedisdeterminedbyfittingtheinputintoataxonomyofgeneralization techniques. In the next section this taxonomy ofgeneralization is presented,
followed by a briefdiscussion ofwhen to generalize. Finally, an example is given
demonstrating a working ESA system.
19.2 TAXONOMY OF GENERALIZATIONS
Therearefoursituationsthatwhenrecognizedinthetexteitherindividuallyor
incombination oughtto invoke the generalization routines. They are the following:
Schema Composition
Secondary Effect Elevation
Schema Alteration
Volitionalization
In the first part ofthis section each of these situations will be illustrated with an
example.
19.2.1 Schema Composition
The first situation is called schema composition. Basically, it involves composingknownschematainanovelway. Thiswilltypicallyinvolveaprimaryschema,
essentiallyunchanged, withoneormoreofitspreconditions satisfied inanovel way
by other known schemata.
To clarify the procedure let us consider an example. This example is a story
about a kidnapping. Let us assume that we, the readers ofthis example, do not yet
have a schema for kidnapping or extortion or any similar notion. However, the
knowledgeofaconsiderablequantityofbackgroundinformationaboutstealing, bargaining, the use ofnormal physical objects, and goals ofpeople and institutions is
assumed.
--- PAGE 592 ---
578 CHAPTER 19: LEARNING FROM OBSERVATION
ParispolicedisclosedTuesday thataman who identified himselfasJean Maraneauxabductedthetwelve-year-olddaughterofawealthy Parisianbusinessman
Michel Boullardlatelastweek. Boullard receivedalettercontainingasnapshot
ofthe kidnapped girl. The nextday he received atelegram demanding thatone
millionfrancsbeleftinalobby wastebasketofthecrowdedPompidouCenterin
exchangeforthegirl Askingthatthepolicenotintervene,Boullardarrangedfor
thedelivery ofthe money. Hisdaughter was found wandering blindfolded with
herhandsbound nearhisdowntownofficeon Monday.
In this story the primary schema is BARGAIN, a schema that we assumed the
systemalready knew. Oneofthepreconditions specifiedinthe BARGAIN schemais
thatbothpartiestothebargainmustconvinceeachotherthattheycanindeeddeliver
their sides ofthe bargain. For Maraneaux, this corresponds to making Boullard
believe that he (Maraneaux) has control of Boullard's daughter and can therefore
relinquish the girl to him. Maraneaux achieves this by actually establishing control
overthedaughter(viaaninstanceoftheSTEALschema)andthensendingBoullarda
photograph. Tothe system, this is anovel way to satisfy BARGAIN'S preconditions.
19.2.2 Secondary Effect Elevation
Considerthe following scenario:
FredwantedtodateonlySue,butSuesteadfastlyrefusedhisovertures. Fredwas
onthe vergeofgiving up whenhe saw what happened tohis friend, John: John
wanted to date Mary but she also refused. John started seeing Wilma. Mary
becamejealous,andthenexttimeheaskedher,Maryeagerlyaccepted. Fredtold
Sue he was going to make adate with Lisa.
Here Fred has used an existing schema (DATE) in a new way. This is called secondaryeffectelevation. Fred's DATE schema already contains all ofthe knowledge
necessaryforresolvinghisdilemma. TheproblemisthatthenormalDATEschemais
organized in the wrong way. In secondary effect elevation situations a new schema
canbeconstructedby modifyinganexisting schematoindicate thatthe schema may
be used to achieve a result that is normally neutral or negative.
The main purpose ofthe DATE schema is to satisfy certain recurring social
goals(likecompanionship, sex,andsoon). DATEcontainssecondaryeffectsaswell.
These are often undesirable effects accompanying the main, planned effects. For
example, one is usually monetarily poorer aftera date. Another secondary effect is
that an old girlfriend may becomejealous ofthe new date. What Fred learned from
John'sexperience isthat it isoccasionallyusefultoinvokethe DATEschemainorder
to cause one ofits secondary effects (jealousy) while completely ignoring the usual
main goal.
Just as with schema composition, the existing schema is changed to reflect a
generalization made from a specific instance. In this case, the specific instance is
John's interactions with Mary. Notice, however, that Freddid not simply copyJohn's
--- PAGE 593 ---
DEJONG 579
actions. Johnactually madeadatewithWilma, but Fredonlyexpressedan intention
todate Lisa. This is notanearth-shakingdifference, but inthecontext ofdating itis
extremely significant. Inthe normal DATE situation, expressingan intentiontodate
someone isnotnearly sosatisfyingasanactualdate. Oncedating ismodified forthe
purpose ofcausingjealousy, however, expressing an intention to date and actually
carrying it out can be equally effective.
One might argue that the distinction between main and secondary effects ofa
schema is otiose and, in situations such as this, even deleterious. After all, DATE
already had all the information necessary for solving Fred's problem. If a system
simply treats all the effects ofa schema the same, then any effect can be singled out
duringtheplanning processtobe usedasthe maingoal. There is, however, a strong
argument againstthis position. Thepossibledesiredeffects ofaschemado notexist
only withinthe schema itself. They areusedto organize and select among schemata
in both understanding and planning applications. Many effects (like feeling more
tired after a date than before) will not be used in the normal planning or understandingprocess. Iftheyaretreatedthesameaslegitimatemaingoalsthesystemwill
be swamped in a combinatorial quagmire ofundifferentiated possibilities, most of
which are wildly implausible.
Forexample, wedonotwanttheunderstandingprocesstopredictthatJohnwill
takeanapwhen itistoldthatJohndatedMary. Giventheinput "Johntookanap," the
systemoughttobeabletojustifyit. However, itoughtnotactivelypredictit. Thereis
a multiplicity ofindividual actions making up the DATE schema (each with its own
set ofeffects), and the vast majority ofthe effects from this schema (and any other
schema) are simply irrelevant to overall planning and understanding processes.
Instead, we would like the system to single out the plausible volitional effects ofits
schemata and use only those for schema organization and selection. Thus in the
example, Fred has constructed, via secondary effect elevation, a new use of the
DATE schema.
19.2.3 Schema Alteration
Schemaalterationinvolvesmodifyinganearlycorrectschemasothatitfitsthe
requirements of a new situation. The alteration process is guided by the system's
world model. This is illustrated by the following anecdote:
Ihadoccasiontoreplacetemporarilyabrokenwindowinmybackdoorwitha
plywood panel. The plywood sheet from which the panel was to be cut had a
"good"sideanda"bad" side(asdoesmostrawlumber). Thegoodsidewasreasonablysmooth, butthebadsidehadseveralrutsandknotholes. Iautomatically
examined both sides ofthe sheet (presumably as part of my SAWING or
CUTTING-A-BOARD-TO-FIT schema) and selectedthegood side to face into
thehouse, leavingthebadsidetobeexposedtotheelements. AfterIhadcutthe
panel and fitted it in place I noticed that several splinters had been torn out,
leavingrutsinthegoodside. Iimmediatelysawtheproblem. Handsawsonlycut
--- PAGE 594 ---
580 CHAPTER 19: LEARNING FROM OBSERVATION
inonedirection. Withhandsaws,thedownwardmotiondoesthecuttingandthe
upwardmotiononlyrepositionsthecuttingbladeforanotherdownwardmotion.
Ihadcutthewoodpanel withthegoodsidefacingdown. Thedownwardcutting
action has a tendency to tear splinters ofwood out ofthe lower surface ofthe
board. Sincethegoodsidewasthelowersurface, itsufferedthelossofsplinters.
IfIhadtoperformthesameactionagain,Iwouldnotmakethesamemistake.
wouldcuttheboard withthegood side facingup. However, whatI learned was
notjust a simple specialized patch to handle this particular instance of splintering. SinceI knewthecauseofthesplintering, I knewthatit wouldnotalways
be a problem: it is only a problem when (1) the lumber is prone to splintering,
(2)thereisagoodsideoftheboardthatistobepreserved,and(3)oneismakinga
crosscut(acrossthewood'sgrain) ratherthanaripcut (alongthegrain). Moreover, the solution is not alwaysto positionthe wood with the good side up. My
electricsabersaw(alsoareciprocatingsaw)cutsduringtheupwardblademotion
rather than during the downward motion. Clearly, the solution when using the
sabersaw is the opposite: toposition theboardwith thegood sidedown. Now,
these are not hard-and-fast rules: with a sheet ofplywood ofsufficiently poor
quality, splintering would likely always be a problem. Rather, these are useful
heuristics thatleadtoarefinementofthe SAWING schema.
Notethatthisrefinementtothe SAWINGschemaisfarmoregeneralthanrequiredto
handletheparticularproblemthatgaverisetoit. Therefinementcontainscontingenciesrelevanttotheuseofsabersawseventhoughnosabersawwasusedinthe immediate problem. This is possible because the refinement is driven by a world model,
notjust the problem. The SAWING schema was altered by identifying and eliminating the offending cause in the underlying knowledge-based explanation of the
phenomenon.
19.2.4 Volitionalization
This situation involves transforming a schema for which there is no planner
(likeVEHICLE-ACCIDENT, ROULETTE, etc.)intoaschemathatcanbeusedbya
planner to attain a specific goal. Considerthe following story:
Herman, whowasmarriedtoJoyforfifteenyearshadfalleninlovewithhissecretary, Heather. WhenJoy refusedtodivorcehim, Hermancutahydraulicbrake
hoseinJoy'scar. Thenexttimeshedrovedownthewindingroadtotownshelost
control ofhercarand struckatree. Thecarburst into flames.
This story describes a vehicle accident. However, unlike most vehicle accidents, thisone hasanactiveagent. The VEHICLE-ACCIDENT schema is normall\
nonvolitional; that is, it dictates what toexpect in a vehicle accident situation, but it
docs not allow planned invocation. This schema cannot be involved in problemsolving planning.
--- PAGE 595 ---
DEJONG 581
The story illustrates that an event that was previously only attributable to
accidental causes can, in fact, be controlled. Thus a volitional counterpart can be
constructed forthepreviously nonvolitional schema. Thenewvolitional schemahas
certaineffectsthatcanbelabeledas itsmaingoals, newconstraintsontheactorsand
objects,andsoon. Itis, infact, anewschemainits ownright. Furthermore, usingthe
underlying explanation ofthe story to drive the generalization process, the schema
canencompass situations significantly different fromthe one described inthe story.
Forexample, itcould handle a story abouta student weakening the steering rod ofa
professor's car after receiving a failing grade.
19.3 TO GENERALIZE OR NOT TO GENERALIZE
Therearefiveaspectstoconsiderwhenoneisdecidingwhetherornottogeneralize an input intoanew schema. By hypothesis itwillbeassumedthatthe inputdid
not match an existing schema. (If it had, then the system would have already possessedthe desired schema, and indeedthat schemawouldhavebeen usedtoprocess
the story.) Ifany ofthese five conditions does not hold, constructing a new schema
from this input is inappropriate.
The criteria are as follows:
1. Is the main goal ofa character achieved?
2. Is the goal a general one?
3. Are the resources required by the goal achiever generally available?
4. Isthisnewmethodofachievingthegoalatleastaseffectiveastheotherknown
volitional schemata to achieve this goal?
5. Does the input match one ofthe known generalizable patterns?
These criteriaaretested forall goals inthe story. The firstcriterion-Was the
goal achieved?-is self-explanatory and easily judged. The second-Is it a general
goal?-andthe third-Are the resources generally available?-require some discussion.
Noveltyaloneinanapproachtoachievingagoalisnotsufficienttowarrantthe
construction ofa new schema. Consider, forexample, the plots inthe "Mission Impossible" television series. These plots are noteworthy in that they are very novel.
Theyall usebizarremethodstoachieve ratherpeculiargoals. Furthermore, theyare
alwayssuccessful. However, thegoalsachievedarenotthetypethatariseinordinary
life, and the resources and skills needed are so specialized and uncommon that the
samesolutionwouldneverbeapplicableagain. Clearlyanewschemashouldbeconstructedonly ifthere is a reasonableexpectation that it will be helpful in futureprocessing. Ifaschemawill neverbeusedagain, itshouldnotbeconstructed inthefirst
place.
--- PAGE 596 ---
582 CHAPTER 19: LEARNING FROM OBSERVATION
Howcantheutilityofaparticulargoal bejudged?Theanswertothis isclosely
tiedto where goalscome from. Achieving agoal thatarises from general conditions
important to an individual's well-being and that uses readily available resources is
likely toresult in an interesting new schema, onethatwill ariseagainand again. For
thesolution, anaspectofSchankandAbelson'stheory ofplanning (1977) is used. In
their view themes give rise to the highest-level goals (goals that are not simply subgoalsintheachievementofothergoals). Interpersonalandlifethemesareofinterest
here. Anexample ofthe formerisahusband'soffering (andtherefore, atsomelevel,
wanting) totype aterm paper forhis overworked studentwife. Thisexemplifiesthe
theme of taking on goals of others whom one cares for. It is not necessary to go
beyond this theme in explaining the husband's actions. Examples oflife themes are
attemptingtosatisfyone'shunger,togainmoney, ortorelieveboredom. Themesgive
rise to goals that require no furtherjustification.
The fourth criterion is self-explanatory. The idea is thatthe system should not
botherconstructingschematathataremuchlessefficientthansimilaralready-known
schemata. Inanatural language inputthiswouldoccuronly ifacharacterwereusing
a highly suboptimal plan.
The fifth criterion-Does the input match one of the known generalizable
patterns?-simplystatesthattheinputmustbeidentifiedwithinthetaxonomyofgeneralizationtechniques, asgivenintheprevioussection. Thisisnecessarytoallowthe
system to bring its technique-specific generalization knowledge to bear.
AN EXAMPLE
19.4
Here an example of the first-pass implementation of the system is given. It
illustrates a new schema acquired by volitionalization. The system does not have a
natural language front end. Rather, the inputs are given in an internal conceptual
form. This is the representation that wouldbe the output ofa natural language front
end. Inputsaredenotedbylinesstartingwith "Processing- ..." Therearejustfour
inputs. Englishversionsofthe inputsaregivenbelow. Forreadability, thisoutputisa
slightlyalteredformoftheprogram'soutput. ThesystemgeneratesmanymoreGENSYMedatomsandreferstothembythe GENSYM names. TheGENSYMshavebeen
replaced with mnemonic symbols.
ENGLISHIFIED INPUT:
1)CLAUDIUS OWNED AN ISLAND ESTATE.
2)AGRIPPINA FED CLAUDIUS POISONED MUSHROOMS.
3)CLAUDIUS DIED.
4)AGRIPPINA INHERITED THE ISLAND ESTATE.
The first input:
Processing-
$P0SS
ACTOR*CLAUDIUS*
) (
OBJECT(*ESTATE*
--- PAGE 597 ---
) ) ) )
DEJONG 583
New Schema-S00001 ($POSS(ACTOR*CLAUDIUS*) (OBJECT*ESTATE*)
Setting bindings and links
Thefirstproposition is input. The internal story representation is initiallyempty. Thus, noprocessingisrequiredtointegratethisinput. However, an instance ofthe $POSS schema is created. It is called S00001.
Pointersto *CLAUDIUS*and *ESTATE*areestablished. Theseare the
variables, orbindings, ofthenewinstanceofthePOSSess schema.
New Schema-S00002 ($DESIRE-FOR-MATERIAL-THINGS
(ACTOR*CLAUDIUS*)
(OBJECT*ESTATE*))
Setting bindings and links
A new schema S00002, which is an instance of$DESIRE-FORMATERIAL-THINGS, is createdandactivatedin a bottom-up response
tothefirstinput. Thisschema representstheknowledgethatpeopleusuallyactso as topreserve theirpossessions. Links are constructedto tie
this instance toschema S00001.
The second input:
Processing ($FEED(ACTOR*AGRIPPINA*)
(0BJECT*P0IS0N*TYPE*MUSHR00M*)
New Schema-S00003 $FEED ACTOR*AGRIPPINA* OBJECT*P0IS0N*
( ( (
(TO*CLAUDIUS*))
Setting bindings and links
New Schema-S00005 $EAT ACTOR*CLAUDIUS* OBJECT*P0IS0N*
( ( ) (
Activated by S00003
Setting bindings and links
New Schema-S00006($NAIVE-P0IS0N(ACTOR*AGRIPPINA*)
(INSTRUMENT*MUSHROOM*)(OBJECT*CLAUDIUS*)
Activated by S00003
Setting bindings and links
SEATingpoisonbringsin thepoisonschema. Wecallit NAIVE-POISON
toremindourselvesthatitisanincompleteschemacontaininglittlemore
than what is neededforthisstory.
--- PAGE 598 ---
) )) )
584 CHAPTER 19: LEARNING FROM OBSERVATION
New Schema-S00007($M0VE(ACTOR*AGRIPPINA*)
OBJECT*MUSHR00M* FROM OUTSIDE*CLAUDIUS*
( ) ( (
(T0(INSIDE*CLAUDIUS*)))
The system infers that the $NAIVE-POISON involves INGESTing an
object that must be apoison, which in turn involves aphysical MOVEment to the inside ofthe person, and that the person has some initial
$HEALTH
state.
Activated by S00006
Setting bindings and links
New Schema-S00008($B0DILY-HARM(0BJECT*CLAUDIUS*)
Activated by S00006
Setting bindings and links
New Schema
S0Q009($HEALTH(CREATURE*CLAUDIUS*) STATE(*VAR*Sl)
( )
Activated by S00008
Setting bindings and links
New Schema-S00010($INGEST(ACTOR*AGRIPPINA*) (0BJECT*P0IS0N*;
(TO*CLAUDIUS*))
Activated by S00006
Setting bindings and links
New Schema-S00011($DEATH(0BJECT*CLAUDIUS*)
Activated by S00006
Setting bindings and links
This SDEATH ispredictedby thepoisoning. POISONhas two possible
outcomes. One isthatthepersonmaysurvive, theotheristhattheperson
may die. Both are predicted, but only one is allowed to be eventually
satisfied.
Old Schema-S00009($HEALTH(CREATURE*CLAUDIUS*)
(STATE*ALIVE*))
Activated by S00011
Setting bindings and links
--- PAGE 599 ---
) )))) ) ) ) )
DEJONG 585
New Schema-S00012($HEALTH(CREATURE*CLAUDIUS*) (STATE*DEAD*)
Activated by S00011
Setting bindings and links
Ifhesurvives, Claudius mightbecome ill. This isalsopredicted.
New Schema-S00013 ($NAUSEA(ACTOR*CLAUDIUS*)
Activated by S00006
Setting bindings and links
IfClaudius dies, Agrippina, the volitional actor ofS00006 ($NAIVEPOISON)mighthavehadthisoutcomeasagoal. Thisactivatesamurder
schema.
New Schema-S000l4($NAIVE-PREMEDITATED-MURDER
(ACTOR*AGRIPPINA*) (OBJECT*CLAUDIUS*)
Activated by S00006
Setting bindings and links
Old Schema-S00011($DEATH(OBJECT*CLAUDIUS*)
Activated by S00014
Setting bindings and links
Old Schema-S00006($NAIVE-P0IS0N(ACTOR*AGRIPPINA*)
(INSTRUMENT*MUSHROOM*) (OBJECT*CLAUDIUS*)
Activated by S00014
Setting bindings and links
New Conjunction Node S00015(AND($DEATH(ACTOR*CLAUDIUS*))
(NOT($PUNISH(OBJECT*AGRIPPINA*)
The murder schema contains information that the killer willprobably
have thegoalofescapingpunishmentforthe crime.
Old Schema-S0001l($DEATH(0BJECT*CLAUDIUS*)
The third input:
Processing- $DEATH OBJECT*CLAUDIUS*
( ( )
Old Schema-S00011($DEATH(0BJECT*CLAUDIUS*)
This matches the dyingprediction. Thus, very littleprocessing needbe
donehere.
--- PAGE 600 ---
) ) ) ))) )
586 CHAPTER 19: LEARNING FROM OBSERVATION
The fourth input:
Processing-($NAIVE-INDIVIDUAL-INHERIT(HEIR*AGRIPPINA*)
(BEQUESTS*ESTATE*))
Thesystem mustjustifythatthe conditionsforINHERlTanceare indeed
fulfilled.
New Schema-S00018($NAIVE-INDIVIDUALINHERIT HEIR*AGRIPPINA*
(BEQUESTS*ESTATE*))
Setting bindings and links
Old Schema---S00001($P0SS(ACT0R*CLAUDIUS*) (OBJECT*ESTATE*)
Activated by S00018
Setting bindings and links
Old Schema---S00011($DEATH(0BJECT*CLAUDIUS*)
Activated by S00018
Setting bindings and links
New Schema-S0002l($HEALTH(CREATURE*AGRIPPINA*)
STATE*ALIVE*))
Activated by S00018
Setting bindings and links
Old Schema-S00012($HEALTH(CREATURE*CLAUDIUS*) (STATE*DEAD*)
Activated by S00018
Setting bindings and links
New Schema-S00022 $TRANSFER-OF-POSSESSION(FROM*CLAUDIUS*
(TO*AGRIPPINA*) (OBJECT*ESTATE*)
Activated by S00018
Setting bindings and links
Since the conditions are all true, the conclusions ofINHERITarc
asserted.
Old Schema-S00001($P0SS(ACT0R*CLAUDIUS*) (OBJECT*ESTATE*)
Activated by S00022
Setting bindings and links
--- PAGE 601 ---
) )) )) ) ) ) )
DEJONG 587
New Schema-S00025($P0SS(ACTOR*AGRIPPINA*) (OBJECT*ESTATE*)
Activated by S00022
Setting bindings and links
New Schema-S00026($MATERIAL-DESIRE(ACTOR*AGRIPPINA*)
(OBJECT*ESTATE*))
Activated by S00025
Setting bindings and links
Finally, thesystemhasamotivationforthemurder. Generalbackground
knowledgeaboutpeopledictatesthateveryonemaybeassumedtodesire
materialobjects. Anaction ofAgrippinahasjustresultedinAgrippina's
acquisition ofa new material object. The explanation ofhow she
achieved this result is examined. The minimal necessary conditions to
preservethevalidityoftheexplanationarepropagatedtotheinitialstate
ofthe world. A new schema is then constructed with these general
requirementsaspreconditions.
VOLITIONALIZATION TRIGGERED
NEW SCHEMA V00035:
(FORM SCHEMA
VARS
BENEFACTOR(*VAR*V00031
PRIOR-HEIRS(*VAR*V00033
BEQUESTS(*VAR*V00029
HEIR(*VAR*V00030)
ACTOR(*VAR*V00030)
OBJECT(*VAR*V00031))
(P0INT-0F-VIEW(*VAR*V00030)
(GOAL V00032)
(ACTIVATE
(V00032($P0SS(ACT0R(*VAR*V00030))
OBJECT(*VAR*V00029)
V00033
($NAIVE-INDIVIDUAL-INHERIT(BENEFACT0R(*VAR*V00031))
PRIOR-HEIRS(*VAR*V00033
BEQUESTS(*VAR*V00029
HEIR(*VAR*V00030))
V00034
($NAIVE-PREMEDITATED-MURDER ACTOR(*VAR*V00030)
0BJECT(*VAR*V00031))
COMPLEX (V00033 V00034
)) ) )
--- PAGE 602 ---
588 CHAPTER 19: LEARNING FROM OBSERVATION
19.5 CONCLUSION
The example shows a new schema acquired by the system. The form ofthis
schemaisidenticaltothosepreprogrammedintothesystem. Theonlydifferenceisin
the lack of mnemonic naming of variables. There are several problems with this
implementationthatarethe subjectofcurrentresearch. First, there is an inadequate
treatment oftime. The system assumes (whenever it cares) that the inputs are presented in the same order as the events they report. The system performs (through
schema expansion) a good deal ofcausal analysis. This also implies some temporal
orderings. However, by and large, difficult time problems do not surface for this
simple story.
A secondproblem is withvariableconstraints. Thisimplementation doesvery
little reasoningaboutobjects. Mostofitsgeneralization is performed onconstituent
events rather than the objects participating in those events. These generalizations
(such as generalizing POISON to MURDER) seemed less obvious and therefore
moreinteresting. Furthermore,therehasbeenpreviousworkongeneralizingobjects
(Fikes, Hart, and Nilsson, 1972). Finally, thereisagooddeal ofworktobedone on
goal manipulationofstorycharactersonschemarepresentation. Therepresentations
used here are ad hoc and cannoteasily handlethe next taskthe authorhas chosen (a
sequence ofkidnapping stories).
Unlike the similarity-based approaches (Fox and Reddy, 1977; Michalski,
1977; Langley, 1981) and many other knowledge-based approaches (Soloway, 1977;
Lebowitz, 1980; Schank, 1982; Michalski and Stepp, 1983), this procedure is
capable ofconstructing a schema from one input example. The procedure is not
"failure-driven" (Kolodner, 1980; Lebowitz, 1980; Schank, 1982). Nor is it primarily analogical (Winston, 1980; Carbonell, 1982). The LEX system (Mitchell,
Utgoff, and Banerji, 1983) is also a significantly different approach. In that system
concepts are acquired that arejudged to be useful through search and experimentation, not through an analysis of why an operator was useful. In ESA concepts are
formedentirelyonthebasisofthiskindofanalysis. Finally, it mustbeacknowledged
thattheknowledge-basedconceptapproachowesmuchtotheearlierworksofFikes.
Hart, and Nilsson (1972), Lenat (1976), and Soloway (1977).
ACKNOWLEDGMENTS
ThisworkwassupportedinpartbytheAirForceOfficeofScientific Research
under Grant F49620-82-K-0009 and in part by the National Science Foundation
under Grant NSF 1ST 81-20254.
The author is indebted to the members ofthe Coodinated Science Laboratory
Learning Group at the University of Illinois: Alberto Segre, Paul O'Rorke, and
Ashwin Ram. Paul O'Rorke implemented the system that performed the example.
--- PAGE 603 ---
DEJONG 589
References
Carbonell,J., "Experiential Learning inAnalogical ProblemSolving," ProceedingsoftheNCAI, Pittsburgh, Pa.,pp. 168-71, 1982.
Charniak, E., "MS. MALAPROP,aLanguageComprehensionSystem,"ProceedingsoftheFifthIJCAI,
Cambridge, Mass., pp. 1-7, 1977.
Cullingford,R., "ScriptApplication: ComputerUnderstandingofNewspaperStories," ResearchReport
No. 116, DepartmentofComputerScience, YaleUniversity, 1978.
DeJong, G., "Generalizations Based on Explanations," Proceedings ofthe Seventh IJCAI, Vancouver,
Canada, pp. 67-70, 1981.
DeJong,G., "PredictionandSubstantiation: ANewApproachtoNaturalLanguageProcessing," CognitiveScience, Vol. 3, pp. 251-73, 1979.
Doyle,J., "ATruthMaintenanceSystem,"ArtificialIntelligence, Vol. 12, No. 2, pp. 231-72, 1979.
Fikes, R., "Deductive Retrieval MechanismsforState Description Models," ProceedingsoftheFourth
IJCAI, Tiblisi,Georgia, USSR, pp. 99-106, 1975.
Fikes,R.,Hart,P.,andNilsson,N., "LearningandExecutingGeneralizedRobotPlans,"ArtificialIntelligence, Vol. 3, No. 2, pp. 251-88, 1972.
Fox, M., andReddy, R., "Knowledge-guided LearningofStructural Descriptions," Proceedingsofthe
FifthIJCAI, Cambridge, Mass., pp. 318-19, 1977.
Granger, R., "Adaptive Understanding: Correcting Erroneous Inferences," Research Report No. 171,
DepartmentofComputerScience, YaleUniversity, 1980.
Kolodner, J., "Retrieval and Organizational Strategies in Conceptual Memory: A Computer Model,"
ResearchReportNo. 187, DepartmentofComputerScience, YaleUniversity, 1980.
Langley, P., "Data-drivenDiscoveryofPhysical Laws," CognitiveScience, Vol. 5, pp. 31-54, 1981.
Lebowitz, M., "GeneralizationandMemory inanIntegratedUnderstandingSystem," ResearchReport
No. 186, DepartmentofComputerScience, YaleUniversity, 1980.
Lenat, D, "AM:AnArtificialIntelligenceApproachtoDiscoveryinMathematicsasHeuristicSearch,"
AIM-286, AILaboratory, StanfordUniversity, 1976.
Michalski, R., "ASystemofProgramsforComputer-aidedInduction: ASummary," Proceedingsofthe
FifthIJCAI, Cambridge, Mass., pp. 319-20, 1977.
Michalski,R.
andStepp,R., "AutomatedConstructionofClassifications:ConceptualClusteringVersus
NumericalTaxonomy," IEEETransactionsonPatternAnalysisandMachineIntelligence, Vol. 5,
No. 4, pp. 396-410,July 1983.
Minsky, M. "AFrameworkfortheRepresentationofKnowledge," AIReportTR-306, MIT, 1974.
Mitchell, T., Utgoff, P., and Banerji, R., "Learning by Experimentation: Acquiring and Refining
Problem-solving Heuristics," in Machine Learning: An Artificial Intelligence Approach,
R. S. Michalski,J. G. Carbonell,andT. M. Mitchell(Eds.),Tioga, PaloAlto,Calif., 1983.
Newell, A., andSimon, H., HumanProblemSolving, Prentice-Hall, EnglewoodCliffs, N. J., 1972.
Nilsson, N., PrinciplesofArtificialIntelligence, Tioga, PaloAlto, Calif., 1980.
--- PAGE 604 ---
590 CHAPTER 19: LEARNING FROM OBSERVATION
Schank, R.,DynamicMemory, CambridgeUniversity Press, Cambridge, 1982.
Schank,R.,andAbelson,R.,Scripts,Plans, Goals,andUnderstanding,Erlbaum,Hillsdale,N.J., 1977.
Schmidt, C, and Sridharan, N., "Plan Recognition Using a Hypothesize and Revise Paradigm: An
Example,"ProceedingsoftheFifthIJCAI, Cambridge, Mass., pp. 480-86, 1977.
Soloway, E., "Knowledge-directed LearningUsing Multiple LevelsofDescription," Ph.D. diss., ComputerScienceDepartment, UniversityofMassachusetts, Amherst, 1977.
Winston, P., "Learning and Reasoning by Analogy," Communications ofthe ACM, Vol. 23, No. 12,
pp. 689-702, 1980.
--- PAGE 605 ---
PART
SIX
AN EXPLORATION OF
GENERAL ASPECTS OF
LEARNING
--- PAGE 606 ---
--- PAGE 607 ---
ESCAPING BRITTLENESS:
The Possibilities of General-Purpose Learning
Algorithms Applied to Parallel Rule-Based
Systems
John H. Holland
UniversityofMichigan
Abstract
Message-passing, rule-based production systems in which many rules are
active simultaneously offer attractive possibilities for the exploitation of generalpurposemachinelearningalgorithms. Insuchsystemseachrulecanbelookedupon
as a tentative hypothesis about some aspect of the task environment, competing
againstotherplausiblehypothesesbeingentertainedatthesametime. Inthiscontext
there are two major tasks for machine learning algorithms: (1) apportionment of
credit and (2) rule discovery.
The apportionment-of-credit algorithm(s) must assign "strength" to rules on
the basis oftheirobserved usefulness tothe system. The problem is complicatedby
the difficulty ofdetermining which ofa cluster ofrules active in an early, "stagesetting" capacity has contributed to a later useful outcome (e.g., rules controlling
early moves that make possible a later triplejump in checkers). Ifstrengths can be
assigned appropriately, then they can be used to determine a rule's ability to win
against competing rules, and they can be used to determine the rule's likelihood of
beingusedasa "parent" fornew rules. Surprisingly, forcreditapportionmentalgorithmsofthebucket-brigadevariety,onecanprovefixed-pointtheoremsthatprovide
some guarantees ofan appropriate apportionment.
The task of rule discovery depends critically upon the discovery ofgood
"building blocks" for generating plausible rules (hypotheses). A parallel system
designedwith machinelearning in mind mustpermitaconstant flux ofnew rules to
--- PAGE 608 ---
594 CHAPTER 20: ESCAPING BRITTLENESS
betestedandexploitedordiscarded. Moreoverthisfluxmustnotdisturbthesystem's
behavior in taskenvironments forwhich ithas well-practiced, effectiveprocedures.
Geneticalgorithms, using the strengths as "fitnesses," offer subtle ways ofdiscoveringgoodbuildingblocks, andthereare new versionsoftheorems from mathematical genetics that enable us to understand this discovery process.
20.1 INTRODUCTION
The research that has culminated in the design of expert systems is a solid
achievement for artificial intelligence: Given appropriately restricted domains, expertsystemsdisplaythe reasonedconsiderationofalternativesthatoneexpectsofan
expert. The source ofthis success, the domain-specific character ofthe systems, is
also a source oflimitations. The systems are brittle in the sense that they respond
appropriately only innarrowdomainsand requiresubstantial human interventionto
compensate for even slight shifts in domain (see Duda and Shortliffe, 1983). This
problemofbrittlenessandwaystotemper itarethemainconcernofthischapter. The
overall theme isthatinduction isthebasic-andperhaps only-way ofmaking large
advances in this direction.
Togainaclearerideaofthescopeoftheoverallproblem, considersomeofthe
specific problems induction faces in this context. At the top ofthe list is the task of
generatingusefulwaysofcategorizinginput. Incomplexdomainsthereisaperpetual
novelty tothe input sothatexperiencecan guide futureactiononly ifthe systemdiscoversregularitiesorrecurrencesintheinput. Thecategoriesinducedmustbebroad
enoughto"cover"thelikelypossibilitiesparsimoniously; atthesametimetheymust
bespecificenoughtodistinguishsituationsrequiringdifferentbehaviors. Categories
mustbe incorporated intorulesthat "point" bothtoactionsandtoanauraofassociated categories. That is, as the categories are induced, they must be arranged in a
"tangled hierarchy" (see Fahlman, 1979) that enables the system to model its environment appropriately.
On a larger scale induction must provide plausible alternatives and changes in
thehierarchiesandmodelsbaseduponthesecategories. Inthisstructure, creditmust
be apportioned to the all-important categories that point to "stage-setting" actions
necessary forlatersuccess. Because ofthe uncertainty ofany induction, the process
must be carried out in such a way that the system can absorb new, tentative rules
withoutdestroyingcapabilitiesinwell-practicedsituations. Inallbutthesimplestsituationsacomplexcombinationofcompetingruleswillbeactivatedsothatthesystem
must select a subset ofrules that provides a coherent "picture" (model) ofthe situation. This picture in turn directs behavior and attempts at confirmation. At the
highestlevel, thesystemmustmakeeffectiveuseofmetaphorandanalogytotransfer
inferences from familiar to unfamiliar situations (a capacity only touched upon in
thischapter). The first twosectionsofthechapterwill expand upon these problems.
--- PAGE 609 ---
HOLLAND 595
Section 20.2 takes a closer look at the notions ofdomain and environment, and section 20.3 examines (informal) criteria bearing on the escape from brittleness.
The approach advocated in this chapter is based upon a class of messagepassing, rule-based systems, called classifier systems, in which large numbers of
rulescanbeactivesimultaneously. Individual rulescanbekeptsimpleand standardized because combinations of rules are used to define complex situations. This
approachresultsinbothparsimonyandflexibility, becausethesamerulecanbeused
inmanycontexts(seecriterion 1 insection20.3). Moreover, itgivesadifferentslant
to the induction task-the object becomes that of finding rules that serve well in a
variety oftasks.
All rulesareincondition/actionform. Eachconditionspecifiesthesetofmessagessatisfyingit, andeachactionspecifiesthemessagesentwhentheconditionpart
is satisfied. Because messages are kept to a standard length, it is possible to define
conditions using strings ofstandard length, and this is done in such a way that it is
simple to setthe generality ofa condition. As aconsequence defaulthierarchies are
easytogenerateanduse. Rulescanbetiedtogetherintonetworksofvariouskindsby
appropriate use oftagging. Section 20.4 describes classifier systems in detail.
Simplicity ofthe component rules also eases the tasks ofthe learning algorithms. Firstamongthesetasksisthatofratingtheusefulnessofexisting rules. This
is the task ofthe bucket-brigade algorithm; it assigns a strength to each individual
rule, modifyingthestrengthonthebasisoftherule'soverallusefulnessasthesystem
accumulatesexperience. Ineffectthe algorithmtreatseach rule as amiddleman ina
complex economy, its survival being dependent upon "making a profit" in its local
interactions. Inthelongrun, suchprofitswill recuronly iftheruleistiedintochains
ofinteractions leading to successful actions. Bucket-brigade algorithms are defined
and described in the first part ofsection 20.5.
Themostdifficultinductivetaskisthatofgeneratingplausiblenewrules. Here
thattaskiscarriedoutby ageneticalgorithm. Ituseshigh-strengthclassifiersasthe
"parents" of new classifiers to be tested under the bucket brigade. Although the
genetic algorithmactsdirectly uponthe stringsdefining classifiers, itcanbe shown
that it is implicitly searching andusingaspaceof"buildingblocks." Moreover, itis
searchingthisspaceordersofmagnitudemorerapidlythanwouldbeindicatedbythe
rateatwhich itisprocessingstrings. Rulesgeneratedbythegeneticalgorithmdonot
displacetheirparents; rathertheydisplacelow-strengthrules, enteringintocompetition with the other rules in the system. This competition gives the overall system a
gracefulwayofhandlingconflictsandtentativehypotheses. Thelatterpartofsection
20.5 describes genetic algorithms and theireffects upon classifiers.
Systems organized along these lines have been tested successfully in a variety
of contexts. For example, a poker-playing version of the system (Smith, 1980),
starting with classifiersembodying only the rulesofthegame, competed with overwhelming success againstWaterman's learning pokerplayer (Waterman and HayesRoth, 1978). RecentlyGoldberg(1983)testedasystemthat, startingwithacleanslate
--- PAGE 610 ---
596 CHAPTER 20: ESCAPING BRITTLENESS
(randomly generated classifiers), confronted a gas pipeline transmission problem
involving diurnal variation, seasonal variation, and leaks. The system generated
successful control procedures embedded in a (discovered) default hierarchy distinguishingnormaloperationfrom "leaky" operation. Additionaltestsarediscussedin
section 20.6.
DOMAINS AND ENVIRONMENTS
20.2
A closerlookattheroleofinductionbeginswithacloserlookatthedomainsthe environment-in which the system is to operate. The environment provides the
gristfortheinductivemill, therebysettingthepossibilitiesfor, andtheultimatelimitations on, the inductive process. An environment with no regularities (however
defined) offersnoopportunities forinduction. Humanexperience indicatesthat real
environments abound in regularities. The problem is to uncover and exploitthem.
This chapter will restrict itselfto environments that, implicitly or explicitly,
present problems in terms ofgoals to be attained. In this context the system "closes
theloop" throughtheenvironment, receivinginformationfromtheenvironmentand
acting upon the environment to bring about changes therein. The environment signals the solution ofa problem by feeding back a quantity calledpayoff. (This term
from game theory, chosen for its neutrality, is the cognate ofutility in economics,
errorsignalsincontroltheory,fitness ingenetics, rewardinpsychology, andsoon.)
This format cleanly exposes most ofthe difficult problems in planning and problem
solving, ranging from game playing though the design ofmobile robots to abstract
taskssuchastheproductionofacorpusofusefultheorems. Thesystemusesthestates
ofthe environment as "stepping stones" to reach goal states that feed back payoff.
The problem, simply, is togoefficiently from "here," a nongoal state, to "there," a
goal state. The subtleties underlying this simple statement increase rapidly as the
complexity ofthe state graph ofthe environment increases. One need go no further
than the game trees and simply defined goals ofchess or go to see deep subtleties;
real-world situationstypifiedbythedesignofflexible robotsorinteractive information retrieval systems offereven deeper problems.
Thesystemcanbethoughtofasreceivinginformationaboutthecurrentstateof
its environment in the form ofmessages generated by an input interface. The input
interfacetypicallyconsistsofasetoffeaturedetectors, andthe messageconsistsofa
stringoffeaturevalues. The systemsdealtwith heregenerallydonothavehigh-level
interpreters forthese messages. That is, the rules ofthe system workdirectly on the
message strings, acting on the presence or absence ofcertain bits. Whatever meanings there are, are supplied by the actions ofthe rules and, ultimately, by the effects
produced on the environment.
The contrast between this "environment-oriented" approach and a "languageoriented" approach isworth pointing up. Considerthe game ofcheckers. A languageoriented approach would use a language (symbols, grammar, etc.. based, say, on
--- PAGE 611 ---
HOLLAND 597
standardcheckersmovenotation)tospecifylegalmoves,desirableconfigurations, and
soon. The language, with an interpreterproviding propertiesofboardconfigurations
and the like, would then be used, along with deductive inference, to develop a goaloriented plan. The environment-oriented approach uses detectors (cf. the "parameters" usedbySamuel, 1959)togeneratebitstringsbasedonthecheckerboardconfiguration. These messages are processed by rules (arranged in a complex default
hierarchy; see below) to determine plans and moves. An environment-oriented
approachdoesnotexplicitlyassignabstractsymbolstoboardconfigurations, nordoes
itexplicitly search forand apply grammatical rules to such symbols.
Note that the environment-oriented approach is not more restricted in its
powersofdefinitionthanthelanguage-orientedapproach. Theultimatelimitsonthe
definitional powers of either approach are set by the input interface. The system
cannot distinguish environmental state configurations assigned identical values by
the input interface, be they symbols or feature strings. (This sets aside certain
sequentialtests, buttheargumentremainsthesameeven iftheseareused. Moreformally, the input interface groups environmental states intoequivalence classes; elements ofthe same equivalence class arethe same as faras the system is concerned.)
Allthatdefinitioncandoundereitherapproachistocategorizethedistinguishable. It
divides the distinguishable elements into two classes-those that satisfy the definition and those that do not.
If the system is computationally complete (can define any procedure) with
respect to sorting the input messages into classes, then it has reached the limits of
whatdefinition cando for it relativetodistinguishability. Stated anotherway, iftwo
systems are computationally complete with respect to input interfaces that set identical restrictionsondistinguishability, thenthe systemshavethe samelimitsontheir
powers of definition. This is true even ifone system is language-oriented and the
otherisenvironment-oriented. Theenvironment-orientedsystemsthatwillbeexamined shortly accomplish definition by a combination of conditions, tags, and
recoding (seesection20.4.3); theyarecomputationallycompleterelativetothesetof
messages produced by any input interface.
20.3 CRITERIA
This investigation ofways ofavoiding brittleness has been guided by several
informal criteriaderivedprimarily from ruminations about flexible natural systems
andconsiderationofvarious landmarks inmachinelearning. The systemsdefined in
the next section are intendedas procedural implementations ofthesecriteria, which
are as follows:
J. Recombinationandparallelism. In ordertoavoid adistinct rule foreach situation
(a "visitinggrandmother" rule, a "yellow Volkswagon with a flattire" rule, etc.), it
--- PAGE 612 ---
598 CHAPTER 20: ESCAPING BRITTLENESS
is imperative that the system's response to any situation be mediated by the concurrent activation of a set of relevant rules. By activating several elementary rules in
response to a complex set ofconditions, rather than relying on anticipation of the
overall situation by provision ofa single preformed rule, the system sets combinatoricstoworkforitratherthanagainstit. Asasimpleexample, by selectingoneeach
from ten hairlines, ten eye configurations, ten noses, ten mouths, and tenjawlines,
the system can match any one ofone hundred thousand distinct faces at the cost of
retaining only fifty elementary rules. Under this criterion, it is incumbent upon
induction and learning to search for rules that are useful "building blocks" in a
variety ofcontexts. Ifthebuildingblocks are well chosen, the system may be able to
functionwellinsituationsnotpreviouslyencountered. Forinstance, ifthesystemhas
rules categorizing and handling input messages according to the usual notions of
hooved,four-legged, and horned, it is conceivable that it would infer that a unicorn
(observed forthe first time) is herbivorous.
2. Categorization and default hierarchies. Categorization is the system's major
weaponforcombatingtheenvironment'sperpetualnovelty. Thesystemmustreadily
generatecategoriesforinputmessages, and it mustbeabletogeneratecategoriesrelevanttoitsinternalprocesses. Thesecandidatesmustbetestedrepeatedlyforusefulness and used with increasing or decreasing frequency in accord with the outcome
(see criterion 5, "Competition, confirmation, and gracefulness." below).
Moreover, theremustbesomecriterionofplausibility sothatthe system is not
overwhelmedwithpoorcandidates. Appropriatebottom-upprocedures (e.g. generalization ofinput messages) and top-down procedures (e.g., recombination ofparts
of the definitions of extant categories) can go far toward implementing this constraint. The categories generated should spontaneously arrange themselves into a
default hierarchy (much like the skeleton ofFahlman's NETL, 1979), so that details
invoke "sketches" ofthesituations, allowingtransferofinformationbetweenexperiencesactivatingsimilarsketches. (Themore rulesheld in commonbytheclustersof
rules defining two sketches, the more similar they are.) High-level interpreters for
determiningcategories shouldbe avoided wherepossible because they imposecomplex relations between syntax and semantics, greatly complicating the induction ot
categories.
3. Association. The use ofcategories as building blocks is much enhanced if. as the
categoriesdevelop, an auraofassociations with othercategories alsodevelops. Various "triggers," such as theco-occurrence ofa pairofcategories in agivenenvironmental situation, can limit the formation of associations to plausible candidates.
Associations are recorded by synchronicpointers pointers that do not impl) temporal sequence-andthesepointers mustbetested repeatedly forusefulness (see criterion 5). The generation and selection ofthe categories and pointers that serve as
building blocks are processesthat provide the system witha wide range of"structures
--- PAGE 613 ---
HOLLAND 599
thatactmuch like virtualcopies (Fahlman, 1979). Touseabiological analogy, these
virtualcopiesplaythe role of"species" fillingthe "niches" definedbythe regularities (opportunities for exploitation) uncovered in the system's experience. The
meaning ofthe virtual copies stems from the process ofcompetition and selection
that determines their emergence. This contrasts strongly with attempts to arrive at
such structures a priori, which is much like attempting to develop a taxonomy for
species without understanding their ontogeny.
4. Diaehronicpointing, models, andprediction. Although Samuel's paper (1959) is
oftencitedinmachinelearning, hisuseofmodelbuildingtosolveproblemsisalmost
always overlooked. (This may be because he modeled strategies by using linear
forms, formsthattypicallyserveonlyaslinearpatternrecognitiondevices.)Because
ofthe modelbuilding, Samuel'scheckersplayercan refine its strategy whileplaying
the game, when there is no payofffrom the environment. This greatly enhances the
system's flexibility. When a system uses a model to generate expectations orpredictions, it can use subsequent verification or falsification ofthe predictions to guide
revisions ofthe model (toward better prediction) even in the absence ofpayoff.
In the present context, the construction of a model requires that the system
include a second kind of pointer-the diaehronicpointers to indicate temporal
sequences ofcategories. In short, the system forms temporal associations. Trigger
conditionsservetorestrictthegenerationofcandidates, astheydidinthecaseofsynchronic pointers. Forexample, ifa well-established category Yconsistently follows
well-establishedcategory
XwhenthesystemmakesresponseR,
thenitisplausibleto
induceadiaehronicpointerbetweenXand Y (Notethatageneralcategory willoften
describe an environmental situation that persists over an extended period, as in the
case ofagoinghome orpursuitofpreycategory, allowing thetriggerto linkcategories well separated in time.) As in the case of synchronic pointers, the diaehronic
pointers must be subjected to continued selection for usefulness.
5. Competition, confirmation, and gracefulness. The previous criteria have
exploitedparallelismtoprovideclustersofrulesthatservebothasvirtualcopiesand
as models. Parallelism neatly sidesteps the priority issues ofone-rule-at-a-time systems but leaves open questions concerning conflict and consistency. Of all the elementary rules that are candidates for activation in a given situation, which ones get
the nod?
The foundation for an answer is set by an effective apportionment-of-credit
algorithm. Strengthsmustbeassignedtorules inaccordwiththeirpastusefulnessin
the situations in which they have been invoked. Once again Samuel (1959) leads the
way. Theproblem isoneofstrengthening stage-settingrulesthat makepossiblelater
actionsyieldingpayoff. Theexploitationofpredictionsprovidesamechanism. Letus
assume, following Samuel, that the strength ofa rule amounts to a prediction ofthe
averagepayoffthesystemwill receivelateriftherule is invokedconcurrentlyaspart
--- PAGE 614 ---
600 CHAPTER 20: ESCAPING BRITTLENESS
ofa cluster. Assume furtherthat a second rule is coupled to the given rule by a diachronicpointer. Ifthissecondrulehasastrength(prediction)verydifferentfromthat
ofthe first rule, then the strength ofthe first rule can be revised to bring it into line
with the later prediction (see discussion in Samuel, 1959, and the definition ofthe
bucket-brigade algorithm in section 20.5). When the system has such an algorithm
for revising strengths, then the invocation ofrules can be decided by a competition
based on strength and the degree to which the rule's conditions are satisfied by the
current situation.
Ineffectthe various rules heldby the system are treated as competing hypotheses. Thewinnersarethesystem'sestimateofthecurrentsituation. Itiscriticaltothe
system's performance and flexibility that its rules represent a wide range of competing, even conflicting, hypotheses. The competition replaces a criterion ofglobal
consistency-acriterionthatisinfeasibleforanyverylargesystemofrules-withone
ofprogressiveconfirmationundertheapportionment-of-credit algorithm. With this
outlook, rules that consistently make poor predictions when invoked have their
strength steadily decreased tothe pointthatthey are displaced by newercandidates.
Thenewercandidates must inturncompete, usually doing well in "niches" notwell
handledby rules already inthe system. The combination ofcompetition and confirmation contributes to the system's gracefulness: Large numbers ofnew candidates
can be injected without disturbing performance in well-practiced domains.
20.4 CLASSIFIER SYSTEMS
20.4.1 Overview
Classifiersystemsaregeneral-purposeprogrammingsystemsdesignedtomeet
the objectives and criteria set forth in sections 20.2 and 20.3. They have been designedfromtheoutsettobeamenabletomodificationbylearningalgorithms. Particularattentionhasbeengiventoquestionsofgracefulnessandtotheprovisionof"natural" building blocks. The systems have already been tested in a variety ofcontexts
(see section 20.6).
Classifier systems have many affinities to the rule-based (production system)
approach to expert systems (see, for example, Davis and King, 1977. or Waterman
and Hayes-Roth, 1978) but with the following majordifferences:
1. Any number ofrules, called classifiers, can be active at the same time. There
canbenodirectconflictbetweenclassifiersbecausetheonly action ofaclassifieristopostamessagetoaglobal messagelist-themoreclassifiersacthated,
themore messagesonthe message list. The resultingconflict-freeconcurrency
sidestepsthedifficultconflict resolution problems ofonc-rule-at-a-time systems
(see McDermott and Forgy, 1978), allowing the system to use man) rules concurrently to summarize and act upon a situation. The rules become building
--- PAGE 615 ---
HOLLAND
blocks that can be combinedto handle a wide variety ofsituations. Moreover,
theparallelismmakes iteasiertospecifyandcontroltheparallelprocessesthat
pervade the real world.
2. Messages are strings offixed lengthkovera fixedalphabet, takentobe {1,0}
in the definitions that follow. Classifiers, as is usual with production systems,
consist ofa condition part and an action part, but the conditions are all specified by strings oflength k over the alphabet {1,0,#}. With this provision it is
possibletouseasimplematchingoperatortotestwhetherornotsomemessage
satisfies a condition. From the architectural viewpoint, the fixed lengths
encourage organizations exploiting fixed-length registers, an important consideration in simulations or physical realizations.
Whentheconditionpartofaclassifierissatisfiedbysomemessageonthemessage list, the action part uses the message to form a new message, which is
posted on the new message list. Thus the basic procedure ofthe system is a
simple loop in which all classifiers access thecurrentmessage list, determine
iftheirconditionshavebeen satisfied, and ifso, post messages tothe message
listforthe nexttime-step. As mentionedearlier, any numberofclassifiers can
beactive simultaneously without conflict, because actions only add messages
to the new message list.
4. Allexternal communication (inputand output) is viamessagestothe message
list. As a result, all internal control information and external communication
reside in the same data structure.
5. Becausetheorderinwhichclassifiersareexecutedis independentoftheorder
in which the classifiers are stored, and because satisfaction of conditions is
determinedbyasimplematchingoperation, thereisnoneedforaninterpreter.
This makes itpossible to design local syntactic operators that modify systems
of classifiers ("programs") in useful ways, something difficult to do for
standardlanguagesorproductionsystemsbutveryimportantifthesystemisto
be modified by learning algorithms orexpert advice.
6. Becauseoftheglobalnatureofthemessagelist, taggingandrelatedtechniques
becomeefficientwaysof"coupling" classifiers, forcingpredeterminedexecution sequences, and so on. The combination ofconcurrency and a global list
avoids the limitations on tagging discussed by Davis and King (1977) in their
review ofproduction systems.
20.4.2 Definition of the Basic Elements: Classifiers and Messages
Classifiers have the same role in classifier systems that instructions have in
computer language. They are called classifiers because they can be used to classify
messages into general sets, but they are broader in concept and application than
this name would indicate, providing both processing and recoding. The message
--- PAGE 616 ---
602 CHAPTER 20: ESCAPING BRITTLENESS
specified by theaction part oftheclassifierchangesthe internal state ofthe system,
thereby influencinglateraction, and it maycauseexternal (effector)action. Provided
withsomesimplemessage-processingcapabilities, classifierscancarryoutarbitrary
operationson messages, including recursions. Itfollowsthatthereareclassifiersystems that are computationally universal.
Themajortechnicalhurdleinimplementingamessage-processingversionofa
productionsystemisthatofprovidingasimplewayofdefiningconditionsintermsof
messages. Each condition specifies a subset ofthe set ofall possible messages-the
set ofmessages that satisfies the condition. There is no simple and compact way of
specifying an arbitrary subset ofa large set; that is, most subsets must be specified
element by element. Nevertheless there is one large and important class ofsubsets
thatcan be simply specified, and any othersubsetcanbedefined as aunion ofthese
subsets. Each subset in this special class is specified by a string oflength koverthe
three-letteralphabet {1,0,#}. (Recallthatmessages, forpresentpurposes, arestrings
oflengthkoverthealphabet {1,0}.) The ft symbolplaystherolehereofa "don'tcare*7
inthesensethatwhereveraft occursinthespecifyingstringonecansubstituteeither
a 1 ora andstillhaveamemberofthesubset. Forexample,thestring 11 . . . 1ftspecifiesthesubsetofexactlytwoelements, namely, themessages {11 ... 11, 11 ... 10},
and the string \tttt . . . tttt specifies the subset consisting ofall messages that start
with a 1.
More formally, let
<S], 5 2, . . . , Sj, . . . Sk>, SjE{\,0,tt}
be a string ofk symbols specifying a subset, and let
m m G{
<mi, ra2, . . . , k>, y 1,0}
be ak-b\i message. The message belongs to the specified subsetjust in case
= = =
1. ifSj 1 orSj 0, then w, Sj
2. ifSj = ft, then m } can be either 1 or0.
Thesubsetconsistsofall messages satisfyingthisrequirement; that is, each subset is
a hyperplane in the space ofmessages.
In this notation, classifier conditions are specified using strings of length k
over the alphabet {1,0,#}. We extend the notation by allowing the string to be prefixed by a minus sign (-), with the intended interpretation that the prefixed condition is satisfied only ifno message ofthe given subset is present on the message list.
That is, ifstring cspecifiessubsetSofthesetofall messages, thecondition - cisnot
satisfiedjust in case the message list contains a message belonging to S. Combinations of classifiers can be used to implement conditions over arbitrary subsets in
much the same way that AND, OR, and NOT can be combined to yield arbitrary
Boolean functions (see section 20.4.3).
--- PAGE 617 ---
HOLLAND 603
Whentheconditionpartofaclassifierissatisfied, itproducesamessagespecifiedby itsaction (ormessage specification) part. The actionpart, like the condition
part, isspecifiedbyastringoflengthkthatcontainsthe#symbol, butthe#hasadifferent meaning. Now itplaysthe role ofa "pass through" inthe sense that wherever
the #symbol occurs inthe actionpart, thecorrespondingbit inamessage satisfying
the condition part is passed through into the outgoing message. For example, consider the message specification 11 . . . 1# in the action part ofthe classifier, and
assumethe message00 ... 00 satisfiesthe condition partofthe classifier. Then the
outgoing message will be 11 . . . 10.
More formally, let
<au a 2, . . . , ah . . . a k>, aj G {1,0,#}
be a string ofk symbols in the action part ofa classifier, and let
<m 1? m 2, . . . , rtij, . . . , m k>, rrijG {1,0}
be a message satisfying the condition part ofthe classifier. Then the outgoing message, atpositionj, has value
1. a }, ifcij = 1 or
2. nij, ifcij #.
Inbrief, ifamessagesatisfiestheconditionofaclassifier, anewmessageisgenerated
fromtheactionportionoftheclassifierby usingthe l'sand0'softheactionpartand
passing through bits ofthe satisfying message at the pass through positions ofthe
action part.
Itisusefultogeneralizethe notionofaclassifiertoallow an arbitrary number
ofconditions. Condition i ofan r-condition classifier C is specifiedby a string c, of
lengthkoverthe symbols {1,0,#} , possiblyprefixedby a - ; the actionpart is specifiedbyasinglestringaofalengthkoverthesymbols {1,0,#}. Notationally, theconditionsintheconditionpartareseparatedby "," andtheactionpartisseparatedfrom
theconditionpartby "/". Thusthespecificationofanr-conditionclassifierwillhave
the form
c,, c 2, . . . , c r la.
The condition part ofthe classifier C is satisfied ifeach condition c, is satisfied by
somemessageM,onthecurrentmessagelist. Whentheclassifierissatisfied, anoutgoingmessageM* isgeneratedasbeforeusingthemessageM,satisfyingconditionc
andtheactionparta. Ateachpositionwhereahasabit or 1, A/*getsthatbit; ateach
position where a has a pass through #, M* gets the corresponding bit ofM,.
These definitions are sufficient to define the basic elements of a classifier
system. A classifier system consists ofa list ofclassifiers {C\, C 2, . . . , C N}, a
--- PAGE 618 ---
604 CHAPTER 20: ESCAPING BRITTLENESS
messagelist, an input interface, andanoutput interface. Thebasicexecution cycleof
this system proceeds as follows:
1. Place all messages from the input interface on the current message list.
2. Compare all messages to all conditions and record all matches.
3. Foreach match generate a message forthe new message list.
4. Replace the current message list by the new message list.
5. Process the new message list through the output interface to produce system
output.
6. Return to step 1.
A classifier system may be augmented by algorithms for "look-ahead," inference, and learning. Several methods for doing these things will be described in the
next two subsections and in section 20.5. For some ofthese, weights are associated
with classifiers and messages, and wherever a match is made a record is kept ofthe
classifierthatissatisfiedandofthecombinationsofmessagesthat satisfieditsothat
theseweightscanbemodifiedperiodically. Suchenrichedsystemswillstillbecalled
classifier systems unless there is some distinction to be pointed up by using a
different name.
Because matching messagesagainstconditions isa simple process, the central
loop ofthe process (step 2 above) proceeds rapidly even on standard von Neumann
architectures. One simulation currently in operation executes a time-step involving
256conditionsandthirty-twomessagesinlessthan0.1 second. Parallelarchitectures
offer speedups in proportion to the parallelism.
20.4.3 Tagging and Networks
Pointers, action sequences, and otherprocessesdependent upon "addressing"
are attained using tags to couple classifiers.
A classifier C 2 is coupled toa classifier C\ ifsome condition ofC : is satisfied
by the message(s) generated by theactionpartofC, Note that aclassifierwith very
specificconditions(few#'s) will typicallybecoupledtoonly a fewotherclassifiers,
andaclassifierwithverygeneralconditions(many#'s)willbecoupledtoman) other
classifiers. When used to implement part of a "semantic network" or neural network, a classifier with very specific conditions has few incoming branches, and a
classifier with very general conditions has many incoming branches.
Tags are a simple way ofproviding coupling. For example, any message with
theprefix 1101 will satisfyaconditionofthe form 1101#. #. soaclassifierwiththis
. .
condition ineffecthasanaddress: tosenda messagetothisclassifieroneemplo)sthe
prefix 1101. Sincebbitsyield2*tagsandtagscanbeplacedanywhere inacondition, a
great number ofconditions can be addressed uniquely.
By using appropriate prefixes, one can define a classifier that attends to a specific set ofclassifiers. Consider a pair ofclassifiers C\ and C that send messages
--- PAGE 619 ---
HOLLAND 605
taggedwith 1101 and 1001, respectively. A classifierwiththecondition 1101## ##
. . .
willattendonlyto C]
whereasaclassifierwiththeconfiguration 1#01##
. . .
##will
attend to both C\ and C 2. Using a combination ofpass throughs (#'s in the action
parts)andrecodings(inwhichtheprefixontheoutgoingmessagediffersfromthatof
thesatisfyingmessages), onecancircumvent, usuallywithlittleeffort, thelimitation
that the conditions be hyperplanes in the message space.
Boolean compounds ofconditions-and hence the specification ofconditions
satisfiedbyarbitrarilychosensubsetsofmessages-arereadilyachievedbytags. An
ANDconditionisexpressedbyasingle multico
nditionclassifiersuchasM\
, 2
IM,
for is only added to the list ifboth M\ and M2 are satisfie M d. An OR-condition is
expressed by a M set ofclassifiers such as {M\IM; 2 IM} , for is added tothe list if
either M\ or 2 is satisfied. With these primitives, any Boolean form can be
expressed by a set ofclassifiers. For example,
(M, 2) V [(M 3 &(-Af 4)]
is achieved by the classifiers
M M
{M IM ,-M/M
l9 2 5 ; 3 4 5}.
The judicious use of #'s and recoding again reduces the number of classifiers
required whenthe Booleanexpressions are complex. By assigning tags to the input,
internal, andoutputstatesofafinitesystem, onecanrealizearbitrary statetransition
diagrams.
The use oftags to couple classifiers forpurposes ofcontrol and sequencing is
illustratedindetailinthenextsubsection. Theexamplealsoillustratestheuseofconcurrency and distributed control in classifier systems.
20.4.4 A Simple Classifier Control System
Figure 20-1 gives the schematic of a simple control routine for a classifier
system operating in a two-dimensional environment. When there is an object of a
specifiedtype anywhere inthe system's fieldofvision, this classifier routine acts to
bring the system next to the object and hold it there.
The environmentcontains objectsdistributedoveraplanarsurface. The input
interface produces amessage foran object inthe field ofvision. This message indicates the relative position ofthe object in the field ofvision (left-of-center, center,
right-of-center) and whether it is distant from or adjacent to the system. The classifiersprocessthis informationand issuecommandstothe output interface (ROTATE
VISION VECTOR [LEFT, RIGHT], COUPLE MOTION VECTOR TO VISION
VECTOR, MOVEFORWARD, STOP). Thecontrolroutineproceedsbystages, first
centering the object, then aligning the direction ofmotion to that vision direction,
next moving forward in that direction, and finally stopping when adjacent to the
object. The operations ofthe system take place over successive execution cycles or
time-steps.
--- PAGE 620 ---
606 CHAPTER 20: ESCAPING BRITTLENESS
New Messages
Generatedby
Matches
SJj
Q.&
:3 ,C C/O
--- PAGE 621 ---
HOLLAND 607
To define the classifier system, one first defines the input messages, then the
conditionpartsofclassifiers, andthentheactionpartsofclassifiers. Eachoftheseis
16 bits long, though the presentexample is based on actual simulation in which they
are 32 bits long.
Theleftmostbitofamessageisatag, 1 foraninputmessageand foranyother
kind ofmessage. The next 12 bits ofan input message specify the properties ofan
object. (Note that these 12 bits can be used for entirely different purposes for messages with initial tag 0.) There are twelve independent properties, with 1 indicating
thepresenceand indicatingtheabsenceofapropertyinanobject. Forconcreteness
we will stipulate that the system is searching for objects-goal objects-that satisfy
thecondition#111000#########. Thatis, itissearchingforobjectsthathavethefirst
three properties and lack the next three, whether or not they have the remaining six
properties.
Thelast3 bits inaninterface messagegive informationabouttherelativeposition ofthe object in the field ofvision. They are interpreted as follows:
bits 14, 15: 1,0 object left-of-center
0,1 object right-of-center
0,0 object centered
bit 16: 1 object adjacent
object not adjacent.
Thus, the message 11110001 01011100 indicates the presence in the visual field ofa
goalobjectthatisleft-ofcenterandnotadjacent, onlytheunderlinedbitsbeingrelevant to this interpretation.
Classifierconditions will be abbreviated as follows:
x = desired objectx is present in the field ofvision
c object is centered
/ object is left-of-center
r object is right-of-center
a = object is adjacent
- a = object is not adjacent
Following these conventions, [x,l,-a] specifies the condition 1111000# #####100,
and so on.
The action part ofeach classifier specifies a 16-bit message issued when the
conditions ofthe classifierare satisfied. Each such message will simply be abbreviated as the corresponding 16-bit integer. That is, "[4]" abbreviates the message
0000000000000100, the tag atthe firstposition indicatingthatthis isnotan input
message.
Theclassifierroutinecontrolsthreeeffectors: aneffectortomovethedirection
ofvision incrementally (15 degrees in the simulation) to the left or right, a second
effector to set the direction ofmotion parallel to the direction ofvision, and a third
--- PAGE 622 ---
608 CHAPTER 20: ESCAPING BRITTLENESS
effectortocausethesystemtomoveforwardoneunitinthedirectionofmotion. Ifno
command is issuedtoagiveneffectorduringanexecutioncycle, thateffectorretains
its last setting. In presenting theactioneffectedby messagestoeffectors we will use
L= rotate vision vector 15 degrees to the left
R= rotate vision vector 15 degrees to the right
P= set the move vector parallel to the vision vector
G= move one unit forward in the move vectordirection
Therearenineclassifiersinthis illustrativesystem. Thefirstfourleadtooperationsbytheremainingfive,thenextthreecauseoutputactions, theeighthcausesthe
system to halt, and the ninth will be explained shortly.
CI [*,l]/[4]
C2 [x,r]/[5]
C3 [x,c,-a]/[6]
C4 [x,c,a]/[l]
C5 [4]/[8] [8] causes effector actionL
C6 [5]/[9] [9] causes effector action R
CI [6]/[10] [10] causes effector actions Pand G
C8 [7]/[ll] [11] causes the cycling to halt
C9 [4or5or6or7]/[0]
(Note that the condition [4or5or6or7] is specified by the string 00000000
000001##.)
Ifan object ofthe desired typexappears at the far left ofthe field ofvision at
executioncyclet, classifierC\ isactivated, placingmessage [4] onthemessagelistat
cycle t + 1. Assuming the objectx is still left-of-center, the classifiers CI, C5, and
C9becomeactiveatcyclet + 1 andthemessagelistconsistsof4messages: [4], [8],
[0], andthe message fromthe input interface. Thislistofmessagescontinues until.v
iscentered as a result ofthe repetitions oftheLcommand, whereupon C3 would be
activated, and so on (see table 20-1).
Notethatmessage [4] providesarecordingofthemessagefromthe input interface, coupling this information to the classifier C5 ([4]/[8]), which causes effector
action L. Any message [m] could have been used for this purpose; for example, the
pairofclassifiers [x,1\l\m] and [w]/[8] wouldhave produced the sameactionL. It is
this "internal" rccoding that permits the classifier systems to carry out arbitrary
computations, so that formally speaking classifier languages are computationally
complete.
In detail, the execution sequence ofthe classifiersystem proceeds as shown in
table20-1. It isclearthattheclassifier[4or5or6or7]/[0] playsnorole inthisexample.
It is inserted to illustrate the concept ofasupportclassifier, which is useful when the
bucket-brigade algorithm (see section 20.5) is incorporated into this classifier
--- PAGE 623 ---
) )
HOLLAND 609
Table20-1: Exampleofatypical execution sequence.
MajorCycle Active
(Time) Classifiers MessageList
CI 11110001 10000100
[4]
t + 1 CI, C5, C9 11110001 10000100
[4]
[8]
[0]
t + 2 CI, C5, C9 1111000 1 10000100
[4]
[8]
[0]
(t + cisthetimeatwhichobjectxisfirstcentered.)
t + c C3, C9 11110001 10000000
[6]
[0]
t + c + 1 C3, C7, C9 11110001 10000000
[6]
[10]
[0]
(t + aisthetimeatwhichthesystemisfirstadjacenttoobjectx.
t + a C4, C9 11110001 10000001
[7]
[0]
t + a + 1 C4, C8. C9 11110001 10000001
[7]
[11]
[0]
(Thesystemhasnowhaltedadjacenttoobjectx.
system. In that case the classifier [4or5or6or7]/[0] serves to reinforce the whole set
ofclassifiers. With further additions such a classifier can be used to call the whole
routine when an objectx appears.
20.4.5 Use of Classifiers to Define Complex Entities and Hierarchies
The introductiontothischaptermadethepointthattheultimatelimitationson
definition are no greater for the environment-oriented approach than for the
--- PAGE 624 ---
610 CHAPTER 20: ESCAPING BRITTLENESS
language-orientedapproach, thatlimitbeing setbydistinguishability. Section20.4.3
showed thatclustersofcoupledclassifierscan bearrangedto respondto any chosen
subsetofthe setofpossiblemessages. Because messagesarethe unifyingelementof
classifier systems-providing internal communication as well as communication
fromtheenvironment-thiscapabilityprovidesbroadpowersofdefinition. There is
not room here for a detailed exposition, but the possibility of defining objects
involvingcomplexcombinationsofcategoriesandrelations-Winston'sdefinitionof
an arch (1975) is a simple example-can at least be made plausible.
First, networklike interactions ofcoupled classifiers, through the use oftags
and conditions ofvarying generality, have already been discussed (section 20.4.3).
When the condition part ofa rule is satisfied and it is coupled into such an array, it
acts by pointing to other classifiers that are to have their condition parts tested in
turn. Thatis, theoutgoingmessageistaggedsothatitisattendedtobytheclassifiers
to which the rule is to be coupled. This operation is quite analogous to passing a
markeroveralinkin NETL(Fahlman, 1979)ortomovingdownoneofthelinksina
linked list.
Because messages are involved, notjust markers, a great deal ofinformation
canbecarriedfrompointtopointinthenetwork. Forinstance, thetag ofamessage
can indicate its point oforigin, and other bits carry information passed through or
recoded(seesection20.4.2). Becauseoftheparallelismofclassifiersystems, clusters
ofcoactive rules canbe usedtodefine categories andobjects (see section20.4.3 and
20.4.4). The pointingtechnique canbe extendedto include relations, coupling some
classifiers in a cluster to other related clusters. Finally, default hierarchies develop
naturallyunderthebiddingprocessdiscussedinsection20.5. Underthebiddingprocess, when two classifiers are satisfied, say by the same message, the one with the
more specific condition (fewer#'s) usually becomes active. As the induction procedures add new candidates to the system (see section 20.5), the "specialists" (fewer
#'s)serveasexceptionstothe "generalists" (more#'s)underthecompetitioninduced
bythebiddingprocess. A specialistmayinturnserveasadefaultforastillmorespecific classifier, whence the developing default hierarchy arises.
The combination ofthe default hierarchy, so realized, with the clusters of
coupledclassifiersprovidesaneffectmuchlikeFahlman'svirtualcopy (1979). Environmental messages cause the activation ofa cluster ofclassifiers that provides the
"frame" and specifics wherein the system builds its responses to the situation. The
tagontheoutgoingmessagefromaclustercanindicatethepresenceofsomecomplex
object, such as an arch, while the pass through bits (see section 20.4.2) carry incidental information(color, size,etc.)possiblyrelevanttofurtherprocessing. Theprocessing can include expectations (classifiers satisfied by messages from the virtual
copy but not yet supported by messages from the environment) and plans (coupled
sequencesofclassifierswhereinonlythe firstelementofthesequence isactivatedby
messages from the environment).
--- PAGE 625 ---
HOLLAND
The object ofthe next section is to show how such structures can emerge, in
responsetoexperience, undertheimpetusofcompetitionandlearningandinduction
rules. Some early uses of classifier systems for realistic problems (Wilson, 1982;
Goldberg, 1983) show that default hierarchies do emerge and that sequences of
coupled classifiers sensitive to stage-setting situations do develop.
20.5 LEARNING AND INDUCTION
The essence of classifier systems is a parallelism and a standardization that
permitbotha"buildingblock" approachtotheprocessingofinformationandtheuse
ofcompetitionto resolve conflicts. It is the latterproperty, competition, that makes
possible an approach to learning that is both general and powerful.
Twokindsoflearning algorithms are required ifaclassifier system is toadapt
tochangesinthedomainsandgoalspresentedtoit. Thefirstisanalgorithmthatreinforces or apportions credit to rules already available to the system. (Samuel's 1959
paperisfullofinsightsconcerningthisproblem.)Thesecondisanalgorithmforgeneratingnew, plausible ruleswhentherulesavailableprove inadequate. (Samuelcalls
this "the parameter problem," and it is the one problem on which he did not really
makeprogress. RecentlybothLenat [1983] andHofstadter [1983] haveofferedinteresting approaches to it.) Here, in order to exploit competition between classifiers,
two new kinds ofalgorithm are introduced. The first, the apportionment-of-credit
algorithm, is called a bucket-brigade algorithm. The second, the rule generation
algorithm, is called ageneticalgorithm.
20.5.1 Bucket-Brigade Algorithms
Thebucket-brigade algorithm is designedto assign creditto each classifier in
the system according to its overall usefulness in attaining system goals. Tothis end,
eachclassifierinthesystemisassignedavalue, calleditsstrength, anditisthisvalue
thatthebucket-brigadealgorithmadjusts. Theproblemiseasyenoughwhenaclassifierparticipatesdirectly ingoal-achievingactionthatproducespayoff, butitisquite
difficulttodecidewhichoftheclassifiersactiveearlyinasequencesetsthestagefor
latersuccessfulactions. (InSamuel'sterms, itiseasyenoughtocreditclassifiersthat
combine to produce a triplejump at some point in the game; it is much harder to
decide which classifiers active earlier were responsible for changes that made the
laterjump possible.) By a combination ofanalysis, and simulation (Wilson, 1982;
Goldberg, 1983), we can show that the bucket-brigade algorithm actually accomplishes this task.
Thealgorithmworks, viaamodificationofthebasicexecutioncycle, byintroducing a competition between classifiers. Recall that, during the execution cycle,
each classifier scans all the messages on the global message list, producing a new
message from each message satisfying its conditions. That procedure is now
--- PAGE 626 ---
612 CHAPTER 20: ESCAPING BRITTLENESS
modified sothatsatisfiedclassifiers mustcompetetogettheirmessagesonthe message list. Each satisfied classifier makes a bid based on its strength, and only the
highestbiddingclassifiersgettheirmessagesonthelist. The sizeofthebiddepends
notonly ontheclassifier's strength butalso on its specificity. (Recall thatthe specificityofaclassifierismeasuredbythenumberofnon-#'s initsconditionpart.) Specifically, thebidproducedbyaclassifierisproportionaltotheproductofitsstrength
("past usefulness") and its specificity ("relevance"-the amount of information
about the current situation incorporated in its condition part).
Formally, when the condition partofaclassifier C is satisfied, it makes abid
Bid(C,r) c/?(C)Strength(C,f)
whereR(C)isthespecificity,equaltothenumberofnon-#'sintheconditionpartofC
dividedby thelengththereof; S(C,t) isthe strengthofC attimet, and cis aconstant
considerably less than 1 (e.g., 1/8 or 1/16).
The winning (high) bidders place their messages on the message list and have
their strength reducedby the amount ofthe bid (they are paying forthe privilege of
posting a new message):
Strength(C,r + 1) = Strength(C,r) - B(C,t)
for a winning classifier C. The classifiers { } that sent the messages matched by
thiswinnerhavetheirstrengthsincreasedbytheamountofthebid(itissharedamong
them in the simplest version):
Strength(CU + 1) = Strength(Ct) + aBid(C,t)
wherea = 1 /(numberofmembersof { C }). (Thesendersarerewardedforsettingup
a situation usable by C.
Thebucket-brigadealgorithmtreatseachclassifierasakindofmiddlemanina
complexeconomy, thestrengthofaclassifiermeasuringitsabilitytoturna "profit."
Asamiddleman, theclassifieronlydealswithitssuppliers-theclassifiersthatsend
messagessatisfyingitsconditions-andits consumers-theclassifiersthatareinturn
satisfiedbythemessagesitsends. Wheneveraclassifierwinsabiddingcompetition,
itinitiatesatransactioninwhich itpaysoutpartofitsstrengthtoitssuppliersandthen
receives similar payments from its consumers.
Theclassifier's strength is akindofcapital. Ifaclassifierreceives more from
its consumers than it paid out, it has made a profit, that is, its strength is increased.
This is likely to occuronly ifthe consumer in turn is profitable. This chain leads to
the ultimate consumers, the classifiers that attain goals directly and receive payoff
directly from theenvironment. That is, certain actionsare immediately rewardedor
reinforced by the environment; this payoff for goal attainment is added to the
strengths ofall classifiers active at that time. The profitability ofother classifiers
dependsupontheirbeingcoupled intosequencesleadingtopayoff. Thus, thebucketbrigade assures that early-acting, stage-setting classifiers receive credit ifthey (on
average) make possible later, overtly rewarding acts.
--- PAGE 627 ---
HOLLAND 613
It is worth noting that some of the fixed-point theorems of mathematical
economics provide a way ofproving the above for environments that have "stable"
statistics.
20.5.2 Genetic Algorithms
Once strengthscanbe assignedtoclassifiers, abasisexists forgenerating new
classifiers to enter the competition. In broadest terms the genetic algorithm uses
high-strength classifiers as progenitors for new classifiers to be tested under the
bucket brigade. Because of the parallelism of classifier systems, newly generated
classifiers can be inserted into the "population" without the system's repertoire in
well-practiced situations being seriously disrupted (see below). It is vital to the
understandingofgeneticalgorithmstoknowthateventhesimplestversionsactmuch
more subtly than "random search with preservation of the best" (contrary to
commonmisreadingofgeneticsasaprocessprimarilydrivenby mutation). Genetic
algorithms have been studied intensively through analysis (Holland, 1975; Bethke,
1980) and simulation (DeJong, 1980; Smith, 1980; Booker, 1982; and others).
Although genetic algorithms act subtly, thebasicexecutioncycle (the "central
loop") is quite simple:
1. Select pairs from the set ofclassifiers according to strength-the strongerthe
classifier, the more likely its selection.
2. Apply genetic operators to the pairs, creating "offspring." Chief among the
genetic operators is crossover, which simply exchanges a randomly selected
segment between the pairs (see figure 20-3).
3. Replace the weakest classifiers with the offspring.
The effect ofthis procedure is to emphasize various combinations ofdefining
elements-schemata-asbuildingblocks fortheconstructionofnewclassifiers. The
tentative nature of the classifiers constructed in this way is pointed up by step 3
above. They will be displaced if they do not acquire strength under the bucketbrigade algorithm. Note that a newly constructed classifier gains or loses strength
(aside from certain "taxations") only when its condition is satisfied and it wins the
bidding competition tobecome active. As will be seen, this has much todo with the
overall system's gracefulness relative to the insertion ofnew rules.
20.5.2.1 Definitions
Tobegin, letusconsidertheset Cofall stringsoflengthkoveranalphabetof/?
letters. Forexample, thealphabetcouldbe {1,0,#} sothatthestringsdesignatecondition parts or message parts for classifiers. In the standard terminology ofgenetics
--- PAGE 628 ---
614 CHAPTER 20: ESCAPING BRITTLENESS
these strings would be called genotypes and the values for individual letters in a
string would be calledalleles. Thesetofstringsbeing tested at agiven time (e.g., a
classifier system) is called apopulation. In brief, and very roughly, a genetic algorithmcanbelookeduponasasamplingprocedurethatdrawssamplesfromthesetC;
each sample drawn hasavalue, thefitnessofthecorrespondinggenotype. From this
viewpoint, the classifier system at time r call it B(t)-is a set ofclassifiers drawn
from C, and the fitness ofeach classifier is its strength. The genetic algorithm uses
the fitnesses ofthe genotypes in B(t) to generate new genotypes fortest.
As will soon be seen in detail, the genetic algorithm uses the familiar "reproduction according to fitness" in combination with certain genetic operators (e.g.,
crossover; see below) togenerate the new genotypes (classifiers). This process progressively biases the sampling procedure toward the use ofcombinations ofalleles
(building blocks) associated with above-average fitness. Surprisingly, in a populationofsizeM, thealgorithmeffectivelyexploitssomemultipleof 3combinationsin
exploring C. (Howthis happens will be seen in a moment.) Forpopulations ofmore
than a few individuals this number, 3 , is vastly greater than the total number of
alleles in the population. The corresponding speedup in the rate of searching C, a
property called implicitparallelism, makes possible very high rates ofadaptation.
Moreover,becauseageneticalgorithmusesadistributeddatabase(thepopulation)to
generate new samples, it is all but immune to some ofthe difficulties-false peaks,
discontinuities, high-dimensionality, and so on- that commonly attend complex
problems.
The task now is to give some substance-and intuition - to this outline. An
understandingofsomeoftheadvantagesandlimitationsofgeneticalgorithmscanbe
reached via three short steps. First, in order to describe the nonuniform sampling
proceduregeneratedbyageneticalgorithm, aspecialclassofsubsetsofC isdefined.
Then, in the second step, an explicit sampling procedure emphasizing the sampling
ofcombinationsisusedtoexaminetheroleofthesespecialsubsetsinthenonuniform
sampling procedure. The final step is to show how the genetic algorithm accomplishes implicitly and rapidly what is an intolerable computational burden for the
explicit procedure.
Forthe first step, the subsets (combinations) ofinterest, called schemata, can
be characterized as follows: Much as in the definition ofconditions for classifiers
(sec section 20.4.2), values are first fixed at a selected set of positions in the kposition strings. Note that for classifiers the strings are over the alphabet {l.0.#}
ratherthan the alphabet { 1,0}. By using a (new) "don't care" symbol * for positions
not fixed, one can specify schemata quitecompactly. Thus *0**#** . . . ** is the set
ofallconditions (oractions) having a at position 2 and a # at position 5. The set o\
schemata is the set ofall collections that can be defined in this way. Note that schemata lorclassifiers define subsets ofthe spaceofpossible conditions (oractions), in
contrast totheconditionsthemselves, which define subsets ofthe spaceofmessc
Thus a schemaconstitutes a building block from which one canconstruct classifiers.
--- PAGE 629 ---
HOLLAND
Parenthetically, schematacanalsobecharacterizedinawayfamiliartomathematicians: Ifwelookuponthe^-positionstringsasvectors ina^-dimensional space
(each component having one ofn values 0, 1, . . . , n - 1), then the schemata are
hyperplanesofthe/r-dimensional space. Schematanameparticularsubsetsofthe set
C of^-position strings. These subsetsareofinterestbecausetheycorrespondtoparticularcombinationsoflettersandbecausetheyareeasily andcompactly definedby
strings on an n + 1-letteralphabet {0, 1, . . . , n - 1, *}.
(For simplicity, n = 2, implying binary strings, will be used throughout the
rest ofthis subsection. For theoretical reasons it is usually advantageous to recode
strings over large alphabets, n > 2, into binary strings. To apply the discussion
directly toclassifiers, simply increasento3, using theparticularalphabet {1,0,#}.)
Now it istime forthe secondstep. Forabetterillustrationofthe way inwhich
schematacanaidasearch,analgorithmwillbeconsideredthatmanipulatesschemata
explicitly. Although this algorithm is an aidto understanding, itis impractical from
the computational point of view because of the enormous amounts of time and
storage it would require. The genetic algorithm, to be described in the third step
accomplishes the same actions concisely and rapidly via an implicit manipulation.
The explicit version involves the following steps:
1. Set t = and generate, at random, a setB(t) of M strings.
2. Observe the value ofv(C), the "fitness," ofeach string C inB(t).
(Fromamoreformalpointofview, steps 1 and2amounttosamplingthe
random variable v, using a sample ofsize taken from C.)
3. LetM(s, t)bethenumberofstrings inB{t)belongingtothe schemas (i.e., the
strings are instances of s). If M(s, t) for s, calculate the average value
v(s, t) of the strings in B{t) belonging to that schema. Calculate, also, the
average value v{i) ofthe set of samples.
(There will be somewhere between 2k and M*2k schemata with one or
more instances in B(t). More formally, v(s, t) is the marginal sample
average ofvoverthe subset s.)
4. Selectanew setB(t + 1) of strings sothatthe numberofinstances ofeach
schemas inthe new set is equal to
M(s, t + 1) = [v(s, r)/v(r)]*M(s, t)
for as many schemata as possible.
(Informally, this recursion says that schemata observed to be above
average, v(s,t) > v(t), receivemoresamplesonthenexttime-step. Similarly, below-average schemata receive fewer samples. At first sight it
may seem impossible to meet this requirement in any meaningful way
because there are so many schemata, but see below.)
5. Set fto f + 1 and return to step 2.
--- PAGE 630 ---
616 CHAPTER 20: ESCAPING BRITTLENESS
It isdifficultto satisfy the requirementofstep4 becausethe schemata (hyperplanes) intersecteach other over and over again. In fact there are so many intersections that each string belongs to 2k distinct schemata. Thus any sample allocated to
one schema is as well a sample of2k - 1 other schemata. However, a little thought
and some calculation show that it is possible to distribute new samples so that all
schemata with more than afew elements receive the requisite number of samples.
(Notethatthismeansthatschematawith morethanafew *'sintheirdefiningstrings
canbesampledaccordingtothedictatesofstep4.) Actuallytocarryoutthedistributionexplicitly, allocatingsamplesschemabyschemasothatstep4issatisfied, would
require an enormous amount ofcomputation.
Settingasidethedifficultiesofimplementation, wefindthatthealgorithmuses
very plausible inferences in generating "fit" strings. Most importantly, it samples
with increasing intensity schemata that contain strings of above-average strength.
The net effect of increasing the proportion of samples allocated to above-average
schemata is to move the overall average v(t) upward. Because the average v(t)
increases with time, this sampling procedure is a global "force" driving the search
into subsets observed to contain valuable strings. Moreover, because the algorithm
worksfromadatabaseofMpointsdistributedoverC, itisnoteasilycaughton "false
peaks" (local optima). (Standard optimization procedures work well only with
single-peak functions, relying on a uniform random search for "starting points"
when there are multiple peaks.) Overall, this algorithm is much more globally oriented than standard optimization procedures, searching through a great many schemataforregularitiesandinteractionsthatcanbeexploited. Thispointhasbeenestablished, for the algorithm described next, by extensive experimental comparisons
between that algorithm and standard procedures (Bethke, 1980; DeJong, 1980).
Figure 20-2 illustrates the use ofschemata to locate the global optimum ofa
function v(x) on the interval [0,1]. The arguments ofthe function are represented as
binary fractions so that, for example, the argument x = 1/2 is represented as
0.100 ... 0. Ifwe look upon the binary representations as strings, then the schema
1** ... * is the subset of all arguments greater than or equal to 1/2, that is, the
interval [1/2,1]. Similarly, the schema **1* . . . * isasetofintervals (seetophalfof
figure20-2) correspondingtoall thebinary fractionsthat havea 1 inthethird place.
Other schemata are determined accordingly. Regularities of the function, such as
periodicities, trends, and so on, are readily exploited by the biasing of samples
toward appropriate schemata. The genetic algorithm automatically takes advantage
ofsuch schemata, as will be seen in a moment.
Froman intuitivepointofview, goodschemata(schematacontainingstringso\'
above-average fitness) can be thought ofas useful building blocks for constructing
newstrings. Forexample, iftheschemata
*0**
. . .
**and ***001**...** arc both
good, then it seems reasonable to investigate strings constructed \\ith these building
blocks, namely, stringsbelonging to the schema 1*0001** ...**. The powerofthis
--- PAGE 631 ---
Generation
Y777ZA V7777A 7Z77ZA V////X
**i* *
Generation5
OTrials(individuals)
Figure20-2: Exampleoffunctionoptimizationbygeneticalgorithm.
kind ofalgorithm lies in its rapid accumulation of large numbers ofbetter-thanaveragebuildingblocks-buildingblocksthatexploitregularitiesandinteractions in
the sample space C. By carefully choosing TMsamples over Titerations, the algorithm accumulates information about a large numberofpotential building blocks, a
number somewhere between 2kand TM2k
Our objective now is to see how we can obtain the effects ofthis direct algorithm without paying the tremendous computational costs. This third step of the
--- PAGE 632 ---
618 CHAPTER 20: ESCAPING BRITTLENESS
explanation involves, first, a specification ofthe genetic algorithm and, second, an
explanation ofits implicit manipulation ofschemata (see figure 20-3).
The specification is as follows:
1. Set / = and generate, at random, a setB(t) of M strings.
2. Observe the value v(C) ofeach string C in B(t).
3. Computetheaveragestrength vofthe strings inthedatabaseB(t) andassign
a normalized value v(C)/vto each string C in B{t).
4. Assign each string in B(t) a probability proportional to its normalized value
and then select n, n< <M, pairs of strings from B(t) using this probability
distribution.
5. Applygeneticoperatorstoeachpair, forming2nnewstrings. Themostimportantofthegeneticoperatorsiscrossover(seefigure20-3): A positionalongthe
Samples Values Crossover
C v(C) Point
1 1 1 1 1 1 1 . . 1
110 11110.. .-
00•0•1 • T 10 T ...-12
1 1 1
110 10 11
10• 0•01•
101 ...-12
01 1001 100. ..H 2
10 10 10 [~Q~|
1 . . .
100010010...-
I 1
(*00*1* . . .) = 2 + 2+1 X Definingbitsin instances
= 1.67 T ofschema *00*1** ... *
X Definingbits in instances
ofschema ******1*0* *
****** *0* + 2 + 2 . . .
v( . . . )
= 1.33
Figure20-3: Exampleofthe genetic algorithm'seffect on schemata.
--- PAGE 633 ---
HOLLAND 619
stringisselectedatrandom, andthen, inthepairbeingoperatedupon, thesegments to the left of this position are exchanged. This simple operation has
subtleeffectswhenusedincombinationwiththe "emphasis" providedby step
3 aswill be seen shortly. Theotheroperators, such asmutation andinversion,
have lesser roles in this use ofthe algorithm, mainly providing "insurance"
against overemphasis ofa given kind ofschema. (See Holland, 1975, chap. 6,
sees. 2, 3, and4, fordetails.)
6. SelectIn strings fromB(t) and replace them with the 2n new strings resulting
from step 4. (There are some technical issues involved in the selection ofthe
strings to be replaced. These issues primarily concern limitations on the portion ofthe database allocated to strings ofa given type. In effect each string
belongstoanicheinthedatabaseanditsspreadistobelimitedtothesize-i.e.
carrying capacity-ofthat niche. See Bethke, 1980, and DeJong, 1980, for
details.)
7. Set tto t + 1 and return to step 2.
Unlike theearlierdirect algorithm, the genetic algorithm neverdeals directly
with schemata- it only manipulates the strings in B(t). Ifone wishes to explore the
action ofthe algorithm relative to schemata, it is helpful to divide the algorithm's
action into two phases: Phase 1 consists ofsteps 2-4; phase 2 consists ofsteps 5-6.
Consider first what would happen ifphase 1 were iterated without the executionofphase2 (butwiththereplacementofstrings inB(t)). Inparticular, letphase 1
beiteratedMllntimes(assumingforconveniencethatMisamultipleofIn). Itisnot
difficulttoshowthattheexpectednumberofinstancesofaschemasattheendofthis
iteration isjust v(s) times the numberofinstances at the outset (see Holland, 1975).
ThisistrueofeveryschemawithinstancesinB, andthis isjustwhatwas requiredof
the direct algorithm in step4.
What, then, is accomplished by phase 2? The problem is that phase 1 introducesnonewstringsintoB; itmerely introducesadditionalcopiesofstringsalready
there. Phase 1 providesemphasis,butnonewtrials. Thegeneticoperators, appliedin
phase 2, obviously modify the strings in B. It is a fundamental property ofgenetic
algorithms (Theorem6.2.3, Holland, 1975) thattheemphasis providedby phase 1 is
little disturbed by phase 2. That is, afterphase 2, schemata with instances in Bwill
largely have the number of instances provided by phase 1 but they will be new
instances.
Thus the genetic algorithm as a whole generates new samples ofschemata
already present, increasing or decreasing the sampling rate according to the multiplier v(s,r)/v(0, as desired. From the point ofview of sampling theory, these new
samplesincreaseconfidenceintheestimatesv(s)foreachabove-averageschemasin
B. Somecalculation (Holland, 1975, chap. 4) shows that considerably morethan 3
schemataaresotreatedeveryMllntime-steps. Moreover, samplesaregeneratedfor
schemata not previously tried without this procedure being disrupted. All of this
--- PAGE 634 ---
620 CHAPTER 20: ESCAPING BRITTLENESS
comes about through simple-and fast-manipulations ofIn strings per step. This
implicit manipulation of a great many schemata via operations on relatively few
strings is called implicitparallelism.
20.5.2.2 Application to Classifiers
Howdoesallthisapplytothegenerationofnewclassifiers?Asmentioned,
the
strengthsassignedby thebucket-brigadealgorithm serveasthe fitnesses oftheclassifiers. Becausethepartsoftheclassifierarestringsofstandardlengthkoverafixed
alphabet {1,0,#}, threepossibleallelesperlocus, theproceduresofthegenetic algorithm are directly applicable. A combination ofalleles that occurs in several strong
classifiers-forexample, aparticulartag, orpart ofa message, oracombination of
properties-automatically becomes a building block for the construction of new
classifiers. Such combinations amount to schemata that are subject to the theorems
concerning implicit parallelism.
Iftheclassifiersystemisusing classifiers, itcanbeshownthat,asthegenetic
algorithmgeneratesnewclassifiers, itiseffectively selectingamongstmorethan y
building blocks, each rated on the basis ofpast experience! The building blocks so
manipulateddetermineimportantproperties, suchasclassifiercouplingandcontrol
sequencing(seesection20.4.3 and20.4.4). Thusthegeneticalgorithmcanencourage
variants of useful subroutines, and it can generate hierarchical substructures for
testing. In sum, the genetic algorithm offers an inductive procedure that is (1) fast
(becauseoftheimplicitparallelism), (2) relatively immunetomisdirection (because
ofthe distributed database provided by the population), and (3) capable ofsophisticated transfer of knowledge from one situation to another (because of the role of
schemata).
It is important to note that, in general, the candidate rules generated by the
genetic algorithm do not displace the parent rules. The parent rules simply supply
copiesoftheirpartsforusebythegeneticoperators (suchascrossover); they remain
in the system in their original form. The offspring rules typically displace rules of
lowstrength, thuseliminating rulesthathavenotprovedvaluabletothesystem. Asa
consequence the parent rules, because oftheir high strength, will tend to remain in
control ofsituations in which they acquired their strength. New rules typically get
theirchance in situationswherenoneofthehigh-strength rules havetheirconditions
satisfied. That is, theytendtofill new nichescorrespondingtodomains in which the
system has inadequate sets of rules. (This feature goes hand in hand with Scott's
(1983) use of"play" as a means ofreducing uncertainty about the environment.)
Ultimately, ofcourse, new rules may outcompete their parents (or other relatives) ifthey prove superior underthe bucket-brigade algorithm. The explicit parallelism, underwhich a variety ofrules is active simultaneously, encourages the competition. The result is a system that can explore without disturbing well-established
capabilities. In short, the system is graceful rather than brittle.
--- PAGE 635 ---
HOLLAND
20.6 TESTS AND PROSPECTS
Severalyearsagoaseriesoftestsofsimplifiedclassifiersystems (Hollandand
Reitman, 1978) demonstrated simple transfer oflearning from problem to problem
and showed that the genetic algorithm yielded learning, in that context, an order of
magnitude faster than weight-changing techniques alone. The results were encouraging enough to sparka variety ofsubsequenttests at several places. Smith (then at
theUniversityofPittsburgh,nowatCarnegie-MellonUniversity)completedaclassifier system (1980) that competed against Waterman's poker player (Waterman and
Hayes-Roth, 1978)-also a learning program-with overwhelming success. Wilson
(thenatPolaroid, nowattheRowlandInstitute)usedaclassifiersystem(1982)witha
genetic algorithm in a series ofexperiments involving TV-camera-mechanical-arm
coordination, culminating in a successful demonstration ofthe segregation ofthe
classifiers,underlearning, intosets(Wilsoncallsthemdemes)correspondingtocontrol subroutines. Booker has done an in-depth simulation study (1982) ofclassifier
systemsascognitivemodels, withparticularemphasisonthegenerationofcognitive
maps under experience. More recently, Goldberg (1983) has demonstrated emergenceofadefaulthierarchyinastudyoftheuseofclassifiers, underthegeneticalgorithm, as adaptivecontrols forgaspipelinetransmission. There are several ongoing
projects, including one that uses a classifier system to deal with the classification
problem in KL-ONE (Forrest, 1982).
The more advanced properties ofclassifier systems are being tested with the
helpofaprogram, CS1, thatisbotha "compiler," allowingdesignandsimulationof
classifiersystemsonaserialcomputer,anda"test-bed,"providingthemeansofsimulatingawiderangeofenvironmentsfortestingthelearningalgorithms. Thecurrent
version provides the following facilities:
Simulationofataskenvironmentconsistingofupto256objects, eachwithup
to thirty-two distinct features, emplaced on a 65,000-by-65,000 grid. Any or
all ofthe objects may be mobile.
2. Aninputinterfacethatconsistofanarbitrarilyshaped "visioncone"thatviews
alocalpartofthesurface(typicallylessthan 1000gridpoints)andusesfeature
detectors to generate an input message foreach object in the vision cone.
3. An emulator for the classifier system that can retain, in random access
memory, the description ofover 1000 classifiers and a message list ofup to
thirty-two messages. This part ofthe system is written in machine language
andcanexecuteabasictime-step(allclassifiersmatchedagainstall messages)
in about 0.1 second. The overall system runs in close to real time, making it
convenientto run long learning sequences in the simulated environment.
4. Anoutputinterfacethatpermitsmanipulationofobjectsonthegrid, movement
overthegrid, rotationofavisualcone,andinfactanyothereffectoractionconveniently specifiable by a subroutine.
--- PAGE 636 ---
622 CHAPTER 20: ESCAPING BRITTLENESS
5. Parameterized versions of both the bucket-brigade and genetic algorithms,
includingprovisionsforcontingentactivationofthesealgorithms(suchasactivation of the genetic algorithm when no classifier responds to an input
message).
Studying full-fledged classifier systems is much like studying an ecology.
There are niches, adaptations exploiting them, and shifting hierarchies of
interaction-the emergence ofparasitic classifiers has even been observed! Questions abound. Most pressing is the question oflimitations. What is it that such systems cannot learn from experience? The author's observations to date indicate that
general-purpose learning algorithms, given the right grist, can produce organizationsthataredetailed, appropriate, and subtle. This contradictsaccepted wisdom in
AI; somewherethereisaboundary (orsetofthem) thatmarksthelimits ofwhatcan
be accomplished reasonably with so-called weak methods. The author's impression
isthatthedomainofsuchmethodsis muchlargerthanisusuallybelieved. Withinthis
domain brittleness is no longer abete noire.
Although most ofthe studies to date have dealt with systems that start with a
tabula rasa-the most difficult test for a general-purpose learning procedure-this
wouldnotbethetypicaluseofsuch systems. Classifiersystems are general-purpose
systemsthatcanbeprogrammedinitiallytoimplementwhateverexpertknowledgeis
available to the designers and their consultants. Learning then allows the system to
expand, correcterrors, andtransferinformation fromonedomaintoanother. Inthis
contextthequestionbecomesone ofhow flexible-and graceful-such a system can
be. It is important to provide ways ofinstructing such systems so that they can generaterules-hypothesestobeheldtentatively-onthebasisofadvice. Littlehasbeen
done in this direction.
Much more remains tobe discovered about conditions that induce a classifier
system to construct models of its environment for purposes of planning and lookahead. Itisparticularlyimportanttounderstandhowlook-aheadandvirtualexplorationscanbeincorporatedwithoutotheractivitiesofthesystembeingdisturbed. Ultimately the question is whether such systems can develop symbols (cf. Hofstadter,
1983) and use them, via abstract models, to generate plans and expectations.
ACKNOWLEDGMENT
This research wassupported inpartby the National Science Foundationunder
grants IST-8018043, MCS-7826016, and MCS-8305830.
References
Bcthke, A. D., "GeneticAlgorithmsasFunctionOptimizers." Ph.D. diss.. DepartmentofComputerand
CommunicationSciences, University ofMichigan. 1980.
--- PAGE 637 ---
HOLLAND 623
Booker,L., "IntelligentBehaviorasanAdaptationtotheTaskEnvironment,"Ph.D.diss., Departmentof
ComputerandCommunicationSciences, UniversityofMichigan, 1982.
Davis,R.,andKing,J., "AnOverviewofProductionSystems," inMachineIntelligence8, E.W. Elcock,
and D Michie(Eds.), AmericanElsevier, NewYork, 1977.
DeJong,K.A.
"AdaptiveSystemDesign-AGeneticApproach,"IEEETransactions:Systems,Man,and
Cybernetics, Vol. 10, No. 9, 1980.
Duda, R. O., andShortliffe, E. H., "ExpertSystemsResearch," Science, Vol. 220, pp. 261-68, 1983.
Fahlman, S. E.,NETL:ASystemforRepresentingand UsingReal-WorldKnowledge, MITPress, Cambridge, 1979.
Forrest,S., "AParallelAlgorithmforClassificationofKL-ONENetworks," ConsulNoteNo. 15, USC/
InformationSciencesInstitute, 1982.
Goldberg,D., "ComputerAidedGasPipelineOperationUsingGeneticAlgorithmsandRuleLearning,"
Ph.D. diss., DepartmentofCivilEngineering, UniversityofMichigan, 1983.
Hofstadter, D. R., "Artificial Intelligence: SubcognitionasComputation," in TheStudyofInformation,
F. MachlupandU. Mansfield(Eds.),Wiley, NewYork, 1983.
Holland,J. H.,AdaptationinNaturalandArtificialSystems, UniversityofMichiganPress, AnnArbor,
1975.
Holland, J. H., and Reitman, J. S., "Cognitive Systems Based on Adaptive Algorithms," in PatternDirectedInference Systems, D. A. Waterman and F. Hayes-Roth (Eds.), Academic Press, New
York, 1978.
Lenat, D. B., "The Role ofHeuristics in Learning by Discovery: Three Case Studies," in Machine
Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and T M.
Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
McDermott,J., andForgy, C, "ProductionSystemConflictResolutionStrategies," inPattern-Directed
InferenceSystems, D.A. Waterman,andF. Hayes-Roth(Eds.),AcademicPress, NewYork, 1978.
Samuel, A. L., "Some Studies in Machine Learning Using the Game ofCheckers," IBMJournal of
ResearchandDevelopment , Vol. 3, pp. 211-32, 1959.
Scott, P., "Knowledge-Oriented Learning," Proceedingsofthe Eighth IJCAI, Karlsruhe, W. Ger., pp.
432-35, 1983.
Smith,S., "ALearningSystemBasedonGeneticAlgorithms,"Ph.D.diss.,DepartmentofComputerScience, UniversityofPittsburgh, 1980.
Waterman, D. A.,andHayes-Roth, F (Eds.),Pattern-DirectedInferenceSystems, AcademicPress, New
York, 1978.
Wilson,S., "Adaptive 'Cortical' PatternRecognition,"InternalReport,ResearchLaboratories, Polaroid
Corporation, 1982.
Winston, P. H., "Learning Structural Descriptions from Examples," in The Psychology ofComputer
Vision, P. H. Winston(Ed.), McGraw-Hill, NewYork, 1975.
--- PAGE 638 ---
--- PAGE 639 ---
LEARNING FROM
POSITIVE-ONLY EXAMPLES:
The Subset Principle and Three Case Studies
Robert C. Berwick
MassachusettsInstitute ofTechnology
Abstract
A key issue for learning theory has been the relative importance ofdomainindependent learning versus domain-specific learning. How much of learning is
attributabletogenerallearningmethodslikeinductivegeneralization,andhowmuch
isattributabletoparticulartechniquesandrepresentationsthatapplyonlytospecific
domains? Thischapterexploresthisissuethroughananalysis oftheroleofonegeneral principle in the context ofseveral very specific domains. Angluin (1978) established a necessary and sufficient condition for the acquisition ofa (recursive) language (in the sense used by Gold, 1967) from positive-only evidence. The effect of
this condition is to impose an ordering on possible sequences ofguesses about the
targetlanguagecombinedwiththearrayofpossibledatasequencesinsuchawaythat
the acquisition system always guesses the narrowest possible language compatible
with thedatagiven so far. Inthischapter, this "Subset Principle" is appliedtothree
areasinwhichextensivedomain-specificknowledgeispresent. First, conceptacquisition inchildren, as studiedby Keil (1979), isconsidered, followedby anexamination oftheareaofnatural language, where syntactic constructions and phonological
distinctive feature systems are explored. Constraints on these systems are shown to
follow the Subset Principle, providing some evidence that natural learning systems
are designed tobe easily learnable.
--- PAGE 640 ---
626 CHAPTER 21:THESUBSET PRINCIPLE
INTRODUCTION
21.1
What are the scope and power of "general" learning principles? General
methods like inductive generalization or analogy have figured prominently in any
discussion of learning. Those who have embraced such general principles range
acrossthe scientific spectrum, from Skinnerand PiagettoNewell and Simon. More
recently, however, the very existence of general learning principles has been
questioned:
What Iintendtosignify inreferringtothedoctrineofuniformityofmindis
. . .
thatthere
are general principlesoflearning thatunderlie all ofthese systems, accounting fortheir
development: "multipurposelearningstrategies,"astheyarecalled,thatapply "acrossthe
board."Incontrast,itmightbeproposedthatvarious "mentalorgans"developinspecific
ways
. . .
andthatmultipurposelearningstrategiesarenomorelikelytoexistthangeneral
principlesof"growthoforgans" thataccountfortheshape, structure, andfunctionofthe
kidney, theliver,theheart, thevisual system, andsoforth. (Chomsky, 1980, 245)
Chomsky claims that there are no general principles ofhuman learning. Each
domain, beitlanguage, motorcontrol, vision, ormathematics, hasits ownparticular
constraints. Evenifthis weretrue, however, therewould stillbe room foradomainindependent learning theory; it would consist ofgeneral principles supplementing
the particularconstraints ofeach domain.
As a simple example, consider Winston's classic program that learned the
descriptionsofscenesmadefromtoyblocks, suchasarchorhouse (Winston, 1975).
Herethedomain-dependentconstraintsincludethoseoftherepresentation language
primitives,thatis,thebasicvocabularyusedtodescribeblocksworldscenes, suchas
the predicates touch or is-a-brick, and the way that these basic predicates can be
pastedtogether. Thepresumablydomain-independentprinciplesoftheblocksworld
include generalization heuristics such as "require link," a descriptor-modification
introduced when anegativeexample shownby ateacherestablishesthat aparticular
feature ofa block model must be present. For instance, ifWinston's program was
shownanonarchwithitstop(orlintel)lyingontheground, itcouldapplythe require
linkheuristictothedescriptorthatthetwoarchcolumnsmustsupportthe lintel. The
require link heuristic is part ofgeneral learning theory.
The question still remains of the relative contribution of general learning
theory to the explanation of human learning; that is, can it help us account for
observed patterns ofhumancognitivedevelopment? The answer is often assumed to
beyes, butmattersare farfromclear. Theaimofthischapteristoshowthatthereisa
viable general learning theory that helps explain why human learning proceeds the
way itdoes. Itwillbeshownthatthereisapowerfulconstraintontheorderinwhieha
learner should consider hypotheses, called here the Subset Principle. Armed with
thisprinciple, theauthorthenshowshowtoexplainsomeobservedpatternsinhuman
learning in the domains oflanguage and concept acquisition.
--- PAGE 641 ---
BERWICK 627
Toseejustwhy theexplanatory powerofgeneral learningtheory canbequestioned at all, let us consider the example of language. Modern linguists generally
make the idealization that language acquisition is "instantaneous"-that is, they
imaginethatthelanguagelearnerispresentedwithallthe "inputdata" (sentencesof
theirlanguagetolearn) atonce. This isclearly false. Childrenhearsentences strung
out over time. Linguists know that this idealization is false,just as physicists know
thatthereare no frictionlessplanes. Yetthe idealizationhas proved remarkably successful: sofar, nogeneralizationsaboutthepropertiesofnaturallanguageshavebeen
lost because of the idealization of instantaneous acquisition. At least the linguists
would claim that this is true. In fact, it is widely assumed that there is no theory of
language learning at all. John Marshall has put things this way:
Thereis,however,averygeneralproblemwithpracticallyallstudiesoflanguagedevelopment, whether investigated from the standpoint ofrule acquisition, strategy change, or
elaborationofmechanism.Theproblemarisesbothforaccountsthatpostulate"stages"of
development(i.e., afinitenumberofqualitativelydistinctlevelsoforganizationthrough
whichtheorganismpassesenroutefrommoleculetomaturity)andforaccountsthatview
developmentasacontinuousfunctionofsimple accumulation. Thedifficulty isthis: No
one has seriously attempted to specify a mechanism that "drives" language acquisition
through its "stages" or along its continuous function. Or more succinctly: there is no
knownlearningtheoryforlanguage. (Marshall, 1979,443)
This chapter provides just such a theory. The Subset Principle heuristic
"drives" language acquisition through its stages. It is not a constraint particular to
language, just as the require link heuristic is not particular to the domain oftoy
blocks. What is the Subset Principle? Intuitively, it is a strategy of "timid acquisition": Ifpossible guesses can be arranged in a subset relationship, then the learner
shouldmakethesmallestpossibleguessaboutwhat itshouldlearnconsistentwiththe
evidenceithasseensofar. Thisisanexceedinglysimpleidea, yetitisquitepowerful.
As it happens, this constraint is necessary and sufficient for successful acquisition
givenonlypositivetrainingexamples,wheresuccessfulacquisitionisdefinedasconvergence to the correct target description or language after some finite number of
trainingexamples. (Recall thatapositivetrainingexample isanexample oftheconcepttobe learned. In the Winston toy blocks world, ifone is learning about arches,
thenapositiveexampleisanexampleofanarch. A negativeexampleisanexampleof
a nonarch
Thischapterwill focusontwonatural learning systems, language andconcept
development. The Subset Principle can account for a wide variety ofconstraints in
these systems. Inthecaseofconceptdevelopment, the way thatchildren learn about
whatkindsofthingsthereareintheworldwillbeexamined. Theanalysisisbasedon
work by Keil (1979). It will be seen that the Subset Principle actually explains the
developmental stages that children go through as they learn. Language will be considered next, andtwo subareas will beexamined. The first isphonology, thatpartof
linguisticsthatstudiestheinventoryofsoundsinalanguage. Linguistsknowthatthe
--- PAGE 642 ---
628 CHAPTER 21:THESUBSET PRINCIPLE
possiblesoundsinagivennatural languageareactuallyquitelimited-outoffiftyor
so possible sounds, there will be at most a handful ofvowels (like a or u) and a few
dozen consonants (like/?, t, or k). Many languages have far fewer consonants. The
vastmajorityofpossiblecombinationsofconsonants and vowels isneverfound. For
example, no natural language lacks so-called voiceless consonants (consonants pronounced without the vocal cords vibrating, as in a hissed s). Why is this so? The
Subset Principle explains why: the gaps are an artifact of timid acquisition. The
second subarea oflanguage examined here is syntax. Here too the Subset Principle
accounts for a wide variety ofotherwise inexplicable constraints.
Finally, theSubsetPrinciplehashadtwo "practical" applications. First, ithas
beenusedinacomputermodel forlanguageacquisition(Berwick, 1980, 1982). This
model has successfully acquired a large complement ofrules for analyzing English
sentencesandis nowbeingextendedtoChineseandGerman. Second, recentpsycholinguistic evidence has probed for evidence ofthe Subset Principle in young children's acquisition ofsyntax; preliminary results confirm the principle.
Before applications ofthe Subset Principle in specific learning situations are
considered, the principle will be described in the abstract.
21.2 THE SUBSET PRINCIPLE
The Subset Principle is actually quite simple. The intuition behind it will be
presentedfirst. LetususetheWinstonblocksworldsettingasanexampleofatypical
learning situation. We assume thatthe learner has at its disposal a fixed representation language with which to describe observed scenes ofblocks. The learner is presentedwithexamplesandnonexamplesofsometargetconcepttolearn, suchasarch.
In the case oflanguage, the target concept is the rule system ofthe language itself,
such as English orGerman. Acquisition proceeds viathe presentation ofa sequence
ofpositive and negative examples. After each example the learner may make some
responseandchange itscurrentmodelofthetargetconceptorlanguage. A change is
prompted by some difference between the current model and an example. If after
some finite sequence ofpresentations the learner does not change its model and
has settled on the correct target model or language, we say that acquisition has
succeeded.
Considerhowthisworks inthetoyblocksworld. Ifthe system'scurrent model
ofan arch includes two columns supporting a wedge, and if it now receives as an
example ofan arch a setofblocks with twocolumns supporting a rectangularbrick,
then the difference prompts a generalization. Perhaps the top ofthe arch can be any
prismaticsolid. Positiveexamples(examplesofarches) inducegeneralizations. They
rule out descriptions that are too specific. On the other side, negative examples rule
outcertainovergeneralizations. Ifwepresentasanexample ofa nonarehtwoblocks
thattoucheachother(sothatthere isnoholebetweenthem)plusawedgeontop. then
the discrepancy between this and the current model forces the learner to add a must-
--- PAGE 643 ---
BERWICK 629
not-touch or musthave a hole between descriptor to the properties ofthe two supporting columns. In Mitchell's version space framework (1978), positive examples
force the boundary of the "specific descriptions" frontier toward more general
descriptions, and negative examples force the boundary ofthe "maximally general
descriptions" toward more specific descriptions.
An important variant ofthis learning situation restricts the learnerto positive
examples. Thisisacrucialassumptionformodelsoflanguageacquisition, wherethe
existence of negative evidence is problematic. Children do not seem to learn their
native language through explicit, Berlitz-like training sequences (see Brown and
Hanlon, 1970;WexlerandCulicover, 1980). Noonetellsthemthat "sentence X isnot
asentenceinEnglish," correspondingtothe "thisisnotanarch" examplesinthetoy
blocks world.
Unfortunately, a restriction to positive evidence makes learning harder. The
danger is overgeneralization. Suppose a learning program gets only positive examples ofa concept or a language. Ifthe program's model ofthe concept becomes too
general, nofurtherpositiveevidencecandislodge it fromits incorrectperch. This is
simplybecausetherecanbenoinconsistencybetweenatoo-generalmodelandapositiveexample. Wehavenonegativeexamplesthattellusthatwehavegonebeyondthe
correcttargetdescription. Remember, itisthenegativeexamplesinMitchell'sframeworkthattelluswhenwehaveovergeneralized. Inthearchexample, if wearelimited
toexamples ofarches, and ifafterseeingtwocolumns supporting abrickwe generalizeto say thatthe columns may ormay nottouch, then no furtherexamples where
the columns do nottouch will disagree with our description.
A solution is to avoid ever hypothesizing an overly general description. We
shouldorderourhypothesesinsuchawaythatateachstepweareguaranteedneverto
have formedtoogeneral adescription. This way, ifourdescription is incorrect, say,
too specific, then a later positive example will correct it. Ifwe say that arches can
haveonlywedgesontop, thenanarchexamplewithabrickontoptellsusthatweare
wrong. Ifourdescriptionisjustrightandarchesalwayshave wedgesontop, thenwe
simplyneverchangeouroriginalhypothesis. Inaword, wewantourhypothesestobe
maximallydisconfirmable. Inthearchexample, again, weseethattherightchoiceto
makeafterseeinganarchwherethetwocolumnsdonottoucheachotheristousethe
descriptor must-not-touch to begin with. This description is refutable by a positive
example if we are wrong: if someone shows us an arch with the two supporting
columnstouching, thenwechangetomay-or-may-not-touch Inthiscase, ofcourse,
our original description is correct. Note how a description of may-or-may-nottouch" is wrongtobeginwith, since itcannotbedisconfirmedby positiveexamples
ifthe correct target descriptions is maynottouch.
Considerwhat it meanstohave ahypothesisthatcan bedisconfirmedby positive examples. Call hypo the hypothesized model (like the arch description). Let
M hypo be the set ofarches describable by M hypo. Similarly, let M Mtrue and M true be the
truedescriptionandsetofarchessodescribable, respectively. If hypoiswrong, then
--- PAGE 644 ---
630 CHAPTER 21:THESUBSET PRINCIPLE
forittobedisconfirmablebypositiveexamples, someexample in M
true
should show
this; thatis, there shouldbe some finite setofexamplesthatis notcoveredby hypo.
This isjust like the brick-topped arch example that tells us that our description of
arches as all wedge-topped is wrong. Mathematically, then, we can disconfirm our
M M
hypothesis if true is M notaproper subsetof M hypo. Disconfirmation fails only ifany
examplecovered M by true is alsocov M eredby hypo. But M this isjustto say thatdisconfirmationfailsif trueisasubsetof hypo. Inthiscase, hypoistoogeneral, unlessit
happens to be exactly correct.
Of course, in any actual learning situation we do not know what the true
descriptionis. However, wecanstillarrangeforasequenceofhypothesestomeetthe
condition ofdisconfirmability. Consider any two hypotheses h l and h i+], where the
subscriptsmeanthatthehypothesis/ H- 1 isputforthafterhypothesis i. Onceagain,
hypothesis i + 1 should notbe aproper subset ofhypothesis i. Ifwe guess firstthat
column supports foran arch may-or-may-not-touchthen we cannotdisconfirm this,
since the hypothesis may-touch is a proper subset ofthis first guess. Reversing the
logic, whatweshoulddoisorderourguessessothateachpair(i,i + 1)isdisconfirmable. We say that this arrangement ofhypotheses satisfies the Subset Principle.
ItisnothardtoseewhytheSubsetPrincipleisdubbed "timidacquisition." Ifa
learning systemfollowstheprinciple, then it mostoftenmakesthe smallestgeneralization possible at any given step. (It need not be so timid if it can be certain of
receiving disconfirming evidence at some later point. For example, suppose the
learnerhadtochoosebetweentwolanguages, L) = {a? J isodd} + {a,a1
, . . .
,a10
andL 2 = {a', iiseven} 4- {a, a2 , . . . , a10 }. Thenjustasinglepositiveexample, say,
a3 cansparkaninductiveleaptotheguessofL eventhoughthismaybewrong. For
, x,
ifthelearneriswrong, itwilleventuallygetanexamplesuchasa12thatwillprovethis
to be so. On the other hand, a timid generalizer would not be able to make such a
leap.)
Itisalsonothardtoseethatthe SubsetPrincipleissufficienttoguarantee successful acquisition after some finite number of positive examples. (We might not
knowhowmanyexamplesthiswouldtake, however.)Thisisbecauseanystepatallis
a step in the right direction, as stimulated by a positive example. After some finite
numberofsteps, the system must guess the correct target language ordescription.
Morestrikingly, Angluin(1978)hasshownthatthisprincipleisactually necessaryforacquisitiongivenpositive-onlyevidence. Angluinprovesthisresultusingthe
techniques ofrecursive function theory. In her framework, the "hypotheses" are a
family oflanguages, X = {L,, L 2, . . . , L,, . . . , L,, . . . L,,}. The "examples" arc
finitecollectionsofpositiveexamples, perhapssingletonsetsorperhaps not. defined
as T n where 7] is a positive set ofexamples for hypothesis L,. Finally, Angluin uses
the term identifiable to mean that aftersome finite number ofpositive example presentations, thelearningprocedure (1)guessesthe right target language and (2) never
changes its guess after this. This is Gold's (1967) traditional definition ofidentifiability in the limit. The theorem proved is the following:
--- PAGE 645 ---
BERWICK
Theorem 1: Given a family of languages i£, i£ is identifiable from
positive-only evidence ifand only if for each target language L, in £
there exists a computable procedure that enumerates finite sets Tu
T 2, . . . , such that
CL,C #
(1) Tt and
(2) Forally > i, if7) C L,, thenL, is not aproper subset ofL,.
The Subset Principle is a very general and abstract restriction on acquisition
using positive examples. Suppose that hypotheses may be nested, so that each one
completely covers the next. This is usually the case in natural languages, when one
has a rich theoretical vocabulary and is trying to find a correct description ofsome
targetgrammar. Inthissituation, Angluin'sresultsaysthatonemustfindthesmallest
generalization coveringthe samples seen so farorotherwise one risks overgeneralizing. This kind ofproblem is alsodiscussed in amore general contextby Diettrich
and Michalski (1983).
However, thereisanotherwayforhypothesisi + 1 nottobeapropersubsetof
hypothesis i andsomeettheSubsetPrinciple. Hypothesescouldpartiallyoverlap; in
thiscase, thatwouldmeanthatthereissomeexamplenotcoveredbyoneguessthatis
inthesecond. Thenthelearnerneednotsubscribetominimalgeneralization, asdiscussed earlier. Angluin's theorem covers both sorts ofcases.
Since the subset arrangement is the focus in the next two sections, it will be
described in more detail. Suppose that all the possible target concepts or languages
can be arranged in a nested order, like concentric circles. Then to meet the Subset
Principle the acquisition procedure must order its hypotheses so that it always
guessesthenarrowestpossiblehypothesisorlanguageateachstep. Thisisbecauseif
allhypothesesaresonestedthentheonlywayforthehypothesisguessedatstepi +
nottobeapropersubsetofthatguessedatstepi isforittocontainhypothesisi. The
right sequence ofguesses will be monotonically increasing-each description will
covertheonebeforeitinthesequence. Thisiswhatismeantby "timidacquisition";
at each step the system will take the smallestpossible step consistent with evidence
seeninordertoavoidthepossibilityofguessingtoolargealanguage. Itcorresponds
toanincrementalsearchthroughthespaceofhypotheses, startingfromthemostspecific first. The power ofthis principle suggests that we look for evidence that it is
applied in natural learning settings. In the next two sections we shall see ifwe can
find suchevidence. As farascanbedetermined, the Subset Principleexhausts what
can be said about ordering constraints in language acquisition and perhaps in other
domains as well.
21.3 CONCEPT DEVELOPMENT AND THE SUBSET PRINCIPLE
The firstexamplepresentedhere is drawn from research on conceptual development initiatedby Sommers (1971) andpursuedby Keil (1979). Itwill be seenthat
--- PAGE 646 ---
632 CHAPTER 21:THE SUBSET PRINCIPLE
children's developing knowledge about the world obeys the Subset Principle. But
first let us summarize Keil's research. Keil claims that if one arranges a person's
judgments ofwhether a set of "predications" ofterms "makes sense" or not, then
one obtains a characteristically hierarchical tree. By predication here Keil means
suchthingsasisloved, isanhourlong, canbethoughtabout, isgreen, andsoforth.
Thus, anapplecanbegreenbutnotanhourlong; arecesscanbeanhourlongbutnot
green; and both can be thought about. This is really just a way ofdescribing how
people categorize things. For instance, a recess and an apple are different things,
because apples can be green but recesses cannot be. As Keil suggests, following
Sommers, thesefactscanberepresentedbypredicatesplacedatthe nodesofagraph
insuchaway that interiornodes ofthegraphdenotepredicates andleafnodes ofthe
graph (those nodes not dominating any other nodes) denote things in the world like
recessesandapples. A leafisdominatedbythepredicatenodesthatit makessenseto
apply to that leaf. For example, since recess can be an hour long or can be thought
aboutbutcannotbe green, itisplacedbelow the firsttwonodes butnotthe last. But
since an apple can be green or thought about but not an hour long, apple is placed
below the nodes can bethoughtaboutand isgreen. Finally, predicates (the interior
nodesofthegraph)areplacedaccordingtotheleavestheyspan. Anodeisdominated
byanothernodeiftheleavesthefirstnodedominatesareapropersubsetoftheleaves
the second node dominates. Applying thistotheexampleproducesthe graph shown
in figure 21-1. Objects in the world (tree leaves) are shown in italics.
Note that the predicate is thoughtabout will typically be at the root ofsuch a
graph. Moreinterestingly, thegraphisalmostalwaysatree. Thekeypointisthatone
rarely finds a natural conceptual structure where the resulting graph forms M- or
W-shaped patterns-that is, a case where a single term is subsumed by predicates
fromtwoseparatehierarchicaltrees. Thisisbecausesuchastructureleadstoindeterminate things in the world, as Keil notes. This is dubbed the M-constraint.
Forexample, supposethatazorchwasaworddenotingeitherabluepyramidor
a red cube. Further suppose thatpyramids could be blue or thought about and that
cubescouldberedorcoloredorthoughtabout. Thepredicationtreewouldthenlook
like the one shown in figure 21-2.
According tothe M-constraint, zorch could not stand fora natural concept, at
least not in the vocabulary ofblocks used earlier. This is because zorch would fall
isthoughtabout
/ \
isanhourlong isgreen
I I
recess apple
Figure21-1: A simple predication tn
--- PAGE 647 ---
BERWICK 633
isthoughtabout
iscolored
isred isblue
isacube isapyramid
zorch
Figure21-2: AnunnaturalconceptresultsinanM-constraintviolation.
under two separate hierarchy trees, violating the M-constraint; note the distinctive
partial W-shaped arrangement ofthe links ofthe graph at the bottom.
Keil developed a method to describe the trees ofdeveloping children. Basically, heaskedchildrenwhetherrecessescouldbegreen, oriftheycouldbeanhour
long, and so forth. He found that predication trees "grow" by the refinement of
existing tree links without the destruction ofexisting domination relationships. No
radical surgery occurs in which a pattern of domination links is completely
destroyed. To see what this means, consider figures 21-3 and 21-4, which show a
thinkof
tall anhourlong
chair TVshow
alive secret
awake
sorry
person
Figure21-3: Samplepredicationtreeatagefive-six. (Thisisasingleindividual'stree.)
thinkof\
f anhourlong
tall \ \
recess
chair idee
alive \
water
tree
awake
sorry
girl
rabbit
Figure21-4: Foliatedpredicationtreeatageseven-eight.
--- PAGE 648 ---
634 CHAPTER 21:THE SUBSET PRINCIPLE
sampleofevidencethatKeilobtainedofchildren'spredicationtreesatagesfivetosix
and then at ages seven to eight.
At the earliest ages studied (five-six years), some children's predictability
trees looked like the one in figure 21-3.
The trees ofsecond graders were foliated versions ofinitial trees ofthis kind;
that is, thenew predication trees developed withoutexisting domination linksbeing
destroyed; new links were simply inserted between existing predicates. Figure 21-5
illustratesthis. Notehowtheclass {alive,awake, sorry} issplitintotwo. Thisisnota
necessary condition for the development ofpredication trees; for example, trees
coulddevelopby ageneral rearrangementofpredicate links. Itcould have beenthat
children first consider an hour long to fall between tall and alive, only to move it
from this position to the position shown in figure 21-4. But this evidently does not
happen.
WhatKeildidnotexplainwaswhypredicationtreesdevelopbybranching. The
Subset Principle can tell us why. Basically, this kind ofbranching corresponds to a
"timid" refinement strategy, in the sense that there are no other refinements that
could be interposed between the new tree and the old one. In other words, the childrenconstructminimalextensionsoftheirwaysofcategorizingobjects intheworld.
Presumably the extensions are minimal in order to avoid overgeneralization. For
example, afirst-gradechildcouldtakethetreeinfigure21-4andmakean "inductive
leap" to a tree ofthe kind shown in figure 21-5 that splits apart alive, awake, and
sorryall in one step.
Ifthechildmadethisleapinonestep, it mightgoastray: thecorrecttreecould
be one where sorry and awake were collapsed together. Note how this tree can be
interposedbetweenthecurrenttreeandtheoverlygeneralguess. Toavoidthispossibility, children sticktheirontological necks out as little as possible-at least, that is
what is suggested by this evidence. Incremental refinement is not necessary, but
apparently it is observed. The Subset Principle dictates whatthe next possible set of
predicationtrees looks like: it shouldbe some minimal refinement ofexisting trees.
thinkof
anhourlong
Figure21-5: A predication tree that ispossibly toogeneral.
--- PAGE 649 ---
BERWICK 635
21.4 LEARNING SOUND SYSTEMS AND THE SUBSET PRINCIPLE
Let us now turn to the question of how the sound systems of language are
acquired. Once again, some relevant background material and terminology will be
presented first, and itwill thenbe shownthatthe Subset Principle can actually help
explain why only certain sound systems exist in the world's languages. This is an
example ofhow a general learning principle may be used to account for a domainspecific constraint, in this case a constraint on what is apossible human language.
According to the distinctive feature theory ofsound systems originally developedbyPragueschoolstructuralistssuchasJakobsonandpursuedby Chomskyand
Hallein TheSoundPatternofEnglish (1968), allnatural soundssuchasaorpcanbe
describedviaasmallnumberofbinary-valueddistinctivefeatures. Byandlargethese
featureshaveanarticulatoryoranacousticgrounding, withnamessuggestiveofhow
the tongue and lips are placed asthey are pronounced, such as high, back, anterior,
and the like. Back refers to a vowel sound produced in the back ofthe mouth; anterior, in the front ofthe mouth; high, with the tongue raised high. There are about
twenty-four features in all. Given binary values for distinctive features (+ or -),
thereare224possiblesinglelanguagesoundsorsegments(aboutsixteenmillion)and
even more possible subsets ofthese segments, what are called segmental systems.
However, mostofthesesegmental systemsarenotattestedinhumanlanguages. Why
isthis?
Inpart, the reason forthis is thatdistinctive features are notdetermined independently ofone another. Rather, certain distinctive features canbe fixedonlyafter
certain other features are set. For instance, according to the theory ofKean (1974),
thedistinctivefeatureconsonantalmustbesetbeforethefeaturebackorcontinuant.
(Consonantalissimplyabinaryfeaturethatis -I- ifasoundisconsonantand - ifthe
sound is not aconsonant. Continuant is a sound produced like a continuous tone.)
Kean developed this theory as a way to explain some ofthe observed restrictions onpossible sound systems andpossiblephonological rule systems. Butthere is
another way to interpret such a theory, and that is as a developmental program for
howasoundsystemisacquired. Byconstruingthetheoryinthisnewway, onecanin
factexhibitanacquisitionsysteminwhichlargenumbersofdevelopmentalpathways
are eliminated because ofthe orderin which a small number ofparameters are set.
Kean'stheory ofmarkedness forphonological sounds (orsegments) will beoutlined
here to show how this approach works in detail. Kean states the basic aim of her
theory as follows:
Itisassumedherethatthereisarelativelysmallsetofdistinctivefeatureswithbinaryspecificationsintermsofwhichallthemembersofeverysegmentalsystemcanbecharacterized
at every stage ofphonological representation. The postulation ofsuch a set offeatures
makesasubstantiveclaimastotheclassofpossibleelementsinphonological systems.
Ofthesetofpossiblesegmentscharacterizedbythedistinctivefeatures, itisevident
thatsomeare present in nearlyevery language, withothersonly occasionally occurring.
Forexample, the segments t and a are nearly ubiquitous in segmental systems; they are
--- PAGE 650 ---
636 CHAPTER 21:THESUBSET PRINCIPLE
found at all stages ofphonological representation in an overwhelming majority oflanguages, butthesegmentskpanduonly occasionallyenjoyaplacein segmental systems.
Thesimplepostulationofasetoffeaturescannotaccountforsuchfacts. (1974, 6)
To explain the relative frequency or rarity ofcertain sounds, Kean posits "a
hierarchy offeatures which is derivable from the intrinsic ordering ... ofmarkedness conventions" (1974, 81). For example, vowels are usually - anterior, consonantsare + anterior. Itisthereforehighlyunusual, ormarked, foravoweltohavethe
feature + anterior. But vowels alsohavethe feature - consonantaland consonants
the feature + consonantal. Therefore, the featureanterioris correlated with that of
consonantal; in the usual or unmarked case the following rule applies:
unmarkedanterior -* + anterior(if wealreadyhavedeterminedthefeature +
consonantal forthe segment)
unmarked anterior -+ - anterior (if we already have determined -
consonantal)
From the complement ofthis rule, we obtain the convention for determining
whatthe value ofanteriorshould be ifit is marked:
markedanterior -* - anterior(ifwealreadyhavedetermined + consonantal)
unmarked anterior -* + anterior (if we have determined - consonantal)
Determining whethera sound segment is marked for anterioror not logically
demands that the feature consonantalbe determined first. Ifthe sound is - consonantal, then the sound will usually be - anterior (the unmarked case); if + consonantal,thesoundwillusuallybe + anterior. Pursuingthisapproach, Keangoesonto
show that whetherthe feature backis unmarked (expected) ormarked (unexpected)
depends on the value ofthe distinctive feature anterior. One obtains the following
hierarchy of distinctive features: consonantal, anterior, back. If this analysis is
appliedtoall twenty-fourdistinctive featuresthat Keanconsiders, one arrives atthe
dependencydiagramshowninfigure21-6. Thisgivesacompletepictureoftheorder
inwhichfeaturesmustbesetforthefeaturevaluesforanysoundsegmentlike/?ortto
be determined.
Here is what the other feature names mean: sonorant is, literally, a sonorous
sound; low is a sound produced with the tongue low; labial, with the lips: lateral.
withthetongueatthesideofthelipsandmouth; coronal, midway up:flapand trill,
by vibrating the tongue; delayedrelease, by blocking air and then exploding it outwards. The other feature names are mostly self-explanatory.
Eachdistinctive feature inthe networkdependsonthose features immediately
above ittodeterminewhether itismarkedornot. Forexample, todeterminewhether
the feature continuant is marked or unmarked we must know the values ofthe features coronal and nasal; to know whether continuant is marked or not. we must
know the values ofthe features coronal and all features above coronal, nasal, and
sonorant.
--- PAGE 651 ---
BERWICK 637
Segmental
Syllabic
Consonantal
LongStressConstricted
Pharynx
Constricted Spread
Glottis Glottis
Figure21-6: Hierarchydiagramfordistinctivefeatures.
Although Kean did not choose to do so, we may interpret this hierarchical
structureasthespecificationofanacquisitionprocedureforlearningasoundsystem.
ThekeypointisthatthisprocedurefollowstheSubsetPrinciple,andthisexplainsthe
appearance ofthe dependency diagram. We now describe how this works.
According to distinctive feature theory, sounds can be distinguished only if
they havedifferentvalues foratleastoneofthetwenty-fourdistinctive features. For
example, thesoundsaand iaredistinguishablegivenasoundsystemthatsetsalldistinctive features forthe two sounds tothe same value save forthe feature back. The
soundaisunmarkedforback, butiismarkedforback(i.e., isexpected). Asanother
example, the soundaeisalsomarkedforback(- back)butisdistinguishable from i
because it is additionally marked + low.
We seethenthat a sound mustbeexplicitly marked in order for ittobe distinguishedfromthedefaultsetofplusandminusvalues. Otherwise, all soundwouldbe
unmarked forall distinctive features, and hence all would possess the same array of
distinctive feature values. In otherwords, ifthe array ofdistinctive feature marks is
--- PAGE 652 ---
638 CHAPTER 21: THESUBSET PRINCIPLE
regarded as partitioning the universe of possible sounds into equivalence classes,
then ifno sounds were marked there would bejust one class ofsounds, the totally
unmarked one.
This remark is not quite accurate, however, since Kean also assumes a basic
syllabic/nonsyllabicdistinction inadditiontopurelydistinctivefeaturecontrasts. As
aresult, thereisalwaysaninitialdivisionofallpossiblesoundsintotwoclasses, consonants and vowels, according to the following rules:
unmarked consonantal -» + consonantal (ifsegment is already - syllabic)
marked consonantal -> - consonantal (ifsegment is already + syllabic)
Given the initial partition defined by the feature syllabic, we can thus distinguishtwoclassesofsounds, evenifnootherdistinctivefeaturesareusedformarking
sounds:
{/, e, ae, u, o, oe, i, e, au, o) {+ syllabic)
{p, t, ty , ...}(- syllabic)
At this stage then, there are in effect just two sounds, "consonants" and
"vowels," asdefinedby local soundcontext. Togeneratenewclasses, we mustmark
additionaldistinctivefeaturevalues. Thekey ideahereisthatthereisadefiniteorder
in which new features are used to form new sound classes. New classes may be
formedby splittingestablishedclasses, withthesplitbasedontheordergivenbythe
distinctivefeaturehierarchy. Supposewestartwithadivisionintojusttwoclassesof
sounds, consonantsandvowels. Thenextpartitionisbasedonthenextunused(previouslyunmarked)featureinthehierarchy. Thisisanaturalassumption. Wecannotget
a new class ofsounds unless we explicitly mark a distinctive feature contrary to its
expected value-otherwise, we would simply obtain the default feature settings for
all laterfeaturesinthefeaturehierarchy. Sowemustmarkatleastonenewdistinctive
feature. Further, since features lowerdown inthe hierarchy depend on the values of
featuresabovethem, thenatural placetolookforthe nextdistinctive feature to mark
or not is the next feature below consonantalin the hierarchy, i.e., eitherthe feature
anterior or the feature sonorant. We cannot skip either ofthese features to try to
mark, say, the feature labial, because the value oflabialdepends upon whether low
and labialweremarkedornot, andthesefeatureshavenotyetbeenevaluated. Solet
ussaythat, ingeneral, anewpartition mustbeformedby markingexactlyoneofthe
distinctive features immediately below the last feature that was marked.
How is a split triggered? This choice must be "data driven" since different
sound systems will have different sounds (from the adult point of view) that are
marked for a particular distinctive feature. For example, as Kean observes, in
Hawaiian only the sound n is marked for sonorant, but in Wichita, it is r that is SO
marked (1974, 57). A split must therefore be triggered by a detectable difference
between at least one o( the members of an existing sound class and the rest of the
members ofthat class. Presumably, this difference could be detected on a variety o\
--- PAGE 653 ---
BERWICK 639
grounds, articulatoryoracoustic. Thenewsoundmightjustsounddifferent. Nothing
morewillbesaidhereaboutjusthowthismightoccur. Whatonecansay, however, is
justwherethenextdistinctionwill bemade. The SubsetPrinciple statesthatthenext
availableunuseddistinctivefeatureintheKeanhierarchymustbeusedasthepointof
refinement. Otherwise the learner could guess too large a language and go astray.
Asanexample,consideragaintheclassofvowels {i, e,ae, eu, . . .}. According
tothe feature hierarchy diagram, the next splitofthisclass mustbedescribed by the
valueofthenextfeaturebelowconsonantal, namely,anterior As itturnsout,thefeature combination [- consonantal + anterior] is physically impossible (the mouth
and tongue cannot produce both features simultaneously) so that in fact the feature
anterior cannot be freely varied given that the value of the feature consonantal is
minus. So the candidate distinctive features that may be used to split the class
{i, e, a, . . . } becomethefeaturesjustbelowanterior, namely, backorsonorant. The
combination [- consonantal, - sonorant] is also impossible, however, so that a
potential splitmustbepursuedbyconsideringback. Featuressuchasstridentorcontinuantwould not be used as this point.
Suppose then that the feature back is selected for marking, forming the basis
for a new partition of sounds. By marking back we obtain the following potential
classes: markedback {i. e, ae, u, . . .} andunmarkedback {a, etc.}. Kean'smarking
conventionunmarkedback -* + backgiven - anteriorestablishesthatmarkedback
must be - back in this case, and unmarked back, + back. In effect, two kinds of
"vowels"havebeenestablished,correspondingtotwopossiblepathwaysthroughthe
hierarchy diagram.
The importantfeature ofthepartitioningprocess is that splitting occurs atthe
leading edge of the directed hierarchy graph by successive refinement of exiting
classes ofsounds. This is a powerful constraint on possible natural sound systems.
Suppose thatthis constraintdid notexist. Then it wouldbe possible to have a sound
system in which the feature sonorant was not used-not set as either marked or
unmarked-but the feature labial was used. There would be a "gap" in the feature
hierarchyskippingovertheuseofafeature. Nosuchsystemexistsamongtheworld's
languages.
Becauseextensionofclassesoccurs solely viatherefinementofexistingpartitions, the setofsoundclasses at stepiwillbearefinementofall ofthose before it in
the developmental sequence. This is simply the same constraint we saw with the
predicationtrees, nowrepeatedinaquitedifferentdomain. Inparticular,thisrestrictionmeansthatjustonedistinctivefeaturewillbesetaseithermarkedorunmarkedat
any single acquisition step. Again, this is not a necessary constraint, since it is not
clearwhy onecould notdevelopa newclassby markingtwoormore features in one
step.
Theeffectofthisconstraintistoguaranteeincrementalacquisition. Atanystep
iinthedevelopmentofasoundsystem,theclassesofsoundswillbeatmostonemark
(m) different. For instance, this constraint excludes the array ofmarks described in
figure 21-7, where m stands for a marked feature and u for an unmarked feature.
--- PAGE 654 ---
640 CHAPTER 21:THESUBSET PRINCIPLE
XI X2 X3 X4
Consonantal m u u u
Anterior m u u u
Back m u u u
Low u m u u
Labial u u m u
Sonorant u u m u
Figure21-7: Animpossibleconfigurationofuandmmarks.
Fromonepointofviewtheone-markconstraintisapuzzlingone. Itisnotatall
obvious why sound systems shouldbe designed sothatthe alteration ofa single distinctive featurecouldconvertana intoan i. This would seemtobeanunwisedesign
choicefromthestandpointoferrordetectionorerrorcorrection; asiswellknown, in
ordertobeabletocorrecterrorsofkbits, thensoundswouldhavetobeseparatedbya
ballofradius2k + 1 (sinceonemustguaranteethatchangesofuptokbitsinanytwo
sounds still leave one able to determine the original sound).
Importantly, natural sound systems do seem to obey the one-markconstraint,
as Kean observes. In other words, the matrices ofra's and u's ofnatural sound systemscannotlookliketheonedepictedinfigure21-7, with nosoundmorethanonem
away from any other. That this is so may be attributed to a design that follows the
Subset Principle. Let us see why this is so.
As the way in which the hierarchy can be interpreted as an acquisition model
has been described here, only one feature can be used to form a new partition of
sounds-onlyonemark(m)iseveraddedatanygivenstep. Asaresult, atanystagein
theacquisitionofasoundsystemthepartitionscorrespondtosoundsthatareatmost
one m apart, automatically satisfying the distinguishability constraint. So Kean's
observation might well be explained as a side effect ofthe acquisition ofsound systems. Even so, it seems as though a stipulation about the well-formedness of segmental systems has merely been replaced with a stipulation about the acquisition of
segmental systems. Why should acquisition be incremental?
Suppose thatacquisition is notincremental andthattwoormore markscanbe
addedatasinglestep. Itwouldthenbepossibletoformanewclasspartitionbasedon
marking both the features labial and sonorant without having first used the feature
sonorantto form any sound classes.
There would be no class that would correctly accommodate a sound that is
labeled [unmarkedlabial, markedsonorant]. Onewaytoremedythisproblemwould
be to allow the procedure to go back and rebuild classes that have already been
formed, but this would violate the developmental ordering that has been assumed.
The fringe ofthe hierarchy tree would no longer summarize the possible next states
that could be hypothesized, sincetherecouldbe sounds such as// that would demand
the interpolationofnewclassesbetweenolderpartitionsandthecurrent partition. In
--- PAGE 655 ---
BERWICK
other words, the one-ra constraint amounts to the demand that new classes be the
minimallyspecificrefinementsofexistingclasses. Itisimpossibletoguessanoverly
general soundsystem, becauseeachnewguess isthe smallestpossible refinementof
preceding guesses. But this isjust the Subset Principle again. At each step, the narrowest language is hypothesized, consistent with positive evidence seen so far.
21.5 LEARNING SYNTAX AND THE SUBSET PRINCIPLE
The SubsetPrinciple subsumesavariety ofproposalsthathavebeenadvanced
in the linguistic literature that order hypotheses for language acquisition. In fact, it
appearsasthoughtheSubsetPrincipleexhaustswhatcanbesaidaboutorderingconstraintsinacquisition. Insupportofthisclaimseveralproposalsthathavebeenmade
regardingtheorderingofhypothesesintheacquisitionofsyntacticconstructionswill
be reviewed. It is not important that the reader appreciate all the details of these
examples. Theintentistogiveafeelforthevarietyofdifferentkindsofgrammatical
constructions that fall underthe Subset Principle.
21.5.1 An Adjacency Requirement in English
InEnglish, nounphrasedirectobjectsmustbeadjacenttoverbs: Igaveabook
quicklytoBillisfine,butIgavequicklyabooktoBillisnot. (Insomelanguages, such
asFrench, thisconstraintisweakenedsothatanadverbmaybeinterpolatedbetween
verbandobject; inotherlanguages, such asJapanese, thisconstraint is so weakthat
the object can be quite distant from the verb.)
How isthe adjacency requirementacquired? Once again, the Subset Principle
may be invoked. The most restrictive assumption possible is that adjacency holds
since itgeneratesthenarrowestclass oflanguagepossibilities. Toassumeotherwise
wouldbe to guess a language that couldbe too large, hence a possible Subset violation. A language satisfying the adjacency condition could be aproper subset ofone
that was not and yet coverthe same triggering data. The acquisition procedure thus
assumesanadjacencyrequirementasthedefault, unmarkedcase, looseningitonlyif
positiveexamplesareencounteredthatindicateviolationsofadjacency. Sinceexamples violatingadjacency {IhithardlyBill) will neverbeencountered in English, this
strict requirement will neverbe dropped.
Inotherlanguages (likeFrench)positiveexamplesexhibitingadjacencyviolations would prompt a relaxation ofthese conditions, perhaps along a continuum
of possibilities. Thus one might expect to find languages where strict adjacency
was relaxed according to a hierarchy ofphrasal types. This prediction seems to be
confirmed.
--- PAGE 656 ---
642 CHAPTER 21: THESUBSET PRINCIPLE
21.5.2 Arguments of Verbs
Verbs differ in the number of noun phrase objects (or arguments) that they
requireand in whetherthose argumentsareobligatory oroptional. Forexample, eat
may or may not take an argument denoting the thing eaten: John ate an ice cream
cone,Johnate. Incontrast, takemusttakeanargument: Johntookanicecreamcone
isfine,butJohntookisnot. Notethatalanguagewhereaverbmayormaynottakean
argument is a superset of a language where that verb must take an argument. If
hypotheses are to be ordered by the Subset Principle, the first guess to make about
any verb is that ifit appears with an argument, then that argument mustbe assumed
obligatory until apositive example appears in which that argument is not present at
all; ifsuchanexampleappears, theargumentisoptional. Thisstrategyisobservedin
children (Roeper, 1982).
21.5.3 Bounding Nodes for Subjacency (Rizzi, 1978)
Inmostcurrenttheoriesofgenerativegrammar, itisassumedthatgrammatical
rulesobeyacertain "localityprinciple," inthatamovementcannotcrossmorethana
single sentence boundary. For example, in the first sentence below, John is understoodasthesubjectoftheembeddedsentencetolikeicecream. Thesecondsentence
is ill formed ifinterpreted this way. The only difference is that the second sentence
interposes an additional sentence boundary via the it is certain clause. Square
brackets markthese boundaries.
1. John is certain [s trace to like ice cream]
2. John seems [s it is certain [s trace to like ice cream ]]
This called the subjacency constraint. This constraint is also what makes the
followingsentencepoor, wherethewhowordsarelinkedtopositionsas indicatedby
subscriptsiandy*. Forexample,thefirstwho istheobjectofknow. Unfortunately,two
S's must be crossed to linkup to this position-hence the sentence is no good.
The man who I don't know who knows.
The man [who,[ s (first S) I don't know [who, [s (second S)j knows /']]]]
Interestingly, this lastsentenceisgrammatical inItalian, asdiscussedby Rizzi
(1978):
L'uomo [wh, che non so [chi, [sjconosca /JJJ]
According to Rizzi, this is because it is afull clause with that orfor in it that
counts for subjacency in Italian, notjust a simple "S" or sentence. (A full "S" in
English wouldbesomethinglikeForJohn togo
. . .
.)This kindofphrase iscalled an
S-bar. Rizzi claims that S-bar, not S, is whatcounts in Italian. Therefore, the who in
the sentence above can be the object ofconosca (know) because it crosses only a
single full clause boundary that starts at chi. The second boundary is an S. not an
--- PAGE 657 ---
BERWICK 643
S-bar. Apparently, the choice ofabounding node is yet anotherparameterthat must
be set in orderto learn a language.
SupposeRizzi'sanalysisiscorrect. Howcouldthechoiceofboundingnodebe
determined on the basis of evidence received by an acquisition procedure? Once
again, letusapplytheSubsetPrinciple. IftheboundingnodeforsubjacencyisS,then
anarrowerclassoflanguagesisgeneratedthaniftheboundingnodeforsubjacencyis
S-bar. Therefore, bytheSubsetPrinciple, theacquisitionprocedure'sfirsthypothesis
should be to set the bounding node for subjacency to S. In other words, the default
assumption is that all languages are like English inthis regard. Ifthis assumption is
wrong,thenapositiveexamplewillappearthatviolatesS-bounding-asintheItalian
exampleabove. Thentheacquisitionprocedurecanresetthesubjacencyparameterto
the next "largest" value, namely, S-bar.
21.6 SUMMARY AND CONCLUSIONS
Thischapterhasshownthatthereisatleastonequitegeneralprincipleoflearning, the Subset Principle, that applies "across the board" in a domain-independent
fashion. The SubsetPrinciple arrangestheorderofhypothesesthat alearner should
advance in the face ofpositive-only evidence. The principle has wide applications,
showing up in such diverse domains as the acquisition ofcategory concepts, sound
systems, and syntax. It has been used explicitly in at least one model for the acquisition oflanguage (Berwick, 1982) and implicitly in the version space model of
acquisition.
The Subset Principle makes strong predictions about the order of events in
humanlanguageacquisition. Areany ofthesepredictionsconfirmed? In fact, recent
experimentaltestshavebeenmadeoftheorderingconstraintsimpliedbytheacquisitionproposalsdescribedinthissection. Childrenareaskedto "actout"certainsituations with toy animals in order to see ifthey understand particular sentences ofthe
sortdescribed above. The results are preliminary (Wexler, 1984), but so farcorrespond exactly to the predictions ofthe Subset Principle.
IthasalreadybeenseenthatKeil'sworkinconceptacquisitionpointstoconfirmation ofthe Subset Principle. What ofthe acquisition oflanguage sound systems?
Results here are sketchy. However, there is at least one "classic" piece ofevidence,
namely, theobservationsofJakobson (1968). The SubsetPrincipleorderingpredicts
that t, p, and k would be among the first consonants acquired and a, i, the first
vowels. This sequencing appears to be roughly verified by empirical work, though
therehasbeencontroversyregardingJakobson'smorerestrictedandprobablyoverly
strong proposal.
Itremainstobeseenwhetherotherkindsofhumanlearning, suchastheacquisition ofarithmetic skills, abide by the Subset Principle. There is at least some suggestiveevidence(seeVanLehn, 1983) thattheydo. Several natural learning systems,
--- PAGE 658 ---
644 CHAPTER 21: THESUBSET PRINCIPLE
then, obey the Subset Principle, in which positive-only example evidence plays a
dominant role. Machine learning systems would do well to follow this successful
design.
ACKNOWLEDGMENTS
ThischapterdescribesresearchcarriedoutattheArtificialIntelligenceLaboratory at MIT. Support for the Laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency under Office of Naval
Research Contract N00014-80-C-0505.
References
Angluin, D., "InductiveInferenceofFormal LanguagesfromPositiveData," InformationandControl,
Vol. 45, pp. 117-35, 1978.
Berwick, R., "Computational Analogs ofConstraints on Grammars," Proceedings ofthe Eighteenth
AnnualMeetingoftheAssociationforComputationalLinguistics, Philadelphia, Pa., pp. 49-54,
1980.
, "LocalityPrinciplesandtheAcquisitionofSyntacticKnowledge," Ph.D. diss., Departmentof
ElectricalEngineeringandComputerScience, MIT, 1982.
Brown,R.,andHanlon,C, "DerivationalComplexityandtheOrderofAcquisitioninChildSpeech,"in
CognitionandtheDevelopmentofLanguage, J. R. Hayes(Ed.), Wiley, NewYork, 1970.
Chomsky, N.,RulesandRepresentations, NewYork, ColumbiaUniversityPress, 1980.
Chomsky, N., andHalle, M., TheSoundPatternofEnglish, NewYork, HarperandRow, 1968.
Dietterich,T.,andMichalski,R., "AComparativeReviewofSelectedMethodsforLearningfromExamples,"inMachineLearning:AnArtificialIntelligenceApproach,R.S.Michalski,J.G.Carbonell.
andT. M. Mitchell(Eds.),Tioga, PaloAlto, Calif., 1983.
Gold, E., "LanguageIdentificationtotheLimit," InformationandControl, Vol. 10, pp. 447-74. 1967.
Jakobson, R., ChildLanguage , Aphasia, andPhonological Universals, TheHague, Mouton. 1968.
Kean,M., "TheTheoryofMarkednessinGenerativeGrammar,"Ph.D.diss. DepartmentofLinguistics.
MIT, 1974.
Keil, F.,SemanticandConceptualDevelopment:AnOntologicalPerspective, HarvardUniversity Press.
Cambridge, 1979.
Marshall,J., "LanguageAcquisition inaBiological Framework." inLanguageAcquisition. P. Fletcher.
and M. Garman(Eds.), CambridgeUniversity Press, NewYork. 1979.
Mitchell,T., "VersionSpaces: AnApproachtoConceptLearning,"ComputerScienceReportCS-78-711.
Stanford University, 1978.
--- PAGE 659 ---
BERWICK 645
Rizzi,L., "ARestructuringRuleinItalianSyntax,"inTransformationalStudiesinEuropeanLanguages,
S. J. Keyser(Ed.), MITPress, Cambridge, 1978.
Roeper,T., "OntheDeductiveModelandtheRoleofProductiveMorphology,"inTheLogicalProblemof
LanguageAcquisition, C. BakerandJ. McCarthy (Eds.), MITPress, Cambridge, 1982.
Sommers, F, "Structural Ontology." Philosophia, Vol. 1, pp. 79-85, 1971.
VanLehn, K., "ValidatingaModel ofChildren'sArithmetic Skills: Sierra." ProceedingsoftheInternationalMachineLearning Workshop, R. S. Michalski(Ed.), AllertonHouse. UniversityofIllinois
atUrbana-Champaign, June22-24, 1983.
Wexler,K.
"IndependenceandtheSubsetPrinciple."UniversityofMassachusettsConferenceonFormal
ModelsofLanguageAcquisiton, Amherst, 1984, forthcoming.
Wexler.K..andCulicover, P.,FormalPrinciplesofLanguageAcquisition, MITPress.Cambridge. 1980.
Winston, P., "Learning Structural Descriptions ofBlocks World Scenes from Examples.'* in The PsychologyofComputer Vision, P. H. Winston(Ed.). McGraw-Hill, New York. 1975.
--- PAGE 660 ---
--- PAGE 661 ---
PRECONDITION ANALYSIS:
Learning Control Information
Bernard Silver
UniversityofEdinburgh
Abstract
ThischapterdescribesalearningtechniquecalledPreconditionAnalysis. Aprogram
usingthistechniquelearns fromtheworkedexampleofacorrectlyexecutedtask. In
thissense, PreconditionAnalysisisaformof"learningfromexamples,"butitdiffers
from most such learning methods intwo ways:
1. Precondition Analysis is principally used to learn strategies for problem
solving. Thiscontrastswiththemoreclassicalusesoflearningfromexamples,
in which the program learns concepts ordecision rules.
2. Thetechnique allowsaprogramtolearn fromasingleexample, whereas most
other systems require several examples.
Precondition Analysis is described in the context ofa program called LP that
learns new techniques for solving symbolic equations. Precondition Analysis has
been successfully used by LPto learn several new equation-solving strategies.
22.1 INTRODUCTION
This chapter uses symbolic equation solving as an example domain. The program LP uses questions taken from A-level mathematics papers, such as1
sin(2*x) + sin(3*x) + sin(5*x) = 0. (1)
'A-levels are taken at age eighteen in England and Wales and are used as a criterion in selection for
university.
--- PAGE 662 ---
648 CHAPTER 22: PRECONDITION ANALYSIS
Thetaskofsolvinganequationconsistsoftakinganequationandtransforming
it using legal algebraic operations until a solved state is obtained. The equation is
solvedwhen itisoftheform* = ans, wherexistheunknownandansdoesn'tcontain
x, or ifit is adisjunction ofsuch terms.
Equation solving isadifficulttaskbecauseateach stepalargenumberofpossible legal operations canbeperformed. Bundy (1975) shows thatten is a conservativefigureforthebranching rate inthis domain. Possibleoperations includethe use
ofcommutativity, identity, and functional reflexive axioms. For example, equation
(1) can be transformed to many forms, including the following:
sin(2*;c) + sin(3*x) = -sin(5**)
sin _1 (sin(2*jc) -I- sin(3*Jt) + sin(5*x)) = sin _1
sin(2*jc) + 2*sin(4*x)*cos(jt) = (2)
Ofthethreeexamplesshown, onlythetransformationto (2) isagoodchoice, asthis
equation lies on an optimal solution path. The same kind ofchoice occurs at every
step of the solution; that is, there are many ways of transforming equation (2),
including one that transforms it backto (1).
TheaveragelengthofthesolutionpathfortheproblemsusedbyLP isaboutten
steps. This implies that the average search space contains ten billion nodes at the
depth at which the solution occurs. Some ofthe paths rejoin, so the actual search
spacewon'tcontainthisnumberofdifferentnodes. However,thenumberwillstillbe
very large. A program that had to examine all these possibilities would soon get
bogged down in what is known as the combinatorial explosion. As the number of
steps inthe solution increases, the numberofpossiblepaths increasesexponentially.
What is needed is some way ofpruning the search space. The program should only
consider a small subset ofpossible operators at various points in the search.
Skilledhumanscan findthecorrect solutionpathwithalmost no search at all.
Apparently these people are using some technique of search constraint. The LP
project is an attempt to build a program that can learn this type ofsearch constraint
information so that it can be taught to solve new types ofequations.
22.1.1 Precondition Analysis
A program using Precondition Analysis works in two phases, the learning
cycle and theperformancephase. During the learning cycle, the program is given a
workedexample. Theworkedexampleshowshowtosolveanequation. Theexample
consists ofa sequence ofproblem states, snapshots ofthe equation-solving process.
No other information is given; for example, the program isn't told how one state
arises from the previous one.
--- PAGE 663 ---
SILVER 649
LP learns three kinds ofthings during the learning cycle:
1. Rewrite rules. These are algebraic facts, such as
cos(;c) + cos(y) - 2*cos((;c + y)/2)*cos((;t - y)ll).
These are suppliedby the user (see section 22.4.2.1.)
2. Methods. LPhasasetofproblem-solvingoperators, calledmethods. Methods
apply rewrite rules. They have conditions indicating how and when various
rules should be used. Methods are described in more detail in section 22.3.1.
3. Schemata. These are equation-solving plans and are usedby the performance
element.
The learning cycle consists offourphases:
1. Operator Identification. In this phase, LP tries to discover which methods
were used in the worked example. The question is, How was each step in the
worked example performed? LP may discover that a step in the worked
example uses some method that it doesn't have. In this case, LP must learn a
new method.
2. PreconditionAnalysis. Herethequestionis, What isthepurposeofeach step?
3. Method creation. In this phase, LP creates and stores new problem-solving
methods.
4. Schema creation. Finally, LP stores all this information in a schema. The
schema records the sequence of methods that were used to solve a worked
example. The schema also contains the strategic information indicating why
each method was used.
Theschemaisexecutedinaflexiblewaybytheperformanceelement. LPtries
tousethe same sequenceofmethodsasareusedintheworkedexample. Usuallythe
sequence can't be executed exactly. As the schema records the strategic reasons for
each step, the program is able to patch the plan in a sensible way.
LP isabletolearnanewtechniquebyexaminingasingleworkedexample. As
Nevespointsout(1978), manytextbookspresentjustoneworkedexamplebeforeproceeding to the exercises. This consideration rules out many concept-learning
methods (see, e.g., Mitchell, 1978; Winston, 1975), as these programs generally
need toprocess several instances before the concept is learned.
The learning cycle is described in more detail in section 22.4, and the performance phase is discussed in section 22.5. Various parts ofthe precondition analysis
techniqueare similartopartsofthelearning methodusedby otherresearchers, particularly Mitchell (1978, 1983) and DeJong (1983).
--- PAGE 664 ---
650 CHAPTER 22: PRECONDITION ANALYSIS
22.2 BACKGROUND TO LP
LP is built on the equation-solving program PRESS2 (Bundy and Welham,
1981; Sterling et al., 1982). Both programs are written in PROLOG (Clocksin and
Mellish, 1981). PRESS isanonlearningprogram; theonlywaytoincreaseitsability
istowritenewcode. Thereisatestsetofequations, andeachversionofPRESS isprimarilyjudgedbyhowmanyoftheseequationsitcansolve. Thetestsetcontainquestion from A-level mathematics papers.
Other research has dealt with learning equation-solving methods, but this
author'sinterestsaresomewhatdifferent. Mostoftheotherfairlyrecentworkinthis
field (e.g., Brazdil, 1978; Langley, 1983; Neves, 1978) has concentrated on very
simple equations, such as
2*jc + 3 = 5.
Such work has demonstrated that computer programs can learn to solve equations,
building up from very little knowledge. However, the strategies used to solve such
equationsarerathersimple; thedifficultpartistoteachtheprogramalgebraic rules.
TheA-level standardquestionsusedby LParemuchharderthanthoseconsidered by other researchers. Knowledge of algebraic rules is no longer sufficient to
solvesuchequations;thesearchspaceistoolarge. Thesolverrequiressomestrategic
guidance, knowledge ofhowand when to apply the rules.
22.3 THE PROBLEM-SOLVING METHODS
Manylearningprogramsworkindomainsinwhichthebasicoperatorsaresimilar to those of STRIPS (Fikes, Hart, and Nilsson, 1972). LEX (Mitchell, 1983;
Mitchell, Utgoff, and Banerji, 1983), and ALEX (Neves, 1978) are examples.
The equivalent ofoperators in LP are the methods. However, methods do not
haveoneofthedesirablepropertiesofSTRIPS-typeoperators. Thepreconditionsof
a method represent necessary but not sufficient conditions, whereas the preconditions of STRIPS-type operators are both sufficient and necessary. In general, a
method is not certain to succeed even if the preconditions are applicable. This is
becausethepreconditionsaretoogeneral,butstrongerpreconditionscannotbegiven
thatdonotinvolveactuallyapplyingthemethodtotestifitisapplicable! Similarly, it
isdifficultto specify effectsoftheoperator. It seemsthatthis mightbe aproblem in
many domains.
ThefactthatLPmethodslackthesepropertiesisimportant. Ifthemethodswere
aswell behavedasSTRIPSoperators, it wouldbepossibletobuildamuch more powerful planning system, and a lotofthe heuristic nature ofLP would be unnecessary.
'PRolog EquationSolvingSystem. LPstands forLearning Press.
--- PAGE 665 ---
SILVER 651
22.3.1 The Methods of LP
LPbegins with about fifteen methods. Each method has (1) a setofpreconditions; (2)asetofpostconditions; (3)andassociatedsetofrewriterules; and(4) some
information indicating how to apply the rewrite rules-for example, information
aboutwhethertherewriteruleshouldbeappliedtothewholeequationorjusttosome
special subterms. The postconditions are used to specify what must be true after a
methodhasbeenappliedinthedesiredway, butthereisnoguaranteethatthemethod
willmakethesetrue; thatis, it maybepossibletoapplythemethodinanundesirable
way, where the postconditions are false. Postconditions are used to check that the
method has been applied in the right way.
Each LPmethodhasanassociated setofrewrite rules. Amethodcanonly use
the rules initsassociatedset. The rules inasetarerelated inthatthey maybeableto
achieve aparticular kind ofeffect. Some examples are given below.
In the following sections, one method, Isolation, is described in detail, and
fourothers are described briefly.3
22.3.1.1 Isolation
Isolation is a method that solves equations containing exactly one occurrence
oftheunknown. Considerthefollowing(verysimple)example, showingthesolution
ofthe equation
log,(2**) 1.
log,(2**) 1 (3)
2*x = e (4)
x = ell (5)
Thelastlineoftheexample, line(5), isthesolutiontotheequation. Thereisasingle
occurrence ofx, isolated on the left-hand side.
Thepreviousline(4)alsohastheonlyoccurrenceofxontheleft-handside,but
in this case it appears as the first argument ofthe multiplication function. The last
line is obtained from (4) by applying the inverse ofthis function to both sides.
In the case of line (4), it can be said that multiplication is the outermost
function.
Similarly, line (4) is obtained from line (3) by removing the outermost function, which is the logarithmic function in this case. A function is removed by
applyingthe inverseofthatfunctiontoboth sidesoftheequation. Inthiscasethis is
done by exponentiating both sides.
^AlloftheLPmethodsexceptforFactorizationPreparation,describedinsection22.3.1.5,arealsoPRESS
methods. PRESS methodsaredescribed inLangley(1983)andSterlingetal. (1982).
--- PAGE 666 ---
652 CHAPTER 22: PRECONDITION ANALYSIS
This process ofremoving the outermost function is calledIsolation. Isolation
works on equations containing exactly one occurrence ofthe unknown. At each
application it removes the outermost function surrounding the occurrence ofthe
unknown, until the unknown appears alone.
The precondition of Isolation is that the equation Eqn contains exactly one
occurrence ofthe unknown X, written as single-occ(Z,Eqn). The postcondition is
thatthe resultant equation is solved. Isolation rules are ofthe form
C(UU U 2, . . . , U n ,B) & F(U U U 2 ,...,U n) = B=$> U t = F~\B),
where C isa(possiblyempty) setofconditions, U containstheunknown, andF~x is
theinverseofFwithrespecttothe/thargumentofF4Forexample,
thefirstIsolation
step above uses the rule
B > & V\i = B => U 2 = logcjfl
(HereF isthetwo-argumentexponentiationfunction(F(X,Y) = XY ), /is2, andC is
thesingleconditionthat Bmustbepositive.)Isolationrulesareappliedinarestricted
manner. Atany stageIsolationstripsoffonlytheoutermostfunctiondominatingthe
unknown. Forexample, given the equation
sin(cos(jc)) tan(tf),
Isolation strips offthe sin function ratherthanthe tan one.
22.3.1.2 Collection
Ifthe equation contains more than one occurrence ofthe unknown, Isolation
cannotbeapplied. In suchcases LPmaybeabletouse anothermethod, Collection,
that tries to reduce the numberofoccurrences ofthe unknown.
Collection rules include the following:
U*N + U*M - U*(N + M)
U2 + 2*(7* V + V2 => (U + V)2
Inthefirstcasethetermcorrespondingto f/mustcontaintheunknown. EitherUoxV
can contain the unknown in the second case.
Like Isolation, Collection applies its rules selectively. Collection applies its
rules to subterms ofthe equation that have two immediate subterms containing the
unknown. (Thesearecalledleast-dominatingsubtermsinBundyandWelham, 1981.)
Collection can only be applied to equations containing two or more occurrences ofthe unknown, and its effect is to reduce the number ofoccurrences. Note
4Insomecasestherewillbeadisjunctionoftermsontheright-handside. Forexample, if Fisthesquare
function:
x1 = a => x = au2 V x = -(av2 ).
--- PAGE 667 ---
SILVER 653
that Collection need not reduce the numberto one, so Isolation may not apply after
Collection. Sometimes a sequence of Collection applications will be needed to
reduce the number ofoccurrences to one so that Isolation can be applied; in other
cases totally different methods will be needed.
ThepreconditionofCollectionisthattheequationEqncontainmorethanone
occurrenceoftheunknown. Thepostconditionis thatthe new Eqnhas feweroccurrences thanthe original equation.
22.3.1.3 Attraction
Attraction is a method that moves occurrences of the unknown "closer"
together5 inthehopethatCollectionwillthenbeapplicable. Attractionhasthe same
precondition as Collection. Its postcondition is that the unknowns are closer in the
resultant equationthan in the original one. Attraction rules include the following:
log^£/ + log^F- log^(U*K)
U*W + V*W -* (U + K)*W
In these rules the terms U and V are attracted, so these terms must contain the
unknown and the other terms must not. Like Collection rules, Attraction rules are
appliedto least-dominating subterms.
22.3.1.4 Factorization
Anequation ofthe form
a*b*c . . . * k = (6)
canbe transformed to the disjunctive set ofequations
a = 0\/b = 0\/...\/k = 0. (7)
Eachmemberofthedisjunct(7)canbesolvedasanindependentequation. Thesolution ofthe original equation (6) is the disjunction ofthe solutions to the individual
equations in (7).
LP is able to perform this type of transformation using its Factorization
method. The preconditions of Factorization can be expressed as the set {rhszero(Eqn), lhs-prod(jc,Eqn)}; that is, the right-hand side must be zero, and the lefthand side mustbe aproduct intheunknownX.
Factorizationhastheeffectofsplittingacomplexequationintoseveralsimpler
ones. Thenewequationscontainfeweroccurrencesoftheunknownthantheoriginal
5Theconceptof"closeness"hasbeendefinedinapreciseway;seeBundyandWelham(1981)fordetails.
--- PAGE 668 ---
654 CHAPTER 22: PRECONDITION ANALYSIS
one, and thus it is possible that Isolation can apply to the individual factors. For
example, Factorization splits
cos(;c)*sin(jc)*tan(jt)
into three factors, each ofwhich can thenbe solved by Isolation.
Keymethods. Factorization shouldbeused whenever it is applicable. It is oneofthe
keymethods, methodsthatshouldbeappliedwheneverpossible. Isolation is another
key method. Key methods play a special role in Precondition Analysis.
22.3.1.5 Factorization Preparation
ThelastmethodtobedescribedisFactorizationPreparation. Asthenamesuggests, FactorizationPreparationtransformstheequationsothatFactorizationcanbe
applied.
This methodtakes an equation ofthe form
e\ + e 2 + . . . + e n =
andtransforms it to
+ + + =
/*(<?', e'2 . . . e'n) 0, (8)
where/is a common factorofthe e, and
/V, =
The e and/must contain the unknown;/is called the common subterm ofthe lefthand side.
This method uses the distributive law (backwards)
A*B + A*C + . . . + A*M - A*(B + C + + hi). (9)
Note that Factorization can now be applied to equation (8).
The preconditions ofFactorization Preparation are
{rhs-zero(Eqn), mult-occ(X,Eqn), lhs-sum(X,Eqn),
common-subterms(X,Eqn)}
Note that the common subterm must contain the unknown and must be a (top-level)
multiplicative factor ofeach member ofthe sum. This last condition, expressed as
common-subterms (X,Eqn), impliesthatthedistributivelawcanbeapplied and preventsx from being counted as a common factorof
cos(*) -I- cos(2*jc) -I- cos(3*x).
--- PAGE 669 ---
SILVER 655
22.3.2 Using the Methods
Now that some ofthe methods have been described, this section will describe
how LP uses the methods to solve equations before learning has taken place.6
Learning modifies thebehavior; see section 22.5.
The technique used by LP is similarto that used inthe Boyer-Moore theorem
prover. BoyerandMoore (1979)usethetermwaterfalltodescribetheirtop-levelprocess. This terminology will be followed here: the term heuristic waterfall will be
used to describe the control flow ofLP.
The waterfall consistsofa numberofmethods. Atthe topofthe waterfall, LP
checkstosee iftheequationisalready solved. Ifitis, LPreturnstheanswer, andthe
equation is removed from the waterfall. Otherwise, the equation is passed over the
waterfall. As the equation goes down, the methods try to transform it. Ifa method
succeeds in transforming the equation, the new equation is sent to the top of the
waterfallandtheprocessis repeated. IfamethodsuchasFactorizationcreatesmore
than one equation, all such equations are sent to the top.7 Ifa method fails totransformtheequation, theequationfallstothenextlevel, wherethenextmethodistried.
The process terminates with success when there are no more equations to be processed. Ifan equation falls rightthrough the waterfall-that is, no methodcan transform the equation-LP backtracks. Finally, ifall possibilities have been tried and
equationsstillremaininthewaterfall,theprocessterminateswithfailure; thatis, LP
fails to solve the equation.
Generally, the key methods (such as Isolation and Factorization) should be
attempted first, and this is reflected in the ordering. The rest ofthe ordering was
determinedexperimentally. Variousdifferentorderingsweretriedonsometestproblems, and the output was evaluated on several criteria. These criteria included considerationsofefficiencyandofwhethertheoutputresembledtheoutputthathumans
would produce when solving the problems themselves.
Thisorderingcanbeviewedas implementingakindofplan. Forexample, LP
firsttriestoapplyIsolation. Ifthisfails, ittriesCollectionandthenattemptstoapply
Isolation to the result.
Note that the waterfall is a very simple control mechanism. Such a simple
device can be used because the search space of the methods is small and well
behaved. By wellbehavedis meant that ifa method is applicable, it will usually be
right to apply it, and ifthis decision is wrong, dead ends are quickly reached.
Onmorecomplexproblems, however, thewaterfall isjusttoosimple. Theprogram requires more guidance than that provided by the ordering ofthe waterfall.
6PRESSusesthetechniquedescribedhere.
7A11 thesesubproblemshavetobesolved; theprogramattemptstosolvetheminanarbitraryorder.
--- PAGE 670 ---
656 CHAPTER 22: PRECONDITION ANALYSIS
Such information is provided by worked examples. LP uses these to build a schema
that provides a more flexible control.
22.4 THE LEARNING CYCLE
The learning cycle falls into fourphases:
OperatorIdentification
2. Precondition Analysis
3. Methodcreation
4. Schema creation
Each ofthesephases will bedescribedafterthecharacteristics ofthe worked examples used by LP are examined.
22.4.1 Worked Examples
Aworkedexampleshowsthestepsinvolvedinthesolutionofanequation. The
example showsvariouspoints inthe solutionprocess. Itisarrangedas a sequence of
lines, eachlinebeinganequationoradisjunctionorconjunctionofequations. Generally, each line can be transformed into the next by the application ofa sequence of
legal algebraic operations. Such a sequence is called astep.
A worked example should contain enough detail so that the reader (either a
human or a program) can understand the technique being demonstrated, but there
should not be so much detail that the important points are swamped.
Figure22-1 showsatypical instanceofthetypeofworkedexampleusedbyLP.
The example shows the solution ofthe equation
cosOr) + 2*cos(2*jc) + cos(3*x) = 0.
cos(.x) + 2*cos(2*jc) -1- cos(3*x) = (1)
2*cos(2*jt)*cos(x) + 2*cos(2*x) = (2)
2*cos(2*jt) = *(cos V (jt) + 1) + = 1=0 (3)
cos(2*;t) cos(x) (4)
cos(2*;t) (5)
x = 180*A7, = 45 (6)
+1=0
cos(jc) (7)
= -
cos(jc) (8)
x = 180*(2*/? 2 + 1) (9)
Figure22-1: A workedexample.
--- PAGE 671 ---
SILVER 657
Theanglemeasureisdegrees, andn^ andn 2 arearbitrary integers. Thisexamplewill
be used in the discussion here. This worked example is typical ofthose used by LP.
Note that the examples contain no annotations or grouping information.
Generally, itisassumedthateachlineintheworkedexamplefollows fromthe
previous one; for example, the step from (3) to (4) is recognized as a Factorization
step. However, this assumption breaks down ifthe example is presented linearly in
the way adopted here: line (5) doesn't really follow from line (4), and line (7) certainly doesn't follow from line (6).
LP overcomes such problems by looking out for special situations. It knows
that some examples fall intodistinctsections. Examples thatuse Factorization form
one such category. The first section ofa Factorization example consists ofthe steps
leadinguptotheFactorizationstep. Infigure22-1,lines(1)to(4)formthissection. If
the Factorization step gives rise to n factors, the example then falls inton more sections, each section containing the steps that solve one factor.
Each ofthe factor sections is considered separately backto the "parent" Factorization step that formed it. In figure 22-1, lines (5) to (6) form one section, and
lines (7) to (9) form another. Theparentstepis (4). LPneedstoknow howtodivide
examples into sections; such knowledge can be given explicitly.
Eachtypeofsectionhas
purpose. ThepurposeofthefirstsectionofaFactorizationexampleistoallowFactorizationtoapply. Thepurposeoftheothersectionsis
simplytosolveafactor. Theconceptofsectionsandtheirpurposesisusedduringthe
performance phase (see section 22.5).
22.4.2 Operator Identification
Aftersectioning the example, LPhasto discoverhoweach line in the worked
example is transformed into the next.
Theprogramexaminesconsecutivepairsofstepswithineachsection; thatis, it
examinesline(1)andline(2),thenline(2)andline(3),andsoon. NotethatforOperator Identification the example is examined in a forward direction, from the initial
equation. This contrasts with the backward examination that takes place during the
Precondition Analysis phase.
SupposethatLP isworkingonthe stepfromaline1tothenextline, say m. LP
firsttriestoseeifanexisting methodcanaccountforthe step. Todothis, LPtriesto
find a method whose preconditions are satisfied by 1 and whose postconditions are
satisfied by m.
IfLPfindsamethod, itattemptstouseittotransform
intom. Ifthemethodis
successful, LP records that the step from 1 to m was performed by that method and
proceeds to the step from m to the next line.
--- PAGE 672 ---
658 CHAPTER22: PRECONDITION ANALYSIS
Forexample, considerthe workedexample shown in figure22-1. Supposethat
LP is working on the step from line (2) to line (3), that is,
2*cos(2*x)*cos(jt) + 2*cos(2*x) = (2)
2*cos(2*jt)*cos(x) + 1) = (3)
LP discovers that Factorization Preparation can transform line (2) to line (3).
In the interesting case, the program cannot find a method that would account
forthe step and marks the step as not immediatelyparsed.
Supposethattheprogramhasastepthatitcannotaccountfor;thatis, noknown
methodwouldproducethetransformationbetweenlines1 andm. Therearetwodistinct cases. The first case is when LP finds a method, say, M, whose preconditions
andpostconditionsaresatisfiedbythetwolines, butitfindsthat can'tproducethe
transformation. In this case it informs the user that it is probably missing a rewrite
rule forthe method M. Ata laterstagethe user is askedtoprovidethe missing rule,
and this rule is then made available to M.
The second case is when no apparently applicable method is discovered. The
programthenassumesthatanunknownrewriterule(i.e., anidentity)hasbeenused.
Once ithasbeengiventherulebytheuser, itwillneedtocreateanewmethodthatcan
apply the rule.
Ineithercase, the program can usethe worked exampleto conjecture the particularinstanceoftherule. Todothis, LPmakesuseofavery obviousbutextremely
effectiveheuristic. Theheuristiccanbeparaphrasedas follows: Giventwoconsecutive lines of an example, delete all terms common to both and conjecture that the
remaining expressions are equal.
For instance, given
A + B = C
B + D = C
as two consecutive lines of a worked example, it is reasonable to conjecture that
A = D.8
Considertheworkedexample shown infigure22-1. Forexample, supposethat
the program does not understand the step from (1) to (2), that is,
cos(jc) + 2*cos(2*x) + cos(3*x) = (1)
2*cos(2*jc)*cos(jc) -I- 2*cos(2*x) = (2)
*LPknowstactsaboutthecommutative natureof + .etc., soitisnotcontusedb> thechangingtigumem
positionofH.
--- PAGE 673 ---
SILVER 659
Usingtheheuristic, theprogramnotesthatbothlinescontaintheadditiveterm
2*cos(2*x). Itdeletesthisandconsidersthe rest. Nothingelse is common, soitproduces the correct conjecture
cos(x) + cos(3*x) = 2*cos(2*jc)*cos(;c) (10)
The heuristic is notperfect; there areexamples onwhich LPcan makeerrors.
Neves (1978) uses a similarheuristic in hisprogram ALEX. However, ALEX is not
giventhe rule; itguessesthe ruleby generalizingthe conjectureand replacing numbers with variables where this appears to be appropriate. ALEX can be misled by
spurious correlations.
22.4.2.1 Assimilating the Rule
LPhasconjecturedaspecific instanceofthe rewrite rule. It nowaskstheuser
toconfirmtheconjecture. Iftheconjectureisnottrue,theprogramexitswithfailure.
Iftheconjectureistrue, theuserisaskedtoprovidethegeneral rule. Forexample, if
the conjecture was (10) above, the userwould provide the rule
cosG4) + cos(B) - 2*cos((,4 + B)/2)*cos((A - B)I2). (11)
Now the rule must be assimilated. Recall that there are two possible reasons
whyastepmaynotbeimmediatelyparsed. First,thestepmayuseanexistingmethod
M,9 but LP may lack the necessary rewrite rule or, second, the step may use an
entirely new method.
In the first case LP adds the new rule to the rule set of M. Now method M
should be able to perform the step. This process allows LP to add new rules to
methods,therebyincreasingtheirscope. Otherwise, LPmustcreatethenewmethod.
The new rule will eventually be placed in the rule set ofthis new method.
22.4.3 The Precondition Analysis Phase
Figure 22-2 shows the example of figure 22-1 after Operator Identification.
Each line is precededby the name ofthe methodthatproduced it oran explanatory
comment.
Now thePreconditionAnalysisphasebegins. Themethodisessentially simple.
Basicallytheideaistofindthemajoraimofeachstep,thatis,toanswerthequestion,
Whywasthestepperformed?Asindicatedearlier, thisanalysisisexpressedinterms
ofthe satisfactionofpreconditionsofthe subsequent steps. Inthe simplestcase, the
first method is applied to satisfy some preconditions ofthe second method so that
9Note that M need notbe one ofthe original methods. Itcan be a method created by LPearlier in the
session.
--- PAGE 674 ---
660 CHAPTER 22: PRECONDITION ANALYSIS
cos(x) 4- 2*cos(2*jc) + cos(3*x) = (1)
(Applying new rule)
2*cos(2*jc)*cos(jt) + 2*cos(2*x) = (2)
(Factorization Preparation)
2*cos(2*jc)*(cos(jc) 4- 1) (3)
(Factorization)
cos(2*x) = V cos(x) +1=0 (4)
(Solving first factor)
cos(2*x) = (5)
(Isolation)
x = 180*11, + 45 (6)
(Solution)
(Solving next factor)
+1=0
cos(jc) (7)
(Isolation)
= -1
cos(jc) (8)
(Isolation)
x = 180*(2*az 2 + 1) (9)
(Solution)
Figure22-2: TheworkedexampleafterOperatorIdentification.
thatmethodcanbeapplied.10Thesecondmethodisapplied inturninordertosatisfy
somepreconditionsofthethirdmethod, andsoon. Finally,thelastmethodisapplied
in order to produce a solution. Sometimes the situation is somewhat more complicated (see section 22.4.4.1).
Theanalysistakesplaceforeachsectionoftheexample. Ingeneraltheproblem
isto find the purpose ofthe application ofmethod M at line ito produce line i + 1.
The step from (1) to (2) in figure 22-2 will be used as an example:
cos(jc) + 2*cos(2*jc) + cos(3*jc) = (1)
2*cos(2*x)*cos(jc) + 2*cos(2*;t) (2)
The following step is Factorization Preparation.
"'Notethai someofthepreconditionsofthesecondmethodma) alreadybesatisfiedb> theoriginal state,
so in general there is no need for the first method alone to satisf) all the preconditions o\ the second
method.
--- PAGE 675 ---
SILVER 661
LP firstfindsthepreconditionsPofthemethod M ' appliedatlinei + 1 togive
line i + 2. In this case, M' is Factorization Preparation. The set P is
{rhs-zeroCEq^Jhs-sumCXEq^^ommon-subtermsC^Eqn)}.
LPthenfinds whichofthe membersofPare satisfiedatline i. CallthesepreconditionsS.Intheexample,thefirsttwoelementsofParesatisfiedbutthelastisn't.
Sistherefore {rhs-zero(Eqn),lhs-sum(X,Eqn)}
thesetdifferenceP \ Sisthe
setofpreconditionsnotsatisfiedatlineibutsatisfiedati + 1; callthis set ME.n In
this case, ME is {common-subterms(X,Eqn)}. If ME is nonempty, LPassumes that
theaimofapplying M istosatisfy ME sothatM' canbeapplied. Theset ME iscalled
themajoreffectsofthestep (themajoraimofastepistosatisfythemajoreffects). If
LP finds that all the preconditions of M' were satisfied at line /-that is, ME is
empty- it looks foranotherexplanation (see section 22.4.4.1).
The above analysis isperformed foreach "real" step in the worked example.
(Real steps exclude those steps that are artifacts, such as partial applications of
Isolation.)
Thefinalanalysisisshownintable22-1. NotethatEqnineachlinereferstothe
currentequation, that is, a different equation in each case. Similarly X refers to the
currentunknown,whichhappenstobexforeveryline. Thefirstcolumncorresponds
tothelinesinfigure22-2. Notethatnotalllinesappear, suchasline(8);thosethatdo
notappearare superfluoustotheanalysis. Thelastcolumnofthelastthreeentriesis
blank. In the case ofthe Isolation entries, each entry ends its section so no next
methodfollows. Factorizationisakeymethod, sonodetailedanalysisofthepreconditions is required. Theanalysis is usedby LPtocreatenew methodsand schemata.
22.4.4 Creating New Methods
Thenextstage involvescreating newmethods. Newmethodsarecreatedifthe
programhasbeengivennew rulesbytheuser. Since LP is methodbased ratherthan
Table22-1: Analysisofworkedexample.
Line Method Major Satisfied
Effects(ME) Preconditions(S)
(2) Application common- rhs-zero(Eqn)
ofnewrule subterms(XEqn) lhs-sum(X,Eqn)
(3) Factorization lhs-prod(X,Eqn) rhs-zero(Eqn)
Preparation
(4) Factorization (Keymethod)
(6) Isolation (Solvesequation)
(7) Isolation (Solvesequation)
MEmustbesatisfiedat/' + 1 orM' couldnotbeapplied.
--- PAGE 676 ---
662 CHAPTER 22: PRECONDITION ANALYSIS
rule based-that is, it learns control rather than factual knowledge- it must create
newmethodsthatapplythenewrules. Thenewmethodscanbeusedinthesameway
asthe original methods. By creating new methods, theprogram increases its ability
to solve problems.
Anew rulewas appliedatline (1) in figure22-2. Theaboveanalysisgivesthe
aimofthis step, expressedasthe set ME intable22-1. LPcreates anew methodthat
canapplytherule. ThenewmethodallowsLPtoapplytherewriterulewheneverthis
is appropriate.
Findingthepreconditions. Thereisachoiceforthepreconditionsofthenewmethod.
The preconditions can be obtained from the analysis above, from the lines in the
workedexample, orfromsomecombinationofthetwo. LPadoptsthefirstapproach
and makes the preconditions ofthe method S. It is hopedthatapplying the operator
won't undo the satisfaction ofthis set.12 When LP eventually applies the new operator,ifitsucceedsinsatisfyingthemajoreffect andSisalsosatisfied,allthepreconditions ofthe following method will be satisfied.
The postconditions of the method are the preconditions of the following
methodM. WhenLPlaterappliesthenewoperator, itwillthereforetestto see ifthe
applicationoftheoperatordoesinfactpreservethesatisfactionofS. Inthisway, LP
is able to create a method with preconditions that are probably necessary, although
they aren'tsufficient. Thepreconditionsareonlyprobably necessarybecause it may
bethatthe rulesused will automatically satisfy someofSanyway, even ifthesepreconditions aren't satisfied already.
In the example above, a new method is createdthat has as preconditions
{lhs-sum(X,Eqn), rhs-zero(Eqn), mult-occ(X,Eqn)},
wheremult-occ(X,Eqn)issatisfiedwhentherearemultipleoccurrencesofXinEqn.
The postconditions ofthe new method are
{lhs-sum(X,Eqn), rhs-zero(Eqn), mult-occ(X,Eqn),
common-subterms(X,Eqn)}
22.4.4.1 When the Set ME Is Empty
SometimesLPwill findthatthestepitistryingtoanalyze satisfiesnomissing
preconditionsofthefollowingmethod. Inthetermsusedabove, theset isempty.
LP is able to recognize two cases in which this happens:
Manipulating the equation. One possibility isthat the rule is used to manipulate the
equationsothat M' canbeapplied, althoughnonewpreconditionsaresatisfied. This
l2LPchecks torthis-sec nextparagraph.
--- PAGE 677 ---
SILVER 663
kindofbehavioroccursbecause, in general, thepreconditions are necessary but not
sufficient conditions. The vocabulary ofLP is insufficient to express the necessary
conditions.
In general, ifa step from line itoline i + 1 doesn't satisfy any new preconditionsofthemethodM' appliedatlinei + 1, noexistingmethodcanproducethestep
andM' couldn'tbeappliedatlinei. LPthencreatesanewmethod inthefollowing
way:
The userhas already suppliedthe rule R thatallows the steptobe performed.
Ifthe preconditions ofM' are P, the method has P as both its preconditions and
postconditions. M uses rule R and has M' as its indicated next method. The major
effectsof areempty, andLPinformstheuserthatthisstepdoesn'tappeartosatisfy
anynewpreconditionsofthefollowingmethodbutitappearstobeessentialanyway!
The factthat has no majoreffects canbe used when LP is solving newequations.
Parallelsteps. Theothercaseisthat in which two (ormore) stepshavetobeapplied
beforeathird, buttheorderofthefirsttwoisarbitrary. Inthiscase, when LPcomes
toanalyzethefirststep, itwillfindthat isempty. This seems similartothecase
above, butthereisanimportantdifference. Inthe former, M' can'tbeappliedatline
i, so some manipulation is needed. In the case described here, M' can be applied at
linei. LPnotesthatthetwostepscanbeappliedineitherorderandcreatesamethod
that applies both, telescoping the two steps.
22.4.5 Creating the Schema
After the above work is done, the schema can be created easily. The schema
contains three main parts. These are (1) the generating equation, (2) the unknown
that is being solvedfor, and (3) the body ofthe schema.
The body is a list consisting ofall the methods used in the worked example.
Each step is tagged with the following:
The method used
2. The preconditions that it is used to satisfy, that is, the majoreffects
3. Any conditions that must alsobe maintained, that is, the set S.
The schema is divided in the same way as the worked example; that is, all the sectioning is preserved.
Theschema "summarizes" theworkedexampleandisusedtosolvenewequations. The schema isn't a perfect plan. LP hasn't analyzed the example completely
and has made simplifying assumptions. Before it could analyze the example completely. LPwouldneedextensive modification. Forexample, the solution methodof
the standard example used here works exactlyonly on equations ofthe form
cos(/1*jc) + 2*cos(£*x) + cos(C*;c) = 0,
--- PAGE 678 ---
} .
664 CHAPTER 22: PRECONDITION ANALYSIS
whereA B, and C don'tcontainxandareinarithmetic progression IfLPweretobe
, .
abletoderivethiscondition, eitherit wouldhavetohaveconcepts suchasarithmetic
progressioninitsdescriptionspaceandusesomeformofconceptlearningorit would
have to use a technique such as Mitchell's goal-directed learning (1983). However,
thenatureoftheoperatorsofLPmakethelatterapproachdifficult(seesection20.6)
Instead, LP has produced an approximate plan that it executes flexibly.
Part ofthe schema produced by LP for the worked example offigure 22-1 is
shown intable22-2. Theschemaisstoredasamethod, calledaschemamethod. The
preconditionsofthe methodarethe preconditions ofthefirst method inthe schema.
The postconditions are that the equation is solved.
22.5 SOLVING NEW EQUATIONS
Theusergives LPanewequationtosolve, calledthegiven equation. Theprogram first tries to find a schema methodthat might help it solve the given equation.
Todothis, the program finds a schemamethod whosepreconditions are satisfiedby
thegivenequation. This insuresthatatleastthefirstoperatorintheschemamightbe
applicable (recall that the preconditions are necessary but not sufficient).
Oncethe schemahasbeen selected, LPuses ittoguidetheattemptto solvethe
givenequation. The schemaliststhe methods usedto solve the generating equation,
together with other information about preconditions and purposes.
Inessence, LPtriestoapplythemethodslistedintheschematothegivenequation. LPtriestousethe schemafromthetop; thatis, ittriestoapplythe firstmethod
first, and ifit succeeds, it tries to apply the second method, and so on. LP works
down the schema, at any point working on a particular step of the schema. The
methodinthisstepiscalledthecurrentindicatedmethod, and itcanbesaidthatLP is
trying to apply the current indicated method.
Table22-2: Partoftheschema.
Name Preconditions Purpose
Method New {rhs-zero(Eqn). lhs-sum(X,Eqn). Produce common submult-occ(A',Eqn)} terms for Factorization
Preparation
Factorization {rhs-zero(Eqn), lhs-sum(X.Eqn). Produceproducton lhs
Preparation common-subterms(X,Eqn) equationsothatFactorizationcanbeapplied
Factorization (rhs-zero(Eqn). lhs-prod(X.Eqn). (Majorstep)
mult-occ(X.Eqn)}
Isolation (single occ(X,Eqn)] (Solution)
Isolation [single occ(X,Eqn)] (Solution)
cos(.v) + 2*cos(2*v) + cos(3*.v) = 0. v.
--- PAGE 679 ---
SILVER 665
Ifitsucceedsinapplyingthecurrentindicatedmethod, LPmovesontothenext
step,tryingtoapplythenewschemamethod, andsoon. Bytryingtoapplythestepsit
learned from the worked example in exactly the same sequence, LP is using the
resultsofaformoflearningcalledlearningbyrote. This isareasonablefirststrategy:
thestepssolvedthegeneratingequation, andthegivenequationandgeneratingequation both satisfy the preconditions ofthe first method in the schema. Ifthe given
equation is very similartothe generating equation, the attempt may succeed. More
usually, there will comeapoint when a method listed inthe schemacan'tbe applied
to the transformed given equation.
IfLPhas only learned the list ofsteps, it would now be stuck. This illustrates
the weakness oflearning by rote. Since there is no understanding ofthe reasons for
various decisions, it is very difficult to recover from unexpected failure.
However, the schemacontains other information as well. The most important
parts are itsdivision into sections andthe majoraims ofeach step. This information
enables LP to modify the linearexecution ofthe schema.
Basically, LP uses the "algorithm" for solveCEqiijA^AnSjSchjMNjSN),
shown in table 22-3, which defines how to solve Eqn for X to get Ans starting at
Table22-3: Thesolvealgorithm.
Test Action
1. IsEqnsolved? Exitwithsuccess, withAns = Eqn.
2. Isthepurposeofthecurrentsection Proceedtonextsection, i.e.,call
(numberSN)achieved? solve(Eqn,X,Ans,Sch,l,NewS),
whereNewS is SN + 1.
3. Canthecurrentindicatedmethod Proceedtonextmethod, i.e.,call
(numberMN)beapplied? solveCNew^Ans^chjNewlV^SN),
whereNewM is NM + 1, andNew isthe
resultofapplyingmethodnumberMN
Eqn.
4. Canamethod(M')thatachievesthe Proceedtonextmethod, i.e., call
samepurposeasmethodnumberMN be solve(New,*,Ans,Sch,NewM,SN),
applied? whereNewM is NM + 1, andNew isthe
resultofapplyingM' toEqn.
5. Cananothermethod(M')beappliedthat Tryagainwithtransformedequation, i.e.,
doesn'tundoanyalready satisfiedprecondi- call
tionsofmethodnumberMN? solve(New,*,Ans,Sch,NM,SN),
whereNew istheresultofapplyingM' to
Eqn.
6. Isthesetofmajoreffectsofmethod Omitmethod, i.e., call
numberMNempty? solve(Eqn,A;Ans,Sch,NewM,SN),
where NewM is NM -1- 1.
7. Otherwise Fail;backtrackifpossible
--- PAGE 680 ---
666 CHAPTER 22: PRECONDITION ANALYSIS
method number MN ofsection number SN ofthe schema Sch. Theprocess starts at
the first method of the first section of Sch; that is, the original call is
solveCEqn^AnSjSch,!,!)'
LPtrieseachtestinorder, startingfromthetop,untilonesucceeds. Itthenperforms the action specified for that test. The first test is obvious; ifthe equation is
solved LP has finished.
NotethatthefirsttwotestsallowLPtoomitlargenumbersofsteps. Thesetests
look for states that are unexpectedly "good." Placing these tests first allows LP to
takeadvantageofsuchfortuitouscircumstances. TheplannerofSTRIPS, PLANEX
(Fikes, Hart, andNilsson, 1972), alsodoesthis, buttherearedifferences. PLANEX
always triesto apply the last step in aplan, thenthe lasttwo, and so on. This allows
STRIPS toexecute the smallestpart oftheplanthat is applicable. Such an approach
isn't useful for LP, since the methods aren't well behaved.
Thethirdtestistheobviousone;trytoapplythesameoperatorthatwasusedin
the worked example.
The fourthtestallows LPto replaceone operatorintheplanwith anotherthat
achievesthesameeffect. Ifsuchanoperatorcanbefound, thingsmay stillgowrong,
since the original operator may have had an undetected effect, but this substitution
action is a reasonable heuristic.
Thefifthtestislessdirected. LPcan'tapplytheoperatoritwantsto, and itcan't
substitute for the operator. LP tries to find an operator that won't undo any already
satisfied preconditions but that might manipulate the equation in some unspecified
waysothatthedesiredoperatormaythenbeapplicable. LPwouldn'tneedtoperform
such actions if the methods were better behaved. Note that this procedure can be
appliedoverandoveragain, allowing LPtoaddan indefinite numberofstepstothe
plan.
The sixth test allows LP to omit an operator ifit has no major effects. This is
risky, since the operator did something in the original worked example-LP just
couldn'tdescribewhat itwas! Nevertheless, asalastresort, thismethodisreasonable.
22.5.1 An Example
As an example, suppose that LP is given the equation
2*cos(x) + 3*cos(2*jc) + 2*cos(3*jc) = (12)
to solve and that it chooses to use the schema shown in table 22-2. This schema was
generated by the equation
cos(jc) + 2*cos(2*x) + cos(3*x) (13)
The first step is to see iftheequation iscompletely solved. It isn't, sothe next
step istosee ifthe purposeofthecurrent schemasectionhasbeen attained. The purpose ofthe current section is to allow Factorization to apply. Equation (12) isn't a
--- PAGE 681 ---
SILVER 667
product, so Factorization can't be applied; therefore the purpose ofthe current section hasn't been attained yet. Consequently, LP attempts to apply the current indicated method.
The first method in the schema is New, which applies the rule
cosC4) + cos(B) - 2*cos((,4 + B)/2)*cos((A - B)I2)
in ordertoprovide common subterms for Factorization Preparation.
Therulecan'tbeapplieddirectlytoequation(12)asthecosinetermshavemultiplicativefactorsdominatingthem. TheattempttoapplyNew tothisequationtherefore fails.
LPnowtriestofindamethodthatwouldachievethemajoreffectsofNew;that
is, ittriestofindamethodthatproducescommonsubterms forFactorizationPreparation. Supposethatitcan'tfindsuchamethod.13LPnowtriestouseothermethods,
inthehopethat it will thenbe ableto apply New. These methods must not alterany
preconditionsofNew thatarealready satisfiedbytheequation (12). Therearethree
suchpreconditions: theright-handsideis0, theleft-handsideisasum, andtheequation contains multiple occurrences ofthe unknown.
Manymethodscanberuledoutquickly. Eithertheirpreconditionsaren'tsatisfied, ortheyhavealreadybeentried, ortheirpostconditionsdon'tsatisfythepreconditionsofNew. Infact, onlytwomethodsremain, CollectionandAttraction. Collection fails to transform the equation, so Attraction is tried. This succeeds,
transforming the equationto
2*(cos(;t) + cos(5*jc)) + 3*cos(3*jc) = (14)
The equation has been transformed. LP first checks to see ifthe equation is
solved. Itisn't, soLP teststoseeifthepurposeofthecurrentschemasectionhasbeen
attained. Inthiscase, as Factorizationcan'tbe appliedtoequation (14), thepurpose
stillhasn'tbeenachieved. LPnowtriestoapplythecurrentindicatedmethod, New.
Thisattemptsucceedsonthenewequation, (14), andfromthenontheschemacanbe
applied exactly.
Theschemahasbeenmodifiedbyaddinganinitialextrastep. Notethatifequation (12) had initially been given as a workedexample, the schema would havecontainedAttractionasthefirststep. Thisstepinfacthasnomajoreffect. IfLPhadthen
been given equation (13) to solve, it would eventually omit this step. The schema
would then apply exactly.
l3ThiswillbethecaseifLPhasseenonlytheoneworkedexamplebeforebeinggiventhenewequation.
OtherworkedexamplesmightcauseLPtocreateothermethodswiththismajoreffect.
--- PAGE 682 ---
668 CHAPTER 22: PRECONDITION ANALYSIS
22.6 RELATION TO OTHER WORK
LP is related in many ways to the work of DeJong and of Mitchell et al.
DeJong's explanatory schema acquisition technique (1983) works in the domain of
story understanding. DeJong is concerned with knowledge-based generalization
rather than problem solving. His program builds new concepts from the examples;
for example, kidnapping is seen as a novel way ofcombining stealing with bargaining. Theseconceptsarethenusedtounderstandlaterstories,butthereisnosense
in which they areexecuted. Nevertheless, DeJong's work showsthat it ispossible to
analyzepreconditionstoproduceuseful resultsindomainsverydifferentfromequation solving.
Mitchell's LEX2 (Mitchell, 1983; Mitchell, Utgoff, and Banerji, 1983) learns
heuristics-thatis, control information-forsymbolic integration. Histechniqueand
PreconditionAnalysisarebothanalyticratherthanempiricalasdefinedby Mitchell
(1983). However, LEX2 has nothing corresponding to a schema and doesn't learn
sequences of operator applications; it only learns when to apply individual
methods.14 LEX2 alsohasthe restrictionthatthe operators are "invertible;" that is,
given the state afterthe application ofan operator, it should be possible to discover
thestatebeforetheoperatorapplication. TheLPmethodsdon'tsatisfythiscondition,
as a method may have used any ofa large number ofrules. LEX2 also uses several
examples to learn, although in theory it need not.
22.7 RESULTS
LP has successfully learned many equation-solving methods. These have
allowed LPtosolveproblemsthatcan'tbe solvedby PRESS. Ithasalso beenable to
learn some ofits initial methods,15 thus "rebuilding" itselffrom a lower level. The
methods it learns are notquite asefficientastheoriginal methods (which have been
handcrafted for PRESS), but they are quite usable.
Thus Precondition Analysis has proved useful in the domain of equation
solving. However,theauthorbelievesthatthetechniquecanbeappliedinmanyother
domains. He plans inthe nearfuture tobuild a Precondition Analysis shell to which
domain knowledge can be added.
l4Whilc learning whentoapplyan individualoperator. LEX2 ineffectassumesthattheoperator istobe
usedaspartofacertaingivensequence. However, it retainsnoinformationaboutthesequenceandhasto
rederive itduring the problem-solving phase.
'"The user can turn offan) of LP's methods. The program then effectively has no knowledge ofthese
methods.
--- PAGE 683 ---
SILVER 669
CONCLUSIONS
22.8
This chapter has presented the learning technique of Precondition Analysis,
which has been implemented in the equation-solving program LP. Precondition
Analysislearnscontrolinformationintheformofnewmethodsandlearnsstrategies
in the form ofschemata. This knowledge can be obtained from a single example.
LP is a version of PRESS that learns new methods and constructs equationsolvingplans. LPhas successfullylearnedanumberofmethodsandplansthatithas
used to solve new equations. Precondition Analysis and LP are described in more
detail in Silver (1984).
ACKNOWLEDGMENTS
The author would like to thank all members ofthe Mathematical Reasoning
Group at Edinburgh for their help with this work. Alan Bundy, Leon Sterling,
Lawrence Byrd, and Lincoln Wallen have been particularly helpful. He would also
like to thank Tom Mitchell forhis useful comments on adraft ofthis paper.
This work was supported by the Science and Engineering Research Council,
who provided grant GR/C/20826 and a Research Studentship forthe author.
References
Boyer,R. S.,andMoore,J. S.,A ComputationalLogic, ACMMonographSeries, AcademicPress, New
York, 1979.
Brazdil, P., "Experimental Learning Model," AISB/GI, Society forthe Study ofArtificial Intelligence
andtheSimulationofBehavior, pp. 46-60, 1978.
Bundy,A.
'AnalysingMathematicalProofs(orReadingBetweentheLines),"ResearchPaper2,DepartmentofArtificialIntelligence, UniversityofEdinburgh, 1975.
Bundy, A., andWelham, B., "UsingMeta-levelInferenceforSelectiveApplicationofMultipleRewrite
RulesinAlgebraicManipulation,"ArtificialIntelligence, Vol. 16, No. 2, pp. 189-212, 1981.
Clocksin,W. R, andMellish, C. S., ProgramminginProlog, SpringerVerlag, Berlin, 1981.
DeJong, G., "An Approach to Learning from Observation," ProceedingsoftheInternationalMachine
Learning Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois at UrbanaChampaign,pp. 171-76,June22-24, 1983. (Anupdatedversionofthispaperappearsaschapter 19
ofthisvolume.)
Fikes,R. E.,Hart,P. E.,andNilsson,N.J., "LearningandExecutingGeneralizedRobotPlans,"ArtificialIntelligence, Vol. 3, pp. 251-88, 1972.
Langley, P., "Learning Effective Search Heuristics," Proceedings ofthe Eighth IJCAI, Karlsruhe,
W. Ger., pp. 419-21, 1983.
--- PAGE 684 ---
670 CHAPTER 22: PRECONDITION ANALYSIS
Mitchell,T. M., "VersionSpaces: AnApproachtoConceptLearning," Ph.D.diss.,StanfordUniversity.
1978.
, "Learning and Problem Solving," Proceedings ofthe Eighth IJCAI, Karlsruhe, W. Ger.,
pp. 1139-52, 1983.
Mitchell,T. M., Utgoff, P. E.,andBanerji, R., "LearningbyExperimentation: AcquiringandRefining
Problem-Solving Heuristics," in Machine Learning: An Artificial Intelligence Approach,
R. S. Michalski, J. G. Carbonell, andT. M. Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
Neves, D. M., "A Computer Program That Learns Algebraic Procedures by Examining Examples and
Working Problems in a Textbook," Proceedings ofthe SecondNational Conference, Canadian
SocietyforComputationalStudiesofIntelligence, Toronto, pp., 191-95, 1978.
Silver,B., "UsingMeta-LevelInferencetoConstrainSearchandtoLearnStrategiesinEquationSolving,"
Ph.D. diss., DepartmentofArtificial Intelligence, UniversityofEdinburgh, 1984.
Sterling, L.; Bundy, A.; Byrd, L.; O'Keefe, R.; and Silver, B., "Solving Symbolic Equations with
PRESS," ComputerAlgebra, Lecture Notes in Computer Science, No. 144, Springer Verlag.
Berlin, 1982.
Winston,P., "LearningStructuralDescriptionsfromExamples,"inThePsychologyofComputerVision,
P. Winston(Ed.), McGraw-Hill, New York, 1975.
--- PAGE 685 ---
BIBLIOGRAPHY OF RECENT
MACHINE LEARNING RESEARCH
SmadarT. Kedar-Cabelli
Sridhar Mahadevan
Rutgers University
INTRODUCTION
The aim ofthis bibliography is to provide a resource consisting mainly of
recent research contributions in machine learning (1980-1984). It is patterned
closely after the bibliography in Machine Learning: An Artificial Intelligence
Approach (Michalski, Carbonell, and Mitchell, 1983), referred to here as Machine
Learning I. The current bibliography complements the earlier bibliography by
emphasizing recent additions to machine learning research. The emphasis is primarily on research in machine learning from the artificial intelligence perspective,
although some contributions from related disciplines such as psychology, cognitive
science, philosophy, andmathematicsarealsoincluded. Theselectionisrepresentativeratherthanexhaustive. Toroundoutthebibliography, severalclassicsinmachine
learning (seminalpaperspublishedpriorto 1980) anda few carefully selectedoverviews andbackgroundmaterials havebeen included. Itis hopedthatthe readerwill
find the bibliography ahandy reference to machine learning research.
The editors ofthis bibliography gratefully acknowledge the help ofall those
who provided useful suggestions. In particular they thank Tom Mitchell, Ryszard
Michalski, Jaime Carbonell, Tom Dietterich, Paul Utgoff, andRich Keller. In addition, they would like to thank Paul Utgofffor his help in all phases ofthe bibliographic process.
Aswithanyworkofthiskind, somereferencesareinevitablyoverlooked. The
editorsapologizeinadvancetotheauthorsofthoseworks, andtothereaders, forany
omissions.
--- PAGE 686 ---
672 BIBLIOGRAPHY OF RECENT RESEARCH
A ROAD MAP TO THE LITERATURE
Much machine learning research is published in conference proceedings on
artificial intelligence. TheseincludetheProceedingsoftheInternationalJointConferenceonArtificialIntelligence(IJCAI), publishedbiannuallyandtheProceedings
oftheNational ConferenceonArtificialIntelligence (AAAI), publishedannually. In
addition, research has been presented at two workshops on machine learning: the
First International Machine Learning Workshop, held in 1980 in Pittsburgh, Pennsylvania, andtheSecondInternationalMachineLearningWorkshop, heldin 1983 in
Allerton House, Urbana, Illinois. Selected papers from the first workshop were
revised and collected into Machine Learning I. Papers from the second workshop
were initially published in 1983 as the Proceedings ofthe International Machine
Learning Workshop. Selectedpapersfromthesecondworkshopwererevisedandcollectedtoproducethisvolume. Inaddition,thereaderwillfindthechapter "Learning
and Inductive Inference" in The Handbook ofArtificial Intelligence (Cohen and
Feigenbaum, 1982) an important source material, particularly for earlier work.
Machinelearningarticlesarealsofoundinvarious AIjournals, the mostprominent
beingArtificialIntelligence.
EXPLANATION OF THE CATEGORIES
Each bibliographic entry is categorized along three dimensions: the learning
strategyit is describing (orhow thelearning is done); thedomain ofapplication (or
what is being learned); and the research methodology (or how the research is performed). Eachofthesedimensions isdivided into several categories. Eachcategory
is identifiedby acode letter, followedby abriefexplanation and a list ofkey words
commonlyusedinthefieldtodescribeworkinthiscategory. (Inaddition, categories
under "learning strategy" are collected into three supercategories, explained in
chapter 1 ofthis book.) For example, in the reference "A Computational Model of
ProblemSolvingbyAnalogy" byJ. G. Carbonell, thelearningstrategy is "analogy"
(code letter a), the domain ofapplication is "problem solving" (code letter s), and
theresearchmethodologyis"experimentalAI" (whichisgivennocodelettersinceit
is assumed to be the default methodology in machine learning).
A list ofthe categories is provided below. Under each category are listed the
numbers ofthe references belonging to this category. Following that are the references. Nexttoeachreferenceintheleftmarginarethecodelettersofthecategoriesto
which the reference belongs. For example, "A Computational Model of Problem
SolvingbyAnalogy" willbementionedunderboththeaandthescategories, andthe
code letters as will appear next to this reference.
One final note: The categorization used here differs slightly from that used in
thebibliographyofMachineLearning I. This isduepartly tothe fastevolutionofthe
field and partly totheeditors' differingviews ofhow the field is organized. A rough
--- PAGE 687 ---
BIBLIOGRAPHY OF RECENT RESEARCH
correspondence ofthe code letters ofthe categories in the two bibliographies is as
follows:
Categories Categories
MachineLearning I MachineLearning II Meaning
a Learning by analogy
b Background material
e Learning from examples
d Learning by discovery
i Intelligent computer-assisted
instruction
n Learning ofnatural language
s Learning ofskills
k Knowledge acquisition for
expert systems
n Learning ofnatural language
rn Cognitive modeling
o Overviews
P P Learning ofprocedures
q c Learning from observation
r c Conceptual clustering
t Learning by being told
r Learning by reformulation
g Genetic algorithms
1 Research employing formal logic
f Philosophical aspects oflearning
THE CATEGORIES
LEARNING STRATEGY
Learning by Induction:
Category e. Learningfromexamples: Thelearnerusestrainingexamples provided
by ateacher orthe environment in orderto learn. Some commonly used key words
are learningfromexamples, inductiveinference, inductive learning, generalization,
conceptacquisition, characterization, learning by experimentation, empiricalgeneralization, and concept learning.
{2, 13, 14, 15, 17, 18, 22, 23, 24, 25, 26, 27, 29, 30, 36,48, 59, 61, 62, 63, 66, 67,
68, 70, 71, 74, 92, 106, 108, 111, 112, 115, 117, 118, 141, 151, 163, 172, 173, 174,
176, 180, 184, 190, 195, 196, 197, 198, 200, 201, 203, 215, 225, 226, 230, 231,
232, 233, 234, 235, 243, 246, 253, 254, 255, 256, 271, 272, 273, 281, 283, 286,
288, 289, 291, 293, 294, 295, 297, 298, 304, 305, 308, 310, 311, 312}
--- PAGE 688 ---
674 BIBLIOGRAPHYOF RECENT RESEARCH
Category c. Learning from observation: The learner is provided with unclassified
examplesand must itselfclassifyexamplesby observing similaritiesanddifferences
among them. Some commonly used key words are learningfrom observation,
learning by classification, taxonomic classification, conceptual clustering, statisticalapproaches to clustering, and unsupervisedlearning.
{148, 149, 150, 151, 162, 171, 173, 175, 176, 177, 178, 181, 232, 233, 234, 235,
236, 237, 238, 239, 290, 291, 305}
Category d. Learning by discovery: The learner investigates a domain in an
unguided, exploratory fashion and discovers new concepts and relationships among
them. Some commonly used key words arediscoveryand theoryformation.
{2, 3, 4, 28, 55, 58, 70, 72, 73, 77, 132, 133, 136, 137, 139, 140, 142, 144, 149,
152, 153, 154, 156, 157, 159, 160, 161, 241, 268, 269, 270}
Learning by Deduction:
Categoryt. Learningbybeingtold:Thelearnerusesknowledgeprovidedtoit, such
asatheoryofthedomain, advice, orinstructions, inordertolearn. Somecommonly
usedkeywordsarelearningbybeingtold, learningbytakingadvice, operationalization, goal-directedgeneralization, explanation-basedgeneralization, model-driven
learning, learningfrom instructions, andsupervisedlearning.
{8, 10, 11,72, 81,94,95,99, 100, 110, 164, 166, 174, 194,204,206,207,208,209,
210, 221, 251, 252, 289}
Category r. Learning by reformulation: The learner reformulates and restructures
already-available knowledge inordertolearn. Some commonly used key words are
problem reformulation, creating macro-operators, chunking, knowledge compilation, caching (memofunctions),proceduralization, reconstructivememory,failuredriven learning, and shifts in representation.
{1, 19, 54, 59, 61, 62, 63, 82, 86, 103, 119, 120, 121, 122, 123, 124, 126, 127, 151,
155, 158, 169, 212, 214, 218, 227, 242, 244, 245, 246, 247, 251, 252, 255, 257,
258, 259, 263, 264, 291, 294, 295, 297, 298}
Learning by Analogy:
Category a. Learningby analogy: The learner reasons by analogy in orderto learn
concepts and problem-solving strategies. Some commonly used key words are concept learning by analogy, analogicalproblem solving, structure mapping, casebasedreasoning and reminding.
{ 16, 37, 38, 41,42, 43,45,47, 51, 55, 65, 75, 78, 83, 86, 88, 89. 90. 102. 104, 107.
109, 116, 130, 131, 169, 170, 205, 248, 249, 300. 309, 310. 311, 312}
--- PAGE 689 ---
BIBLIOGRAPHY OF RECENT RESEARCH
DOMAIN OF APPLICATION:
Category s. Learning ofskills: Strategies, heuristics, production rules, in problem
solving, planning, game playing, theorem proving.
{7, 8, 10, 11, 15, 16, 17, 19, 32, 34, 36, 39,41,42,43,45,47,49, 51, 66, 74, 75, 77,
82, 85, 89, 93, 99, 110, 111, 112, 122, 123, 124, 126, 127, 128, 134, 135, 141, 145,
146, 163, 164, 194, 195, 196, 198, 201, 202, 203, 204, 206, 207, 208, 209, 210,
213, 215, 218, 221, 230, 231, 234, 236, 237, 238, 239, 240, 245, 247, 249, 256,
262, 271, 272, 273, 280, 281, 284, 292, 294, 295, 298, 300, 301, 302, 303}
Category k. Learning and knowledge acquisition for expert systems: Expertise
acquisition.
{12, 22, 34, 50, 51, 56, 57, 58, 85, 94, 95, 98, 101, 113, 125, 174, 182, 190, 191,
204, 226, 227, 250, 277}
Category n. Learning of natural language: Natural language acquisition, syntax
acquisition, grammatical inference, learning ofword meanings.
{6, 23, 24, 25, 54, 59, 60, 61, 62, 63, 80, 92, 102, 130, 131, 150, 222, 242, 257,
258, 259, 266, 267, 286}
Category p. Learning ofprocedures: Program synthesis, automatic programming
learning ofrobot procedures.
{3, 4, 13, 27, 30, 65, 72, 73, 81, 134, 212, 223, 292, 301}
Category i. Instruction: Intelligent computer-assisted instruction (ICAI).
{31, 39, 50, 250, 278, 279, 282}
RESEARCH METHODOLOGY
Unless otherwise indicated, the research methodology is experimental AI.
Typicallythisinvolvesthedevelopmentofcomputermodelsoflearning(ofteninspecifictaskdomains) andthecreationofcomputerprogramswithlearningcapabilities
as the primary means ofvalidating the research.
Category 1. Research employing formal logic, automatatheory, orother formal systems as its primary means ofvalidation.
{24, 26, 48, 91, 92, 116, 118, 147, 168, 222, 268, 269, 270, 299}
Categorym. Researchemployingpsychologicalexperimentsofhumanlearningasits
primary means ofvalidation. Some common key words are cognitivemodeling and
cognitivepsychology.
{5, 6, 7, 8, 9, 10, 11, 15, 32, 33, 37, 38, 39, 49, 64, 75, 80, 83, 87, 88, 89, 90, 97,
107, 114, 115, 119, 120, 121, 135, 138, 145, 146, 147, 213, 214, 215, 217, 218, 220,
--- PAGE 690 ---
676 BIBLIOGRAPHYOF RECENT RESEARCH
244, 246, 248, 249, 257, 258, 259, 266, 267, 280, 282, 283, 284, 287, 301, 302,
303}
Category g. Research influenced by biological models, in particular genetic or
neural models. Some commonly used key words are genetic algorithms, neural
models, and adaptivesystems.
{40, 96, 105, 165, 224, 239, 285}
OTHER:
Category f. Philosophical orepistemological discussions ofmachine learning.
{52, 55, 91, 129, 168, 229, 241, 263, 264}
Categoryb. Backgroundandgeneralmaterial: Selectedmaterialinartificialintelligence and related disciplines needed as background for machine learning research.
{20,21, 33,53,76,79, 87,93, 101, 104, 128, 129, 130, 131, 166, 167, 168, 179, 188,
192, 193, 203, 205, 216, 217, 219, 220, 223, 228, 229, 257, 258, 261, 265, 274,
275, 306, 307}
Category o. Overviews, critiques, and surveys ofwork in machine learning.
{7,9, 14, 18, 35, 36,40,44,46, 67, 68,69, 71, 84, 106, 113, 114, 117, 138, 143, 153,
179, 180, 183, 185, 186, 187, 188, 189, 199,200,202,211,224,228,240,241,260,
273, 276, 287, 296}
5. References
br 1. Amarel, S., "On Representation ofProblems ofReasoning about
Actions," inMachineIntelligence3, D. Michie(Ed.), Universityof
Edinburgh Press, Edinburgh, 1968.
de 2. Amarel, S., "Problems of Representation in Heuristic Problem
Solving: RelatedIssuesinthe DevelopmentofExpert Systems." in
Methods ofHeuristics, R. Groner, M. Groner, and W. F. Bischof
(Eds.), Erlbaum, Hillsdale, N.J., 1983.
dp 3. Amarel, S., "Expert Behavior and Problem Representation," in
Human and Artificial Intelligence, A. Elithorn and R. Banerji
(Eds.), North-Holland, Amsterdam, 1984.
dp 4. Amarel, S., "Program Synthesis as a Theory Formation Task:
Problem Representations and Solution Methods," In Machine
Learning: An Artificial Intelligence Approach, Vol. II, R. S.
Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.), Morgan
Kaufmann, LosAltos,Calif. 1986. (Anearlierversionappearedin
--- PAGE 691 ---
BIBLIOGRAPHYOF RECENT RESEARCH 677
Proceedings ofthe International Machine Learning Workshop,
R. S. Michalski (Ed.), Allerton House, University of Illinois at
Urbana-Champaign, June 22-24, 1983).
5. Anderson, J. R., Kline, P. J., and Beasley, C. M., "A General
Learning Theory and Its Application to Schema Abstraction," in
The Psychology ofLearning andMotivation, G. H. Bower (Ed.),
Academic Press, New York, 1979.
mn 6. Anderson, J. R., "A Theory of Language Acquisition Based on
General Learning Principles," Proceedings ofthe Seventh IJCAI,
Vancouver, B.C., pp. 97-103, 1981.
mos 7. Anderson, J. R. (Ed.), Cognitive Skills and Their Acquisition,
Erlbaum, Hillsdale, N.J., 1981.
mst 8. Anderson, J. R., Greeno, J. G., Kline, P. J., and Neves, D. M.,
"Acquisition of Problem-Solving Skill," in Cognitive Skills and
TheirAcquisition, J. R. Anderson (Ed.), Erlbaum, Hillsdale, N.J.,
1981.
mos 9. Anderson, J. R., TheArchitectureofCognition, Harvard University Press, Cambridge, 1983.
mst 10. Anderson, J. R., "Acquisition of Proof Skills in Geometry," in
Machine Learning: An Artificial Intelligence Approach, R. S.
Michalski, J. G. Carbonell, andT. M. Mitchell (Eds.), Tioga, Palo
Alto, Calif., 1983.
mst 11. Anderson, J. R., "Knowledge Compilation: The General
LearningMechanism. "InMachineLearning:AnArtificialIntelligence Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and
T. M. Mitchell(Eds.), MorganKaufmann, LosAltos, Calif. , 1986.
(An earlier version appeared in Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, June 22-24,
1983.)
k 12. Andreae, J. H., "The Development of Intelligent Interfaces
Between Operator Environments," Technical Report ManMachine Studies, Progress Report UC-DSE-24, Department of
Electrical and Electronic Engineering, University ofCanterbury,
Christchurch, New Zealand, November 1984.
ep 13. Andreae, P. M., "Constraint Limited Generalization: Acquiring
Procedures from Examples," Proceedings ofAAAI-84, Austin,
Tex., pp. 6-10, 1984.
--- PAGE 692 ---
678 BIBLIOGRAPHYOF RECENT RESEARCH
eo 14. Angluin,D.,andSmith,C. H., "InductiveInference: Theoryand
Methods," ACM Computing Surveys, Vol. 15, No. 3, pp. 237-70,
September 1983.
ems 15. Anzai, Y., and Simon, H. A., "The Theory of Learning by
Doing," PsychologicalReview, Vol. 86, No. 2, pp. 124-40, March
1979.
as 16. Araya, A. A., "Learning by Controlled Transference ofKnowledge between Domains," Proceedings ofthe Eighth IJCAI, Karlsruhe, W. Ger., pp. 439-43, 1983.
es 17. Araya, A. A., "Learning Problem Classes by Means of Experimentation and Generalization," Proceedings ofAAAI-84, Austin,
Tex., pp. 11-15, 1984.
eo 18. Banerji,R. B.,andMitchell,T. M., "DescriptionLanguagesand
LearningAlgorithms: AParadigmforComparison," International
JournalofPolicyAnalysisandInformation Systems, Vol. 4, No. 2,
pp. 124-40, June 1980.
rs 19. Banerji, R. B., "GPS andthe Psychology ofthe Rubik Cubist: A
Study inReasoningAboutActions," in HumanandArtificialIntelligence, A. Elithorn and R. B. Banerji (Eds.), North-Holland,
Amsterdam, 1984.
b 20. Barr,A.,andFeigenbaum,E. A. (Eds.), TheHandbookofArtificialIntelligence, Vol. 1, Kaufmann, Los Altos, Calif., 1981.
b 21. Barr,A.,andFeigenbaum,E. A. (Eds.), TheHandbookofArtificialIntelligence, Vol. II, Kaufmann, Los Altos, Calif., 1982.
ek 22. Benjamin,D. P.,andHarrison,M. C, "AProductionSystemfor
Learning Plans from an Expert," Proceedings ofAAAI-83, Washington, D.C., pp. 22-26, 1983.
en 23. Berwick,R.C, "LearningWordMeaningsfromExamples," Proceedings ofthe Eighth IJCAI, Karlsruhe, W. Ger., pp. 459-61,
1983.
eln 24. Berwick, R. C, "Locality Principles and the Acquisition ofSyntactic Knowledge," Ph.D. diss., DepartmentofComputerScience.
MIT, 1983.
en 25. Berwick, R. C, "Domain-Independent Learning and the Subset
Principle," in Machine Learning: An Artificial Intelligence
Approach, Vol. 11, R. S. Michalski, J. G. Carbonell. and T. M.
Mitchell (Eds.), Morgan Kaufmann, Los Altos, Calif., 1986.
(Original version appeared in Proceedings ofthe International
--- PAGE 693 ---
BIBLIOGRAPHYOF RECENT RESEARCH 679
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, June 22-24,
1983.)
el 26. Blum, L., and Blum, M., "Toward a Mathematical Theory of
Inductive Inference," Information and Control, Vol. 28, pp.
125-55, 1975.
ep 27. Bond, A. H., and Mott, D. H., "Learning of Sensory-Motor
Schemas in a Mobile Robot," Proceedings ofthe Seventh IJCAI,
Vancouver, B.C., pp. 159-61, 1981.
d 28. Bradshaw, G. L., Langley, P. W., andSimon, H. A., "BACON.
4: The Discovery of Intrinsic Properties," Proceedings ofthe
Canadian Societyfor Computational Studies ofIntelligence, Victoria, B.C., pp. 19-25, 1980.
e 29. Brazdil, P., "Experimental Learning Model," Proceedings ofthe
SocietyfortheStudyofAlandSimulationofBehavior AISB) Conference, 1978.
ep 30. Brazdil, P., "AModelforErrorDetectionandCorrection," Ph.D.
diss., DepartmentofComputerScience, University ofEdinburgh,
1981.
i 31. Brown, J. S., and Burton, R. R., "Multiple Representations of
KnowledgeforTutorialReasoning," inRepresentationandUnderstanding, D. G. Bobrow and A. Collins (Eds.), Academic Press,
New York, 1975.
ms 32. Brown, J. S., and VanLehn, K., "Repair Theory: A Generative
Theory ofBugs in Procedural Skills," Cognitive Science, Vol. 4,
No. 4, pp. 379-426, October-December 1980.
bm 33. Bruner, J. S., Goodnow, J. J., and Austin, G. A., A Study of
Thinking, Wiley, New York, 1956.
ks 34. Buchanan, B. G., and Mitchell, T. M., "Model-Directed
Learning ofProductionRules," inPattern-DirectedInferenceSystems, D. A. Watermanand F. Hayes-Roth (Eds.), Academic Press,
New York, 1978.
o 35. Buchanan, B. G., Mitchell, T. M., Smith, R. G., andJohnson,
C. R., Jr., "Models of Learning Systems," Technical Report
STAN-CS-79-692, Department of Computer Science, Stanford
University, 1979.
--- PAGE 694 ---
680 BIBLIOGRAPHYOF RECENT RESEARCH
eos 36. Bundy, A., and Silver, B., "A Critical Survey ofRule Learning
Programs," Proceedings ofthe Fifth European Conference on AI,
Paris, pp. 151-57, 1982.
am 37. Burstein, M. H., "A Model ofLearning by Incremental Analogical Reasoning and Debugging," Proceedings ofAAAI-83, Washington, D.C., pp. 45-48, 1983.
am 38. Burstein, M. H., "ConceptFormationbyIncrementalAnalogical
Reasoning and Debugging," in Machine Learning: An Artificial
Intelligence Approach, Vol. II, R. S. Michalski, J. G. Carbonell,
and T. M. Mitchell (Eds.), Morgan Kaufmann, Los Altos, Calif.,
1986. (An earlier version appeared in Proceedings ofthe International Machine Learning Workshop, R. S. Michalski (Ed.),
AllertonHouse, University ofIllinoisatUrbana-Champaign, June
22-24, 1983.)
ims 39. Burton, R. R., "Diagnosing Bugs in a Simple Procedural Skill,"
in Intelligent Tutoring Systems, D. H. Sleeman and J. S. Brown
(Eds.), Academic Press, New York, 1982.
go 40. Caianiello, E. R., andMusso, G., CyberneticSystems:Recognition, Learning, Self-Organization, Wiley, New York, 1984.
as 41. Carbonell,J. G., "AComputationalModelofProblemSolvingby
Analogy," ProceedingsoftheSeventhIJCAI, Vancouver, B.C., pp.
147-52, 1981.
as 42. Carbonell, J. G., "Experiential Learning in Analogical Problem
Solving," Proceedings ofAAAI-82, Pittsburgh, Pa., pp. 168-71,
1982.
as 43. Carbonell,J. G., "LearningbyAnalogy: FormulatingandGeneralizing Plans from Past Experience," in Machine Learning: An
ArtificialIntelligenceApproach, R. S. Michalski, J. G. Carbonell,
and T. M. Mitchell (Eds.), Tioga, Palo Alto, Calif., 1983.
o 44. Carbonell, J. G., Michalski, R. S., and Mitchell, T. M., An
Overview ofMachine Learning," in MachineLearning: An Artificial IntelligenceApproach, R. S. Michalski, J. G. Carbonell. and
T. M. Mitchell (Eds.), Tioga, Palo Alto, Calif., 1983.
as 45. Carbonell, J. G., "Derivational Analogy and Its Role in Problem
Solving," Proceedings ofAAAI-83, Washington, DC, pp. 64-6^.
1983.
--- PAGE 695 ---
BIBLIOGRAPHYOF RECENT RESEARCH 681
o 46. Carbonell, J. G., Michalski, R. S., and Mitchell, T. M.,
"Machine Learning: A Historical and Methodological Analysis,"
AIMagazine, Vol. 4, No. 3, pp. 69-79, Fall 1983.
as 47. Carbonell, J. G., "Analogy in Problem Solving. " In Machine
Learning: An Artificial Intelligence Approach, Vol. II, R. S.
Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.), Morgan
Kaufmann, LosAltos, Calif.
1986. (Anearlierversionappearedin
Proceedings ofthe International Machine Learning Workshop,
R. S. Michalski (Ed.), Allerton House, University of Illinois at
Urbana-Champaign, June22-24, 1983.)
el 48. Case, J., and Smith, C, "Comparison ofIdentification Criteria
for Mechanized Inductive Inference," Theoretical Computer Science, Vol. 25, No. 2, pp. 193-220, March 1983.
ms 49. Chi, M. T. H., Feltovich, P. J., and Glaser, R., "Categorization
and RepresentationofPhysics Problemsby Experts andNovices,"
Cognitive Science, Vol. 5, No. 2, pp. 121-51, April-June 1981.
ik 50. Clancey, W. J., "Transfer of Rule-Based Expertise Through a
TutorialDialogue,"Ph. D.diss., DepartmentofComputerScience,
Stanford University, 1979. (Also available as Technical Report
STAN-CS-79-769,DepartmentofComputerScience, StanfordUniversity, 1979.)
aks 51. Clements, J., "Analogical Reasoning Patterns in Expert Problem
Solving," ProceedingsoftheFourthAnnualMeetingofthe Cognitive Science Society, AnnArbor, Mich., 1982.
f 53. Cohen,B.,andMurphy, G. L., "ModelsofConcepts," Cognitive
Science, Vol. 8, No. 1, pp. 27-58, January-March 1984.
b 52. Cohen, P. R., and Feigenbaum, E. A. (Eds.), The Handbook of
ArtificialIntelligence, Vol. 3, Kaufmann, Los Altos, Calif., 1982.
nr 54. Cullingford, R. E., Krueger, M. W., Selfridge, M., and
Bienkowski, M. A., "Towards Automating Explanations," ProceedingsoftheSeventhIJCAI, Vancouver, B.C., pp. 362-67, 1981.
adf 55. Darden, L., "Reasoning by Analogy in Scientific Theory Construction," Proceedings ofthe International Machine Learning
Workshop, R. S. Michalski (Ed.), Allerton House, University of
Illinois atUrbana-Champaign, June 22-24, 1983.
k 56. Davis, R., "Applications of Meta Level Knowledge to the Construction, Maintenance and Use of Large Knowledge Bases,"
--- PAGE 696 ---
682 BIBLIOGRAPHYOF RECENT RESEARCH
Ph.D. diss., Department of Computer Science, Stanford University, 1976. (Alsoappearsin Knowledge-BasedSystemsinArtificial
Intelligence, R. Davis and D. Lenat, Eds., McGraw-Hill, New
York, 1981.)
k 57. Davis, R., "InteractiveTransferofExpertise: Acquisition ofNew
Inference Rules," Proceedings ofthe Fifth IJCAI, Cambridge,
Mass., pp. 321-28, 1977.
dk 58. Davis, R., andLenat, D. B., Knowledge-BasedSystemsinArtificialIntelligence. McGraw-Hill, New York, 1981.
enr 59. Dejong, G., "Automatic Schema Acquisition in a Natural Language Environment," Proceedings ofAAAI-82, Pittsburgh, Pa.,
pp. 410-13, 1982.
n 60. Dejong, G., "Skimming Stories in Real Time: An Experiment in
Integrated Understanding," Ph.D. diss., Department ofComputer
Science, Yale University, 1979.
enr 61. Dejong, G., "Generalizations Based on Explanations," Proceedings oftheSeventh IJCAI, Vancouver, B.C., pp. 67-69, 1981.
enr 62. Dejong, G., "Acquiring Schemata through Understanding and
Generalizing Plans," Proceedings oftheEighth IJCAI, Karlsruhe,
W. Ger., pp. 462-64, 1983.
enr 63. Dejong, G., "An Approach to Learning from Observation," in
Machine Learning: An Artificial Intelligence Approach, Vol. II,
R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.),
Morgan Kaufmann, Los Altos, Calif., 1986. (An earlier version
appeared in Proceedings ofthe International Machine Learning
Workshop, R. S. Michalski (Ed.), Allerton House, University of
Illinois at Urbana-Champaign, June 22-24, 1983.)
64. DeKleer,J., andBrown, J. S., "MentalModelsofPhysicalMechanismsandTheirAcquisition," inCognitiveSkillsandTheirAcquisition, J. R. Anderson (Ed.), Erlbaum, Hillsdale, N.J., 1981.
ap 65. Dershowitz, N., "Programming by Analogy," in Machine
Learning: An Artificial Intelligence Approach, Vol. II, R. S.
Michalski, J. G. Carbonell, and T M. Mitchell (Eds.), Morgan
Kaufmann, LosAltos,Calif. 1986. (Anearlierversionappearedin
Proceedings ofthe International Machine Learning Workshop.
R. S. Michalski (Ed.), Allerton House, University of Illinois at
Urbana-Champaign, June 22-24, 1983.)
--- PAGE 697 ---
BIBLIOGRAPHYOF RECENT RESEARCH
es 66. Dietterich, T. G., "Applying General Induction Methods to the
Card Game Eleusis," Proceedings ofAAA 1-80, Stanford, Calif.,
pp. 218-20, 1980.
eo 67. Dietterich, T. G., and Michalski, R. S., "Inductive Learning of
Structural Descriptions: Evaluation Criteria and Comparative
Review of Selected Methods," Artificial Intelligence, Vol. 16,
No. 3, pp. 257-94, July 1981.
eo 68. Dietterich, T. G., andBuchanan, B. G., "The Role oftheCritic
inLearning Systems," Technical ReportNo. HHP-81-19, (also No.
STAN-CS-81-891), Department of Computer Science, Stanford
University, 1981.
o 69. Dietterich, T. G., London, B., Clarkson, K., and Dromey, G.,
"LearningandInductiveInference," in TheHandbookofArtificial
Intelligence, Vol. 3, P. R. Cohen and E. A. Feigenbaum (Eds.),
Kaufmann, Los Altos, Calif., 1982.
de 70. Dietterich, T. G., and Buchanan, B. G., "The Role ofExperimentationinTheoryFormation," ProceedingsoftheInternational
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, pp. 147-55,
June 22-24, 1983.
eo 71. Dietterich, T. G., andMichalski, R. S., "AComparativeReview
of Selected Methods for Learning from Examples," in Machine
Learning: An Artificial Intelligence Approach, R. S. Michalski,
J. G. Carbonell, and T. M. Mitchell (Eds.), Tioga, Palo Alto,
Calif., 1983.
dpt 72. Dietterich, T. G., "Constraint Propagation Techniques for
Theory-Driven Data Interpretation," Ph.D. diss., Department of
Computer Science, Stanford University, 1984.
dp 73. Dietterich, T. G., "Learning About Systems That Contain State
Variables," Proceedings of AAAI-83, Washington, D.C.,
pp. 96-100, 1984.
es 74. Dietterich, T. G., and Michalski, R. S., "Learning to Predict
Sequences," in Machine Learning: An Artificial Intelligence
Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Morgan Kaufmann, Los Altos, Calif., 1986. (An
earlier version appeared in Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, June 22-24,
1982.)
--- PAGE 698 ---
684 BIBLIOGRAPHYOF RECENT RESEARCH
ams 75. Douglas,S.A.,andMoran,T. P., "LearningOperatorSemantics
by Analogy," Proceedings ofAAAI-83, Washington, D.C.,
pp. 100-103, 1983.
b 76. Doyle, J., "A Truth Maintenance System," ArtificialIntelligence,
Vol. 12, No. 3, pp. 231-72, November 1979.
ds 77. Ernst, G. W., and Goldstein, M. M., "Mechanical Discovery of
Classes of Problem-Solving Strategies," Journal ofthe ACM,
Vol. 29, No. 1, pp. 1-23, January 1982.
a 78. Evans, T. G.
"AProgramfortheSolutionofaClassofGeometric
Analogy Intelligence Test Questions," in Semantic Information
Processing, M. Minsky (Ed.), MIT Press, Cambridge, 1968.
b 79. Feigenbaum, E. A., and Feldman, J. (Eds), Computers and
Thought, McGraw-Hill, New York, 1963.
mn 80. Feigenbaum, E. A., "The Simulation of Verbal Learning
Behavior," In Computers and Thought, E. A. Feigenbaum and
J. Feldman (Eds.), McGraw-Hill, New York, 1963.
pt 81. Fickas, S., "Automatic Goal-Directed Program Transformation,"
Proceedings ofAAAI-80, Stanford, Calif., pp. 68-70, 1980.
rs 82. Fikes,R. E., Hart, P. E.,andNilsson,N.J., "LearningandExecuting Generalized Robot Plans," Artificial Intelligence, Vol. 3,
No. 4, pp. 251-88, Winter 1972.
am 83. Forbus, K. D., and Gentner, D., "Learning Physical Domains:
TowardaTheoretic Framework," inMachineLearning:AnArtificial Intelligence Approach, Vol. II, R. S. Michalski. J. G.
Carbonell, and T. M. Mitchell (Eds.), Morgan Kaufmann, Los
Altos, Calif. 1986. (Anearlierversionappearedin Proceedingsof
the International Machine Learning Workshop, R. S. Michalski
(Ed.), Allerton House, University of Illinois at UrbanaChampaign, June 22-24, 1983.)
o 84. Forsyth, R., "Machine Learning Systems," Proceedings ofthe
Associationfor Library and Information Management
ASLIB).
London, pp. 219-27, 1984.
ks 85. Friedland, P. E., "Acquisition of Procedural Knowledge from
Domain Experts," Proceedings ofthe Seventh IJCAI, Vancouver,
B.C., pp. 856-61, 1981.
ar 86. Genesereth, M. R., "Metaphors and Models," Proceedings <;/
AAAI-80, Stanford, Calif., pp. 208-11, 1980.
--- PAGE 699 ---
BIBLIOGRAPHY OF RECENT RESEARCH 685
bm 87. Gentner,D.,andStevens,A. L. (Eds.),MentalModels Erlbaum,
Hillsdale, N.J. 1983.
am 88. Gentner, D., "Structure Mapping: A Theoretical Framework for
Analogy," CognitiveScience, Vol. 7, No. 2, pp. 155-70, April-June
1983.
ams 89. Gick,M. L., andHolyoak, K. J., "Analogical ProblemSolving,"
CognitivePsychology, Vol. 12, pp. 306-55, 1980.
am 90. Gick, M. L., and Holyoak, K. J., "Schema Induction and Analogical Transfer," CognitivePsychology, Vol. 15, pp. 1-38, 1983.
fl 91. Glymour, C, Kelly, K., and Schemes, R., "Two Programs for
Testing Hypotheses ofAny Logical Forms," Proceedings ofthe
InternationalMachineLearning Workshop, R. S. Michalski (Ed.)
Allerton House, University of Illinois at Urbana-Champaign,
pp. 96-98, June 22-24, 1983.
eln 92. Gold, E. M., "LanguageIdentificationintheLimit," Information
and Control, Vol. 10, pp. 447-74, 1967.
bs 93. Groner, R., Groner, M., and Bischof, W. F. (Eds), Methods of
Heuristics, Erlbaum, Hillsdale, N.J., 1983.
kt 94. Haas, N., and Hendrix, G. G., "An Approach to Acquiring and
Applying Knowledge," Proceedings ofAAAI-80, Stanford, Calif.,
pp. 235-39, 1980.
kt 95. Haas, N., and Hendrix, G. G., "Learning by Being Told:
Acquiring Knowledge forInformation Management," in Machine
Learning: An Artificial Intelligence Approach, R. S. Michalski,
J. G. Carbonell, and T. M. Mitchell (Eds.), Tioga, Palo Alto,
Calif., 1983.
g 96. Hampson, S. E., "A Neural Model ofAdaptive Behavior," Ph.D.
diss., DepartmentofComputerandInformation Sciences, University ofCalifornia, Irvine, 1983.
97. Hanson, S. J., and Timberlake, W., "Regulations During Challenge: General Model of Learned Performance Under Schedule
Constraints," PsychologicalReview, Vol. 90, pp. 261-82, 1983.
k 98. Hayes-Roth, F., Klahr, P., and Mostow, D. J., "Knowledge
Acquisition, Knowledge Programming, and Knowledge Refinement," Technical Report R-2540-NSF, Rand Corporation, 1980.
st 99. Hayes-Roth, F., Klahr, P., and Mostow, D. J., "AdviceTaking and Knowledge Refinement: An Iterative View of Skill
--- PAGE 700 ---
686 BIBLIOGRAPHYOF RECENT RESEARCH
Acquisition," inSkillAcquisitionandDevelopment, J. A. Anderson
(Ed.), Erlbaum, Hillsdale, N.J., 1981.
t 100. Hayes-Roth, F., "Using Proofs and Refutations to Learn from
Experience," In Machine Learning: An Artificial Intelligence
Approach, R. S. Michalski, J. G. Carbonell, and T. M. Mitchell
(Eds.), Tioga, Palo Alto, Calif., 1983.
bk 101. Hayes-Roth, F., Waterman, D. A., and Lenat, D. B. (Eds),
BuildingExpertSystems Addison-Wesley, Reading, Mass., 1983.
an 102. Hobbs, J. R., "MetaphorInterpretationasSelectiveInferencing,"
Proceedings ofthe Eighth IJCAI, Karlsruhe, W. Ger., pp. 85-91,
1983.
r 103. Hofstadter, D. R., "The Architecture ofJumbo," Proceedings of
the International Machine Learning Workshop, R. S. Michalski
(Ed.), Allerton House, University of Illinois at UrbanaChampaign, pp. 161-70, June 22-24, 1983.
ab 104. Hofstadter, D., "Analogies and Roles in Human and Machine
Thinking," in Metamagical Themas, D. Hofstadter (Ed.), Basic
Books, New York, 1985.
g 105. Holland, J. H., "Escaping Brittleness: The Possibilities of
General-Purpose Learning Algorithms Applied to Parallel RuleBased Systems," in Machine Learning: An Artificial Intelligence
Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Morgan Kaufmann, Los Altos, Calif., 1986. (An
earlier version appeared in Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, June 22-24,
1983.)
eo 106. Holte, R. C, "Artificial Intelligence Approaches to Concept
Learning," In Digital Information Systems, I. Alexander (Ed.),
Prentice-Hall International, Englewood Cliffs, N.J., 1984.
am 107. Holyoak, K. J., "Analogical Thinking and Human Intelligence."
inAdvances in the PsychologyofHuman Intelligence, R. J. Sternberg (Ed.), Erlbaum, Hillsdale, N.J., 1984.
e 108. Iba, G. A., "Learning Disjunctive Concepts from Examples,"
Master'sthesis, MIT, 1979. (AlsoavailableasAI Memo 548, MIT,
1979.)
a 109. Kedar-Cabelli, S. T., "Purpose-Directed Analogy," Proceedings
oftheCognitiveScienceSociety, Irvine, Calif., pp. 150-159. 1985.
--- PAGE 701 ---
BIBLIOGRAPHYOF RECENT RESEARCH
st 110. Keller, R. M., "LearningbyRe-expressingConceptsforEfficient
Recognition," Proceedings ofAAAI-83, Washington, D.C.,
pp. 182-86, 1983.
es 111. Kibler, D., and Porter, B., "Episodic Learning," Proceedings of
AAAI-83, Washington, D.C., pp. 191-96, 1983.
es 112. Kibler, D., and Porter, B., "Perturbation: A Means for Guiding
Generalization," Proceedings ofthe Eighth IJCAI, Karlsruhe,
Ger., pp. 415-18, 1983.
ko 113. Kitakami, H., Kunifuji, S., Miyachi, T., and Furukawa, K.,
"Methodology for Implementation of a Knowledge Acquisition
System," Technical ReportICOTTR-037, ICOT, Institute forNew
Generation Computer Technology, Tokyo, 1983.
mo 114. Klahr, D., Langley, P., andNeches, R. T. (Eds), Self-Modifying
ProductionSystemModelsofLearningandDevelopment Bradford
Books, Cambridge, Mass., 1983.
em 115. Kline, P. J., "The Superiority of Relative Criteria in Partial
Matching and Generalization," Proceedings ofthe Seventh IJCAI,
Vancouver, B.C., pp. 296-303, 1981.
al 116. Kling, R. E., "A Paradigm for Reasoning by Analogy," Artificial
Intelligence, Vol. II, No. 2, pp. 147-78, Fall 1971.
eo 117. Knapman, J., "A Critical Review of Winston's Learning Structural Descriptions from Examples," AISB Quarterly, Vol. 31,
pp. 319-20, September 1978.
el 118. Kodratoff, Y., and Ganascia, J. G., "Improving the Generalization Step ofLearning," inMachineLearning:AnArtificialIntelligence Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and
T. M. Mitchell(Eds.), MorganKaufmann, LosAltos, Calif., 1986.
(An earlier version appeared in Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, June 22-24,
1983.)
mr 119. Kolodner, J. L., "Retrieval andOrganizational Strategies inConceptual Memory: A Computer Model," Ph.D. diss., Department
of Computer Science, Yale University, 1980. (Also available as
Technical Report 187, Yale University, 1980.)
mr 120. Kolodner, J. L., "Maintaining Organization in a Dynamic LongTerm Memory," Cognitive Science, Vol. 7, No. 4, pp. 243-80,
October-December 1983.
--- PAGE 702 ---
688 BIBLIOGRAPHYOF RECENT RESEARCH
mr 121. Kolodner, J. L., "Reconstructive Memory: A Computer Model,"
Cognitive Science, Vol. 7, No. 4, pp. 281-328, October-December
1983.
rs 122. Korf,R.E., "TowardaModelofRepresentationChanges,"ArtificialIntelligence, Vol. 14, No. 1, pp. 41-78, August 1980.
rs 123. Korf, R. E., "Operator Decomposability: A New Type ofProblem Structure," Proceedings ofAAAI-83, Washington, D.C., pp.
206-9, 1983.
rs 124. Korf, R. E., LearningtoSolveProblemsbySearchingforMacroOperators, Pitman, Marshfield, Mass., 1985. (Also available as
Ph.D. diss., Department of Computer Science, Carnegie-Mellon
University, 1983.)
k 125. Kulikowski, C. A., "Knowledge Acquisition and Learning in
EXPERT," Proceedings ofthe International Machine Learning
Workshop, R. S. Michalski (Ed.), Allerton House, University of
Illinois at Urbana-Champaign, pp. 71-73, June 22-24, 1983.
rs 126. Laird, J. E., and Newell, A., "A Universal Weak Method: Summary of Results," Proceedings ofthe Eighth IJCAI, Karlsruhe,
W. Ger., pp. 771-73, 1983.
es 127. Laird, J. E., and Rosenbloom, P. S., "Toward Chunking as a
General Learning Mechanism," Proceedings ofAAAI-84, Austin,
Tex., pp. 188-92, 1984.
bs 128. Laird, J. E., "Universal Subgoaling," Ph.D. diss., Departmentof
Computer Science, Carnegie-Mellon University, 1984.
bf 129. Lakatos, I., Proofs andRefutations: The Logic ofMathematical
Discovery, Cambridge University Press, Cambridge, 1976.
abn 130. Lakoff, G., and Johnson, M., Metaphors We Live By, Chicago
University Press, Chicago, 1980.
abn 131. Lakoff, G., and Johnson, M., "The Metaphorical Structure of
the Human Conceptual System," CognitiveScience, Vol. 4, No. 2.
pp. 195-208, April-June 1980.
d 132. Langley, P. W., "BACON. 1: A General Discovery System:' Proceedings ofthe Canadian Societyfor Computational Studies of
Intelligence, Toronto, pp. 173-80, 1978.
d 133. Langley, P. W., "Descriptive Discovery Processes: Experiments
in Baconian Science," Ph.D. diss.. Department of Psychology,
Carnegie-Mellon University, 1979.
--- PAGE 703 ---
BIBLIOGRAPHYOF RECENT RESEARCH
ps 134. Langley, P. W., Neches, R., Neves, D., and Anzai, Y., "A
Domain-Independent Framework for Procedure Learning,"
JournalofPolicyAnalysisandInformation Systems, Vol. 4, No. 2,
pp. 163-97, June 1980.
ms 135. Langley,P.W.,Nicholas,D., Klahr, D., andHood, G., "ASimulated World for Modelling Learning and Development," Proceedings ofthe Third Annual Conference ofthe Cognitive Science
Society, Berkeley, 1981.
d 136. Langley, P. W., "Data-DrivenDiscovery ofPhysical Laws," Cognitive Science, Vol. 5, No. 1, pp. 31-54, January-March 1981.
d 137. Langley, P. W., Bradshaw, G., and Simon, H. A., "BACON. 5:
TheDiscovery ofConservation Laws," ProceedingsoftheSeventh
IJCAI, Vancouver, B.C., pp. 121-26, 1981.
mo 138. Langley,P.W.
andSimon,H.A.
"TheCentralRoleofLearning
in Cognition," in Cognitive Skills and TheirAcquisition, J. R.
Anderson (Ed.), Erlbaum, Hillsdale, N.J., 1981.
d 139. Langley, P. W., Bradshaw, G., andSimon, H. A., "Data-Driven
and Expectation-Driven Discovery ofEmpirical Laws," Proceedings ofthe Canadian Societyfor ComputationalStudies ofIntelligence, Saskatoon, Saskatchewan, pp. 137-43, 1982.
d 140. Langley, P. W., Zytkow, J., and Simon, H. A., "Three Facets of
ScientificDiscovery,"ProceedingsoftheEighthIJCAI, Karlsruhe,
W. Ger., pp. 465-68, 1983.
es 141. Langley, P. W., "Learning Effective SearchHeuristics," ProceedingsoftheEighth IJCAI, Karlsruhe, W. Ger., pp. 419-21, 1983.
d 142. Langley,P. W., Simon,H. A., andBradshaw, G. L., "Rediscovering Chemistry withthe BACON System," inMachineLearning:
An Artificial Intelligence Approach, R. S. Michalski, J. G.
Carbonell, and T. M. Mitchell (Eds.), Tioga, Palo Alto, Calif.,
1983.
o 143. Langley, P. W., and Carbonell, J. G., "Approaches to Machine
Learning," Journal ofthe American Societyfor Information Science, Vol. 35, No. 5, pp. 306-16, September 1984.
d 144. Langley, P. W., Zytkow, J., Simon, H. A., and Bradshaw,
G. L., "TheSearchforRegularity: FourAspectsofScientific Discovery," in Machine Learning: An Artificial Intelligence
Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and
--- PAGE 704 ---
690 BIBLIOGRAPHYOF RECENT RESEARCH
T. M. Mitchell (Eds.), Morgan Kaufmann, Los Altos, Calif.,
1986. (An earlier version appeared in Proceedings oftheInternational Machine Learning Workshop, R. S. Michalski (Ed.),
AllertonHouse, University ofIllinoisatUrbana-Champaign, June
22-24, 1983.)
ms 145. Larkin,J. H., McDermott, J., Simon, D. P., andSimon, H. A.,
"Models ofCompetence in Solving Physics Problems," Cognitive
Science, Vol. 4, pp. 317-45, 1980.
ms 146. Larkin, J. H., "Enriching Formal Knowledge: A Model for
Learningto SolveTextbookPhysics Problems," in CognitiveSkills
and TheirAcquisition, J. R. Anderson (Ed.), Erlbaum, Hillsdale,
N.J., 1981.
mn 147. Lebowitz, M., "Generalization and Memory in an Integrated
Understanding System," Ph. D. diss., Department of Computer
Science, Yale University, 1980.
c 148. Lebowitz, M., "TheNatureofGeneralizationinUnderstanding,"
Proceedings ofthe Seventh IJCAI, Vancouver, B.C., pp. 348-53,
1981.
cd 149. Lebowitz, M., "RESEARCHER: An Overview," Proceedings of
AAAI-83, Washington, D.C., pp. 232-35, 1983.
en 150. Lebowitz, M., "Generalization from Natural Language Text,"
Cognitive Science, Vol. 7, pp. 1-40, 1983.
cer 151. Lebowitz, M., "Concept Learning in a Rich Input Domain:
Generalization-Based Memory," in Machine Learning: An Artificial Intelligence Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.), Morgan Kaufmann, Los Altos,
Calif., 1986. (An earlier version appeared in Proceedings ofthe
International Machine Learning Workshop,. R. S. Michalski
(Ed.), Allerton House, University of Illinois at UrbanaChampaign, June 22-24, 1983.)
d 152. Lenat, D. B., "AM: An Artificial Intelligence Approach to Discovery in Mathematics as Heuristic Search," Ph. D. diss.. Department ofComputer Science, Stanford University, 1976.
do 153. Lenat, D. B., "The Ubiquity of Discovery," Artificial Intelligence, Vol. 9, No. 3, pp. 257-85, December 1977.
d 154. Lenat, D. B., "Automated Theory Formation in Mathematics,"
Proceedings ofthe Fifth IJCAI, Cambridge, Mass., pp. 833-42.
1977.
--- PAGE 705 ---
BIBLIOGRAPHYOF RECENT RESEARCH
r 155. Lenat, D. B., Hayes-Roth, F., and Klahr, P., "Cognitive
Economy in Artificial Intelligence Systems," Proceedings ofthe
Sixth IJCAI, Tokyo, pp. 531-36, 1979.
d 156. Lenat, D. B., "The Role ofHeuristics in Learning by Discovery:
Three Case Studies," in Machine Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Tioga, PaloAlto, Calif., 1982.
d 157. Lenat, D. B., "The Nature of Heuristics," Artificial Intelligence, Vol. 19, No. 2, pp. 189-249, October 1982.
r 158. Lenat,D.B.
Hayes-Roth,F.andKlahr,P., "CognitiveEconomy
in a Fluid Task Environment," Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, pp. 133-46,
June 22-24, 1983.
d 159. Lenat,D.B., "TheoryFormationbyHeuristicSearch: TheNature
of Heuristics II: Background and Examples," Artificial Intelligence, Vol. 21, Nos. 1 and2, pp. 31-59, March 1983.
d 160. Lenat, D. B., "EURISKO: A Program That Learns New Heuristics and Domain Concepts: The Nature ofHeuristics III: Program
DesignandResults," ArtificialIntelligence, Vol. 21, Nos. 1 and2,
pp. 61-98, March 1983.
df 161. Lenat,D. B.,andBrown, J. S.,
"WhyAM
andEuriskoAppearto
Work," ArtificialIntelligence, Vol. 23, No. 3, pp. 269-94, August
1984. (AlsoappearsinProceedingsofAAAI-83,Washington, DC.
1983.)
c 162. Loisel, R., and Kodratoff, Y., "Learning (Complex) Structural
Descriptions from Examples," Proceedings ofthe Seventh IJCAI,
Vancouver, B.C., pp. 141-43, 1981.
es 163. MacDonald, B. A., and Andreae, J. H., "The Competence ofa
Multiple Context Learning System," International Journal on
GeneralSystems, No. 7, pp. 123-50, 1981.
st 164. Mahadevan, S., "Verification-Based Learning: A Generalization
Strategy forInferring Problem-Reduction Methods," Proceedings
oftheNinthIJCAI, Los Angeles, Calif., pp. 616-23, 1985.
g 165. Mauldin,M. L., "MaintainingDiversityinGeneticSearch,"Proceedings ofAAAI-84, Austin, Tex., pp. 247-50, 1984.
bt 166. McCarthy, J., "TheAdviceTaker," in SemanticInformationProcessing, M. Minsky (Ed.), MIT Press, Cambridge, Mass., 1968.
--- PAGE 706 ---
692 BIBLIOGRAPHYOF RECENT RESEARCH
b 167. McCarthy, J., "Programs with Common Sense," in Semantic
Information Processing, M. Minsky (Ed.), MIT Press, Cambridge, Mass. 1968.
bfl 168. McCarthy, J., and Hayes, P. J., "Some Philosophical Problems
fromthe StandpointofArtificial Intelligence," inMachineIntelligence 6, B. Meltzer and D. Michie (Eds.), Edinburgh University
Press, Edinburgh, 1969.
ar 169. McCarthy, L. T., and Sridharan, N. S., "The Representation of
an Evolving System ofLegal Concepts: II. Prototypes and Deformations," Proceedings ofthe Seventh IJCAI, Vancouver, B.C.,
pp. 246-53, 1981.
a 170. McDermott, J., "LearningtoUseAnalogies," Proceedingsofthe
Sixth IJCAI, Tokyo, pp. 568-76, 1979.
c 171. Medin, D. L., "Linear Separability and Concept Naturalness,"
Proceedings ofthe International Machine Learning Workshop,
R. S. Michalski (Ed.), Allerton House, University of Illinois at
Urbana-Champaign, pp. 213-17, June 22-24, 1983.
e 172. Michalski, R. S., "Discovering Classification Rules Using
Variable-Valued Logic System VL1," Proceedings ofthe Third
IJCAI, Stanford, Calif., pp. 162-72, 1973.
ce 173. Michalski, R. S., "Pattern Recognition as Rule-Guided Inductive
Inference," IEEE Transactions on Pattern Analysis and Machine
Intelligence, Vol. PAMI-2, No. 4, pp. 349-61, July 1980.
ekt 174. Michalski,R. S.,andChilausky,R. L., "LearningbyBeingTold
andLearningfromExamples: AnExperimentalComparisonofthe
Two Methods ofKnowledge Acquisition in the Context ofDeveloping an Expert System for Soybean Disease Diagnosis," PolicyAnalysis andInformation Systems, Vol. 4, No. 2 (special issue on
knowledge acquisition and induction), June 1980.
c 175. Michalski, R. S., "Knowledge Acquisition Through Conceptual
Clustering: A Theoretical Frameworkand an Algorithm for Partitioning Data into Conjunctive Concepts," Policy Analysis and
Information Systems, Vol. 4, No. 3, pp. 219-44, September 1980.
ce 176. Michalski, R. S., and Stepp, R. E., "An Application ofAI Tech
niques to Structuring Objects into an Optimal Conceptual Hierarchy," Proceedings ofthe Seventh IJCAI, Vancouver. B.C..
pp. 460-65, 1981.
--- PAGE 707 ---
BIBLIOGRAPHY OF RECENT RESEARCH 693
c 177. Michalski,R. S.,Stepp,R.,andDiday,E., "ARecentAdvancein
Data Analysis: Clustering Objects into Classes Characterized by
Conjunctive Concepts," in Progress in Pattern Recognition,
L. Kanal and A. Rosenfeld (Eds.), North-Holland, Amsterdam,
1981.
c 178. Michalski, R. S., and Stepp, R., "Learning from Observation:
Conceptual Clustering," inMachineLearning: AnArtificialIntelligence Approach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Tioga, Palo Alto, Calif., 1983.
bo 179. Michalski, R. S., Carbonell, J. G., and Mitchell, T. M. (Eds),
Machine Learning: An Artificial Intelligence Approach, Tioga,
Palo Alto, Calif., 1983.
eo 180. Michalski, R. S., "A Theory and Methodology of Inductive
Learning," ArtificialIntelligence, Vol. 20, No. 2, pp. 111-61, February 1983. (Amodifiedversionappears in MachineLearning:An
ArtificialIntelligenceApproach, R. S. Michalski, J. G. Carbonell,
andT. M. Mitchell (Eds.).)
c 181. Michalski, R. S., and Stepp, R., 'Automated Construction of
Classification: Conceptual Clustering Versus Numerical Taxonomy," IEEE Transactions on Pattern Analysis and Machine
Intelligence, Vol. 5, No. 4, pp. 396-410, July 1983.
k 182. Michalski, R. S., and Baskin, A. B., "Integrating Multiple
Knowledge Representations and Learning Capabilities in an
Expert System: The ADVISE System," Proceedings oftheEighth
IJCAI, Karlsruhe, W. Ger., pp. 256-58, 1983.
o 183. Michalski, R. S. (Ed.), ProceedingsoftheInternationalMachine
Learning Workshop, Allerton House, University of Illinois at
Urbana-Champaign, June 22-24, 1983.
e 184. Michalski, R. S., "Inductive Learning as Rule-Guided GeneralizationofSymbolic Descriptions: ATheory andImplementation,"
inAutomaticProgram Techniques, A. W. Bierman, G. Guiho, and
Y. Kodratoff(Eds.), Macmillan, New York, 1984.
o 185. Michalski, R. S., "Learning Strategies and Automated Knowledge Acquisition: An Overview," in Knowledge-Based Learning
Systems, L. Bole (Ed.), Springer-Verlag, New York, 1986
(forthcoming).
o 186. Michalski, R. S., "Understanding the Nature of Learning," in
Machine Learning: An Artificial Intelligence Approach, Vol. II,
--- PAGE 708 ---
694 BIBLIOGRAPHYOF RECENT RESEARCH
R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.),
Morgan Kaufmann, Los Altos, Calif., 1986.
o 187. Michalski, R. S., Amarel, S., Lenat, D. B., Michie, D., and
Winston, P. H., "Machine Learning: Challengesofthe Eighties,"
in MachineLearning:AnArtificialIntelligenceApproach, Vol. II,
R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.),
Morgan Kaufmann, Los Altos, Calif., 1986.
bo 188. Michalski, R. S., Carbonell, J. G., and Mitchell, T. M. (Eds),
Machine Learning: An Artificial Intelligence Approach, Vol. II,
Morgan Kaufmann, Los Altos, Calif., 1986.
o 189. Michie, D., "The State oftheArt in Machine Learning," inIntroductoryReadings inExpertSystems
D. Michie (Ed.), Gordon and
Breach, U.K., 1982.
ek 190. Michie, D., "InductiveRuleGenerationintheContextoftheFifth
Generation," Proceedings ofthe International Machine Learning
Workshop, R. S. Michalski (Ed.), Allerton House, University of
Illinois at Urbana-Champaign, pp. 65-70, June 22-24, 1983.
k 191. Michie, D., "Automating the Synthesis of Expert Knowledge,"
ASLIBProceedings, London, pp. 337-43, September 1984.
b 192. Minsky, M., "AFrameworkforRepresentingKnowledge," in The
Psychology ofComputer Vision, P. H. Winston (Ed.), McGrawHill, New York, 1975.
b 193. Minsky,M., "SocietyofMind," Technical Report, Departmentof
Computer Science, MIT, 1985.
st 194. Minton, S., "Constraint-Based Generalization: Learning GamePlaying Plans from Single Examples," Proceedings of AAA1-84,
Austin, Tex., pp. 251-54, 1984.
es 195. Mitchell, T. M., "Version Spaces: A Candidate Elimination
ApproachtoRuleLearning,"ProceedingsoftheFifthIJCAI, Cambridge, Mass., pp. 305-10, 1977.
es 196. Mitchell, T. M., "Version Spaces: An Approach to Concept
Learning," Ph.D. diss., Department of Electrical Engineering.
Stanford University, 1978. (Alsoavailable as Report STAN-CS-78711, HPP-79-2, DepartmentofComputerScience, StanfordUniversity, 1978.)
e 197. Mitchell, T. M., "The Need for Biases in Learning Generalizations," Technical Report CBM-TR-117, Rutgers University. 1980.
--- PAGE 709 ---
BIBLIOGRAPHY OF RECENT RESEARCH 695
es 198. Mitchell, T. M., Utgoff, P. E., Nudel, B., and Banerji, R.,
"Learning Problem-Solving Heuristics Through Practice," ProceedingsoftheSeventhIJCAI, Vancouver, B.C., pp. 127-34, 1981.
o 199. Mitchell,T., Carbonell,J. G., andMichalski, R., "SpecialSection on Machine Learning," SIGARTNewsletter, No. 76, pp.
25-64, April 1981.
eo 200. Mitchell, T. M., "Generalization as Search," Artificial Intelligence, Vol. 18, No. 2, pp. 203-26, March 1982.
es 201. Mitchell, T. M., Utgoff,P. E., andBanerji, R. B., "Learningby
Experimentation: Acquiring and Refining Problem-Solving Heuristics," InMachineLearning:AnArtificialIntelligenceApproach,
R. S. Michalski,J. G. Carbonell, andT. M. Mitchell(Eds.),Tioga,
Palo Alto, Calif., 1983.
os 202. Mitchell,T.M., "LearningandProblemSolving,"Proceedingsof
theEighth IJCAI, Karlsruhe, W. Ger., pp. 1139-51, 1983.
es 203. Mitchell, T. M., "Toward Combining Empirical and Analytic
MethodsforLearningHeuristics," inHumanandArtificialIntelligence, A. Elithorn and R. Banerji (Eds.), North-Holland,
Amsterdam, 1984.
kst 204. Mitchell, T. M., Mahadevan, S., and Steinberg, L., "LEAP: A
Learning Apprentice for VLSI Design," Proceedings oftheNinth
IJCAI, Los Angeles, Calif., pp. 573-80, 1985.
ab 205. Moore, J., and Newell, A., "How Can Merlin Understand?" in
KnowledgeandCognition, L. W. Gregg(Ed.), Erlbaum,Hillsdale,
N.J., 1974.
st 206. Mostow, D. J., and Hayes-Roth, R, "Operationalizing Heuristics: Some AI Methods forAssisting AI Programming," Proceedings ofthe Sixth IJCAI, Tokyo, pp. 601-9, 1979.
st 207. Mostow, D. J., "Mechanical Transformation of Task Heuristics
into Operational Procedures," Ph.D. diss., Department of Computer Science, Carnegie-Mellon University, 1981.
st 208. Mostow, D. J., "Machine Transformation ofAdvice into a HeuristicSearchProcedure," inMachineLearning:AnArtificialIntelligence Approach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Tioga, Palo Alto, Calif., 1983.
st 209. Mostow, D. J., "Operationalizing Advice: A Problem-Solving
Model," Proceedings ofthe International Machine Learning
--- PAGE 710 ---
696 BIBLIOGRAPHYOF RECENT RESEARCH
Workshop, R. S. Michalski (Ed.), Allerton House, University of
Illinois at Urbana-Champaign, pp. 110-16, June 22-24, 1983.
st 210. Mostow, D. J., "A Problem-Solver for Making Advice Operational," Proceedings ofAAAI-83, Washington, D.C., pp. 279-83,
1983.
o 211. Mostow, D. J., "1983 International Machine LearningWorkshop:
An Informal Report," SIGARTNewsletter, No. 86, pp. 24-31,
October 1983.
pr 212. Neches, R. T., "HPM: AComputational FormalismforHeuristic
Procedure Modification," ProceedingsoftheSeventh IJCAI, Vancouver, B.C., pp. 283-88, 1981.
ms 213. Neches, R., "Models of Heuristic Procedure Modification,"
Ph.D. diss., Department of Computer Science, Carnegie-Mellon
University, 1982.
mr 214. Neves, D. M., and Anderson, J. R., "Knowledge Compilation:
MechanismsfortheAutomatizationofCognitiveSkills," inCognitive Skills and TheirAcquisition, J. R. Anderson (Ed.), Erlbaum,
Hillsdale, N.J. 1981.
ems 215. Neves, D. M., "Learning Procedures from Examples," Ph.D.
diss., DepartmentofComputerScience, Carnegie-MellonUniversity, 1981.
b 216. Newell, A., and Simon, H., Human Problem Solving. PrenticeHall, Englewood Cliffs, N.J., 1972.
bm 217. Newell, A., "Reasoning, Problem Solving and Decision Processes: The Problem Space as a Fundamental Category," inAttentionandPerformance VIII, R. S. Nickerson(Ed.), Erlbaum, Hillsdale, N.J. 1980. (Originally appeared in Proceedings ofthe
InternationalSymposium on Attention andPerformance, 1978.)
mrs 218. Newell, A., and Rosenbloom, P., "Mechanisms ofSkill AcquisitionandtheLawofPractice," inCognitiveSkillsandTheirAcquisition, J. R. Anderson (Ed.), Erlbaum, Hillsdale, N.J., 1981.
b 219. Nilsson, N. J., Principles ofArtificial Intelligence, Tioga, Palo
Alto, Calif., 1980.
bm 220. Norman, D. A. (Ed.), Perspectives on Cognitive Science,
Erlbaum, Hillsdale, N.J., 1981.
st 221. O'Rorke, P., "Generalization for Explanation-Based Schema
Acquisition," Proceedings ofAAAI-84, Austin, Tex., pp. 260-63,
1984.
--- PAGE 711 ---
BIBLIOGRAPHYOF RECENT RESEARCH 697
In 222. Osherson, D. N., and Weinstein, S., "Criteria of Language
Learning," Journal ofInformation and Control, Vol. 52, No. 2,
pp. 123-37, February 1982.
bp 223. Paige, R., "Supercompilers-Extended Abstract," in Program
Transformation andProgramming Environment, P. Pepper (Ed.),
Springer-Verlag, New York, 1984.
go 224. Pettit, E., and Swigger, K. M., "An Analysis ofGenetic-Based
Pattern Tracking and Cognitive-Based Component Tracking
Models of Adaptation," Proceedings of AAA I-83, Washington,
D.C., pp. 327-32, 1983.
e 225. Plotkin, G. D., "ANoteonInductiveGeneralization," InMachine
Intelligence5, B. MeltzerandD. Michie(Eds.), EdinburghUniversity Press, Edinburgh, 1970.
ek 226. Politakis, P. G., "Using Empirical Analysis to Refine Expert
System Knowledge Bases," Ph.D. diss., Department ofComputer
Science, Rutgers University, 1982. (Also available as Technical
Report CBM-TR-130, Rutgers University, 1982.)
ek 227. Politakis, P., and Weiss, S. M., "Using Empirical Analysis to
Refine Expert System Knowledge Bases," Artificial Intelligence,
Vol. 22, No. 1, pp. 23-48, February 1984.
b 228. Polya, G., How toSolveIt, 2d. ed., Doubleday, New York, 1957.
bf 229. Popper, K., TheLogicofScientificDiscovery, 2ded. Harperand
Row, New York, 1968.
es 230. Porter,B., "LearningProblemSolving," Ph.D. diss. Department
ofComputer and Information Sciences, University ofCalifornia,
Irvine, 1984.
es 231. Porter,B.,andKibler,D., "LearningOperatorTransformations,"
Proceedings ofAAAI-84, Austin, Tex. pp. 278-82, 1984.
ce 232. Quinlan, J. R., "Discovering Rules from Large Collections of
Examples: A Case Study," in Expert Systems in the Micro Electronic Age, D. Michie (Ed.), Edinburgh University Press, Edinburgh, 1979.
ce 233. Quinlan, J. R., "Semi-Autonomous AcquisitionofPattern-Based
Knowledge," Australian Computer Bulletin, April 1980. (Also
appears in Machine Intelligence 10, D. Michie, J. E. Hayes, and
Y.-H. Pao (Eds.), Ellis Harwood Ltd., New York, 1982.)
ces 234. Quinlan, J. R., "Learning Efficient Classification Procedures
and Their Application to Chess End-Games," in Machine
--- PAGE 712 ---
698 BIBLIOGRAPHYOF RECENT RESEARCH
Learning: An Artificial Intelligence Approach, R. S. Michalski,
J. G. Carbonell, and T. M. Mitchell (Eds.), Tioga, Palo Alto,
Calif., 1983.
ce 235. Quinlan, J. R., "The Effect of Noise on Concept Learning," in
Machine Learning: An Artificial Intelligence Approach, Vol. II,
R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.),
Morgan Kaufmann, Los Altos, Calif., 1986. (An earlier version
appeared in Proceedings ofthe International Machine Learning
Workshop, R. S. Michalski (Ed.), Allerton House, University of
Illinois at Urbana-Champaign, June 22-24, 1983.)
cs 236. Rendell, L. A., "An Adaptive Plan for State-Space Problems,"
Ph.D. diss., Department of Computer Science, University of
Waterloo, Waterloo, Canada, 1981.
cs 237. Rendell, L. A., "A Learning System Which Accommodates Feature Interactions," Proceedings ofthe Eighth IJCAI, Karlsruhe,
W. Ger., pp. 469-72, 1983.
cs 238. Rendell, L. A., "A New Basis for State-Space Learning Systems
andaSuccessfulImplementation," ArtificialIntelligence, Vol. 20,
No. 4, pp. 369-92, July 1983.
cgs 239. Rendell, L. A., "ADoublyLayered, GeneticPenetranceLearning
DC,
System," ProceedingsofAAAI-83, Washington, pp. 343-47,
1983.
os 240. Rendell, L. A., "Toward a Unified Approach for Conceptual
Knowledge Acquisition," AIMagazine, Vol. 4, No. 4, pp. 19-27,
Winter 1983.
dfo 241. Richie,G.D.,andHanna,F. K., "AM: ACaseStudyinAIMethodology," Artificial Intelligence, Vol. 23, No. 3, pp. 249-68.
August 1984.
nr 242. Riesbeck, C. K., "Failure-Driven Reminding for Incremental
Learning," Proceedings ofthe Seventh IJCAI, Vancouver, B.C.,
pp. 115-20, 1981.
e 243. Rissland, E. L., and Soloway, E. M., "Constrained Example
Generation: A Testbed forStudying Issues in Learning," Proceedings ofthe Seventh IJCAI, Vancouver, B.C., pp. 162-64, 1981.
mr 244. Rosch, E., and Mervis, C. B., "Family Resemblances: Studies in
theInternalStructureofCategories," CognitivePsychology, Vol. 7.
No. 4, pp. 573-605, October 1975.
--- PAGE 713 ---
BIBLIOGRAPHYOF RECENT RESEARCH 699
rs 245. Rosenbloom, P. S., and Newell, A., "Learning by Chunking:
Summary ofaTaskand a Model," Proceedings ofAAAI-82, Pittsburgh, Pa., pp. 255-58, 1982.
emr 246. Rosenbloom,P. S., "TheChunkingofGoalHierarchies: AModel
of Practice and Stimulus-Response Compatibility," Ph.D. diss.,
Department ofPsychology, Carnegie-Mellon University, 1983.
rs 247. Rosenbloom,P. S.,andNewell,A., "TheChunkingofGoalHierarchies," in Machine Learning: An Artificial Intelligence
Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Morgan Kaufmann, Los Altos, Calif., 1986. (An
earlier version appeared in Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, June 22-24,
1983.)
am 248. Rumelhart, D. E., andAbrahamson, A. A., "A Model forAnalogical Reasoning," CognitivePsychology, Vol. 5, pp. 1-28, 1973.
ams 249. Rumelhart, D. E., andNorman, D. E., "Analogical Processesin
Learning," in Cognitive Skills and Their Acquisition, J. R.
Anderson (Ed.), Erlbaum, Hillsdale, N.J., 1981.
ik 250. Rychener, M. D., "The Instructive Production System: A RetrospectiveAnalysis," inMachineLearning:AnArtificialIntelligence
Approach, R. S. Michalski, J. G. Carbonell, and T. M. Mitchell
(Eds.), Tioga, Palo Alto, Calif., 1983.
tr 251. Salzberg, S., "Generating HypothesestoExplain Prediction Failures," Proceedings ofAAAI-83, Washington, D.C., pp. 352-55,
1983.
rt 252. Salzberg, S., andAtkinson, D. J., "Learningby BuildingCausal
Explanations," Proceedings ofthe Sixth European Conference on
AI, Pisa, Italy, pp. 497-500, 1984.
e 253. Sammut, C, "Learning Concepts by Performing Experiments,"
Ph.D. diss., Department ofComputer Science, University ofNew
SouthWales, 1981.
e 254. Sammut, C, "ConceptLearningbyExperiment," Proceedingsof
the Seventh IJCAI, Vancouver, B.C., pp. 104-5, 1981.
er 255. Sammut, C, andBanerji, R. B., "LearningConceptsby Asking
Questions," In Machine Learning: An Artificial Intelligence
Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and
--- PAGE 714 ---
700 BIBLIOGRAPHYOF RECENT RESEARCH
T. M. Mitchell(Eds.), MorganKaufmann, LosAltos, Calif. 1986.
(An earlier version appeared in Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, June 22-24,
1983.)
es 256. Samuel, A. L., "Some Studies in Machine Learning Using the
GameofCheckers," inComputersandThought, E. A. Feigenbaum
andJ. Feldman (Eds.), McGraw-Hill, New York, 1963.
bmn 257. Schank, R. C, and Abelson, R. P., Scripts, Plans, Goals and
Understanding, Erlbaum, Hillsdale, N.J., 1977.
bmnr 258. Schank, R. C, "Language and Memory," Cognitive Science,
Vol. 4, No. 3, pp. 243-84, July-September 1980.
mnr 259. Schank, R. C, Dynamic Memory: A Theory ofReminding and
Learning in Computers andPeople, Cambridge University Press,
Cambridge, 1982.
o 260. Schank, R. C, "Looking at Learning," Proceedings ofthe Fifth
European ConferenceonAI, Paris, pp. 11-18, 1982.
b 261. Schank,R. C, "TheCurrentStateofAI: OneMan'sOpinion,"AI
Magazine, Vol. 4, No. 1, pp. 3-8, Winter/Spring 1983.
s 262. Schmidt,C. , Sridharan,N. , andGoodson, J., "ThePlanRecognition Problem," Artificial Intelligence, Vol. 11, Nos. 1 and 2,
pp. 45-83, August 1978.
fr 263. Scott, P. D., and Vogt, R. C, "Knowledge Oriented Learning,"
Proceedings oftheEighth IJCAI, Karlsruhe, W. Ger., pp. 432-35,
August 1983.
fr 264. Scott, P. D., "Learning: The Construction ofAposteriori Knowledge Structures," Proceedings ofAAAI-83, Washington, D.C.,
pp. 359-63, 1983.
b 265. Selfridge,O. G., "Pandemonium: AParadigmforLearning,"Proceedings ofthe Symposium on the Mechanization ofThoughtProcesses, D. Blake and A. Uttley (Eds.), London, pp. 511-29, 1959.
mn 266. Selfridge, M. A., "A Process Model of Language Acquisition,"
Ph.D. diss., Department of Computer Science, Yale University,
1980.
mn 267. Selfridge, M. A., "A Computer Model ofChild Language Acquisition," Proceedings ofthe Seventh IJCAI, Vancouver, B.C..
pp. 92-95, 1981.
--- PAGE 715 ---
BIBLIOGRAPHYOF RECENT RESEARCH
dl 268. Shapiro, E. Y., "Inductive Inference of Theories from Facts,"
Research Report 192, Yale University, 1981.
dl 269. Shapiro, E. Y., "An Algorithm ThatInfers Theories from Facts,"
Proceedings ofthe Seventh IJCAI, Vancouver, B.C., pp. 446-51,
1981.
dl 270. Shapiro, E. Y., Algorithmic Program Debugging, MIT Press,
Cambridge, 1983.
es 271. Silver,B., "LearningEquationSolvingMethodsfromExamples,"
Proceedings oftheEighth IJCAI, Karlsruhe, W. Ger., pp. 429-31,
1983.
es 272. Silver, B., "Precondition Analysis: Learning Control Information," in Machine Learning: An Artificial Intelligence Approach,
Vol. II, R. S. Michalski, J. G. Carbonell, and T. M. Mitchell
(Eds.), Morgan Kaufmann, Los Altos, Calif., 1986. (An earlier
version appeared in Proceedings ofthe International Machine
Learning Workshop, R. S. Michalski (Ed.), Allerton House, University ofIllinois at Urbana-Champaign, June 22-24, 1983.)
eos 273. Simon, H. A., and Lea, G., "Problem Solving and Rule Induction: A UnifiedView," inKnowledgeand Cognition, L. W. Gregg
(Ed.), Erlbaum, Hillsdale, N.J., 1974.
b 274. Simon,H. A., "ArtificialIntelligenceSystemsThatUnderstand,"
Proceedings ofthe Fifth IJCAI, Cambridge, Mass., pp. 1059-73,
1977.
b 275. Simon, H. A., The Sciences ofthe Artificial, MIT Press, Cambridge, 1982.
o 276. Simon, H. A., "Why Should Machines Learn?" in Machine
Learning: An Artificial Intelligence Approach, R. S. Michalski,
J. G. Carbonell, and T. M. Mitchell (Eds.), Tioga, Palo Alto,
Calif., 1983.
k 277. Sleeman, D. H., "A Rule-Based Task Generation System," ProceedingsoftheSeventhIJCAI, Vancouver, B.C., pp. 882-87, 1981.
i 278. Sleeman,D.H.,andBrown, J. S., "IntelligentTutoringSystems:
AnOverview," inIntelligent TutoringSystems, D. H. Sleemanand
J. S. Brown (Eds.), Academic Press, New York, 1981.
i 279. Sleeman,D.H. , andBrown, J. S. (Eds. ), IntelligentTutoringSystems, Academic Press, New York, 1981.
--- PAGE 716 ---
702 BIBLIOGRAPHYOF RECENT RESEARCH
ms 280. Sleeman,D. H.,andSmith,M. J., "ModelingStudents' Problem
Solving," Artificial Intelligence, Vol. 16, No. 2, pp. 171-87, May
1981.
es 281. Sleeman, D. H., Langley, P., and Mitchell, T. M., "Learning
from Solution Paths: An Approach to the Credit Assignment
Problem," AIMagazine, Vol. 3, No. 2, pp. 48-52, Spring 1982.
im 282. Sleeman, D. H., "Inferring Student Models for Intelligent
Computer-AidedInstruction," inMachineLearning:AnArtificial
IntelligenceApproach, R. S. Michalski,J. G. Carboneil, andT. M.
Mitchell (Eds.), Tioga, Palo Alto, Calif., 1983.
em 283. Sleeman, D., "Inferring (MAL) Rules from Pupils' Protocols,"
Proceedings ofthe International Machine Learning Workshop,
R. S. Michalski (Ed.), Allerton House, University of Illinois at
Urbana-Champaign, pp. 221-27, June 22-24, 1983.
ms 284. Smith, R. L., Jr., "Modeling Student Acquisition of Problem
Solving Skills," Proceedings ofAAAI-80, Stanford, Calif.,
pp. 221-23, 1980.
g 285. Smith, S., "A Learning System Based on Genetic Algorithms,"
Ph.D. diss., DepartmentofComputerScience, UniversityofPittsburgh, 1980.
en 286. Smith, D., "Focuser: A Strategic Interaction Paradigm for Language Acquisition," Ph.D. diss., Department of Computer Science, Rutgers University, 1982.
mo 287. Snow, R. E., Fredrico, P. A., andMontague, W. E. (Eds.),Aptitude,Learning,andInstruction, Vols. 1 and2, Erlbaum, Hillsdale,
N.J., 1980.
e 288. Soloway, E. M., and Riseman, E. M., "Knowledge-Directed
Learning," SIGARTNewsletter, Vol. 63 Proceedingsofthe Workshop on Pattern-DirectedInference Systems), pp. 49-55, 1977.
et 289. Soloway, E. M., "Learning = Interpretation -I- Generalization: A
Case Study in Knowledge-Directed Learning," Ph.D. diss..
Department ofComputer and Information Science, University of
Massachusetts, Amherst, 1978. (Also available as Computer and
Information Science Report COINS TR-78-13, U. Mass., 1967.)
c 290. Stepp, R. E., "Conjunctive Conceptual Clustering: A Methodology and Experimentation," Ph.D. diss.. Department of Computer Science, University ofIllinois, Urbana, 1984.
--- PAGE 717 ---
BIBLIOGRAPHYOF RECENT RESEARCH 703
cer 291. Stepp, R. E., and Michalski, R. S., "Conceptual Clustering:
InventingGoal-OrientedClassificationsofStructuredObjects." in
Machine Learning: An Artificial Intelligence Approach, Vol. II,
R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.),
Morgan Kaufmann, Los Altos, Calif., 1986. (An earlier version
appeared in Proceedings ofthe International Machine Learning
Workshop, R. S. Michalski (Ed.), Allerton House, University of
Illinois at Urbana-Champaign, June 22-24, 1983.)
ps 292. Sussman, G.J., A ComputerModelofSkillAcquisition, American
Elsevier, New York, 1975.
e 293. Techuci, G., "Learning Hierarchical Descriptions from Examples," ComputersandArtificialIntelligence, Slovenska Akademia
Vied, Romania, Vol. 3, No. 3, pp. 211-22, 1984.
ers 294. Utgott, R E., and Mitchell, T. M., "Acquisition ofAppropriate
Bias for Inductive Concept Learning," Proceedings ofAAAI-82,
Pittsburgh, Pa., pp. 414-17, 1982.
ers 295. Utgoff, P. E., "AdjustingBias inConceptLearning," Proceedings
oftheEighth IJCAI, Karlsruhe, W. Ger., pp. 447-49, 1983.
296. Utgoff, P. E., and Nudel, B., "Comprehensive Bibliography of
Machine Learning," in Machine Learning: An Artificial Intelligence Approach, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Tioga, PaloAlto, Calif., 1983.
er 297. Utgoff, P. E., "Shift of Bias for Inductive Concept Learning,"
Ph.D. diss., DepartmentofComputerScience, RutgersUniversity,
1984.
ers 298. Utgoff, P. E., "Shift ofBias for Inductive Concept Learning," in
Machine Learning: An Artificial Intelligence Approach, Vol. II,
R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.).
Morgan Kaufmann, Los Altos, Calif., 1986. (An earlier version
appeared in Proceedings ofthe International Machine Learning
Workshop, R. S. Michalski (Ed.), Allerton House, University of
Illinois at Urbana-Champaign, June 22-24, 1983.)
1 299. Valiant, L. G., "A Theory ofthe Learnable," Communications of
theACM, Vol. 27, No. 11, pp. 1134-42, November 1984.
as 300. VanLehn, K., and Brown, J. S., "Planning Nets: A Representation for Formalizing Analogies and Semantic Models of Procedural Skills," in Aptitude Learning and Instruction: Cognitive
Processes ofLearning and Problem-Solving, R. E. Snow,
--- PAGE 718 ---
704 BIBLIOGRAPHYOF RECENT RESEARCH
P. A. Frederico, and W. E. Montague (Eds.), Erlbaum, Hillsdale,
N.J., 1980.
mp 301. VanLehn, K., "Human Procedural Skill Acquisition: Theory,
Model, and Psychological Validation," Proceedings ofAAAI-83,
Washington, D.C., pp. 420-23, 1983.
ms 302. VanLehn, K., "ValidatingaTheoryofHuman SkillAcquisition,"
Proceedings ofthe International Machine Learning Workshop,
R. S. Michalski (Ed.), Allerton House, University of Illinois at
Urbana-Champaign, June 22-24, 1983.
ms 303. VanLehn, K., "Felicity Conditions for Human Skill Acquisition:
ValidatinganAI-BasedTheory," Ph.D. diss., DepartmentofComputer Science, MIT, 1983.
e 304. Vere, S. A., "Multilevel Counterfactuals for Generalizations of
Relational Concepts and Productions," Artificial Intelligence,
Vol. 14, No. 2, pp. 138-64, September 1980.
ce 305. Waterman,D.A.
"GeneralizationLearningTechniquesforAutomating the Learning ofHeuristics," ArtificialIntelligence, Vol. 1,
No. 1/2, pp. 121-70, 1970.
b 306. Waterman, D. A., and Hayes-Roth, F. (Eds.), Pattern-Directed
Inference Systems, Academic Press, New York, 1978.
b 307. Webber, B. L., and Nilsson, N. J. (Eds.), Readings in Artificial
Intelligence, Tioga, PaloAlto, Calif., 1981.
e 308. Winston, P. H., "Learning Structural Descriptions from Examples," in ThePsychologyofComputer Vision, P. H. Winston (Ed.),
McGraw-Hill, New York, 1975.
a 309. Winston,P. H., "LearningandReasoningbyAnalogy," CommunicationsoftheACM, Vol. 23, No. 12,pp. 689-702, December 1980.
ae 310. Winston, P. H., "Learning New Principles from Precedents and
Exercises," Artificial Intelligence, Vol. 19, No. 3, pp. 321-350,
November 1982.
ae 311. Winston, P. H., Binford, T. O., Katz, B., and Lowry, M.,
"Learning Physical Descriptions from Functional Definitions,
Examples, andPrecedents,"ProceedingsofAAAI-83, Washington,
DC,
pp. 433-39, 1983.
--- PAGE 719 ---
BIBLIOGRAPHYOF RECENT RESEARCH 705
ae 312. Winston, P. H., "Learning by Augmenting Rules and Accumulating Censors," in Machine Learning: An Artificial Intelligence
Approach, Vol. II, R. S. Michalski, J. G. Carbonell, and T. M.
Mitchell (Eds.), Morgan Kaufmann, Los Altos, Calif., 1986. (An
earlier version appeared in Proceedings ofthe International
Machine Learning Workshop, R. S. Michalski (Ed.), Allerton
House, University ofIllinois at Urbana-Champaign, June 22-24,
1983.)
--- PAGE 720 ---
--- PAGE 721 ---
UPDATED GLOSSARY OF SELECTED
TERMS MACHINE LEARNING
ThisglossaryisamodifiedandupdatedversionoftheglossaryincludedinMachine
LearningI. Itwaspreparedbytheeditorswiththehelpofsomeleadingresearchersin
thefield. Theanglebrackets " < > " indicatethatthetermusedinadefinitionisitself
an entry in the glossary. In a few cases, terms in the glossary have not been widely
usedintheliteraturebutwereincludedbecausethey seemtobeparticularlydescriptive ofthe important ideas they represent.
Adaptive Systems: Control systems or pattern recognition systems that achieve
desiredperformance by adjusting their internal parameters.
Advice Taking: A formoflearning in whichthelearner modifies its (procedural)
behavioraccording to the (declarative) advice givenby an instructor.
Analogical Inference: Mapping information from a known objectorprocess to a
less knownbut similar one.
Analogical Means-ends Analysis: A problem-solving process operating in the
<Analogical Problem Space) akin to <Means-ends Analysis). A solution to a
new problem is obtained via the transformation of the solution of a similar
problemusingoperatorsthatreducedifferencesbetweencorrespondingsolution
descriptions.
Analogical Problem Space: A problem space whose states are descriptions of
problem solutions and whose operators transform one problem solution into a
closely relatedone.
Attribute: A variableorone-argument <Descriptor> usedtocharacterizeanobject
oraprocess. Forexample, thecolor(ofanobject) ortheduration (ofaprocess)
are attributes.
--- PAGE 722 ---
708 UPDATEDGLOSSARYOFTERMS
Caching(MemoFunctions)
Storingtheanswertofrequentlyoccurringquestions
(problems) in order to avoid a replication ofpast efforts; an example of <Rote
Learning).
Causal Analysis: Tracing the probable causes of observed events, occasionally
used in <Credit (Blame) Assignment).
Characteristic Description: A (Concept Description) that states properties
characterizing all instances of a given concept (or class). (Cf. Discriminant
Description).
Chunking: Grouping lower-level descriptions (patterns, operators, goals) into
higher-level descriptions.
Complete Generalization: A description characterizing all <Positive Examples)
ofagivenclass, whetherornotitalsocharacterizessome (NegativeExamples).
Composition: Grouping a sequence of (Production Rules) or operators into a
single rule oroperator.
ComputerAssistedInstruction(CAI) Thestudyofcomputer-basedteachingand
testing.
Concept Acquisition: See (Learning from Examples).
Concept Description: (also Description, Generalization): A symbolic data structure defining a concept describing the class of all known instances of the
concepts.
Concept Formation: A form oflearning in which the learner generates concepts
useful incharacterizingagivencollectionofobjectsorfactsorasubsetofthem.
Conceptual Clustering: A form of(Learning fromObservation) concerned with
arranging objects (situations, facts, etc.) into classes characterized by simple
descriptiveconcepts ratherthan intoclassesdefinedby apredefinedmeasure of
similarity among their members, as inconventional clustering.
ConsistentGeneralization: A descriptionofsomeorall (Positive Examples) ofa
class that does not include any (Negative Examples) ofthat class.
Consistent Hypothesis: See (Consistent Generalization)
Constraint: A propertyorrelationthatrestrictsthespaceofpossiblesolutionstoa
problem.
Constraint-based Generalization: A method ofgeneralization that explores constraints binding descriptive concepts characterizing or explaining a given
example (intraexample constraints) and produces a generalization ofthat event
that satisfies these constraints. (Cf. (Similarity-based Generalization).)
--- PAGE 723 ---
UPDATED GLOSSARYOFTERMS
Constructive Induction: An (Inductive Learning) process that generates new
<descriptors) notprovided in the description ofinitial facts orobservations.
Credit (Blame) Assignment: Identifying the steps (decisions, operators, etc.)
chiefly responsible for a success (failure) in the overall process ofachieving a
goal.
Decision Tree: A (Discrimination Network) in the form ofatree structure.
Deductive Inference: A mode of reasoning that starts with certain assertions
(premises) and concludes with logical consequences of these assertions. It
employs (Deductive Inference Rules).
Deductive Inference Rule: An (Inference Rule) that, given one or more assertions, produces a logically equivalent or more specific assertion. A deductive
inferenceruleisatruth-preservingtransformationofassertions. (Cf. (Inductive
Inference Rule).)
Derivational Analogy: A case-based problem-solving method in which derivations of solutions to similar problems are replayed and modified to solve new
problems.
Descriptor: Anattribute, function, orpredicateusedasanelementaryconceptfor
describing objects or situations.
Discriminant Description: A (Concept Description) that states properties that
distinguish the given concept from other concepts under consideration. (Cf.
(Characteristic Description).)
DiscriminationNetwork: Anetworkencodingasetofteststoclassifyacollection
ofobjects (situations, events, etc.) into fixed categories according to predetermined features ofthe objects.
Domain ofa Descriptor: (also Value Set ofa Descriptor): The set ofadmissible
valuesthata (Descriptor) maytakeasacomponentofa (ConceptDescription)
Expert System: A computer system that achieves performance comparable to a
human expert at solving problems in some task domain by utilizing a large
amount of domain-specific knowledge. Because of the substantial amounts
of knowledge required, the (Knowledge Acquisition) task assumes major
proportions.
Expertise Acquisition: See (Knowledge Acquisition).
Feature: See (Attribute).
--- PAGE 724 ---
710 UPDATEDGLOSSARYOFTERMS
Generalization: Extending the scope of a concept description to include more
instances(theoppositeofSpecialization)). Thistermissometimesalsousedas
a noun, synonymous with <Concept Description).
Generalization Rule: An (Inductive Inference Rule) used in generalizing examples in <Learning from Examples).
Grammatical Inference: A form of <Inductive Inference) concerned with inferring the grammar ofalanguage from a setofsentences labeled "grammatically
correct" and a second (optional) set labeled "grammatically incorrect."
Heuristics: Imperfect but useful pieces ofknowledge employed in reasoning and
problem-solving tasks, such as <Plausible Inference), discovery, and so on, in
lieu ofprecise knowledge. Also, an approximate inference Rule).
Heuristic Search: Aproblem-solving method for finding a sequence ofoperators
thattransforms an initial state intoadesiredgoal state. <Heuristics) are usedto
generate, test, and prune operator sequences.
Incremental Learning: Multistage learning in which knowledge learned at one
stage is modifiedto accommodate new facts provided in subsequent stages.
Inductive Inference: A mode ofreasoning that starts with some assertions, e.g.,
specific observations, and concludes with more-general and -plausible assertions, i.e., hypotheses explaining the initial assertions. It employs <Inductive
Inference Rules).
Inductive Inference Rules: An inference Rule) that, given one or more assertions, produces an assertion that logically implies the original assertion(s). An
inductiveinferenceruleisafalsity-preservingtransformationofassertions. (Cf.
<Deductive Inference Rule).)
Inductive Learning: Learning by drawing <Inductive Inferences) from facts and
observations obtained from a teacher or environment (that is, learning by
(Inductive Inference)).
InferenceRule: A rulethatproducesnew assertions fromold, eitherby the application ofstrict logical principles orby more imperfect, plausible methods. (See
also (Inductive Inference Rule) and (Deductive Inference Rule).)
Intelligent CAI (ICAI): The application ofAI techniques in building (Computer
Assisted Instruction) systems.
Knowledge Acquisition: (also Expertise Acquisition): A form o( machine
learningconcernedwithtransferring knowledge from humansorataskenvironment into computers; often associated with constructing or augmenting the
knowledge base ofan (Expert System).
--- PAGE 725 ---
. .
UPDATEDGLOSSARY OFTERMS
KnowledgeCompilation: (also <Operationalization> ofKnowledge): Translating
knowledgefromadeclarativeformthatcannotbeuseddirectly intoaneffective
procedural form; for example, converting the advice "Don't get wet" into specific instructions that recommend howto avoid getting wet in a given situation.
(See also <Skill Acquisition).)
Learning by Being Told: See <Learning from Instruction)
LearningfromExamples: Inferringageneral <ConceptDescription) fromexamples and (optionally) counterexamples of that concept. This is a form of
<Inductive Learning).
Learning from Instruction: (also Advice Taking, and Learning by Being Told):
Theprocessoftransformingandintegratinginstructionsfromanexternalsource
(such as ateacher) into an internally usable form.
Learning from Observation: (also Learning without a Teacher, Unsupervised
Learning): Constructing descriptions, hypotheses ortheories about a given collectionoffactsorobservations. Inthisformoflearningthereisnoaprioriclassification ofobservations into sets exemplifying desired concepts.
Machine Learning: A subdomain ofartificial intelligence concerned with developing computational theories of learning and constructing machines with
learning capabilities.
Macro-operator: An operator composed ofa sequence ofmore primitive operators. Appropriate macro-operators can simplify problem solving by allowing a
more "coarse-grained" problem-solving search.
Means-ends Analysis: A problem-solving method that searches at each step for
operators that maximally reduce the difference between the current state and a
known goal state.
NearMiss: A counterexampleofaconceptthatisquitesimilartopositiveexamples
ofthis concept. Near misses are very useful in isolating significant features in
<Learning from Examples).
Near-missAnalysis: Theprocessofexploiting < NearMisses) toboundthe scope
ofGeneralization) in learning from examples.
NegativeExample: In <Learning fromExamples)
acounterexampleofaconcept
that may boundthe scope ofGeneralization)
Neural Network: A network ofneuron-like elements that performs some simple
logical function, typically a logic threshold function.
Operationalization: See <Knowledge Compilation).
--- PAGE 726 ---
712 UPDATEDGLOSSARYOFTERMS
ParameterAdjustment: Changingtherelativeweightofdifferenttermsinamathematicalexpression, asafunctionofcredit (blame) forpastsuccesses (failures);
akind ofincremental curve fitting.
Partially Learned Concept: In concept learning, an underdetermined concept;
thatis, aconceptwhoseprecisedescriptioncannotbeinferredonthebasisofthe
learner's current data, knowledge, and assumptions. (See also incremental
Learning) and <Version Space).)
Partial Matching: A technique for comparing structural descriptions by identifyingtheircorrespondingcomponents; usefulinvariouskindsofinference, such
as <Analogical Inference).
Path Constraint In problem solving, a <Predicate) that must be satisfied by any
solution sequences; a type of<Constraint>.
Plausible Inference: A derivation oflikely conclusions from incomplete, imperfect, assumed, or indirectly relevant premises. This includes <Inductive>,
approximate, default, and <Analogical) inference.
PositiveExample: In <LearningfromExamples)
anexampleorinstanceofaconcepttobe learned in <Learning from Examples).
Predicate: statementthat is eithertrue or false; abasic building blockofpredicate logic.
ProblemReformulation: Translatingaproblemstatementintoanalternativelogicallyequivalentformsothatanappropriatesolutionmethodcanbeapplied. This
may include reformulating data representations and restating problem
constraints.
ProductionRule: A condition-actionpair, statingthattheactionistobeperformed
ifthe condition is matched or satisfied.
Production System: An inference system comprised ofalarge set of (Production
Rules) aworkingmemoryagainstwhichproductionsarematched, andthecontrol structure to apply the productions to working memory.
Proceduralization: Theconversionofdeclarativeknowledgeintoproceduralform
(see also (Knowledge Compilation)).
Rote Learning: Learning by direct memorization offacts without generalization
(see also (Caching)).
Schema: A symbolic structure that can be filled in by specific information
("instantiated'') todenote an instance ofthe generic concept represented by the
structure.
--- PAGE 727 ---
UPDATEDGLOSSARYOFTERMS 713
Similarity-based Generalization: A methodofgeneralizationthatexplores similarities and differences between examples of the same concept (interexample
similarities) in order to create a description characterizing or explaining all
examples ofthatconcept (Cf. <Constraint-based generalization)).
SimilarityMetric
Either(1)acontext-freemathematicalmeasureonpropertiesof
object descriptions used in clustering-minimized for objects within a cluster
andmaximizedforobjectsspanningclusters, or(2)acontext-sensitivesymbolic
expression capturing relevant similarities between two object or process
descriptions-usedto establish mappings in <Analogical Inference).
Skill Acquisition: (and Refinement): Acquiring or improving a procedural skill
(such as touchtyping) by <Knowledge Compilation) and repeated practice.
Specialization: Narrowing the scope ofa <Concept Description), thus reducing
the sets ofinstances itdescribes (opposite ofGeneralization)).
StructuralDescription:
Asymbolicrepresentationforobjectsandconceptsbased
ondescriptions oftheirparts andthe relationships among them.
Supervised Learning: A termusually used inthe context of<Adaptive Systems)
or <Neural Networks) to denote learning processes in which input signals are
accompanied by a classification decision provided by a teacher (see <Learning
from Examples)).
Transformational Analogy: A problem-solving method in which a solution to a
similarproblemistransformedincrementallyintoasolutiontothenewproblem.
UnsupervisedLearning: See <Learning fromObservation)
VersionSpace: (ofaconcept): The setofalternativecandidate <ConceptDescriptions) thatareconsistentwiththetrainingdata, knowledge, and assumptions of
the concept learner. This setdefines a <Partially Learned Concept) and can be
representedintermsofitsmaximallygeneralandmaximally specificmembers.
WeakMethods: Generalmethodsforproblemsolvingapplicableintheabsenceof
specific knowledge oftheproblemdomain required formoredirect orefficient
algorithmic solutions. Forexample, see <Means-ends Analysis) and <Heuristic
Search).
--- PAGE 728 ---
--- PAGE 729 ---
ABOUT THE AUTHORS
SaulAmarel isProfessorofComputerScienceatRutgersUniversity. Hereceivedhis
B.S. inElectrical Engineering (1948) fromTechnion, Israel, InstituteofTechnology
andhisM.S. (1953)andD. Eng. Sci. (1955)fromColumbiaUniversity. From 1957to
1969 he was Head ofComputer Theory Research at RCS Laboratories, Princeton,
N.J., and from 1969 to 1984 he was Chairman ofComputer Science at Rutgers. He
organizedtheLaboratoryforComputerScienceResearchatRutgersandfoundedthe
Rutgers Research Resource on Computers in Biomedicine, which has been a major
centerforresearchonknowledge-based systems since 1971. Hehasbeen involved in
artificial intelligence research since the early sixties. In 1983 he was General
Chairman ofthe International Joint Conference on Artificial Intelligence. His currentresearchinterestsincludeproblemsofrepresentationinproblemsolving, theory
formation processes, model-guided reasoning, learning and expertise acquisition,
planninganddesignproblems, andapplicationsofAItomedicine, engineering, and
science. He istheauthorofnumerousjournalarticlesandbookchapters. Hiscurrent
address is Department ofComputer Science, Rutgers University, New Brunswick,
NJ 08903.
John R. Anderson is Professor ofPsychology and Computer Science at CarnegieMellon University. He receivedhisB.A. fromtheUniversity ofBritish Columbia in
1968 and his Ph.D. from Stanford University in 1977. Beforejoining the faculty at
CMU, Dr. AndersonwasaProfessoratYaleUniversity. His research interestsare in
human learning and memory and intelligent tutoring. His books include Human
Associative Memory (with G. Bower, 1973), Language, Memory, and Thought
(1976), CognitivePsychologyandItsImplications(1980), CognitiveSkillsandTheir
Acquisition (1981), and TheArchitectureofCognition (1983). His currentaddress is
DepartmentofPsychology, Carnegie-Mellon University, Pittsburgh, PA 15213.
RananB. BanerjiisProfessorofMathematicsandComputerScienceatSt. Joseph's
University. Hereceivedhis Ph.D. fromtheUniversityofCalcutta in Physicsandhas
--- PAGE 730 ---
716 ABOUTTHEAUTHORS
worked on ionospheric physics and propagation, coding theory, languages, and
automata theory prior to his present research in AI. His major interest is in mathematical models ofproblems and inductive logic involved in learning heuristics for
problemsolving. He istheauthoroftwobooksonthesubject. Hiscurrentaddressis
Department of Mathematics and Computer Science, St. Joseph's University, 5600
City Avenue, Philadelphia, PA 19131.
Robert C. Berwick is Assistant ProfessorofComputer Science and Engineering at
the Massachusetts Institute ofTechnology in the Artificial Intelligence Laboratory.
HereceivedhisA.B. in 1976fromHarvardCollegeandhisS.M. andPh.D. fromMIT
in 1980and 1982, respectively. Hiscurrentresearchcentersonthelearningofsyntax,
wordmeaning, andthesemanticsoftense; formalmodelsofinductiveinference; and
the relationship between computational complexity and language processing. ProfessorBerwick is the author ofover fifty technical articles in the areas ofcomputational linguisticsand learning. Hehaspublishedtwobooks: ComputationalModels
ofDiscourse, editedwithJ. MichaelBrady(1983)and, withA. Weinberg, TheGrammaticalBasis ofLinguisticPerformance (1984). His current address is MIT Artificial Intelligence Laboratory, Room 820, 545 Technology Square, Cambridge, MA
02139.
Gary L. Bradshaw is Assistant Professor in Psychology at the University ofColorado, Boulder. He received his B.A. fromthe University ofMissouri, Columbia, in
1974andhisPh.D. fromCarnegie-MellonUniversityin 1984. Hisresearchinterestis
in building intelligent systems capable of discovering knowledge through experience. Two majorprojects include BACON, aprogram capable ofdiscovering scientific laws, and NEXUS, a program that identifies important components ofspeech
and incorporatesthis information intoaworking speech recognition system. He has
authored or coauthored fourteen technical papers in the areas of human learning,
humanmemory, infantmotorbehavior, scientificdiscovery, andspeech recognition.
Hiscurrentaddress isPsychology Department, CampusBox345, UniversityofColorado, Boulder, 80309.
Mark H. Burstein is a Research Scientist at Bolt Beranek and Newman, Inc. He
received his B.S. in Mathematics from the Massachusetts Institute ofTechnology in
1976 and his Ph.D. in Computer Science from Yale University in 1984. He is interested in cognitive modeling, learning, and knowledge representation. His dissertationdescribesacognitivemodelofanalytical processesbywhichstudentsapplyand
integrate knowledge from several domains in learning new concepts. His current
address is BBN Laboratories, 10 Moulton Street, Cambridge, MA 02238.
JaimeG. Carbonell isAssociateProfessorofComputerScienceatCarnegie-Mellon
University. Priortojoiningthe facultyatCarnegie-Mellon in 1978. he wasa research
fellow at Yale University, where he received his M.S. and Ph.D. in Computer
Science, specializing in Artificial Intelligence and Natural Language Processing.
--- PAGE 731 ---
ABOUTTHEAUTHORS
Previously, hehadreceiveddegreesinMathematicsandPhysicsattheMassachusetts
InstituteofTechnology. Hisresearchinterestsspanvariousaspectsofartificial intelligence, computational linguistics, knowledge engineering and cognitive science,
especially natural language interfaces, machine learning, and analogical reasoning.
Dr. Carbonell istheauthorofSubjective Understanding: ComputerModelsofBelief
Systems, the coeditor ofMachine Learning: An Artificial Intelligence Approach
(1983) and Machine Learning: An Artificial Intelligence Approach, Vol. II (1986),
and the author or coauthor of over eighty technical papers and reports. He is a
founder and director ofCarnegie Group, Inc., one ofthe nation's leading AI firms.
He is chairman ofSIGART (the Special Interest Group in Artificial Intelligence of
the ACM). His current address is Department of Computer Science, CarnegieMellon University, Pittsburgh, PA 15213.
GeraldDejong isAssociate ProfessorofElectrical EngineeringandComputerScience atthe University ofIllinois. He alsohas a research appointmentatthe CoordinatedScience Laboratory. HereceivedhisB.S. fromtheUniversity ofSouthDakota
in 1974andhis Ph.D. fromYale University in 1979. Afterayear'spostdoctoral work
atYalehebeganhiscurrentappointmentin 1981. In 1982hereceivedanExxonJunior
Faculty Award and in 1984, an Arnold O. Beckman Award. His research interests
include natural language processing, machine learning, and robotics. He designed
the FRUMP natural language system, one ofthe few AI systems to work well on
unconstrained novel inputs. More recently he has been working on explanationbasedlearning. AlongwithhisgraduatestudentsheiscurrentlyworkingonAIprojectsinnaturallanguageprocessing, robotics, theoremproving, andphysicsproblem
solving. His current address is Coordinated Science Laboratory, University ofIllinois at Urbana-Champaign, Urbana, IL 61801.
Nachum Dershowitz is Associate Professor of Computer Science at the University ofIllinois at Urbana-Champaign. He studied in Israel, receiving his B.S. and
Ph.D. inAppliedMathematicsfromtheWeizmannInstitutein 1975and 1979, respectively. His doctoral research in automated program development took him to the
Stanford Artificial Intelligence Laboratory; that work is described in his book The
Evolution ofPrograms (1983). His research interests also include program verification and logic programming. His current address is Department ofComputer Science, University ofIllinois at Urbana-Champaign, 1304 West Springfield Avenue,
Urbana, IL61801.
Thomas G. Dietterich is Assistant Professor ofComputer Science at Oregon State
UniversityinCorvallis, Oregon. HereceivedhisA.B. fromOberlinCollege in 1977,
hisM.S. fromtheUniversityofIllinoisin 1979, andhisPh.D. fromStanfordUniversity in 1984. He has written a number ofarticles on machine learning, including a
survey entitled "Learning and Inductive Inference," which constitutes chapter 14 of
volume3ofTheHandbookofArtificialIntelligence(1982). Dr. Dietterich'sresearch
concernsmachinelearningfromtheperspectiveofscientifictheory formation. He is
--- PAGE 732 ---
718 ABOUTTHEAUTHORS
currently investigating issues that involve forming theories about systems that contain statevariables, including the relationshipbetweentheory formationandexperimentdesign. His currentaddress is DepartmentofComputer Science, Oregon State
University, Corvallis, 97331.
Kenneth D. Forbus is Assistant ProfessorofComputer Science atthe University of
IllinoisatUrbana-Champaign. HereceivedhisdegreesfromtheMassachusettsInstitute of Technology (B.S., 1977, M.S., 1981, Ph.D., 1984). His research concerns
commonsense and qualitative reasoning, with a focus on reasoning about physical
processes and space as well as learning. His currentaddress is DepartmentofComputer Science, University ofIllinois, 1304 W. Springfield Ave., Urbana, IL 61801.
Jean-Gabriel GanasciaisAssistantProfessorattheUniversityofParis-Sud, Orsay,
whereheisworkingintheLaboratoiredeRechercheenInformatique. Heobtaineda
degree as Docteur Ingenieur in 1983. His research involves modeling experts'
behavior in domains in which data may be uncertain and reasoning inexact, such as
medicine orgeology. Atpresent ProfessorGanasciais completing a Doctoratd'Etat
degreeinComputerScience, concentratingonmachinelearningasappliedtoexpert
systems. His current address is Laboratoire de Recherche en Informatique,
Universite de Paris-Sud, Orsay, 91405, France.
Dedre Gentner is Associate ProfessorofPsychology at the University ofIllinois at
Urbana-Champaign. She received her A.B. from the University of California at
Berkeley in 1969 and her Ph.D. from the University ofCalifornia at San Diego in
1974. Herwork isincognitivepsychologyandcognitivedevelopment, withafocuson
analogy, learning, andacquisitionofmeaning. PriortojoiningthefacultyattheUniversityofIllinois, shewasaseniorscientistatBoltBeranekandNewman, Inc. Sheis
the coeditor ofMental Models. Her current address is Department of Psychology,
University ofIllinois, 603 E. Daniel, Champaign, IL 61801.
John H. Holland is ProfessorofComputer Science and Engineering at the UniversityofMichigan. HereceivedhisB.S. inPhysicsfromthe MassachusettsInstitute of
Technology (1950), his M.A. in Mathematics from the University of Michigan
(1954), andhisPh.D. inCommunicationSciences(nowComputerScienceandEngineering) from the University of Michigan (1959). Dr. Holland's research interests
center on adaptive systems, both natural and artificial, including genetic processes,
economic systems, cognitive processes, induction, cellular automata, machine
learning, and relevant computer architectures. He has published Adaptation in NaturalandArtificialSystemsand iscurrentlycompleting, withthree coauthors. Induction:Learning, Discovery, andthe Growth ofKnowledge. Dr. Holland haspublished
over thirty papers in these areas, including several book chapters. His current
address is Department ofElectrical Engineering and Computer Science. University
ofMichigan, Ann Arbor, MI 48109.
--- PAGE 733 ---
ABOUTTHEAUTHORS
Smadar T. Kedar-Cabelli is a doctoral candidate at Rutgers University, pursuing
research in machine learning underthe direction ofDr. Tom Mitchell. Herdissertation explores techniques for concept learning by analogy, guided by explicit knowledge ofthe purpose ofthe analogy. Her other interests in machine learning include
analogyinlegalreasoning, therelationshipoflearningandnonmonotonicreasoning,
andgoal-directedlearning. She receivedaB.A. in Mathematics (1976) andan M.Sc.
inComputerScience (1982) fromRutgersUniversity. Hercurrentaddressis Department ofComputer Science, Rutgers University, New Brunswick, NJ 08903.
Yves Kodratoffis Associate Professor (maitre de recherche) at the Laboratoire de
Recherche en Informatique, University of Paris-Sud, Orsay. He obtained his Doctorat d'Etat in 1967 in the field ofparamagnetic resonance appliedto chemistry. At
presentProfessorKodratoffheadsagroupworkingoninference, learning, andautomatic programming. His current address is Laboratoire de Recherche en Informatique, Universite de Paris-Sud, Orsay, 91405, France.
Pat Langley is Associate Professor ofComputer Science at the University ofCaliforniaatIrvine. Hisresearchfocusesoncomputationalmethodsforlearninganddiscovery. He isthe authorofBACON, anAI systemthathas rediscoveredanumberof
laws fromthe history ofscience; SAGE, asystemthat learns heuristics fordirecting
search; and a number of other machine learning systems. His current research
focuses on combining the results ofprevious efforts into integrated models of the
learning process. Dr. Langley has published over thirty-five papers in the areas of
artificial intelligence and cognitive science. He is Executive Editor ofthe journal
MachineLearning, andheiscoauthoringabookdescribingtheresultsofthe BACON
Project. His current address is Department ofInformation and Computer Science,
University ofCalifornia, Irvine, CA 92717.
MichaelLebowitzistheHerbertM. SingerAssistantProfessorofComputerScience
atColumbiaUniversity. HereceivedhisS.B. (1975) fromtheMassachusettsInstitute
of Technology and his M.S. (1977) and Ph.D. (1980) from Yale University, all in
ComputerScience. AtColumbiaUniversityheleadsaresearchgroupstudying intelligentinformationsystemsinvolvingtheautomaticconstructionoflarge information
bases from assorted kinds ofdata. His current research interests include learning,
naturallanguageprocessing,humanmemorymodels, andextendedstorygeneration.
Hiscurrentaddressis DepartmentofComputerScience, Columbia University, New
York, New York 10027.
DouglasB. LenatisPrincipal ScientistattheMicroelectronicsandComputerTechnology Corporation (MCC) and is a faculty member at Stanford University. His
research has focused on machine learning through automated discovery and on the
nature ofheuristics, and it has earned IJCAI's Computers and Thought Award and
AAALsBest PaperAward. HecoeditedthebookBuildingExpertSystemsandcoauthored the book Knowledge Based Systems in Artificial Intelligence. Dr. Lenat's
--- PAGE 734 ---
720 ABOUTTHEAUTHORS
numerous articles include the September 1984 overview ofAI in Scientific American. His current address is Microelectronics and Computer Technology Corporation, 9430 Research Boulevard, Echelon Building #3, Austin, TX 78759.
SridharMahadevan isadoctoralcandidateintheComputerScienceDepartmentat
Rutgers University. He received his M.Tech. in Electrical Engineering from the
Indian Institute of Technology, Kanpur, in 1983. He is a research assistant in the
Learning Apprentice Project at Rutgers University. He has written a number of
articlesinmachinelearningongeneralizationtechniquesforlearningapprenticesystems. His current address is Department ofComputer Science, Rutgers University,
New Brunswick, NJ 08903.
Ryszard S. Michalski is Professor ofComputer Science and Medical Information
Science and Director ofthe Artificial Intelligence Laboratory at the University of
IllinoisatUrbana-Champaign. HestudiedattheCracowandWarsawTechnicalUniversities and received his M.S. from the Leningrad Polytechnic Institute and his
Ph.D. fromtheUniversityofSilesiainPoland. PriortocomingtotheUnitedStatesin
1970, he was aResearch Scientistatthe Polish Academy ofSciences in Warsaw. His
research interests include machine learning, inductive inference, expert systems,
modeling ofhuman plausible reasoning, computer vision, and application ofartificial intelligence to life sciences, particularly to medicine and agriculture. Dr.
Michalskiauthoredorcoauthoredoverninetyresearchandtechnicalpapersonthese
topics. He coedited Machine Learning: An Artificial Intelligence Approach (1983)
and Machine Learning: An Artificial Intelligence Approach, Vol. II (1986). He
coorganized, with Carbonell and Mitchell, three International Workshops on
Machine Learning (1980, 1983, and 1985). Dr. Michalski iscoeditoroftheMachine
LearningJournal. Hiscurrentaddress is DepartmentofComputerScience, University ofIllinois, 1304 W. Springfield, Urbana, IL61801.
Donald Michie is DirectorofResearch and Advanced Study at the Turing Institute,
an Academic Associate ofthe University ofStrathclyde, and Professor Emeritus of
theUniversityofEdinburgh. He receivedhisM.A., D. Phil., andD.Sc. from Balliol
College, Oxford. During the Second World War hejoined the Bletchley Park codebreakingestablishment,andafterpursuingapostwarcareerinexperimentalgenetics
and immunology, he returned to machine intelligence in the early 1960s. In 1967 he
was elected to a Personal Chair ofMachine Intelligence in the University ofEdinburgh. Hehasbeen Visiting Professoratseveral universities inthe United States and
is Adjunct ProfessorofComputer Science at the University ofIllinois. He is editorin-chiefoftheMachineIntelligenceseriesandtheauthorofbooksonmachineintelligenceandofscientificpublicationsinbiologyandcomputerscience. Hismost recent
workhasbeenconcernedwiththestructuredinductionofsituation-actionplansfrom
example decisions. His current address is the Turing Institute. George House. 36
North Hanover Street, Glasgow G12AD, Great Britain.
--- PAGE 735 ---
ABOUTTHEAUTHORS
TomM. MitchellisAssociateProfessorofComputerScienceatRutgersUniversity.
He earned his B.S. degree (1973) from MIT and his M.S. (1975) and Ph.D. (1978)
degrees from Stanford University. He is the recipient ofthe 1983 IJCAI Computers
andThoughtAwardinrecognitionofhisresearchonmachinelearningandofa 1984
NSFPresidentialYoungInvestigatorAward.
Dr. MitchellhastaughtArtificialIntelligence at Rutgers and in tutorial courses for the past six years. He is coeditor of
Machine Learning: An Artificial Intelligence Approach (1983) and Machine
Learning: An ArtificialIntelligenceApproach, Vol. II (1986). His current research
lies in the areas oflearning heuristics through experimentation, knowledge-based
aids forcircuitdesign, and frameworks for incorporating learning into knowledgebasedsystems. HiscurrentaddressisDepartmentofComputerScience, RutgersUniversity, New Brunswick, NJ 08903.
AllenNewellistheU. A. andHelenWhitakerUniversityProfessorofComputerScience at Carnegie-Mellon University. He received his B.S. in physics in 1949 from
StanfordUniversity, didoneyearofgraduateworkatPrincetonin 1950, andreceived
hisPh.D. inindustrialadministrationin 1957 fromCarnegieInstituteofTechnology
(now CMU). From 1957 to 1961 he was amemberofthe scientific staffatthe Rand
Corporation, andhesubsequentlyjoinedCarnegie-MellonUniversity. Dr. Newell is
a recipient of several distinguished awards, among them the 1971 Harry Goode
AwardoftheAmericanFederationofInformationProcessingSocieties (AFIPS)and
the 1975 A.M. Turing Award ofthe Association ofComputing Machinery (jointly
withHerbertS. Simon). He isamemberoftheNationalAcademyofSciencesandthe
NationalAcademyofEngineering. Hehasauthoredorcoauthoredseveralbooksand
approximately 200 publications. His active research interests are artificial intelligence, humancognition, human-computerinteraction, andthesocialimpactofcomputers. Hiscurrentaddressis ComputerScienceDepartment, Carnegie-MellonUniversity, Pittsburgh, PA 15213.
J. Ross Quinlan isthe Head ofthe School ofComputing Sciences atthe New South
Wales Institute ofTechnology in Sydney. He obtainedhis B.Sc. fromthe University
ofSydney in 1965 andhis Ph.D. fromthenewly formedComputerScience Groupat
the University ofWashington in 1968. Dr. Quinlan spent the 1968-1969 academic
yearasaVisitingAssistantProfessoratCarnegie-Mellon University, the 1970-1980
period in the Basser Department ofComputer Science at the University ofSydney,
and 1981-1982 asaComputerScientistatthe Rand Corporation in Los Angeles. He
has worked in the area ofartificial intelligence since 1965, first in problem solving
and learningandmorerecently inexpertsystems, inductive inference, andplausible
reasoning. HiscurrentaddressisSchoolofComputing Sciences, N.S.W. Instituteof
Technology, Broadway, N.S.W. Australia 2007.
Paul S. Rosenbloom is Assistant ProfessorofComputer Science and Psychology at
StanfordUniversity. HereceivedhisB.S. (1976)inMathematicalSciencesfromStanford University and his M.S. (1978) and Ph.D. (1983) in Computer Science from
--- PAGE 736 ---
722 ABOUTTHEAUTHORS
Carnegie-Mellon University. He spent the 1978-1979 academic year as a visiting
graduate student in Psychology at the University ofCalifornia, San Diego, and the
year 1983-1984 as a Research Computer Scientist at Carnegie-Mellon University.
Hiscurrentresearchfocusesonthedesignofcognitivearchitecturesasbothpsychological models and artifically intelligent systems, with particular attention to the
interactionsamongknowledge,learning,andproblemsolving. Hiscurrentaddressis
Heuristic Programming Project, Stanford University, 701 Welch Road (BuildingC),
Palo Alto, CA 94304.
Claude Sammut is a Lecturer in Computer Science atthe University ofNew South
Wales. HereceivedhisB.Sc. fromtheUniversityofNewSouthWalesin 1978,andhis
Ph.D. fromthesameinstitutionin 1982. AfterreceivinghisPh.D. Dr. Sammuttooka
postdoctoralfellowshipatSaintJoseph'sUniversityinPhiladelphiaandin 1983wasa
VisitingAssistantProfessorintheDepartmentofComputerScienceattheUniversity
of Illinois, Urbana-Champaign. His current research interests are in knowledge
acquisitionforexpertsystemsandinlogicprogramming. Dr. Sammut istheauthorof
the UNSW PROLOG system. His current address is Department ofComputer Science, University of New South Wales, P.O. Box 1, Kensington, N.S.W. Australia
2033.
BernardSilverisaSeniorMemberoftheTechnicalStaffat GTE Laboratories,Inc.
in Waltham, Mass. He received his B.Sc. in Mathematics from Imperial College,
London, in 1980, andhisPh.D. inArtificialIntelligencefromtheUniversityofEdinburghin 1984. Hisinterestsincludecontrollingsearch,computeralgebra, PROLOG,
and strategy learning. His doctoral dissertation, "Using Meta-Level Inference to
Constrain Search and to Learn Strategies in Equation Solving," supervised by Dr.
AlanBundy,combinedthesefieldsintheLPprogram. Dr. Silveriscurrentlyat GTE,
continuinghisresearchonmachinelearning. HiscurrentaddressisInformationSciences, FundamentalResearchLaboratory, GTE Laboratories, Inc., 40Sylvan Road,
Waltham, 02254.
Herbert A. Simon is Professor ofComputer Science and Psychology at CarnegieMellonUniversity, wherehehastaughtsince 1949. HewaseducatedattheUniversity
ofChicago (B.A., 1936, Ph.D., 1943). He is a member ofthe National Academy of
Sciences, hashonorarydoctoratesfromanumberofuniversities intheUnited States
and abroad, and has received awards for distinguished service from the American
Psychological Association, the Association for Computing Machinery, and the
American Economic Association. In 1978 he received the Alfred Nobel Memorial
Prize in Economics. Among his numerous books are Administrative Behavior,
Human Problem Solving (with Allen Newell), The New Sciences ofManagement
Decision, Models of Thought, The Sciences ofthe Artificial, Models ofBounded
Rationality(vols. 1 and2), Reasonin Human Affairs, andProtocolAnalysis(with K.
A. Ericsson). His current address is Department of Psychology. Carnegie-Mellon
University, Pittsburgh, PA 15213.
--- PAGE 737 ---
ABOUTTHEAUTHORS
RobertE. SteppIII is Assistant ProfessorofElectrical and Computer Engineering
atthe University ofIllinois at Urbana-Champaign. He received his A.B. (1970) and
M.S. (1971) from the University of Nebraska and his Ph.D. in Computer Science
fromthe University ofIllinois (1984). Dr. Stepphas authored orcoauthored several
publications in the areas ofinductive learning and conceptual clustering. His interests include machine learning, conceptual data analysis, software engineering, and
applicationsofpersonalcomputersforhelpingtheblind. Hiscurrentaddress is University of Illinois, Coordinated Science Laboratory, 1101 W. Springfield Avenue,
Urbana, IL61801.
GailE. Thornburg is adoctoral candidate inthe Department ofLibrary and Information Science atthe University ofIllinois at Urbana-Champaign. She received her
M.S. fromKentStateUniversityin 1982andisexpectedtoreceiveherPh.D. in 1986.
Her interests include expert systems, information retrieval, and structure ofknowledge. Herdissertationinvolvesdevelopmentofanadvisorysystemforon-linebibliographicinformationretrieval. HercurrentaddressisGraduateSchoolofLibraryand
Information Science, University ofIllinois, 1407 W. Gregory, Urbana, IL 61801.
Paul E. Utgoffis Assistant Professor in the Department ofComputer and Information Science atthe University ofMassachusetts at Amherst. He received his B.Mus.
(1974) from Oberlin College Conservatory ofMusic and his M.S. (1979) and Ph.D.
(1984) in Computer Science from Rutgers University. His current research focuses
on generalizing relations from examples. Additional research interests include
learning fromexplanations, apprentice systems, deductive learning, and distributed
systems. His current address is Department ofComputer and Information Science,
Lederle Graduate Research Center, University of Massachusetts, Amherst,
01003.
Patrick H. Winston is Professor ofElectrical Engineering and Computer Science
and Directorofthe Artificial Intelligence Laboratory atthe Massachusetts Institute
ofTechnology. HereceivedhisB.S. in 1965, hisM.S. in 1967, andhis Ph.D. in 1970.
all fromtheMassachusettsInstituteofTechnology. Hiscurrentresearch interestsare
concentratedinartificial intelligenceandalliedfields. He isparticularly involvedin
thestudyoflearningbyanalogy, commonsenseproblemsolving, expertsystems, and
robots. He is the author ofArtificial Intelligence (two editions), coauthor of LISP,
editorofThePsychologyofComputer Vision, andcoeditorofArtificialIntelligence:
AnMITPerspectiveand TheAIBusiness. Hiscurrentaddressis MITArtificialIntelligence Laboratory, 545 Technology Square, Cambridge, 02139.
JanM.Zytkow isAssociateProfessorofComputerScienceatWichitaState University and Associate Professor ofPhilosophy ofScience at the University ofWarsaw,
Poland. He received his M.S. in Physics (1967), Ph.D. in Philosophy (1972), and
Habilitation in Philosophy of Science (1979), all at the University of Warsaw. His
research interests include semantics of scientific theories, patterns of change and
--- PAGE 738 ---
724 ABOUTTHEAUTHORS
choiceofscientificconceptsandtheories, systemsofscientificdiscovery, expertsystems, andphilosophicalfoundationsofartificialintelligence. In 1982-1984hewasa
Visiting Professor at Carnegie-Mellon University in Pittsburgh. He has published
sometwenty-fiveresearchpapers. Hiscurrentaddress isComputerScienceDepartment, Wichita State University, Wichita, KS 67208.
--- PAGE 739 ---
AUTHOR INDEX
Abbot,R.,66 Brown,A. L.,325
Abelson,R. P.,71,356,572,582 Brown, F. M.,394
Ackley,D. H.,5, 12 Brown,J. R.,353
Adams,M.J.,327 Brown,J. S.,7,59,340,352,394,446
Albrecht,R.,353 Brown,R.,629
Amarel,S.,394,500,502,503,505,509,510, Brown,R. H.,394
512,519,520,522,537 Bruner,J.S.,326
Anderberg,M.R.,472 Buchanan,B.G.,6, 13,64,68,69,374,384,
Anderson,J. R.,248,250,264,265n,267n, 461
276,281,289,290,291,292,293,300,304, Buckley, S., 320
306,307,308,341,388 Bundy,A.,648,650,652,653
Andreae,P. M., 16 Burr,B.,250
Angluin,D.,625,630 Burstein,M. H.,328,364,394,416,474
Caianiello, E. R., 12, 14
Bairn,P.W., 102 Campione,J. C,325
Balzer,R.,377,378 Carbonell,J. G..5, 10, 12, 14,64,304,328,
Banerji,R.B., 14,64, 142, 169, 199,326,387, 359,372,373,374,375,376,382n,383,
588,650,668 385,387,394,416,474,588
Barstow,D. R.,377 Card,S.K.,250
Beasley,C.M.,290,300 Carey,S.,330
Berkson,W.,4 Carlson,R. A.,308
Berwick,R.,56,627,643 Chang,C.L.,222
Bethke,A. D.,616,619 Charniak,E., 198,572
Binford,T.0.,57 Chase,W. G.,252
Birkhoff,G.,506 Chen,D.T.W.,394
Blikle,A.,414,415 Chen,K., 15
Bobrow, D.G., 199 Chi,M.T. H.,326,342
Booker, L.,621 Chilausky,R. L,64
Boulanger,A. G., 102 Chomsky,N.,626,635
Bower,G. H.,252,289 Clark, E. V.,326
Boyer,R. S.,655 Clements,J.,328, 373
Bradshaw,G. L.,340,429n,436,438n,501 Clocksin,W.F.,650
Bradzil,P.,650 Cohen,B. L., 169, 189
Brooks,L.,304,341 Cohen,P. R., 15, 197,202,216
--- PAGE 740 ---
726 AUTHOR INDEX
Collins,A.,202,333, 343n,352 Fox,M.,588
Conrad,M., 12 Friedland,P. E.,71
Craig,A. T., 154 Gardner,M.,66
Crossman,E. R. F. W.,250 Gentner,D.,312,319,326,328,333,352,356,
Culicover,P.,629 358,374,394,468
Cullingford,R.,577 Gentner,D.R.,319,328,333,356,358
Gerhart,S. L,395
Darden,L.,328 Gibson,E.J.,326
Davis,J., 102 Gick,M. L.,326,328
Davis,R.,6,501,537,600,601 Glaser,R.,326,342
DeGroot,A. D.,252 Gold,E.,625,630
DeJong,G. F., 197,473,483,572,577,616, Goldberg,D.,595,611,621
619,649,668 Goldin,S.E.,333
deKleer,J.,337n,340,343 Goldstein,I. P.,46,59
Dershowitz, N.,394,395,396,398,400,413 Goldstein,M.,544
Dewey,G. I.,308 Granger,R.,577
Dietterich,T.G., 15,64, 194,216,335,474, Greenfield,P.,326
631 Grimson,E.,46,59
Dijkstra,E.W.,414,415,418
Habel,C.H.,446
diSessa,A.,329n,330
Halle,M.,635
Doyle,J.,381,386,577
Hanlon,C.,629
Duda,R.0.,374,594
Dulany,D. E.,308 Hart,P.E., 130,276,279,395,588,650
Duncan,J.,275 Hayes,P.J., 85, 199,317,317n,341
Hayes-Roth,F.,4,6, 185,216,217,228,262n,
290,383,384,595,600,621
Elio,R.,303
Emde,W.,446 Hedrick,C. L.,65
Englemore,R.,71
Hesse,M.B.,343n,394
English,W.K.,250 Hillis,W. D.,5
Ericsson,K.A.,252 Hinton,G. E.,5, 12
Hoff,W.A.,481
Ernst,G.W.,248,254,326,544
Evans,T. G.,59,352,360,394
Hoffman,R. R.,328
Hofstadter,D. R.,9,65,611,622
Hogg,R.V., 154
Fahlman,S. E.,594,598,599,610
Hollan,J.,332
Farrell,R.,265n,276,290,292,293,300
Feigenbaum,E. A.,6, 13, 15, 197,202,216,
Holland,J. H.,6, 12,619,621
Holte,R. C.,216
374,384,461
Holyoak,K.J.,326,328,394
Feltovich,P.J.,326,342
Horn,B.K. P.,293
Fikes,R. E., 130,276,279,299,308,372,395,
577,588,650 Hume,D., 8
Findler,N. V.,394
Inhelder,B.,331
Finkel,L.,353
Fisk,A. D.,341 Jakobson,R.,643
Fitts, P. M.,275 Jeffries, R.,297
Flavell,J.H.,342n Johnson, N. F.,252
Floyd,R. W.,395
Fogel,L., 12 Kahneman,D 341
Forbus,K.,312,318,337 Kant. E., 377
Forgy,C. L.,263,268,600 Karpinski.J.,68
Forrest,S.,621 Katz,B..57
Forrin, B.,275 Kat/.S. M..397
--- PAGE 741 ---
n n
AUTHOR INDEX 727
Kawanobe,K.,5 229,241,290,330,335,387,445,472,473,
Kean,M.,635,636,638 474,475,481,481n,482,487,488,489,
Keil,F.,625,627,631 490,
Keller,R.M., 118,473 588,631
Kelley,V. E.,130 Michie,D. M.,6,276,278
Kempton,W.,319 Miller,G.A.,252
King,J.,600,601 Minsky,M.,5, 10,46,56,59, 198,383,572
Kline,P.J.,290,300,303 Mitchell,R. F.,327,330
Kling,R.E.,374,394 Mitchell,T.M.,5, 10, 12, 14,64,68,69,93,
Ko,H.,15 108, 113, 119, 189, 194, 196, 197,218,290,
Kodratoff,Y.,218,220,239,241,243 335,340,387,473,503,544,558,629,649,
Kolers,P.A.,250 650,664,668
Kolodner,J.L., 194,364,577,588 Moore,J.A.,394
Korf,R.E.,276,280,283,544 Moore,J.S.,655
Kotovsky,K.,64 Moran,T. P.,250,265
Kowalski,R.A.,420 Morin,R.E.,275
Kuhn,T. S.,4 Mostow,D.J., 118, 197,537,544
Musso,G., 12, 14
Laird,J.E.,282,374
Lakatos,I.,4 Neisser,U.,248,250
Langley,P.W., 14, 194,290,335,340,429n, Neves,D.M.,248,250,276,290,292,308,
436,438n,501,588,650,651n 649,650
Larkin,J.H.,6,326,342,373,374 Newell,A.,248,250,254,257,261,262,263,
Larson,J.B.,48In 268,274,275,282,371,374,394,426,500,
Lazar,R.,248,250 544,572
Lea,G.,30n,503 Nickerson,R.S.,327
Lebowitz,M., 193, 194, 195, 199,202,207, Nilsson,N.J., 12, 130,254,276,279,299,372,
208,364,572,577,588 395,572,588,650
Lederberg,J.,374,384,461 Norman,D. A.,341,394
Lee,R.C.T.,222 Novick,R.,248,250
Lehnert,W.G.,58,364 Nudel,B.,558
Lenat,D.B.,6,9, 13, 14,36,69,340,384,
501,537,588,611 01ver,R.,326
Lewis,C.H.,276,291 Owens,A., 12
Lewis,M.,307
Lindsay,K.,501 Papert,S.,5
Lowry,M.R.,57 Persson,S.,70
Piaget,J.,330,331,342n,503,506
McCarthy,J.,4, 10, 80,85,317 Plotkin,G. D.,220
McDermott,D.V.,372 Polya,G.,372
McDermott,J., 185,216,217,228,290,374, Popper,K.R.,4,8, 16
384,394,600 Posner,M.I.,327,330
MacLane,S.,506
Manna,Z.,394,395,398,416,544 Quillian,M.R., 198
Marcus,M. P.,7 Quinlan,J.R., 13, 151, 196, 197
Marsh,D.,278,627
Medin.D. L.,8,327,482,489 Reber,A. S.,327,341
Mellish,C. S.,650 Reddy,R.,588
Michalski,R. S.,5,8,9, 10, 12, 13, 14, 15, 16, Reed,S.K.,326
17,57,64,65,68,69,92,93,95, 112, 149, Reif,F.,373,374,378
186, 189, 194, 196, 199,216,220,221,223, Reitman,J.S.,621
--- PAGE 742 ---
728 AUTHOR INDEX
Rendell,L.A.,6,474,489 Steinberg,L.I. , 130
Rieger,C.,58 Stepp,R. E., 16,65, 199,290,330,445,472,
Riesbeck,C.K.,194, 197 475,481,487,489,490,588
Rizzi,L.,642 Sterling,L.,650,65In
Robinson,J. A.,5, 173 Sternberg,R.J.,394
Roeper,T.,642 Stevens,A.,333
Rollinger,C.,446 Stevens,A. L,332
Rosch,E.,327 Sussman,G.J., 130,332,395,544
Rosenblatt,F., 12 Swartout,W.,378
Rosenbloom,P. S.,248,249,250,257,261,
262,262n,263,264,272n,273,274,275, Tarnlund,S. A.,394
Terry,A.,71
Rumelhart,D. E.,341,394 Tsypkin,J.Z., 12
Tversky,A.,320,341
Sacerdoti,E. D.,353,372,544
Sagiv,Y.,395
Sammut,C.A., 185, 189, 199 Ulrich,J.W.,396
Samuel,A. L.,276,278,279,597,599,600, Utgoff,P. E., 8, 14,64, 119, 129, 189,387,
588,650,668
Sauers,R.,265n,276,290,292,293,
300,303,308 VanLehn,K.,59,394,643
Schank,R. C.,4,7, 15,71, 193,194, 197, 198, Vere,S.A.,216,217,218,228,237,240,290
211,356,359,375,383,384,386,572,
582,588
Scherlis,W. L,378
Waldinger,R.J., 130,396,416,544
Walsh,M., 12
Schlosberg,H.,249
Warren,D.H. D.,544
Schmidt,C,577
Wasserman,K., 199
Schneider,W.,341
Waterman,D. A.,6,262n,384,595,600,621
.,275
Wattenmaker,W.S., 8,482,489
Seibel,R.,249,250,252,255,274n
Weizenbaum,J.,9
Welham,B.,650,652,653n
Sejnowski,T.J.,5, 12
Wensley,J.H.,413,415
Selfridge,M., 12
Wettersten,J.,4
Shapiro,E. Y., 171,186,189
Wexler,K.,629,643
Shepp,B. E.,327
Wilenski,R.,353,372
Shortliffe,E. H.,6,202,374,384,594
Williams,M.,332
Siegler,R. S.,331,332
Wilson,S.,611,621
Simon,H. A., 10,30n,64,248,252,340,371,
Winner,E.,326
426,429n,436,438n,449,454,501,503,
Winograd,T.,7,54, 199
544,572
Winston,P. H.,6, 13,46,49,52,56,57,58.
Sleeman,D. H.,7
59,64, 194, 195, 196,209,210,239,272.
Smith,E. E.,327
293,328,352,374,394,395,416.468.473.
Smith,S.,595,621
474,588,610,626,649
Snoddy,G. S.,250
Winzenz,D.,252
Solomonoff, R. S.,65
Soloway,E. M.,69,588 W
Sommers,F.,631
Woodworth,R. S.,249
Springston,F.,252
Sridharan, N.,577
Stallman, R. M., 130 Zagoruiko. N., 16
Stefik, M., 130,544 Zytkow,J., 340.438n.449.454
--- PAGE 743 ---
1 1 1
SUBJECT INDEX
A*search,572 inautomaticprogramming,394-96,402-4,
Abstractionlevelofknowledgerepresentation, 1 408,415-19
Abstractionmapping,319,320,324,326 inautomaticprogramming,problemsin,417
Abstractionofprograms,395,409-10 inautomaticprogramming,stepsof,417-18
AC3
474 andclassification,305-6
Accessibility(ofstructurematches),325-28 debateonuseofterm,30-31
Achievestatement(inprogramspecification),396 andknowledgeacquisition,35
Acids,discoveryoftheoryof,437-46 withmultiplemodels,354-56,365-66
ACT,291,292,293,303-8 bypatternmatching,352
ACT*theory,290,300,303 problemswith,25
Actiondiscrimination,304 roleof,inproblemsolving,371-74
Adjacencyrequirement(ofEnglishsyntax),641 inStructureMappingtheory,319-24,326,
Aeronautics,historyof,comparedtoML,33-34 328,333
ALEX,650,659 useof,inhumanlearning,312,319
Algebraicequality(analogyforassignment), SeealsoAnalogicalreasoning
355-56,365-66 AND/ORhierarchy,254,519
Alkalis,discoveryoftheoryof,437-46 AnnotatedPredicateCalculus,473,487-88
AM,32,501,537 Annotation,487
AMAPs,362 Anomaly,324
Analogicalmapping,353 Anterior(phonologyfeature),635-40
Analogicalproblemsolving,374-76 ApplicationsofML,3
needsof,374,375-76,384 A 4*algorithm,68,93
Analogicalreasoning,46,49-50,394 AQ11,64
usingcensors,55 Arch(blocksworldconcept), 178-84
Analogicalshifthypothesis,328 Architectures,computer,inML,5,28,39
Analogicaltransformation,372-74 Argumentsofverbs,642
Analogy,290,304,351-67 Arithmeticconceptslearned, 185
assimilationandrepairstrategiesof,3 Array-search,404-8
Artificialintelligence,roleof ML in,37-38
A-rules,490,495
Assertstatement(inprogramspecification),396
Note:Inthisindex, ML isanabbreviationforma- Assi
escriptions, 118,
chinelearning,AIforartificialintelligence.
--- PAGE 744 ---
1 1
730 SUBJECTINDEX
Assistants,intelligent.SeeIntelligentassistants CA. SeeClassifyingattributes
Associationsofcategories,598-99 Calorictheory,460
Associativelearning,289 Candidateeliminationalgorithm,69, 113, 114,
Associativityusedininductiveinference,227 120, 122
Atom(analogytosolarsystem),357-60 CARL,352,356-67
Attraction(equation-solvingtechnique), analogymappingprocessin,362-63
653,667 Case-basedreasoning,381,386
Attribute-basedconjunctiveconceptual Categorizationofrules,598
clustering,474 Causalcorpus,313,330-35,342-43
Attributes, 150 Causalstructure
derivedattributes,74,88-89 ofif-thenrules,49
importanceof,defined, 155, 164-65 ofrelations,358-59,362,364,366
informationmeasuresof, 151-52 CAUSEstatement(ofthecausalcorpus),330-33
Augmentedif-thenrules,46,51-57,59 Censors,46,51-55
contrastedwithinferencerules,53 effectof,51-53
openproblemswith,57-58 Chainofinferences,476
Automaticprogramming,377,378,393-419 Checkers,599
Chess, 142-45
Bachelor(concepttobelearned),54-56,59 endgames,classificationof, 154-55
Backgroundknowledge, 16 Chi-squaretestfornoisyattributes, 153-54, 158
inanalogicallearning,353 Chunking,257-62,270-73,281-83,444-45
inconceptualclustering,472,473,474,488, Chunkingtheoryoflearning,248,252-54
489-90 Chunks
Backwardchaining,52 acquisitionof,272-73
Backwardoperators, 130-31, 135 useof,270-72
BACON,426-37,461-68,495,501 Classification,303-4,472,481-87
BARGAINschema,578 algorithm, 151-54
BASICprogramming,learningof,modeled, byanalogy,305-6
351-56,362-66 metric, 109
Biasinlearningofconcepts, 107-46 withnoisydata, 152-65
correctvs. incorrect, 109-10, 140-41 Classifiers, 13
defined, 107 couplingof,604
initial,calculated, 146 Classifiersystems,600-622
kindsof , 11 defined,603
selectionof, 110 learningin,620
shiftingof, 114-18 results,621-22
sourcesof, 112-14 vs. rule-basedexpertsystems,601-2
strongvs. weak, 109-10, 128, 140-41 vision/motion,exampleof,605-9
Binaryrelations,330 Classifyingattributes(inconceptualclustering),
Binarysearch,395,41 487,493-96
Blockingprinciple,49,51-55 Classsplitting,638-39
Blocking(sequencetransformation),76-79 Clausedeletion,310
Blocksworld, 169-73, 178-84, 186-89,216, Climbingthegeneralizationtree,488
237-39,240,626-28 CLUSTER,65
Bottom-upsearch,353 Clusteranalysis,472
Box(analogytoBASICvariable),353-56 Clustering.SeeConceptualclustering
Brittleness,6 Clusteringphase,475
inexpertsystems,594-600,620 CLUSTER/S,485,492,493
B-rules,490 CLUSTER/2,489,491-92
Bucket-brigadealgorithm,611-13 Cognitivemodeling, 17
--- PAGE 745 ---
SUBJECTINDEX
Coherence-drivenlearning,332 Context-sensitiveconstraints, 139
Collection(equation-solvingtechnique), Contiguityofexamples,289-90,308
652-53,667 Continuant(phonologyfeature),635
Column(blocksworldconcept), 171-73 Correctnessinclassification, 108-9
Combinationmethod(inprogramformation), Correlationalapproachtolearning,571-72
538-50 Counterexamples,80,218-19,237-39,
Combinatorialexplosion,490,648 441,490
Commonsenseknowledge,36-37 Counterfactualdescriptions, 117, 129
Commutativityusedininductiveinference,227 Credibilitytowardsmachines,9
Competitionamongrules,599-600 CRITTER, 130
Componentsofsubstances,discoveryof,446-54 Crypto-informationconstraint,263
Compositionofproductions,291-92,298-300 CS1,621
Comprehensibilityprinciple,9 Cube-rootprogram,404-6
ComputervisionandML,6
Concept,definitionof, 108 DALTON,454-61,462-68
Conceptevaluation,201-4 Datadependencies,577
Concepthierarchies, 197-99 inderivationalanalogy,379-83
Conceptlearning, 167-90, 193-211,481-87, Data-drivenapproachestoprogramformation,
490,627,628,632 537-61,565
byanalogy,356-59 Datareduction,331
detectingerrorsin, 186-89 DATEschema,578-79
bydiscovery,216-17,222-23,550 Debugginginautomaticprogramming,399-2
noisein, 149-65 Decisiontrees, 150-54
SeealsoLearningfromexamples Decomposablesystems,544
Concepts Decompositionalgorithm,93-96
assets, 108 Decompositionmodelforsequences, 82,91
generalityof,221-22 Deductiveinferencevs. inductiveinference,4
synonymous, 108 Deductivelearning, 13
Conceptualclustering, 15,65,330,445-46, DEFUN (aLISPfunction),machineattemptsto
472-96 learnuseof,293-97
Conceptualcohesiveness,475 DENDRAL,461
ConceptualDependency,356 Derivationalanalogy,374,376-84
Concreteexamples,294 efficiencyof,377-79
Confidencelevelsofconceptsandattributes, method,379-83
202-4 needfor,377-79
Confirmationofrules'usefulness,599-600 Descriptionlanguagesforhypotheses, 112-13
CONFUCIUS, 189 Descriptionspacetransformationproblem,67,
Conjunctiveconcepts,473 68-69
Consistency Descriptivegeneralization, 15, 196
ofconcepts, 108-11 Descriptors,initialvs.described,479
ofrules, 168, 174 Diachronicpointerstoexpectedcategories,599
Consonantal(phonologyfeature),635-40 Disconfirmabilityofhypotheses,629
Constraintback-propagation(bias-shifting Discovery,425-68
technique), 118, 129-45 integratedsystemof,461-68
Constraint-basedlearning, 16,574-77 Discrimination,304-8
Constraintpropagation, 130, 142-45 Discriminationnets, 198-99,364
Constrainttransfer,46 Diskdrives(domainforconceptlearning),
Constructiveinduction, 13,335-37 209-10
Context-freegrammars, 120 Distinguishabilitycondition,640
Context-freesimilaritymeasures,475 Divide-and-conqueralgorithm,256
--- PAGE 746 ---
732 SUBJECTINDEX
D-NETs, 198-99 Factorization,653-54,655,661,666-67
DNFsequencemodel,81-82,91-92 Factorizationpreparation,654,658,
Domainknowledge,31 660,661,667
Domain-independentgeneralizations,337,341 Falsitypreservation,8, 16
Domain-specificapproaches,31 Familiarityofstructurematches,325
vs.generalapproaches,27-28,32, 33-34 Firediagnosis,385
Domainspecificityinexpertsystems,594-95 First-orderlogicusedtorepresentconcepts,
Domain-specificrules,384 169-70
Droppingconditionrule,problemswith,217-18 Forwardchaining,52
Fractioningderivations,388-90
Functionaldependencies,316
Effectivenessofknowledgerepresentation, 11
EL, 130
GBM.SeeGeneralization-BasedMemory
Elaboration(partofgeneralization), 172-73
GDN.SeeGoalDependencyNetwork
Eleusis,66-67,73-85, 87-101
Generalityofdescriptions, 122, 134
Eliminationmethodinprogramformation,
Generalityrelationbetweenformulas,225
550-61
Generalization, 168, 170-74,215-39,297-98,
ELIZA,9
300-304,308,462-63,544,576-87
Empiricallaws
algorithmfor,233-36
qualitative,437-46
definitionsof,224-25,243
quantitative,426-37
offormulas,225
Encapsulatedhistories,317-18
Engineeringorientationin
MLandAI,
pragmatic, 196,200
Generalization-BasedMemory, 193-211
Environment-orientedapproachtoexpert
described, 197-201
systemsI/O,596-97
maintenanceof, 199,201-4
Environment,rangeofexpertsystemsdomains
Generallearningalgorithms, 17,27-28,
in,596-97
289-301,626
Environmentsforproblemsolving,332
Generate-and-test,282,519,531
arningof,648-68
GENERATE(subprocessofCLUSTER/ S),495
Geneticalgorithmsforrulegeneration,613
Equivalence-maintainingtransformations, 118
Genetics,460-61,465-66
Errors
GEN-NODEs, 198-200
inclassification,sourceandeffectof, 150
Geometricrepresentations,550-51,566
inderivedrules,48
Geometry,learningof,307
ESA.SeeExplanatoryschemaacquisition
GLAUBER,437-46,461-68
EURISKO,501,537
limitationsof,443-44
Exchangeableknowledgemodule, 13
Goal-architecturalityconstraint,264
Experientiallearning,311-12,328,340,342
Goal-contextrefractioninproduction
Expertiseacquisition. SeeKnowledgeacquisition
recognition,268
Expertmodels,313,337-42
GoalDependencyNetwork,473,477-80.487,
Expertsystems,594
492-95
knowledgeacquisitionfor,6, 31,41,384-86
Goal-freevs.goal-sensitivelearningmethods.
problem-solvingapproachof,374
123, 129, 146
Explanation-basedgeneralization, 16, 197
Goalhierarchies,249,254-62
Explanationfacilities,9
Goalprocessing,269-70
Explanatoryschemaacquisition,571-88,668
Goalregression, 130
criteriaforgeneralizationin,581
GPS,544
Extensionofanalogies(inautomatic
Gracefulnessofrulesystems,definitiono\\
programming),414-17
600,620
Grammarrules,conceptslearned in, 185
--- PAGE 747 ---
SUBJECTINDEX 733
Grammarsforprogramformation,512-19 INDUCE/2,481,489-91
GRAPES,276-77,278,292-98 Inductiveinference, 1
Graphs,representationforprogramconstruction, conjecturalnatureof, 8
509-11 vs.deductiveinference,4
instance-to-class,64
HACKER,395 part-to-whole,64-65
Hawaiiansoundsystem,638-39 Inductiveleap, 109, 116
Heatflow(exampleofanalogy), 321-24,327 Inductivelearning,64-66, 149, 168,290,
Heuristics, 119, 120, 123, 124, 128 297,308
acquisitionof,333,340 Informationcompression,knowledgeas,6
Heuristicsearch,353,372n,445,447-50, Informationmeasuresofattributes, 151-52
453-54,519-23,572 Inherentgoals,299
Heuristictransformations,69 Inheritanceofattributes,364-65
inautomaticprogramming,405 Inspectabilityofstructurematches,325,327-28
Hierarchicalgoaltrees,290,297 Instance-to-classgeneralization, 15,64
Hierarchies,default,inrulecategorization,598 Instantiationofschemata,410-1
Hierarchy-buildingphase,475 Integerdivision,413-15
Hierarchyofpredicates,useof,ininductive Integersquareroot,410-13
inference,228-29 Integralcalculus, 1 19-40
Hillclimbing,430,519-23,565,566 Intelligentassistants,6,32,436
Hornclauses, 169, 184,488 Internalconjunctionordisjunctionin
HPSA77architecture,268 selectors,488
Humanconstraintsonlearningresults,9 Interpersonalthemesinschemata,582
Humanlearning,4,28,248,293-97,303, Intrinsicproperties,434-36
307-8,353-56,377-78,572,626-44 INTSUM,68
resultsof,comparedtoML,292,307-8, IPP, 194, 197
325-27,331,353-56,481-87,482-84, Isolation(equation-solvingtechnique).
628 651-52,655
Human-machineinterface,35-36, 112 Isolationofknowledge,296
Humanmemory(analogytocomputermemory),
355 KDL. SeeKnowledge-intensive,domain-specific
Humanscrutinyof ML results,9 learning
Hypotheses, 173 Kidnappingasexampleofschemaacquisition,
orderinwhichconsidered,aspartofbias, 483,577-78
107, 111 K-lines,383
orderingof,629,643 KL-ONE,621
spaceof, 107, 111, 116, 120, 127-28 Knowledgeacquisition, 31, 139, 384-90
asadefinitionoflearning, 10
Idempotencyusedininductiveinference,227-28 bottleneck,6,9, 384-86
1D3, 151 useofanalogyin,35
If-relevantpartsofrules,57-58 Knowledgebases,36
i-implication,241-43 Knowledgecliff,6
Implicitparallelism,614,620 Knowledgecompilation,290-308, 388
Improvementcriterion, 10 Knowledgedecompilation, 388
Incrementallearning,384-88 Knowledgeengineering, 384-86
ofanalogicalinferences, 354, 363-65 Knowledge-intensive,domain-specificlearning,
ofconcepts, 196,200 13, 18
ofhierarchies,634 Knowledge, isolationof,296
Individualviews, 316-17 Knowledgeratificationbottleneck,9
INDUCE, 189 Knowledgerepresentation, 10-11
--- PAGE 748 ---
734 SUBJECTINDEX
KRL, 199 LISPprogramming,learningof, modeled,
293-97,298-300
Listmanipulation,learned, 185
Languageacquisition,627,635-43
Literalsimilarity, 319,320,324,325-28
Languageunderstandingsystem,359,364
accessibilityof,326
Largedomains,learningin, 196-97,200
Lawofcombiningvolumes,427
lowinspectabilityof,327
Lawofconstantproportions,427
Logicalinferencerules,481
Logic-basedprogramminglanguages,26
Lawofmultipleproportions,427,462
LP,647-68
Learning.SeeDeductiveinference,Discovery,
Generalization,Humanlearning,Inductive
MACBETH,59
inference,Knowledgeacquisition, Machine
Macbeth(storyof,aslearningprecedent),47-53
learning,andtheLearningentriesbelow
Learningbyanalogy, 15, 19-20, 351-67
Machine-humaninteraction,7,9
Machinelearning
Learningbyexperimentation, 15
Learningbyhumans. SeeHumanlearning
currentabilitiesof,5
dimensionsof,37
Learningbyinstruction, 14
domain-specificvs. generalapproaches,
Learningbyrote,655
27-28,40
Learning,cognitiveaspectsof, 19
futureof,7-9
Learning,definitionsof, 10-12
lackof,inAIsystems,4
Learningfromexamples, 15, 18-19, 107-46,
needfor,3-9
194,237-39,294
needfor,inAI,215
effectsofnoiseon, 149-65
purevs.applied,28,29,38
frompositive-onlyexamples,95,627-31
researchin,conditionof,38-39
fromasingleexample,297,303-4,588
singlevs. integratedstrategies,28,36,38,
Learningfromobservation, 15,20, 196-211,
461-68
472,481,571
undesirableconsequencesof,7-9
incomplexdomains, 196-97
Macro-operators,279-80,299,561-65
Learning,gradualnatureof, 167,351
Macroproduction,291-92
Learningorientations, 17-18
Majoritymethodofhandlingclassification
Learningproblems,spectrumof,69
errors, 152-53, 156-57, 165
Learningstrategies, 14-17
Mapping
Learningthroughquestions, 167-90
analogical,353,357-63
Learningtolearn, 112
oftemplates,294
Leastdisjunction(bias-shiftingtechnique), 118, MARVIN, 167-90
123-29
componentsof, 169
Least-dominatingsubterms,652
learningalgorithm, 177-78
LEF.SeeLexicalevaluationfunction
Mathematicalmodels,340-43
Levelsofabstractionofrelations,360-61
Maximalcommongeneralization,300. 303
LEX, 119-23, 133-40, 189,588,650
Maximalgeneraldescriptions,629
Lexicalevaluationfunctions,479,487,489,
Mazes,learningtotraverse,307
495,496
LEX2,668
M-constraint(inhierarchytrees),632-33
Means-endanalysis,372n,572
Lifethemes,582
Measurementinterpretation,318-19
Limitanalysis,318
Memofunctions,278-79
Linksinobjectdescription,216-19
Mereappearancemapping. 319. 320. 324
Liquidflow(domainforphysicalworld
Messagesinclassifiersystems,601-4
reasoning),314-19,327
syntax,602-3
LISP, 121, 123, 132,267,269 Meta-DENDRAL,68.501
LISPprogramming(exampleofanalogy),378
asexampleofgoodproject. 31 32
--- PAGE 749 ---
SUBJECTINDEX 735
Methodacquisition,282-83 Partialpreferenceorderingofhypotheses, 11
Methods(problem-solvingoperators),649, Part-to-wholeinduction, 15,64-65
651-56,661-62 Pascal,programmingin(exampleofanalogy),
creationof,661-62 378
keymethods,654,655,661 Patternmatchingandanalogicallearning,352
MIS, 189 Payoff(feedbackquantity),596-97,612
MissionariesandCannibalsproblem,566 Periodicmodelofsequences,83,93
"MissionImpossible"asexampleof Phlogiston,theoryof,446,450-53
ungeneralizableschema,581 Phonology,learningof,627,635-41
Model-buildinginproblem-solving,69-71, Physicaldomainforhumanlearning,311-43
537,599 Planacquisition,386-90
Model-guidedsearch,79-85,533-37 Planinstantiationinproblemsolving,372-74
spectrumofapproaches,70 Planninggoals,299,301,582
Modelinstantiation,71 Poisonedmushroomexampleofschema
Modificationofprograms,394-96 acquisition,582-87
MOLGEN, 130 POLITICS,372
Monitoringsystems,32 Portabilityof
MLprograms,36
MOPs, 194, 198,384 Positiveexamples, 108, 120, 168,387-88
Multiplemodellearningproblem,67,69-71 Powerlawofpractice,248,250-52,281,
283,284
Naivephysics,313,333-37 Power,sourcesof,in ML research,34-37
Naturallanguageunderstanding,576 Practice,248-49,250-52
NDPproblems,65-66,71,72,86
"Pragmatic"generalization, 196,200
solutionto,71-79 Precedents,46-56
Nearmisses,56-57,59,239,388 SeealsoLearningfromexamples
failuretodeterminecorrectdifference,56-57
PreconditionAnalysis,648-49,659-64,668-69
Negativeexamples, 108, 120, 168,387-88 Predictability, 199-200
usedtocheckandcreatelinks,218
Predictiveopacityinmachinelearning
NETL,610
systems,8
Neuralmodeling, 12-13 Predictivepotentialofaclass,444
Newton'smethod,418 Predictivepowerofaclass,444
Noise Preferencecriteria,473,474
effectsof,onconceptlearning, 149-65 PRESS,650,668-69
resultsof, 16-18 Primafacieconjecture,49,51-52
Nonmonotoniclogic,381 ProbabilisticLearningSystem,474
Novice-expertshiftinadultlearning,326, Probabilitymethod, 152-53, 156-57, 165
340,342 Problemdecomposability,30
Occam'srazor,96
Problemsolving,29, 120,248,290,297,308,
One-markconstraint(inphonology),640-41
359,500-504,596,599
Ontologyinmentalmodelsoftheworld,333,
hierarchicalstructurein,290,297
andmodel-building,599
Operatorcollapsing,290
representationin,500-504
OPS,263,268
roleofanalogyin,371-374
withschemata,571-574
Orderingofhypotheses,629,643
Problemspacehypothesis,282
Overgeneralization,219
Proceduralization,291-92,298
avoidanceof, 199,201
Processes(in QPtheory),312,313-317
Parallelismofrules,597-98,601 componentsof , 314
Parameter-passingbottleneckinproduction Productioncomposition,276-78
recognition,268 Productionrules,291-92,297,305
--- PAGE 750 ---
736 SUBJECTINDEX
Productionsystems,248,601-2,458 Richnessinearlyhumanknowledge
SeealsoXAPS2,XAPS3 representations,312-13, 326
Programcombinationproblem,543 Ringarchitecture,72,85-87
Programformation inSPARC, 87-97
domainspecification,506-8 Rotelearning, 14,655
problemformulationof,503-21,565 Rubik'scube,280
Programmodification,544-49 Ruleacquisition,388-90
Programspace,528-37,538 Rule-basedreasoning,46-47
Progressiverefinement(automaticprogramming
approach),377 Salts,discoveryoftheoryof,437-46
PROLOG, 169 Samuel'scheckersprogram,279,599-600
Protohistories,313,329-30,332,339,340,343 SAWINGschema,579-80
Prototypelearning,326-27, 329-30 SCA. SeeSymbolicconceptacquisition
Purposestatement(inprogramspecification), Schemaabstraction,411-12
397 Schemaalteration,579-80
Schemacomposition,571-IS
QualitativeProcess(QP)theory,312,313-19 Schemainstantiation,71,412-13
Qualitativeproportionality,316 Schemata, 198, 199,438,571-88,649,663-64
Qualitativesimulation,318 inautomaticprogramming, 395,409-11,416
Quantificationofclasses,439 ofclassifiers,614-20
Quarks,59 usedinproblemsolving,571-74
Quicksort, 378 Scientificdiscovery,426-68
Scientist'saide,436
RD. SeeRepeateddiscrimination Search
Reaction-timetasks,249,273-75 withbacktracking,429
Realdivision,396-402 beamsearch,430
Reasoningbyexclusion,333 best-first,440
Recognitionpredicates, 120-22, 133 ofdataspace,427-29,432-34
Recognize-actcycle,268 depth-first, 102,255,282,428,456
Recombinationofrules,597-98 forward, 120
Recommendationofnewconceptdescriptions, withlook-ahead,441 444
115-17, 123-24, 129-32 withmultiplealternatives,444
Reconstructivememory,383 ofspaceoflaws,429-34
Relations,algebraof,522-30 specific-to-general, 168
Relations,useof,inrepresentingsituations,47 SeealsoHeuristicsearch
Remindinginproblemsolving, 359 SEARCH(subprocessofCLUSTER/S).495
Repeateddiscrimination(methodforconceptual Secondaryeffectelevation,578-79
clustering),487,490-93,496 Seesaw(exampleofhumanlearning).329.
Replacement, 171 333-36
Representationlanguage,240-41 Segmentingofsequences,74-75, 89-90
useofpropertiesof,227-28 SelectorinAPCorVL ;:, 73 . 81. 487
Representationsandanalogicalreasoning,352 Semanticnets,48-50, 198.496
RESEARCHER, 194-95, 197,207-11 Sententialforms, 120
Researchparadigmsinmachinelearning, 12-14 Sequencegeneration.63-102
Responsesymbol(forchunks),253 SHAKEY,34
Restauranttable(exampleforconceptual Signaturetables. 279
clustering),475-81 Similarity-basedgeneralizationtechniques, 16
Rewriterules, 139-40, 171,229-31,649, Similarity function.4^4
651-54 Similarityofstructurematches, 325
Richdomains,572 Skillacquisition. 290
--- PAGE 751 ---
1 1
SUBJECTINDEX 737
asadefinitionoflearning, 10 Tee(blocksworldconcept), 169-70
SMOKEY, 385 Telephonenumbers(exampleofknowledge
SOAR2,282-83 compilation),291-92
Solarsystem(analogytoatom),357-60 Templatemapping,294
Solemechanismassumption,318 TerminologyinML,28,30,38,40
Sorting,conceptof,learned, 185 Themes,asasourceofhigh-levelgoals,582
Soybeandiseases,classificationof,64 Theorems,generalityof,220
Spaces,solutionvs.problemcondition,in Theoryformation,29-32,500-503
problemsolving,29-30 Theoryofmarkedness(inphonology),635
SPARC/E,69,72,87-101 Thermostat,useof(exampleofhumanlearning),
Specialization, 168, 174-75 319
Speechunderstandingandmachinelearning,6 Timeinschema-basedstoryunderstanding,588
Splittingofclasses,638-39 Timidacquisition,627,630
Splittingofsequences,76 TMS,381
Spreadingactivation,306 Top-downprogramming,26
STABB, 118-45 Totalpreferenceorderingofhypotheses, 11
STAHL,447-54,461-68 Trainingsets, 151, 162-63
Star(descriptionstructureforconcepts),93,490 Trains(examplesforconceptualclustering),
StatesoftheUnitedStates(domainforconcept 481-87,492-94
learning), 194,203-7 Transformationalanalogy,374-76
STEALschema,578 Transformationofdescriptionspaces,69
Stimulussymbol(forchunks),253 Transformationofprogramsandtheir
Strategyacquisition,387-88 specifications,394-95
Strategyingeneralization,303 Translationofconceptrecommendationsinto
Strengthsassignedtorules,599-600 descriptions, 117-18, 124-25, 133-34
STRIPS, 130,279-80,299, 395,650 Trialconcept, 168
Strongimplication, 149 Truthpreservation, 8,409
Structuralanalogy,293,297 Tutoringsystems,7
Structuraldescriptions,learningof, 195,209-11
Structuralmatching,226-33,491-92 Uniformityofmind,626
algorithmfor,233-36 UNIMEM, 194-95, 197,201-7
Structuralmodels,454-61,464-65,466-67 Universalsubgoaling,282
StructureMappingtheory,312,319-28 Universalweakmethod,282,374
Subjacencyconstraint(inEnglishsyntax), Unlessconditions,473
642-43 Unlesspartsofrules,51-55
SubsetPrinciple,626-44 Usableknowledge, 118
defined,627,629-30 Usefulnessofstructurematches,325,327-28
predictionsof,643
Suggeststatement(inprogramspecification),397 Validityofknowledgerepresentation, 1
Symbolicconceptacquisition, 13, 18 Variablebindinginobjectdescriptions,216-19
Synchronicpointersforcategoryassociation,598 VEHICLE-ACCIDENTschema,580
Synergyin ML research,34-35, 38 Verificationofproposedsolutions,575-76
Syntax, learningof,628,641-43 Virtualcopiesofcategories,599,610
Systematicityconditioninanalogical Virtualrelation,361
mapping,358 Visionsystem(exampleofclassifiersystem),
Systematicityprinciple,328 605-90
VL22,81
Tablelook-up,278,279 Volitionalizationinschemaacquisition,580-81
Tabularasa,622
Targetconcepts, 108, 168,472 Waterfall(acontrolprocess),655
--- PAGE 752 ---
738 SUBJECTINDEX
Waterreaction(domainofdiscovery),456-59
Weakmethods
oflearning,254,282,622
inproblemsolving,372-74
SeealsoGenerate-and-test,Hillclimbing,
Means-endanalysis,Search
Wichitasoundsystem,638
XAPS3,250,262-73,283
XAPS2,248,250,263-64,266
--- PAGE 753 ---
--- PAGE 754 ---
--- PAGE 755 ---
--- PAGE 756 ---
--- PAGE 757 ---
--- PAGE 758 ---
--- PAGE 759 ---
COWOBATIOW
WCOH
$OA J0M1
--- PAGE 760 ---
MACHINE LEARNING
Artificial Intelligence Approach
VOLUME
The ability to learn is one of the most fundamental and intriguing aspects ofintelligence. In the realm ofartificial intelligence, the taskofendowingcomputerswith
learningabilityis an
immensely challengingand importantstep inthe developmentof
powerfulintelligentsystems. Recognizingthis, expertsintherapidly growing field ofmachine learning are attempting to understandthecomputationalprinciplesoflearninginanefforttobuild
anewgenerationofcomputers.
Becauseofthesignificanceofthis
effort, machine learning has become a central research area of
artificial intelligence.
Thisbookcontinuesinthetraditionofthehighlysuccessfulfirst
book in this area, Machine Learning: An Artificial Intelligence
Approach(alsopublishedbyMorganKaufmann),preparedbythe
sameeditors. Thissecondvolume, whichcanbestudiedindependentlyofthefirst,reflectstherecentexpansionofmachinelearning
through presentation of major advances by the field's leading
researchers. These experts provide a comprehensive account of
current research directions for researchers, developers, and studentsofartificialintelligence,cognitivescience,andrelatedareas.
The topicscovered include the following:
• Classification ofmachine learningresearch
• Learning concepts and rules from examples
• Cognitive aspectsoflearning
• Learning and programming by analogy
• Learning from observation and discovery
• explorationofgeneral aspectsoflearning
• Directions forfuture research
extensive, indexed bibliography of recent contributions and
major landmarks of earlier research is included, as well as an
updated glossary ofmachine learningterms.
AlsopublishedbyMorganKaufmann:
Machine Learning: An Artificial IntelligenceApproach
EDITED BY MlCHALSKI, CARBONELL, AND MITCHELL
Principles ofArtificial Intelligence by Nils J. Nilsson
Readings in Artificial Intelligence edited by Webber
AND NlLSSON
Readings in Knowledge Representation edited by Brachman
AND LEVESQUE
ISBN D-13ML13-00-1