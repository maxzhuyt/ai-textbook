--- PAGE 1 ---
国外著名高等院校
信息科学与技术优秀教材
人工智能
----一种现代方法
Artificial Intelligence:
A Modern Approach
人民邮电出版社
POSTS & TELEC.OMMUNICATIONS PRESS
A- <b
Lb<B
-428
دلع 
+ىب = و۴)
L<B
σκεπάσματος δέομαι,
ἱμάτιον δὲ σκέπασμα·
ἱματίου δέαμαι. οὗ δέο
οὐ δέσμαι
ποιητέον· ἱματίου δέομαι·
ἱμάτιον ποιητέου.
τέον. καὶ τὸ
συμπέρασμα, ἱμάτιον
τοιητέον, πρᾶξίς ἐστιν.
英文版
Stuart Russell Peter Norvig
Prentice
Hall
Pearson Education
出版集团
--- PAGE 2 ---
封面设计:胡平利
国外著名高等院校信息科学与技术优秀教材
人工智能----一种现代方法
Artificial Intelligence:
A Modern Approach
英文版
σκεπάσματος δέσμος,
ἱμάτιου δὲ σκέπασμο
ἱματίου δέσμου.
δέομαι, οὐ δέομο
《人工智能--一种现代方法》以智能Agent思想为基础,为AI领域描绘了一幅
统一、清晰的画卷,并展示了如何使用 Al方法来构建智能 Agent。
*该书条理清晰、引人入胜,书中充满全新的内容;整体结构连贯、统一。我认为,该书
极有可能重新构造AI观点,给该领域带来引人注目的影响......祝贺您!
-国际计算机科学协会 Steve Omohundro
*该书全面、深刻地对 AI领域进行了探讨。Russell 和Norvig 围绕构建智能Agent 这一任
务组织素材、将AI呈现为一个相关设计原理的有机体,而不是技术和窍门的松散堆积.......这无
疑是教育学领域的一项伟大成就。
--密执根大学Mike Wellman
* 我正用您们的图书作教材、无论如何赞扬它都不过分。关于Agent的第二章充满才气,以
前瞻性的观点介绍了AI的历史,并解释了人们为何要重新构建已经构建好的算法.......确实是一
本好书!
----康奈尔大学Devika Subramanian
这本有关技术现状的图书介绍了最有效的现代技术,并演示了如何将这些技术用
于解决这一领域的实际问题。
*这是我阅读到的关于人工智能的书籍中内容最为全面、见解最为深刻的一本。该书围绕
理性决策范例进行组织,为这一领域提供了一个统一的观点,书中涵盖了诸如调查、逻辑、规
划、知识表达等传统主题以及不确定性下的推理、机器学习、机器人技术等方面的当前研究成
―普度大学Elisha Sacks
* Russell和 Norvig编写的这本书籍填补了一项空白,该书不但均衡地涵盖了现代主题
为大量重要算法提供了一流的伪代码。
-华盛顿大学 Daniel Weld
* Russell和Norvig编写的这本书是我阅读到的关于人工智能的书籍中内容最为全面的一本。
书中高深的内容很合我的胃口。该书表述清晰、措词准确,学生们对它称赞有加。另外。本人
也从中学到了很多。
-美国国家航空航天局 Steve Minton
*该书清晰而准确地介绍了当前 AI研究的几乎所有领域。
--AT&T贝尔实验室 Bart Selman
ISBN 7-115-10202-3
Stuart Russell 和Peter Norivg 采用了独特的方式---基于智能Agent 设计和实
际项目,这使得该书内容全面、权威。
9787115 102027 >
Russell 和Norvig 编写的该书非常好:措词准确、组织良好、内容全面........很乐意使用该
书作教材。
*使用该书作教材很愉快。
-匹茨堡大学 MarthaPollack
一哈佛大学Barbara Grosz
ISBN7-115-10202-3/TP-2834
定价:75.00元
人民邮电出版社
http: // www.ptpress.com.cn
--- PAGE 3 ---
国外著名高等院校信息科学与技术优秀教材
人工智能--一种现代方法
(英文版)
Artificial Intelligence: A Modern Approach
人民邮电出版社
Stuart Russell
Peter Norvig
Prentice
Hall
Pearson Education 出版集团
--- PAGE 4 ---
图书在版编目(CIP)数据
人工智能:一种现代方法/(英)拉塞尔(Russell, S.)(美)诺文(Norvin, P.)著.
一北京:人民邮电出版社,2002.4
国外著名高等院校信息科学与技术优秀教材
ISBN 7-115-10202-3
I. 人... II. ①拉...②诺... III. 人工智能-高等学校-教材-英文
中国版本图书馆CIP 数据核字(2002)第015670号
版权声明
IV. TP18
English Reprint Edition Copyright © 2002 by PEARSON EDUCATION NORTH ASIA LIMITED and PEOPLE'S
POSTS & TELECOMMUNICATIONS PRESS.
Artificial Intelligence: A Modern Approach
By Stuart Russell, Peter Norvig
Copyright © 1995
All Rights Reserved.
Published by arrangement with the original publisher, Pearson Education, Inc., publishing as Prentice-Hall
This edition is authorized for sale only in People's Republic of China (excluding the Special Administrative Region
of Hong Kong and Macau).
本书封面贴有Pearson Education出版集团激光防伪标签,无标签者不得销售
国外著名高等院校信息科学与技术优秀教材
人工智能--一种现代方法(英文版
Stuart Russell
Peter Norvig
责任编辑
人民邮电出版社出版发行
邮编100061
北京市崇文区夕照寺街14号
电子函件315@ptpress.com.cn
网址http://www.ptpress.com.cn
读者热线
010-67180876
北京汉魂图文设计有限公司制作
北京朝阳展望印刷厂印刷
新华书店总店北京发行所经销
开本:800×1000 1/16
印张:60.25
字数:1342千字
印数:1-3000册
2002年4月第1版
2002年4月北京第1次印刷
著作权合同登记图字: 01-2002 - 1554号
ISBN 7-115-10202-3/TP 2834
定价:75.00元
本书如有印装质量问题,请与本社联系 电话: (010) 67129223
--- PAGE 5 ---
我国人工智能(AI)的教学和科研起步于上个世纪 70年代末,正是我国实施改革开放
政策,打开国门的时候。因此,随后不久国外有关的教材就先后引进到国内来,并很快出版
了中译本,以满足教学的需要。其中影响较大的有,美国尼尔森的《人工智能原理》(N.J. Nilsson,
1980;中译本,1983),温斯顿的《人工智能》(P.H.Winston, 1977;中译本,1983),以及里
奇的《人工智能》(E.Rich, 1983,中译本,1992)等。以后,国内学者也相继编写和出版了
自己的书籍,如清华大学出版社出版的《人工智能导论》(林尧瑞等,1989)和《人工智能原
理》(石纯一等,1993)等。这些书籍曾先后作为各个高等院校本科生或研究生的教材,为培
养和造就我国AI 科研和教学人才起到了很好的作用。
由于人工智能是一门发展中的学科,尚未形成系统的理论体系,理论和技术均在不断的
变化中。同时它又同许多领域相互交叉,形成各自独立的研究分支,内容繁杂,其中包括计
算机科学、数学、心理学与认知科学等,这些都给教材的编写带来一定的困难。比如,(1)
由于AI历史上各个分支领域独立发展,缺乏联系,将各种不同的主题放在一部书中,很容
易造成内容零散且互不相关的现象;(2)有的研究分支还不成熟,难以进行系统和深入的介
绍:(3)内容更新很快、很容易过时等。
这里向读者推荐的《人工智能--一种现代方法》(S. Russell, P. Norvig)是一部 AI 的教
科书。显然,作者认识到编写人工智能教材存在的上述困难。在该书的序言中特别介绍了他
们的意图以及所做的努力。正是在他们的努力下,使该书具有以下明显的特色。
(1)作者试图在智能 Agent(译为自主体、智能体等,由于目前还没有合适的译名,就
使用原文)的概念框架下,把 AI 中相互分离的领域统一起来。书的主体内容共分为六大部
分,即问题求解、知识与推理、合乎逻辑的行动、不确定知识与推理、学习,以及通信、感
知与行动。作者通过Agent 从感知外部环境、到实施行动、并最后对外部环境施加影响的全
过程,将这六部分组织起来,形成一个相互联系的整体,使读者对 AI有一个完整的概念,
达到较好的效果。这种组织方式是新颖的,并得到同行的认可,如最近尼尔森编著的《人工
--- PAGE 6 ---
智能--一种新的综合法》(N.J.Nilsson, Artificial Intelligence: A New Synthesis, 1998;中译
本,2000)也采用了类似的组织方法和思路。
(2)人工智能是一门新兴的交叉学科,因此对于什么是 AI,不同的研究者有不同的认
识。由于认识上的差异,因此作为介绍 AI的书籍,也就有了不同的定位和重点。大体说来,
AI具有科学和工程两个方面。人们的研究可以侧重于 Agent的认知过程,也可以侧重于行为
的效果。显然,前者对应于 AI 的科学问题;后者对应于工程实现。同样是研究认知过程,
人人可以把机器与人类思维过程是否相似作为评判“智能”的标准,也可以把智能机器是否
达到某一个给定的客观性能(如合理性,rationality)作为目标。从而分别产生了4种不同的
方法论,即图灵测试法、认知建模法、思考规则法,以及理性 Agent 法等。相应地也就有了
4种不同类型的教科书。这部书把 AI定义为:设计和建造智能的理性(rational) Agent,并
把Agent行为的合理性作为评定智能的标准,而不注重这些行为的认知和心理学依据。显见,
它是一部注重形式体系并面向工程的书。从方法论看,属于第四类。这也是目前大部分 AI
教科书的编写方式,比较适合于信息科技和其它工程领域的学生和技术人员阅读。
(3)作者十分重视各个部分内容的系统性和相互关联性,采取“突出重点,以点带面”
的编写方法,一定程度上避免了 AI书籍通常存在的内容零散和肤浅的毛病。比如,在知识
与推理部分,作者重点分析了比较成熟的一阶逻辑的表示和推理,对它的表达能力和存在的
局限性均作了详细的介绍。在此基础上,简单地讨论了其它相关的表示方法,如产生式系统、
框架表示和语义网络等。在不确定性和推理部分,用主要篇幅详细分析了概率表示和推理及
其相关的信度网络(Belief Networks),而对于默认推理(Default reasoning)、Dempster-Shafer
理论、模糊逻辑等其它可能的方法只作了简单的介绍。从而使整体内容保持严谨、充实与深
入,但又不失完整性。
(4)该书对 AI的重要内容都作了详尽、全面的介绍,而且尽可能采用最新的研究成果。
比如,不少书籍在介绍规划(Planning)和学习(Learning)问题时内容比较浅显,往往回避
一些关键性的问题(可能由于当时对这些问题的研究还不够深入)。该书在介绍以上两个问题
时都尽量引用最新的研究成果,在规划问题上,深入分析了规划与推理的关系,分析了不同
规划方法优劣。并根据当前的研究状况,对规划中需要解决的几个难题,如画面问题(Frame
Problem,有的译为“框架问题”),条件问题 (Qualification Problem)和分支问题(Ramification
Problem)等,都给出比较满意的解答。近年来,机器学习研究取得了很大的进展,书中也有
较全面的介绍,除决策树以外,还包括了神经网络学习、再励学习(Reinforcement Learning)
--- PAGE 7 ---
等内容。同时讨论了学习中若干基本问题,如学习复杂性、学习精度与泛化(预测)能力等。
不过遗憾的是,可能由于篇幅及时间的关系,书中未能包括 Vapnik 等人在统计学习理论上近
期所取得的研究成果。这些成果对于理解机器学习的本质会有很大的帮助,希望读者在学习
这部分时,可以参考一些最新的文献资料。
(5)理论与实际并重。作者一方面尽量运用数学(或形式化)的语言,对各个领域的原
理和方法进行表述,力图让它们建立在严格的理论基础之上。同时,列举了大量的应用实例,
这些实例不只限于积木世界和玩具世界,也有不少现实世界的应用。对于那些想把 AI 技术
应用于自己所从事的领域的读者来说,无疑会有很大的启发和鼓舞作用。该书每章的后面附
有相当数量的练习题和该领域发展历史和介绍,这对于广大读者也很有益处。
(6)虽然这是一部有关智能机器设计的理论基础和方法的书,但作者在最后一章还提出
一些AI中十分敏感并富有争议的哲学问题:如机器会思考?人工智能能否实现?等等。并
讨论了AI 的现状和未来。这些内容也是广大读者所感兴趣的。
本书可以作为信息领域及相关领域的高等院校本科生和研究生的教科书或教学参考书,
也可以作为相关领域的科研与工程技术人员的参考书。
中国科学院院士清华大学教授
--- PAGE 8 ---
Preface
There are many textbooks that offer an introduction to artificial intelligence (AI). This text has
five principal features that together distinguish it from other texts.
1. Unified presentation of the field.
Some texts are organized from a historical perspective, describing each of the major
problems and solutions that have been uncovered in 40 years of AI research. Although
there is value to this perspective, the result is to give the impression of a dozen or so barely
related subfields, each with its own techniques and problems. We have chosen to present
Al as a unified field, working on a common problem in various guises. This has entailed
some reinterpretation of past research, showing how it fits within a common framework
and how it relates to other work that was historically separate. It has also led us to include
material not normally covered in AI texts.
2. Intelligent agent design.
The unifying theme of the book is the concept of an intelligent agent. In this view, the
problem of AI is to describe and build agents that receive percepts from the environment
and perform actions. Each such agent is implemented by a function that maps percepts
to actions, and we cover different ways to represent these functions, such as production
systems, reactive agents, logical planners, neural networks, and decision-theoretic systems.
We explain the role of learning as extending the reach of the designer into unknown environments, and show how it constrains agent design, favoring explicit knowledge representation
and reasoning. We treat robotics and vision not as independently defined problems, but
as occurring in the service of goal achievement. We stress the importance of the task
environment characteristics in determining the appropriate agent design.
3. Comprehensive and up-to-date coverage.
We cover areas that are sometimes underemphasized, including reasoning under uncertainty, learning, neural networks, natural language, vision, robotics, and philosophical
foundations. We cover many of the more recent ideas in the field, including simulated
annealing, memory-bounded search, global ontologies, dynamic and adaptive probabilistic
(Bayesian) networks, computational learning theory, and reinforcement learning. We also
provide extensive notes and references on the historical sources and current literature for
the main ideas in each chapter.
4. Equal emphasis on theory and practice.
Theory and practice are given equal emphasis. All material is grounded in first principles
with rigorous theoretical analysis where appropriate, but the point of the theory is to get the
concepts across and explain how they are used in actual, fielded systems. The reader of this
book will come away with an appreciation for the basic concepts and mathematical methods
of AI, and also with an idea of what can and cannot be done with today's technology, at
what cost, and using what techniques.
5. Understanding through implementation.
The principles of intelligent agent design are clarified by using them to actually build agents.
Chapter 2 provides an overview of agent design, including a basic agent and environment
--- PAGE 9 ---
Preface
project. Subsequent chapters include programming exercises that ask the student to add
capabilities to the agent, making it behave more and more interestingly and (we hopе)
intelligently. Algorithms are presented at three levels of detail: prose descriptions and
pseudo-code in the text, and complete Common Lisp programs available on the Internet or
on floppy disk. All the agent programs are interoperable and work in a uniform framework
for simulated environments.
This book is primarily intended for use in an undergraduate course or course sequence. It
can also be used in a graduate-level course (perhaps with the addition of some of the primary
sources suggested in the bibliographical notes). Because of its comprehensive coverage and the
large number of detailed algorithms, it is useful as a primary reference volume for AI graduate
students and professionals wishing to branch out beyond their own subfield. We also hope that
AI researchers could benefit from thinking about the unifying approach we advocate.
The only prerequisite is familiarity with basic concepts of computer science (algorithms,
data structures, complexity) at a sophomore level. Freshman calculus is useful for understanding
neural networks and adaptive probabilistic networks in detail. Some experience with nonnumeric
programming is desirable, but can be picked up in a few weeks study. We provide implementations
of all algorithms in Common Lisp (see Appendix B), but other languages such as Scheme, Prolog,
Smalltalk, C++, or ML could be used instead.
Overview of the book
The book is divided into eight parts. Part I, "Artificial Intelligence," sets the stage for all the others,
and offers a view of the Al enterprise based around the idea of intelligent agents-systems that
can decide what to do and do it. Part II, "Problem Solving," concentrates on methods for deciding
what to do when one needs to think ahead several steps, for example in navigating across country
or playing chess. Part III, "Knowledge and Reasoning," discusses ways to represent knowledge
about the world-how it works, what it is currently like, what one's actions might do-and how
to reason logically with that knowledge. Part IV, "Acting Logically," then discusses how to
use these reasoning methods to decide what to do, particularly by constructing plans. Part V,
"Uncertain Knowledge and Reasoning," is analogous to Parts III and IV, but it concentrates on
reasoning and decision-making in the presence of uncertainty about the world, as might be faced,
for example, by a system for medical diagnosis and treatment.
Together, Parts Il to V describe that part of the intelligent agent responsible for reaching
decisions. Part VI, "Learning," describes methods for generating the knowledge required by these
decision-making components; it also introduces a new kind of component, the neural network,
and its associated learning procedures. Part VII, "Communicating, Perceiving, and Acting,"
describes ways in which an intelligent agent can perceive its environment so as to know what is
going on, whether by vision, touch, hearing, or understanding language; and ways in which it can
turn its plans into real actions, either as robot motion or as natural language utterances. Finally,
Part VIII, "Conclusions," analyses the past and future of AI, and provides some light amusement
by discussing what AI really is and why it has already succeeded to some degree, and airing the
views of those philosophers who believe that AI can never succeed at all.
--- PAGE 10 ---
Preface
Using this book
This is a big book; covering all the chapters and the projects would take two semesters. You will
notice that the book is divided into 27 chapters, which makes it easy to select the appropriate
material for any chosen course of study. Each chapter can be covered in approximately one week.
Some reasonable choices for a variety of quarter and semester courses are as follows:
One-quarter general introductory course:
Chapters 1, 2, 3, 6, 7, 9, 11, 14, 15, 18, 22.
One-semester general introductory course:
Chapters 1, 2, 3, 4, 6, 7, 9, 11, 13, 14, 15, 18, 19, 22, 24, 26, 27.
One-quarter course with concentration on search and planning:
Chapters 1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13.
One-quarter course with concentration on reasoning and expert systems:
Chapters 1, 2, 3, 6, 7, 8, 9, 10, 11, 14, 15, 16.
One-quarter course with concentration on natural language:
Chapters 1, 2, 3, 6, 7, 8, 9, 14, 15, 22, 23, 26, 27.
One-semester course with concentration on learning and neural networks:
Chapters 1, 2, 3, 4, 6, 7, 9, 14, 15, 16, 17, 18, 19, 20, 21.
One-semester course with concentration on vision and robotics:
Chapters 1, 2, 3, 4, 6, 7, 11, 13, 14, 15, 16, 17, 24, 25, 20.
These sequences could be used for both undergraduate and graduate courses. The relevant parts
of the book could also be used to provide the first phase of graduate specialty courses. For
example, Part VI could be used in conjunction with readings from the literature in a course on
machine learning.
We have decided not to designate certain sections as "optional" or certain exercises as
"difficult," as individual tastes and backgrounds vary widely. Exercises requiring significant
programming are marked with a keyboard icon, and those requiring some investigation of the
literature are marked with a book icon. Altogether, over 300 exercises are included. Some of
them are large enough to be considered term projects. Many of the exercises can best be solved
by taking advantage of the code repository, which is described in Appendix B. Throughout the
book, important points are marked with a pointing icon.
If you have any comments on the book, we'd like to hear from you. Appendix B includes
information on how to contact us.
Acknowledgements
Jitendra Malik wrote most of Chapter 24 (Vision) and John Canny wrote most of Chapter
25 (Robotics). Doug Edwards researched the Historical Notes sections for all chapters and wrote
much of them. Tim Huang helped with formatting of the diagrams and algorithms. Maryann
Simmons prepared the 3-D model from which the cover illustration was produced, and Lisa
Marie Sardegna did the postprocessing for the final image. Alan Apt, Mona Pompili, and Sondra
Chavez at Prentice Hall tried their best to keep us on schedule and made many helpful suggestions
on design and content.
--- PAGE 11 ---
Preface
Stuart would like to thank his parents, brother, and sister for their encouragement and their
patience at his extended absence. He hopes to be home for Christmas. He would also like to
thank Loy Sheflott for her patience and support. He hopes to be home some time tomorrow
afternoon. His intellectual debt to his Ph.D. advisor, Michael Genesereth, is evident throughout
the book. RUGS (Russell's Unusual Group of Students) have been unusually helpful.
Peter would like to thank his parents (Torsten and Gerda) for getting him started, his advisor
(Bob Wilensky), supervisors (Bill Woods and Bob Sproull) and employer (Sun Microsystems)
for supporting his work in AI, and his wife (Kris) and friends for encouraging and tolerating him
through the long hours of writing.
Before publication, drafts of this book were used in 26 courses by about 1000 students.
Both of us deeply appreciate the many comments of these students and instructors (and other
reviewers). We can't thank them all individually, but we would like to acknowledge the especially
helpful comments of these people:
Tony Barrett, Howard Beck, John Binder, Larry Bookman, Chris Brown, Lauren
Burka, Murray Campbell, Anil Chakravarthy, Roberto Cipolla, Doug Edwards, Kutluhan Erol, Jeffrey Forbes, John Fosler, Bob Futrelle, Sabine Glesner, Barbara Grosz,
Steve Hanks, Othar Hansson, Jim Hendler, Tim Huang, Seth Hutchinson, Dan Jurafsky, Leslie Pack Kaelbling, Keiji Kanazawa, Surekha Kasibhatla, Simon Kasif,
Daphne Koller, Rich Korf, James Kurien, John Lazzaro, Jason Leatherman, Jon
LeBlanc, Jim Martin, Andy Mayer, Steve Minton, Leora Morgenstern, Ron Musick,
Stuart Nelson, Steve Omohundro, Ron Parr, Tony Passera, Michael Pazzani, Ira
Pohl, Martha Pollack, Bruce Porter, Malcolm Pradhan, Lorraine Prior, Greg Provan,
Philip Resnik, Richard Scherl, Daniel Sleator, Robert Sproull, Lynn Stein, Devika
Subramanian, Rich Sutton, Jonathan Tash, Austin Tate, Mark Torrance, Randall
Upham, Jim Waldo, Bonnie Webber, Michael Wellman, Dan Weld, Richard Yen,
Shlomo Zilberstein.
--- PAGE 12 ---
III
6 Agents that Reason Logically
7 First-Order Logic
8 Building a Knowledge Base
9 Inference in First-Order Logic
10 Logical Reasoning Systems
11 Planning
Summary of Contents
I Artificial Intelligence
1 Introduction
2 Intelligent Agents
II Problem-solving
Solving Problems by Searching
4 Informed Search Methods
5 Game Playing
Knowledge and reasoning
Acting logically
12 Practical Planning
13 Planning and Acting
Uncertain knowledge and reasoning
14 Uncertainty
15 Probabilistic Reasoning Systems
16 Making Simple Decisions
17 Making Complex Decisions
Learning
18 Learning from Observations
19 Learning in Neural and Belief Networks
20 Reinforcement Learning
21 Knowledge in Learning
VII
Communicating, perceiving, and acting
22 Agents that Communicate
23 Practical Natural Language Processing
24 Perception
25 Robotics
VIII Conclusions
26 Philosophical Foundations
27 AI: Present and Future
A Complexity analysis and O() notation
B Notes on Languages and Algorithms
Bibliography
Index
--- PAGE 13 ---
Contents
I Artificial Intelligence
Introduction
1.1
What is AI?
Acting humanly: The Turing Test approach
Thinking humanly: The cognitive modelling approach
Thinking rationally: The laws of thought approach
Acting rationally: The rational agent approach
1.2
The Foundations of Artificial Intelligence
Philosophy (428 B.C.-present)
Mathematics (c. 800-present)
Psychology (1879-present)
Computer engineering (1940-present)
Linguistics (1957-present)
1.3
The History of Artificial Intelligence
The gestation of artificial intelligence (1943-1956)
Early enthusiasm, great expectations (1952-1969)
A dose of reality (1966-1974)
Knowledge-based systems: The key to power? (1969-1979)
AI becomes an industry (1980-1988)
The return of neural networks (1986-present)
Recent events (1987-present)
1.4 The State of the Art
1.5
Summary
Bibliographical and Historical Notes
Exercises
2 Intelligent Agents
2.1 Introduction
2.2
How Agents Should Act
The ideal mapping from percept sequences to actions
Autonomy
2.3 Structure of Intelligent Agents
Agent programs
Why not just look up the answers?
An example
Simple reflex agents
Agents that keep track of the world
Goal-based agents
Utility-based agents
2.4 Environments
--- PAGE 14 ---
Contents
Properties of environments
Environment programs
2.5 Summary
Bibliographical and Historical Notes
Exercises
II Problem-solving
3 Solving Problems by Searching
3.1 Problem-Solving Agents
3.2 Formulating Problems
Knowledge and problem types
Well-defined problems and solutions
Measuring problem-solving performance
Choosing states and actions
3.3
Example Problems
Toy problems
Real-world problems
3.4 Searching for Solutions
Generating action sequences
Data structures for search trees
3.5
Search Strategies
Breadth-first search
Uniform cost search
Depth-first search
Depth-limited search
Iterative deepening search
Bidirectional search
Comparing search strategies
3.6
Avoiding Repeated States
3.7 Constraint Satisfaction Search
3.8 Summary
Bibliographical and Historical Notes
Exercises
4 Informed Search Methods
4.1 Best-First Search
Minimize estimated cost to reach a goal: Greedy search
Minimizing the total path cost: A* search
4.2
Heuristic Functions
The effect of heuristic accuracy on performance
Inventing heuristic functions
Heuristics for constraint satisfaction problems
4.3 Memory Bounded Search
--- PAGE 15 ---
Contents
Iterative deepening A* search (IDA*)
SMA* search
4.4 Iterative Improvement Algorithms
Hill-climbing search
Simulated annealing
Applications in constraint satisfaction problems
4.5 Summary
Bibliographical and Historical Notes
Exercises
5 Game Playing
5.1 Introduction: Games as Search Problems
5.2
Perfect Decisions in Two-Person Games
5.3
Imperfect Decisions
Evaluation functions
Cutting off search
5.4
Alpha-Beta Pruning
Effectiveness of alpha-beta pruning
5.5 Games That Include an Element of Chance
Position evaluation in games with chance nodes
Complexity of expectiminimax
5.6
State-of-the-Art Game Programs
Chess
Checkers or Draughts
Othello
Backgammon
5.7
Discussion
5.8
Summary
Bibliographical and Historical Notes
Exercises
6 Agents that Reason Logically
III Knowledge and reasoning
6.1 A Knowledge-Based Agent
6.2 The Wumpus World Environment
Specifying the environment
Acting and reasoning in the wumpus world
6.3 Representation, Reasoning, and Logic
Representation
Inference
Logics
6.4 Propositional Logic: A Very Simple Logic
--- PAGE 16 ---
Contents
Syntax
Semantics
Validity and inference
Models
Rules of inference for propositional logic
Complexity of propositional inference
6.5
An Agent for the Wumpus World
The knowledge base
Finding the wumpus
Translating knowledge into action
Problems with the propositional agent
6.6 Summary
Exercises
Bibliographical and Historical Notes
7 First-Order Logic
7.1
Syntax and Semantics
Terms
Atomic sentences
Complex sentences
Quantifiers
Equality
7.2
Extensions and Notational Variations
Higher-order logic
Functional and predicate expressions using the A operator
The uniqueness quantifier 3!
The uniqueness operator
Notational variations
7.3
Using First-Order Logic
The kinship domain
Axioms, definitions, and theorems
The domain of sets
Special notations for sets, lists and arithmetic
Asking questions and getting answers
7.4 Logical Agents for the Wumpus World
7.5
A Simple Reflex Agent
Limitations of simple reflex agents
7.6 Representing Change in the World
Situation calculus
Keeping track of location
7.7
Deducing Hidden Properties of the World
7.8 Preferences Among Actions
7.9 Toward a Goal-Based Agent
7.10
Summary
--- PAGE 17 ---
Contents
Bibliographical and Historical Notes
Exercises
8 Building a Knowledge Base
8.1 Properties of Good and Bad Knowledge Bases
8.2 Knowledge Engineering
8.3
The Electronic Circuits Domain
Decide what to talk about
Decide on a vocabulary
Encode general rules
Encode the specific instance
Pose queries to the inference procedure
8.4
General Ontology
Representing Categories
Measures
Composite objects
Representing change with events
Times, intervals, and actions
Objects revisited
Substances and objects
Mental events and mental objects
Knowledge and action
8.5 The Grocery Shopping World
Complete description of the shopping simulation
Organizing knowledge
Menu-planning
Navigating
Gathering
Communicating
Paying
8.6
Summary
Bibliographical and Historical Notes
Exercises
Inference in First-Order Logic
9.1 Inference Rules Involving Quantifiers
9.2
An Example Proof
9.3 Generalized Modus Ponens
Canonical form
Unification
Sample proof revisited
9.4
Forward and Backward Chaining
Forward-chaining algorithm
Backward-chaining algorithm
--- PAGE 18 ---
Contents
9.5
Completeness
9.6 Resolution: A Complete Inference Procedure
The resolution inference rule
Canonical forms for resolution
Resolution proofs
Conversion to Normal Form
Example proof
Dealing with equality
Resolution strategies
9.7 Completeness of resolution
9.8 Summary
Bibliographical and Historical Notes
Exercises
10 Logical Reasoning Systems
10.1 Introduction
10.2 Indexing, Retrieval, and Unification
Implementing sentences and terms
Store and fetch
Table-based indexing
Tree-based indexing
The unification algorithm
10.3
Logic Programming Systems
The Prolog language
Implementation
Compilation of logic programs
Other logic programming languages
Advanced control facilities
10.4
Theorem Provers
Design of a theorem prover
Extending Prolog
Theorem provers as assistants
Practical uses of theorem provers
10.5 Forward-Chaining Production Systems
Match phase
Conflict resolution phase
Practical uses of production systems
10.6
Frame Systems and Semantic Networks
Syntax and semantics of semantic networks
Inheritance with exceptions
Multiple inheritance
Inheritance and change
Implementation of semantic networks
Expressiveness of semantic networks
--- PAGE 19 ---
Contents
10.7 Description Logics.
Practical uses of description logics
10.8 Managing Retractions, Assumptions, and Explanations
10.9 Summary
Bibliographical and Historical Notes
Exercises
IV Acting logically
11 Planning
11.1 A Simple Planning Agent
11.2 From Problem Solving to Planning
11.3 Planning in Situation Calculus
11.4
Basic Representations for Planning
Representations for states and goals
Representations for actions
Situation space and plan space
Representations for plans
Solutions
11.5 A Partial-Order Planning Example
11.6 А Partial-Order Planning Algorithm
11.7 Planning with Partially Instantiated Operators
11.8 Knowledge Engineering for Planning
The blocks world
Shakey's world
11.9 Summary
Bibliographical and Historical Notes
Exercises
12 Practical Planning
12.1
Practical Planners
12.2
Spacecraft assembly, integration, and verification
Job shop scheduling
Scheduling for space missions
Buildings, aircraft carriers, and beer factories
Hierarchical Decomposition.
Extending the language
Modifying the planner
12.3 Analysis of Hierarchical Decomposition
Decomposition and sharing
Decomposition versus approximation
12.4 More Expressive Operator Descriptions
Conditional effects
Negated and disjunctive goals
--- PAGE 20 ---
Contents
Universal quantification
A planner for expressive operator descriptions
12.5 Resource Constraints
Using measures in planning
Temporal constraints
12.6 Summary
Bibliographical and Historical Notes
Exercises
13 Planning and Acting
13.1 Conditional Planning
The nature of conditional plans
An algorithm for generating conditional plans
Extending the plan language
13.2 A Simple Replanning Agent
Simple replanning with execution monitoring
13.3 Fully Integrated Planning and Execution
13.4 Discussion and Extensions
Comparing conditional planning and replanning
Coercion and abstraction
13.5 Summary
Bibliographical and Historical Notes
Exercises
V Uncertain knowledge and reasoning
14 Uncertainty
14.1 Acting under Uncertainty
Handling uncertain knowledge
Uncertainty and rational decisions
Design for a decision-theoretic agent
14.2 Basic Probability Notation
Prior probability
Conditional probability
14.3 The Axioms of Probability
Why the axioms of probability are reasonable
The joint probability distribution
14.6 Summary
14.4 Bayes' Rule and Its Use
Applying Bayes' rule: The simple case
Normalization
Using Bayes' rule: Combining evidence
14.5 Where Do Probabilities Come From?
Bibliographical and Historical Notes
--- PAGE 21 ---
Contents
Exercises
15 Probabilistic Reasoning Systems
15.1 Representing Knowledge in an Uncertain Domain
15.2 The Semantics of Belief Networks
Representing the joint probability distribution
Conditional independence relations in belief networks
15.3 Inference in Belief Networks
The nature of probabilistic inferences
An algorithm for answering queries
15.4 Inference in Multiply Connected Belief Networks
Clustering methods
Cutset conditioning methods
Stochastic simulation methods
15.5 Knowledge Engineering for Uncertain Reasoning
Case study: The Pathfinder system
15.6 Other Approaches to Uncertain Reasoning
Default reasoning
Rule-based methods for uncertain reasoning
Representing ignorance: Dempster-Shafer theory
Representing vagueness: Fuzzy sets and fuzzy logic
15.7 Summary
Bibliographical and Historical Notes
Exercises
16 Making Simple Decisions
16.2 The Basis of Utility Theory
Constraints on rational preferences
... and then there was Utility
16.3 Utility Functions
The utility of money
Utility scales and utility assessment
16.1 Combining Beliefs and Desires Under Uncertainty
16.4 Multiattribute utility functions
Dominance
Preference structure and multiattribute utility
16.5 Decision Networks
Representing a decision problem using decision networks
Evaluating decision networks
16.6
The Value of Information
A simple example
A general formula
Properties of the value of information
Implementing an information-gathering agent
--- PAGE 22 ---
Contents
16.7 Decision-Theoretic Expert Systems
16.8 Summary
Bibliographical and Historical Notes
Exercises
17 Making Complex Decisions
17.1 Sequential Decision Problems
17.2 Value Iteration
17.3 Policy Iteration.
17.4 Decision-Theoretic Agent Design
The decision cycle of a rational agent
Sensing in uncertain worlds
17.5 Dynamic Belief Networks
17.6 Dynamic Decision Networks
Discussion
17.7 Summary
Bibliographical and Historical Notes
Exercises
VI Learning
18 Learning from Observations
18.1 A General Model of Learning Agents
Components of the performance element
Representation of the components
Available feedback
Prior knowledge
Bringing it all together
18.2
Inductive Learning
18.3
Learning Decision Trees
Decision trees as performance elements
Expressiveness of decision trees
Inducing decision trees from examples
Assessing the performance of the learning algorithm
Practical uses of decision tree learning
18.4 Using Information Theory
Noise and overfitting
Broadening the applicability of decision trees
18.5 Learning General Logical Descriptions
Hypotheses
Examples.
Current-best-hypothesis search
Least-commitment search
Discussion
--- PAGE 23 ---
Contents
18.6 Why Learning Works: Computational Learning Theory
How many examples are needed?
Learning decision lists
Discussion
18.7 Summary
Bibliographical and Historical Notes
Exercises
19 Learning in Neural and Belief Networks
19.1 How the Brain Works
Comparing brains with digital computers
19.2
Neural Networks
Notation
Simple computing elements
Network structures
Optimal network structure
19.3
Perceptrons
What perceptrons can represent
Learning linearly separable functions
19.7 Summary
Exercises
19.4 Multilayer Feed-Forward Networks
Back-propagation learning
Back-propagation as gradient descent search
Discussion
19.5 Applications of Neural Networks
Pronunciation
Handwritten character recognition
Driving
19.6 Bayesian Methods for Learning Belief Networks
Bayesian learning
Belief network learning problems
Learning networks with fixed structure
A comparison of belief networks and neural networks
Bibliographical and Historical Notes
20 Reinforcement Learning
20.1 Introduction
20.2
Passive Learning in a Known Environment
Naïve updating
Adaptive dynamic programming
Temporal difference learning
20.3 Passive Learning in an Unknown Environment
20.4 Active Learning in an Unknown Environment
--- PAGE 24 ---
Contents
20.5 Exploration
20.6 Learning an Action-Value Function
20.7 Generalization in Reinforcement Learning
Applications to game-playing
Application to robot control
20.8 Genetic Algorithms and Evolutionary Programming
20.9 Summary
Bibliographical and Historical Notes
Exercises
21 Knowledge in Learning
21.1 Knowledge in Learning
Some simple examples
Some general schemes
21.2 Explanation-Based Learning
Extracting general rules from examples
Improving efficiency
21.3 Learning Using Relevance Information
Determining the hypothesis space
Learning and using relevance information
21.4 Inductive Logic Programming
An example
Inverse resolution
Top-down learning methods
21.5 Summary
Bibliographical and Historical Notes
Exercises
VII Communicating, perceiving, and acting
22 Agents that Communicate
22.1 Communication as Action
Fundamentals of language
The component steps of communication
Two models of communication
22.2 Types of Communicating Agents
Communicating using Tell and Ask
Communicating using formal language
An agent that communicates
22.3 A Formal Grammar for a Subset of English
The Lexicon of E0
The Grammar of E
22.4 Syntactic Analysis (Parsing)
22.5 Definite Clause Grammar (DCG)
--- PAGE 25 ---
Contents
22.6 Augmenting a Grammar
Verb Subcategorization
Generative Capacity of Augmented Grammars
22.7
Semantic Interpretation
Semantics as DCG Augmentations
The semantics of "John loves Mary"
The semantics of E
Converting quasi-logical form to logical form
Pragmatic Interpretation
22.8 Ambiguity and Disambiguation
Disambiguation
22.9 A Communicating Agent
22.10 Summary
Bibliographical and Historical Notes
Exercises
23 Practical Natural Language Processing
23.1
Practical Applications
Machine translation
Database access
Information retrieval
Text categorization
Extracting data from text
23.2
Efficient Parsing
Extracting parses from the chart: Packing
23.3
Scaling Up the Lexicon
23.4 Scaling Up the Grammar
Nominal compounds and apposition
Adjective phrases
Determiners
Noun phrases revisited
Clausal complements
Relative clauses
Questions
Handling agrammatical strings
23.5
Ambiguity
Syntactic evidence
Lexical evidence
Semantic evidence
Metonymy
Metaphor.
23.6 Discourse Understanding
The structure of coherent discourse
23.7 Summary
--- PAGE 26 ---
Bibliographical and Historical Notes
Exercises
24 Perception
Contents
24.1
Introduction
24.2
Image Formation
Pinhole camera
Lens systems
Photometry of image formation
Spectrophotometry of image formation
24.3 Image-Processing Operations for Early Vision
Convolution with linear filters
Edge detection
24.4
Extracting 3-D Information Using Vision
Motion
Binocular stereopsis
Texture gradients
Shading
Contour
24.5 Using Vision for Manipulation and Navigation
24.6 Object Representation and Recognition
The alignment method
Using projective invariants
24.7
Speech Recognition
Signal processing
Defining the overall speech recognition model
The language model: P(words)
The acoustic model: P(signallwords)
Putting the models together
The search algorithm
Training the model
24.8 Summary
Bibliographical and Historical Notes
Exercises
25 Robotics
25.1
Introduction
25.2 Tasks: What Are Robots Good For?
Manufacturing and materials handling
Gofer robots
Hazardous environments
Telepresence and virtual reality
Augmentation of human abilities
25.3 Рarts: What Are Robots Made Of?
--- PAGE 27 ---
Contents
Effectors: Tools for action
Sensors: Tools for perception
25.4
Architectures
Classical architecture
Situated automata
25.5 Configuration Spaces: A Framework for Analysis
Generalized configuration space
Recognizable Sets
25.6 Navigation and Motion Planning
Cell decomposition
Skeletonization methods
Fine-motion planning
Landmark-based navigation
Online algorithms
25.7 Summary
Bibliographical and Historical Notes
Exercises
VIII Conclusions
26 Philosophical Foundations
26.1 The Big Questions
26.2 Foundations of Reasoning and Perception
26.3 On the Possibility of Achieving Intelligent Behavior
The mathematical objection
The argument from informality
26.4 Intentionality and Consciousness
The Chinese Room
The Brain Prosthesis Experiment
Discussion
26.5 Summary
Bibliographical and Historical Notes
Exercises
27 AI: Present and Future
27.1 Нave We Succeeded Yet?
27.2 What Exactly Are We Trying to Do?
27.3 What If We Do Succeed?
A Complexity analysis and O() notation
A.1 Asymptotic Analysis
A.2 Inherently Hard Problems
Bibliographical and Historical Notes
--- PAGE 28 ---
B Notes on Languages and Algorithms
B.1 Defining Languages with Backus-Naur Form (BNF)
B.2 Describing Algorithms with Pseudo-Code
Nondeterminism
Static variables
Functions as values
B.3 The Code Repository
B.4
Comments
Bibliography
Index
Contents
--- PAGE 29 ---
Part I
ARTIFICIAL INTELLIGENCE
The two chapters in this part introduce the subject of Artificial Intelligence or AI
and our approach to the subject: that AI is the study of agents that exist in an
environment and perceive and act.
--- PAGE 30 ---
--- PAGE 31 ---
ARTIFICIAL
INTELLIGENCE
INTRODUCTION
In which we try to explain why we consider artificial intelligence to be a subject most
worthy of study, and in which we try to decide what exactly it is, this being a good
thing to decide before embarking.
Humankind has given itself the scientific name homo sapiens-man the wise-because our
mental capacities are so important to our everyday lives and our sense of self. The field of
artificial intelligence, or AI, attempts to understand intelligent entities. Thus, one reason to
study it is to learn more about ourselves. But unlike philosophy and psychology, which are
also concerned with intelligence, AI strives to build intelligent entities as well as understand
them. Another reason to study AI is that these constructed intelligent entities are interesting and
useful in their own right. AI has produced many significant and impressive products even at this
early stage in its development. Although no one can predict the future in detail, it is clear that
computers with human-level intelligence (or better) would have a huge impact on our everyday
lives and on the future course of civilization.
AI addresses one of the ultimate puzzles. How is it possible for a slow, tiny brain, whether
biological or electronic, to perceive, understand, predict, and manipulate a world far larger and
more complicated than itself? How do we go about making something with those properties?
These are hard questions, but unlike the search for faster-than-light travel or an antigravity device,
the researcher in AI has solid evidence that the quest is possible. All the researcher has to do is
look in the mirror to see an example of an intelligent system.
AI is one of the newest disciplines. It was formally initiated in 1956, when the name
was coined, although at that point work had been under way for about five years. Along with
modern genetics, it is regularly cited as the "field I would most like to be in" by scientists in other
disciplines. A student in physics might reasonably feel that all the good ideas have already been
taken by Galileo, Newton, Einstein, and the rest, and that it takes many years of study before one
can contribute new ideas. AI, on the other hand, still has openings for a full-time Einstein.
The study of intelligence is also one of the oldest disciplines. For over 2000 years, philosophers have tried to understand how seeing, learning, remembering, and reasoning could, or should,
--- PAGE 32 ---
Chapter 1.
Introduction
be done. The advent of usable computers in the early 1950s turned the learned but armchair
speculation concerning these mental faculties into a real experimental and theoretical discipline.
Many felt that the new "Electronic Super-Brains" had unlimited potential for intelligence. "Faster
Than Einstein" was a typical headline. But as well as providing a vehicle for creating artificially
intelligent entities, the computer provides a tool for testing theories of intelligence, and many
theories failed to withstand the test-a case of "out of the armchair, into the fire." AI has turned
out to be more difficult than many at first imagined, and modern ideas are much richer, more
subtle, and more interesting as a result.
AI currently encompasses a huge variety of subfields, from general-purpose areas such as
perception and logical reasoning, to specific tasks such as playing chess, proving mathematical
theorems, writing poetry, and diagnosing diseases. Often, scientists in other fields move gradually
into artificial intelligence, where they find the tools and vocabulary to systematize and automate
the intellectual tasks on which they have been working all their lives. Similarly, workers in AI
can choose to apply their methods to any area of human intellectual endeavor. In this sense, it is
truly a universal field.
1.1 WHAT IS AI?
RATIONALITY
We have now explained why AI is exciting, but we have not said what it is. We could just say,
"Well, it has to do with smart programs, so let's get on and write some." But the history of science
shows that it is helpful to aim at the right goals. Early alchemists, looking for a potion for eternal
life and a method to turn lead into gold, were probably off on the wrong foot. Only when the aim
changed, to that of finding explicit theories that gave accurate predictions of the terrestrial world,
in the same way that early astronomy predicted the apparent motions of the stars and planets,
could the scientific method emerge and productive science take place.
Definitions of artificial intelligence according to eight recent textbooks are shown in Figure 1.1. These definitions vary along two main dimensions. The ones on top are concerned
with thought processes and reasoning, whereas the ones on the bottom address behavior. Also,
the definitions on the left measure success in terms of human performance, whereas the ones
on the right measure against an ideal concept of intelligence, which we will call rationality. A
system is rational if it does the right thing. This gives us four possible goals to pursue in artificial
intelligence, as seen in the caption of Figure 1.1.
Historically, all four approaches have been followed. As one might expect, a tension exists
between approaches centered around humans and approaches centered around rationality.2 A
human-centered approach must be an empirical science, involving hypothesis and experimental
A more recent branch of philosophy is concerned with proving that Al is impossible. We will return to this interesting
viewpoint in Chapter 26.
2 We should point out that by distinguishing between human and rational behavior, we are not suggesting that humans
are necessarily "irrational" in the sense of "emotionally unstable" or "insane." One merely need note that we often make
mistakes; we are not all chess grandmasters even though we may know all the rules of chess; and unfortunately, not
everyone gets an A on the exam. Some systematic errors in human reasoning are cataloged by Kahneman et al. (1982).
--- PAGE 33 ---
Section 1.1.
What is AI?
"The exciting new effort to make computers
think... machines with minds, in the full
and literal sense" (Haugeland, 1985)
"[The automation of] activities that we associate with human thinking, activities such as
decision-making, problem solving, learning
..." (Bellman, 1978)
"The art of creating machines that perform
functions that require intelligence when performed by people" (Kurzweil, 1990)
"The study of how to make computers do
things at which, at the moment, people are
better" (Rich and Knight, 1991)
Figure 1.1
"The study of mental faculties through the
use of computational models"
(Charniak and McDermott, 1985)
"The study of the computations that make
it possible to perceive, reason, and act"
(Winston, 1992)
"A field of study that seeks to explain and
emulate intelligent behavior in terms of
computational processes" (Schalkoff, 1990)
"The branch of computer science that is concerned with the automation of intelligent
behavior" (Luger and Stubblefield, 1993)
Some definitions of AI. They are organized into four categories:
Systems that think like humans. Systems that think rationally.
Systems that act like humans.
Systems that act rationally.
TURING TEST
NATURAL LANGUAGE
PROCESSING
KNOWLEDGE
REPRESENTATION
AUTOMATED
REASONING
MACHINE LEARNING
confirmation. A rationalist approach involves a combination of mathematics and engineering.
People in each group sometimes cast aspersions on work done in the other groups, but the truth
is that each direction has yielded valuable insights. Let us look at each in more detail.
Acting humanly: The Turing Test approach
The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory
operational definition of intelligence. Turing defined intelligent behavior as the ability to achieve
human-level performance in all cognitive tasks, sufficient to fool an interrogator. Roughly
speaking, the test he proposed is that the computer should be interrogated by a human via a
teletype, and passes the test if the interrogator cannot tell if there is a computer or a human at the
other end. Chapter 26 discusses the details of the test, and whether or not a computer is really
intelligent if it passes. For now, programming a computer to pass the test provides plenty to work
on. The computer would need to possess the following capabilities:
natural language processing to enable it to communicate successfully in English (or some
other human language);
<◇ knowledge representation to store information provided before or during the interrogation;
◇automated reasoning to use the stored information to answer questions and to draw new
conclusions;
machine learning to adapt to new circumstances and to detect and extrapolate patterns.
Turing's test deliberately avoided direct physical interaction between the interrogator and the
computer, because physical simulation of a person is unnecessary for intelligence. However,
--- PAGE 34 ---
Chapter 1. Introduction
TOTAL TURING TEST
COMPUTER VISION
ROBOTICS
COGNITIVE SCIENCE
the so-called total Turing Test includes a video signal so that the interrogator can test the
subject's perceptual abilities, as well as the opportunity for the interrogator to pass physical
objects "through the hatch." To pass the total Turing Test, the computer will need
◇computer vision to perceive objects, and
robotics to move them about.
Within AI, there has not been a big effort to try to pass the Turing test. The issue of acting
like a human comes up primarily when Al programs have to interact with people, as when an
expert system explains how it came to its diagnosis, or a natural language processing system has
a dialogue with a user. These programs must behave according to certain normal conventions of
human interaction in order to make themselves understood. The underlying representation and
reasoning in such a system may or may not be based on a human model.
Thinking humanly: The cognitive modelling approach
If we are going to say that a given program thinks like a human, we must have some way of
determining how humans think. We need to get inside the actual workings of human minds.
There are two ways to do this: through introspection-trying to catch our own thoughts as they
go by-or through psychological experiments. Once we have a sufficiently precise theory of
the mind, it becomes possible to express the theory as a computer program. If the program's
input/output and timing behavior matches human behavior, that is evidence that some of the
program's mechanisms may also be operating in humans. For example, Newell and Simon, who
developed GPS, the "General Problem Solver" (Newell and Simon, 1961), were not content to
have their program correctly solve problems. They were more concerned with comparing the
trace of its reasoning steps to traces of human subjects solving the same problems. This is in
contrast to other researchers of the same time (such as Wang (1960)), who were concerned with
getting the right answers regardless of how humans might do it. The interdisciplinary field of
cognitive science brings together computer models from AI and experimental techniques from
psychology to try to construct precise and testable theories of the workings of the human mind.
Although cognitive science is a fascinating field in itself, we are not going to be discussing
it all that much in this book. We will occasionally comment on similarities or differences between
AI techniques and human cognition. Real cognitive science, however, is necessarily based on
experimental investigation of actual humans or animals, and we assume that the reader only has
access to a computer for experimentation. We will simply note that Al and cognitive science
continue to fertilize each other, especially in the areas of vision, natural language, and learning.
The history of psychological theories of cognition is briefly covered on page 12.
Thinking rationally: The laws of thought approach
SYLLOGISMS
The Greek philosopher Aristotle was one of the first to attempt to codify "right thinking," that is,
irrefutable reasoning processes. His famous syllogisms provided patterns for argument structures
that always gave correct conclusions given correct premises. For example, "Socrates is a man;
--- PAGE 35 ---
Section 1.1. What is AI?
LOGIC
LOGICIST
all men are mortal; therefore Socrates is mortal." These laws of thought were supposed to govern
the operation of the mind, and initiated the field of logic.
The development of formal logic in the late nineteenth and early twentieth centuries, which
we describe in more detail in Chapter 6, provided a precise notation for statements about all kinds
of things in the world and the relations between them. (Contrast this with ordinary arithmetic
notation, which provides mainly for equality and inequality statements about numbers.) By 1965,
programs existed that could, given enough time and memory, take a description of a problem
in logical notation and find the solution to the problem, if one exists. (If there is no solution,
the program might never stop looking for it.) The so-called logicist tradition within artificial
intelligence hopes to build on such programs to create intelligent systems.
There are two main obstacles to this approach. First, it is not easy to take informal
knowledge and state it in the formal terms required by logical notation, particularly when the
knowledge is less than 100% certain. Second, there is a big difference between being able to
solve a problem "in principle" and doing so in practice. Even problems with just a few dozen
facts can exhaust the computational resources of any computer unless it has some guidance as to
which reasoning steps to try first. Although both of these obstacles apply to any attempt to build
computational reasoning systems, they appeared first in the logicist tradition because the power
of the representation and reasoning systems are well-defined and fairly well understood.
AGENT
Acting rationally: The rational agent approach
Acting rationally means acting so as to achieve one's goals, given one's beliefs. An agent is just
something that perceives and acts. (This may be an unusual use of the word, but you will get
used to it.) In this approach, Al is viewed as the study and construction of rational agents.
In the "laws of thought" approach to AI, the whole emphasis was on correct inferences.
Making correct inferences is sometimes part of being a rational agent, because one way to act
rationally is to reason logically to the conclusion that a given action will achieve one's goals,
and then to act on that conclusion. On the other hand, correct inference is not all of rationality,
because there are often situations where there is no provably correct thing to do, yet something
must still be done. There are also ways of acting rationally that cannot be reasonably said to
involve inference. For example, pulling one's hand off of a hot stove is a reflex action that is
more successful than a slower action taken after careful deliberation.
All the "cognitive skills" needed for the Turing Test are there to allow rational actions. Thus,
we need the ability to represent knowledge and reason with it because this enables us to reach
good decisions in a wide variety of situations. We need to be able to generate comprehensible
sentences in natural language because saying those sentences helps us get by in a complex society.
We need learning not just for erudition, but because having a better idea of how the world works
enables us to generate more effective strategies for dealing with it. We need visual perception not
just because seeing is fun, but in order to get a better idea of what an action might achieve-for
example, being able to see a tasty morsel helps one to move toward it.
The study of AI as rational agent design therefore has two advantages. First, it is more
general than the "laws of thought" approach, because correct inference is only a useful mechanism
for achieving rationality, and not a necessary one. Second, it is more amenable to scientific
--- PAGE 36 ---
LIMITED
RATIONALITY
Chapter 1. Introduction
development than approaches based on human behavior or human thought, because the standard
of rationality is clearly defined and completely general. Human behavior, on the other hand,
is well-adapted for one specific environment and is the product, in part, of a complicated and
largely unknown evolutionary process that still may be far from achieving perfection. This
book will therefore concentrate on general principles of rational agents, and on components for
constructing them. We will see that despite the apparent simplicity with which the problem can
be stated, an enormous variety of issues come up when we try to solve it. Chapter 2 outlines
some of these i3sues in more detail.
One important point to keep in mind: we will see before too long that achieving perfect
rationality-always doing the right thing-is not possible in complicated environments. The
computational demands are just too high. However, for most of the book, we will adopt the
working hypothesis that understanding perfect decision making is a good place to start. It
simplifies the problem and provides the appropriate setting for most of the foundational material
in the field. Chapters 5 and 17 deal explicitly with the issue of limited rationality-acting
appropriately when there is not enough time to do all the computations one might like.
1.2 THE FOUNDATIONS OF ARTIFICIAL INTELLIGENCСE
In this section and the next, we provide a brief history of AI. Although AI itself is a young field,
it has inherited many ideas, viewpoints, and techniques from other disciplines. From over 2000
years of tradition in philosophy, theories of reasoning and learning have emerged, along with the
viewpoint that the mind is constituted by the operation of a physical system. From over 400 years
of mathematics, we have formal theories of logic, probability, decision making, and computation.
From psychology, we have the tools with which to investigate the human mind, and a scientific
language within which to express the resulting theories. From linguistics, we have theories of
the structure and meaning of language. Finally, from computer science, we have the tools with
which to make AI a reality.
Like any history, this one is forced to concentrate on a small number of people and events,
and ignore others that were also important. We choose to arrange events to tell the story of how
the various intellectual components of modern AI came into being. We certainly would not wish
to give the impression, however, that the disciplines from which the components came have all
been working toward AI as their ultimate fruition.
Philosophy (428 B.C.-present)
The safest characterization of the European philosophical tradition is that it consists of a series
of footnotes to Plato.
-Alfred North Whitehead
We begin with the birth of Plato in 428 B.C. His writings range across politics, mathematics,
physics, astronomy, and several branches of philosophy. Together, Plato, his teacher Socrates,
--- PAGE 37 ---
Section 1.2. The Foundations of Artificial Intelligence
DUALISM
MATERIALISM
EMPIRICIST
INDUCTION
and his student Aristotle laid the foundation for much of western thought and culture. The
philosopher Hubert Dreyfus (1979, p. 67) says that "The story of artificial intelligence might well
begin around 450 B.C." when Plato reported a dialogue in which Socrates asks Euthyphro,3 "I
want to know what is characteristic of piety which makes all actions pious... that I may have it
to turn to, and to use as a standard whereby to judge your actions and those of other men."4 In
other words, Socrates was asking for an algorithm to distinguish piety from non-piety. Aristotle
went on to try to formulate more precisely the laws governing the rational part of the mind. He
developed an informal system of syllogisms for proper reasoning, which in principle allowed one
to mechanically generate conclusions, given initial premises. Aristotle did not believe all parts
of the mind were governed by logical processes; he also had a notion of intuitive reason.
Now that we have the idea of a set of rules that can describe the working of (at least part
of) the mind, the next step is to consider the mind as a physical system. We have to wait for
René Descartes (1596-1650) for a clear discussion of the distinction between mind and matter,
and the problems that arise. One problem with a purely physical conception of the mind is that
it seems to leave little room for free will: if the mind is governed entirely by physical laws, then
it has no more free will than a rock "deciding" to fall toward the center of the earth. Although a
strong advocate of the power of reasoning, Descartes was also a proponent of dualism. He held
that there is a part of the mind (or soul or spirit) that is outside of nature, exempt from physical
laws. On the other hand, he felt that animals did not possess this dualist quality; they could be
considered as if they were machines.
An alternative to dualism is materialism, which holds that all the world (including the
brain and mind) operate according to physical law.5 Wilhelm Leibniz (1646-1716) was probably
the first to take the materialist position to its logical conclusion and build a mechanical device
intended to carry out mental operations. Unfortunately, his formulation of logic was so weak that
his mechanical concept generator could not produce interesting results.
It is also possible to adopt an intermediate position, in which one accepts that the mind
has a physical basis, but denies that it can be explained by a reduction to ordinary physical
processes. Mental processes and consciousness are therefore part of the physical world, but
inherently unknowable; they are beyond rational understanding. Some philosophers critical of
AI have adopted exactly this position, as we discuss in Chapter 26.
Barring these possible objections to the aims of AI, philosophy had thus established
tradition in which the mind was conceived of as a physical device operating principally by
reasoning with the knowledge that it contained. The next problem is then to establish the
source of knowledge. The empiricist movement, starting with Francis Bacon's (1561-1626)
Novum Organum, is characterized by the dictum of John Locke (1632-1704): "Nothing is in
the understanding, which was not first in the senses." David Hume's (1711-1776) A Treatise
of Human Nature (Hume, 1978) proposed what is now known as the principle of induction:
3 The Euthyphro describes the events just before the trial of Socrates in 399 B.C. Dreyfus has clearly erred in placing it
51 years earlier.
4 Note that other translations have "goodness/good" instead of "piety/pious."
In this view, the perception of "free will" arises because the deterministic generation of behavior is constituted by the
operation of the mind selecting among what appear to be the possible courses of action. They remain "possible" because
the brain does not have access to its own future states.
6 An update of Aristotle's organon, or instrument of thought.
--- PAGE 38 ---
LOGICAL POSITIVISM
OBSERVATION
SENTENCES
CONFIRMATION
CONFIRMAT
THEORY
MEANS-ENDS
ANALYSIS
Chapter 1. Introduction
that general rules are acquired by exposure to repeated associations between their elements.
The theory was given more formal shape by Bertrand Russell (1872-1970) who introduced
logical positivism. This doctrine holds that all knowledge can be characterized by logical
theories connected, ultimately, to observation sentences that correspond to sensory inputs. The
confirmation theory of Rudolf Carnap and Carl Hempel attempted to establish the nature of the
connection between the observation sentences and the more general theories-in other words, to
understand how knowledge can be acquired from experience.
The final element in the philosophical picture of the mind is the connection between
knowledge and action. What form should this connection take, and how can particular actions
be justified? These questions are vital to Al, because only by understanding how actions are
justified can we understand how to build an agent whose actions are justifiable, or rational.
Aristotle provides an elegant answer in the Nicomachean Ethics (Book III. 3, 1112b):
We deliberate not about ends, but about means. For a doctor does not deliberate whether he
shall heal, nor an orator whether he shall persuade, nor a statesman whether he shall produce
law and order, nor does any one else deliberate about his end. They assume the end and
consider how and by what means it is attained, and if it seems easily and best produced
thereby; while if it is achieved by one means only they consider how it will be achieved by
this and by what means this will be achieved, till they come to the first cause, which in the
order of discovery is last... and what is last in the order of analysis seems to be first in the
order of becoming. And if we come on an impossibility, we give up the search, e.g. if we
need money and this cannot be got; but if a thing appears possible we try to do it.
Aristotle's approach (with a few minor refinements) was implemented 2300 years later by Newell
and Simon in their GPS program, about which they write (Newell and Simon, 1972):
The main methods of GPS jointly embody the heuristic of means-ends analysis. Means-ends
analysis is typified by the following kind of common-sense argument:
I want to take my son to nursery school. What's the difference between what I
have and what I want? One of distance. What changes distance? My automobile.
My automobile won't work. What is needed to make it work? A new battery.
What has new batteries? An auto repair shop. I want the repair shop to put in a
new battery; but the shop doesn't know I need one. What is the difficulty? One
of communication. What allows communication? A telephone... and so on.
This kind of analysis-classifying things in terms of the functions they serve and oscillating
among ends, functions required, and means that perform them-forms the basic system of
heuristic of GPS.
Means-ends analysis is useful, but does not say what to do when several actions will achieve the
goal, or when no action will completely achieve it. Arnauld, a follower of Descartes, correctly
described a quantitative formula for deciding what action to take in cases like this (see Chapter 16).
John Stuart Mill's (1806-1873) book Utilitarianism (Mill, 1863) amplifies on this idea. The more
formal theory of decisions is discussed in the following section.
7 In this picture, all meaningful statements can be verified or falsified either by analyzing the meaning of the words or
by carrying out experiments. Because this rules out most of metaphysics, as was the intention, logical positivism was
unpopular in some circles.
--- PAGE 39 ---
Section 1.2. The Foundations of Artificial Intelligence
ALGORITHM
INCOMPLETENESS
THEOREM
INTRACTABILITY
Mathematics (c. 800-present)
Philosophers staked out most of the important ideas of AI, but to make the leap to a formal
science required a level of mathematical formalization in three main areas: computation, logic,
and probability. The notion of expressing a computation as a formal algorithm goes back to
al-Khowarazmi, an Arab mathematician of the ninth century, whose writings also introduced
Europe to Arabic numerals and algebra.
Logic goes back at least to Aristotle, but it was a philosophical rather than mathematical
subject until George Boole (1815-1864) introduced his formal language for making logical
inference in 1847. Boole's approach was incomplete, but good enough that others filled in the
gaps. In 1879, Gottlob Frege (1848-1925) produced a logic that, except for some notational
changes, forms the first-order logic that is used today as the most basic knowledge representation
system.8 Alfred Tarski (1902-1983) introduced a theory of reference that shows how to relate
the objects in a logic to objects in the real world. The next step was to determine the limits of
what could be done with logic and computation.
David Hilbert (1862-1943), a great mathematician in his own right, is most remembered
for the problems he did not solve. In 1900, he presented a list of 23 problems that he correctly
predicted would occupy mathematicians for the bulk of the century. The final problem asks
if there is an algorithm for deciding the truth of any logical proposition involving the natural
numbers-the famous Entscheidungsproblem, or decision problem. Essentially, Hilbert was
asking if there were fundamental limits to the power of effective proof procedures. In 1930, Kurt
Gödel (1906-1978) showed that there exists an effective procedure to prove any true statement in
the first-order logic of Frege and Russell; but first-order logic could not capture the principle of
mathematical induction needed to characterize the natural numbers. In 1931, he showed that real
limits do exist. His incompleteness theorem showed that in any language expressive enough
to describe the properties of the natural numbers, there are true statements that are undecidable:
their truth cannot be established by any algorithm.
This fundamental result can also be interpreted as showing that there are some functions
on the integers that cannot be represented by an algorithm-that is, they cannot be computed.
This motivated Alan Turing (1912-1954) to try to characterize exactly which functions are
capable of being computed. This notion is actually slightly problematic, because the notion
of a computation or effective procedure really cannot be given a formal definition. However,
the Church-Turing thesis, which states that the Turing machine (Turing, 1936) is capable of
computing any computable function, is generally accepted as providing a sufficient definition.
Turing also showed that there were some functions that no Turing machine can compute. For
example, no machine can tell in general whether a given program will return an answer on a
given input, or run forever.
Although undecidability and noncomputability are important to an understanding of computation, the notion of intractability has had a much greater impact. Roughly speaking,
a class of problems is called intractable if the time required to solve instances of the class
grows at least exponentially with the size of the instances. The distinction between polynomial
and exponential growth in complexity was first emphasized in the mid-1960s (Cobham, 1964;
Edmonds, 1965). It is important because exponential growth means that even moderate-sized inTo understand why Frege's notation was not universally adopted, see the cover of this book.
--- PAGE 40 ---
Chapter 1.
Introduction
REDUCTION
NP-COMPLETENESS
DECISION THEORY
stances cannot be solved in any reasonable time. Therefore, one should strive to divide the overall
problem of generating intelligent behavior into tractable subproblems rather than intractable ones.
The second important concept in the theory of complexity is reduction, which also emerged in
the 1960s (Dantzig, 1960; Edmonds, 1962). A reduction is a general transformation from one
class of problems to another, such that solutions to the first class can be found by reducing them
to problems of the second class and solving the latter problems.
How can one recognize an intractable problem? The theory of NP-completeness, pioneered
by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp showed
the existence of large classes of canonical combinatorial search and reasoning problems that
are NP-complete. Any problem class to which an NP-complete problem class can be reduced
is likely to be intractable. (Although it has not yet been proved that NP-complete problems
are necessarily intractable, few theoreticians believe otherwise.) These results contrast sharply
with the "Electronic Super-Brain" enthusiasm accompanying the advent of computers. Despite
the ever-increasing speed of computers, subtlety and careful use of resources will characterize
intelligent systems. Put crudely, the world is an extremely large problem instance!
Besides logic and computation, the third great contribution of mathematics to Al is the
theory of probability. The Italian Gerolamo Cardano (1501-1576) first framed the idea of
probability, describing it in terms of the possible outcomes of gambling events. Before his time,
the outcomes of gambling games were seen as the will of the gods rather than the whim of chance.
Probability quickly became an invaluable part of all the quantitative sciences, helping to deal
with uncertain measurements and incomplete theories. Pierre Fermat (1601-1665), Blaise Pascal
(1623-1662), James Bernoulli (1654-1705), Pierre Laplace (1749-1827), and others advanced
the theory and introduced new statistical methods. Bernoulli also framed an alternative view
of probability, as a subjective "degree of belief" rather than an objective ratio of outcomes.
Subjective probabilities therefore can be updated as new evidence is obtained. Thomas Bayes
(1702-1761) proposed a rule for updating subjective probabilities in the light of new evidence
(published posthumously in 1763). Bayes' rule, and the subsequent field of Bayesian analysis,
form the basis of the modern approach to uncertain reasoning in Al systems. Debate still rages
between supporters of the objective and subjective views of probability, but it is not clear if the
difference has great significance for AI. Both versions obey the same set of axioms. Savage's
(1954) Foundations of Statistics gives a good introduction to the field.
As with logic, a connection must be made between probabilistic reasoning and action.
Decision theory, pioneered by John Von Neumann and Oskar Morgenstern (1944), combines
probability theory with utility theory (which provides a formal and complete framework for
specifying the preferences of an agent) to give the first general theory that can distinguish good
actions from bad ones. Decision theory is the mathematical successor to utilitarianism, and
provides the theoretical basis for many of the agent designs in this book.
Psychology (1879-present)
Scientific psychology can be said to have begun with the work of the German physicist Hermann
von Helmholtz (1821-1894) and his student Wilhelm Wundt (1832-1920). Helmholtz applied
the scientific method to the study of human vision, and his Handbook of Physiological Optics
--- PAGE 41 ---
Section 1.2.
The Foundations of Artificial Intelligence
BEHAVIORISM
COGNITIVEcy
COGNITIVE
PSYCHOLOGY
is even now described as "the single most important treatise on the physics and physiology of
human vision to this day" (Nalwa, 1993, p.15). In 1879, the same year that Frege launched firstorder logic, Wundt opened the first laboratory of experimental psychology at the University of
Leipzig. Wundt insisted on carefully controlled experiments in which his workers would perform
a perceptual or associative task while introspecting on their thought processes. The careful
controls went a long way to make psychology a science, but as the methodology spread, a curious
phenomenon arose: each laboratory would report introspective data that just happened to match
the theories that were popular in that laboratory. The behaviorism movement of John Watson
(1878-1958) and Edward Lee Thorndike (1874-1949) rebelled against this subjectivism, rejecting
any theory involving mental processes on the grounds that introspection could not provide reliable
evidence. Behaviorists insisted on studying only objective measures of the percepts (or stimulus)
given to an animal and its resulting actions (or response). Mental constructs such as knowledge,
beliefs, goals, and reasoning steps were dismissed as unscientific "folk psychology." Behaviorism
discovered a lot about rats and pigeons, but had less success understanding humans. Nevertheless,
it had a strong hold on psychology (especially in the United States) from about 1920 to 1960.
The view that the brain possesses and processes information, which is the principal characteristic of cognitive psychology, can be traced back at least to the works of William James
(1842-1910). Helmholtz also insisted that perception involved a form of unconscious logical inference. The cognitive viewpoint was largely eclipsed by behaviorism until 1943, when Kenneth
Craik published The Nature of Explanation. Craik put back the missing mental step between
stimulus and response. He claimed that beliefs, goals, and reasoning steps could be useful valid
components of a theory of human behavior, and are just as scientific as, say, using pressure and
temperature to talk about gases, despite their being made of molecules that have neither. Craik
specified the three key steps of a knowledge-based agent: (1) the stimulus must be translated into
an internal representation, (2) the representation is manipulated by cognitive processes to derive
new internal representations, and (3) these are in turn retranslated back into action. He clearly
explained why this was a good design for an agent:
If the organism carries a "small-scale model" of external reality and of its own possible actions
within its head, it is able to try out various alternatives, conclude which is the best of them,
react to future situations before they arise, utilize the knowledge of past events in dealing with
the present and future, and in every way to react in a much fuller, safer, and more competent
manner to the emergencies which face it. (Craik, 1943)
An agent designed this way can, for example, plan a long trip by considering various possible routes, comparing them, and choosing the best one, all before starting the journey. Since
the 1960s, the information-processing view has dominated psychology. It it now almost taken
for granted among many psychologists that "a cognitive theory should be like a computer program" (Anderson, 1980). By this it is meant that the theory should describe cognition as consisting
of well-defined transformation processes operating at the level of the information carried by the
input signals.
For most of the early history of AI and cognitive science, no significant distinction was
drawn between the two fields, and it was common to see AI programs described as psychological
William James was the brother of novelist Henry James. It is said that Henry wrote fiction as if it were psychology
and William wrote psychology as if it were fiction.
--- PAGE 42 ---
Chapter 1. Introduction
results without any claim as to the exact human behavior they were modelling. In the last decade
or so, however, the methodological distinctions have become clearer, and most work now falls
into one field or the other.
Computer engineering (1940-present)
For artificial intelligence to succeed, we need two things: intelligence and an artifact. The
computer has been unanimously acclaimed as the artifact with the best chance of demonstrating
intelligence. The modern digital electronic computer was invented independently and almost
simultaneously by scientists in three countries embattled in World War II. The first operational
modern computer was the Heath Robinson, 1 built in 1940 by Alan Turing's team for the single
purpose of deciphering German messages. When the Germans switched to a more sophisticated
code, the electromechanical relays in the Robinson proved to be too slow, and a new machine
called the Colossus was built from vacuum tubes. It was completed in 1943, and by the end of
the war, ten Colossus machines were in everyday use.
The first operational programmable computer was the Z-3, the invention of Konrad Zuse
in Germany in 1941. Zuse invented floating-point numbers for the Z-3, and went on in 1945 to
develop Plankalkul, the first high-level programming language. Although Zuse received some
support from the Third Reich to apply his machine to aircraft design, the military hierarchy did
not attach as much importance to computing as did its counterpart in Britain.
In the United States, the first electronic computer, the ABC, was assembled by John
Atanasoff and his graduate student Clifford Berry between 1940 and 1942 at Iowa State University.
The project received little support and was abandoned after Atanasoff became involved in military
research in Washington. Two other computer projects were started as secret military research:
the Mark I, II, and III computers were developed at Harvard by a team under Howard Aiken; and
the ENIAC was developed at the University of Pennsylvania by a team including John Mauchly
and John Eckert. ENIAC was the first general-purpose, electronic, digital computer. One of its
first applications was computing artillery firing tables. A successor, the EDVAC, followed John
Von Neumann's suggestion to use a stored program, so that technicians would not have to scurry
about changing patch cords to run a new program.
But perhaps the most critical breakthrough was the IBM 701, built in 1952 by Nathaniel
Rochester and his group. This was the first computer to yield a profit for its manufacturer. IBM
went on to become one of the world's largest corporations, and sales of computers have grown to
$150 billion/year. In the United States, the computer industry (including software and services)
now accounts for about 10% of the gross national product.
Each generation of computer hardware has brought an increase in speed and capacity, and
a decrease in price. Computer engineering has been remarkably successful, regularly doubling
performance every two years, with no immediate end in sight for this rate of increase. Massively
parallel machines promise to add several more zeros to the overall throughput achievable.
Of course, there were calculating devices before the electronic computer. The abacus
is roughly 7000 years old. In the mid-17th century, Blaise Pascal built a mechanical adding
10 Heath Robinson was a cartoonist famous for his depictions of whimsical and absurdly complicated contraptions for
everyday tasks such as buttering toast.
--- PAGE 43 ---
Section 1.2. The Foundations of Artificial Intelligencе
and subtracting machine called the Pascaline. Leibniz improved on this in 1694, building a
mechanical device that multiplied by doing repeated addition. Progress stalled for over a century
until Charles Babbage (1792-1871) dreamed that logarithm tables could be computed by machine.
He designed a machine for this task, but never completed the project. Instead, he turned to the
design of the Analytical Engine, for which Babbage invented the ideas of addressable memory,
stored programs, and conditional jumps. Although the idea of programmable machines was
not new-in 1805, Joseph Marie Jacquard invented a loom that could be programmed using
punched cards-Babbage's machine was the first artifact possessing the characteristics necessary
for universal computation. Babbage's colleague Ada Lovelace, daughter of the poet Lord Byron,
wrote programs for the Analytical Engine and even speculated that the machine could play chess
or compose music. Lovelace was the world's first programmer, and the first of many to endure
massive cost overruns and to have an ambitious project ultimately abandoned. Babbage's basic
design was proven viable by Doron Swade and his colleagues, who built a working model using
only the mechanical techniques available at Babbage's time (Swade, 1993). Babbage had the
right idea, but lacked the organizational skills to get his machine built.
Al also owes a debt to the software side of computer science, which has supplied the
operating systems, programming languages, and tools needed to write modern programs (and
papers about them). But this is one area where the debt has been repaid: work in AI has pioneered
many ideas that have made their way back to "mainstream" computer science, including time
sharing, interactive interpreters, the linked list data type, automatic storage management, and
some of the key concepts of object-oriented programming and integrated program development
environments with graphical user interfaces.
Linguistics (1957-present)
In 1957, B. F. Skinner published Verbal Behavior. This was a comprehensive, detailed account
of the behaviorist approach to language learning, written by the foremost expert in the field. But
curiously, a review of the book became as well-known as the book itself, and served to almost kill
off interest in behaviorism. The author of the review was Noam Chomsky, who had just published
a book on his own theory, Syntactic Structures. Chomsky showed how the behaviorist theory did
not address the notion of creativity in language-it did not explain how a child could understand
and make up sentences that he or she had never heard before. Chomsky's theory-based on
syntactic models going back to the Indian linguist Panini (с. 350 В.С.)-could explain this, and
unlike previous theories, it was formal enough that it could in principle be programmed.
Later developments in linguistics showed the problem to be considerably more complex
than it seemed in 1957. Language is ambiguous and leaves much unsaid. This means that
understanding language requires an understanding of the subject matter and context, not just an
understanding of the structure of sentences. This may seem obvious, but it was not appreciated
until the early 1960s. Much of the early work in knowledge representation (the study of how to
put knowledge into a form that a computer can reason with) was tied to language and informed
by research in linguistics, which was connected in turn to decades of work on the philosophical
analysis of language.
11 She also gave her name to Ada, the U.S. Department of Defense's all-purpose programming language.
--- PAGE 44 ---
Chapter 1.
Introduction
Modern linguistics and AI were "born" at about the same time, so linguistics does not play
a large foundational role in the growth of AI. Instead, the two grew up together, intersecting
a hybrid field called computational linguistics or natural language processing, which
concentrates on the problem of language use.
1.3 THE HISTORY OF ARTIFICIAL INTELLIGENCE
With the background material behind us, we are now ready to outline the development of AI
proper. We could do this by identifying loosely defined and overlapping phases in its development,
or by chronicling the various different and intertwined conceptual threads that make up the field.
In this section, we will take the former approach, at the risk of doing some degree of violence
to the real relationships among subfields. The history of each subfield is covered in individual
chapters later in the book.
The gestation of artificial intelligence (1943-1956)
The first work that is now generally recognized as AI was done by Warren McCulloch and
Walter Pitts (1943). They drew on three sources: knowledge of the basic physiology and
function of neurons in the brain; the formal analysis of propositional logic due to Russell and
Whitehead; and Turing's theory of computation. They proposed a model of artificial neurons in
which each neuron is characterized as being "on" or "off," with a switch to "on" occurring in
response to stimulation by a sufficient number of neighboring neurons. The state of a neuron
was conceived of as "factually equivalent to a proposition which proposed its adequate stimulus."
They showed, for example, that any computable function could be computed by some network
of connected neurons, and that all the logical connectives could be implemented by simple
net structures. McCulloch and Pitts also suggested that suitably defined networks could learn.
Donald Hebb (1949) demonstrated a simple updating rule for modifying the connection strengths
between neurons, such that learning could take place.
The work of McCulloch and Pitts was arguably the forerunner of both the logicist tradition
in AI and the connectionist tradition. In the early 1950s, Claude Shannon (1950) and Alan
Turing (1953) were writing chess programs for von Neumann-style conventional computers.12
At the same time, two graduate students in the Princeton mathematics department, Marvin
Minsky and Dean Edmonds, built the first neural network computer in 1951. The SNARC, as
it was called, used 3000 vacuum tubes and a surplus automatic pilot mechanism from a В-24
bomber to simulate a network of 40 neurons. Minsky's Ph.D. committee was skeptical whether
this kind of work should be considered mathematics, but von Neumann was on the committee
and reportedly said, "If it isn't now it will be someday." Ironically, Minsky was later to prove
theorems that contributed to the demise of much of neural network research during the 1970s.
12 Shannon actually had no real computer to work with, and Turing was eventually denied access to his own team's
computers by the British government, on the grounds that research into artificial intelligence was surely frivolous.
--- PAGE 45 ---
Section 1.3. The History of Artificial Intelligence
Princeton was home to another influential figure in AI, John McCarthy. After graduation,
McCarthy moved to Dartmouth College, which was to become the official birthplace of the
field. McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring
together U.S. researchers interested in automata theory, neural nets, and the study of intelligence.
They organized a two-month workshop at Dartmouth in the summer of 1956. All together there
were ten attendees, including Trenchard More from Princeton, Arthur Samuel from IBM, and
Ray Solomonoff and Oliver Selfridge from MIT.
Two researchers from Carnegie Tech, 13 Allen Newell and Herbert Simon, rather stole the
show. Although the others had ideas and in some cases programs for particular applications
such as checkers, Newell and Simon already had a reasoning program, the Logic Theorist (LT),
about which Simon claimed, "We have invented a computer program capable of thinking nonnumerically, and thereby solved the venerable mind-body problem."14 Soon after the workshop,
the program was able to prove most of the theorems in Chapter 2 of Russell and Whitehead's
Principia Mathematica. Russell was reportedly delighted when Simon showed him that the program had come up with a proof for one theorem that was shorter than the one in Principia. The
editors of the Journal of Symbolic Logic were less impressed; they rejected a paper coauthored
by Newell, Simon, and Logic Theorist.
The Dartmouth workshop did not lead to any new breakthroughs, but it did introduce all
the major figures to each other. For the next 20 years, the field would be dominated by these
people and their students and colleagues at MIT, CMU, Stanford, and IBM. Perhaps the most
lasting thing to come out of the workshop was an agreement to adopt McCarthy's new name for
the field: artificial intelligence.
Early enthusiasm, great expectations (1952-1969)
The early years of AI were full of successes-in a limited way. Given the primitive computers
and programming tools of the time, and the fact that only a few years earlier computers were
seen as things that could do arithmetic and no more, it was astonishing whenever a computer did
anything remotely clever. The intellectual establishment, by and large, preferred to believe that "a
machine can never do X" (see Chapter 26 for a long list of X's gathered by Turing). AI researchers
naturally responded by demonstrating one X after another. Some modern Al researchers refer to
this period as the "Look, Ma, no hands!" era.
Newell and Simon's early success was followed up with the General Problem Solver,
or GPS. Unlike Logic Theorist, this program was designed from the start to imitate human
problem-solving protocols. Within the limited class of puzzles it could handle, it turned out that
the order in which the program considered subgoals and possible actions was similar to the way
humans approached the same problems. Thus, GPS was probably the first program to embody
the "thinking humanly" approach. The combination of Al and cognitive science has continued
at CMU up to the present day.
13 Now Carnegie Mellon University (CMU).
14 Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler, and translated it
into machine code by hand. To avoid errors, they worked in parallel, calling out binary numbers to each other as they
wrote each instruction to make sure they agreed.
--- PAGE 46 ---
LISP
Chapter
Introduction
At IBM, Nathaniel Rochester and his colleagues produced some of the first Al programs.
Herbert Gelernter (1959) constructed the Geometry Theorem Prover. Like the Logic Theorist,
it proved theorems using explicitly represented axioms. Gelernter soon found that there were
too many possible reasoning paths to follow, most of which turned out to be dead ends. To help
focus the search, he added the capability to create a numerical representation of a diagram-а
particular case of the general theorem to be proved. Before the program tried to prove something,
it could first check the diagram to see if it was true in the particular case.
Starting in 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that
eventually learned to play tournament-level checkers. Along the way, he disproved the idea that
computers can only do what they are told to, as his program quickly learned to play a better game
than its creator. The program was demonstrated on television in February 1956, creating a very
strong impression. Like Turing, Samuel had trouble finding computer time. Working at night, he
used machines that were still on the testing floor at IBM's manufacturing plant. Chapter 5 covers
game playing, and Chapter 20 describes and expands on the learning techniques used by Samuel.
John McCarthy moved from Dartmouth to MIT and there made three crucial contributions
in one historic year: 1958. In MIT AI Lab Memo No. 1, McCarthy defined the high-level language
Lisp, which was to become the dominant Al programming language. Lisp is the second-oldest
language in current use. 15 With Lisp, McCarthy had the tool he needed, but access to scarce and
expensive computing resources was also a serious problem. Thus, he and others at MIT invented
time sharing. After getting an experimental time-sharing system up at MIT, McCarthy eventually
attracted the interest of a group of MIT grads who formed Digital Equipment Corporation, which
was to become the world's second largest computer manufacturer, thanks to their time-sharing
minicomputers. Also in 1958, McCarthy published a paper entitled Programs with Common
Sense, in which he described the Advice Taker, a hypothetical program that can be seen as the
first complete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthy's
program was designed to use knowledge to search for solutions to problems. But unlike the others,
it was to embody general knowledge of the world. For example, he showed how some simple
axioms would enable the program to generate a plan to drive to the airport to catch a plane. The
program was also designed so that it could accept new axioms in the normal course of operation,
thereby allowing it to achieve competence in new areas without being reprogrammed. The Advice
Taker thus embodied the central principles of knowledge representation and reasoning: that it
is useful to have a formal, explicit representation of the world and the way an agent's actions
affect the world, and to be able to manipulate these representations with deductive processes. It
is remarkable how much of the 1958 paper remains relevant after more than 35 years.
1958 also marked the year that Marvin Minsky moved to MIT. For years he and McCarthy
were inseparable as they defined the field together. But they grew apart as McCarthy stressed
representation and reasoning in formal logic, whereas Minsky was more interested in getting
programs to work, and eventually developed an anti-logical outlook. In 1963, McCarthy took
the opportunity to go to Stanford and start the AI lab there. His research agenda of using
logic to build the ultimate Advice Taker was advanced by J. A. Robinson's discovery of the
resolution method (a complete theorem-proving algorithm for first-order logic; see Section 9.6).
Work at Stanford emphasized general-purpose methods for logical reasoning. Applications of
15 FORTRAN is one year older than Lisp.
--- PAGE 47 ---
Section 1.3.
The History of Artificial Intelligence
MICROWORLDS
logic included Cordell Green's question answering and planning systems (Green, 1969b), and the
Shakey robotics project at the new Stanford Research Institute (SRI). The latter project, discussed
further in Chapter 25, was the first to demonstrate the complete integration of logical reasoning
and physical activity.
Minsky supervised a series of students who chose limited problems that appeared to require
intelligence to solve. These limited domains became known as microworlds. James Slagle's
SAINT program (1963a) was able to solve closed-form integration problems typical of first-year
college calculus courses. Tom Evans's ANALOGY program (1968) solved geometric analogy
problems that appear in IQ tests, such as the one in Figure 1.2. Bertram Raphael's (1968) SIR
(Semantic Information Retrieval) was able to accept input statements in a very restricted subset
of English and answer questions thereon. Daniel Bobrow's STUDENT program (1967) solved
algebra story problems such as
If the number of customers Tom gets is twice the square of 20 percent of the number of
advertisements he runs, and the number of advertisements he runs is 45, what is the number
of customers Tom gets?
is to
is to:
Figure 1.2
An example problem solved by Evans's ANALOGY program.
The most famous microworld was the blocks world, which consists of a set of solid blocks
placed on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.3. A task
in this world is to rearrange the blocks in a certain way, using a robot hand that can pick up one
block at a time. The blocks world was home to the vision project of David Huffman (1971),
the vision and constraint-propagation work of David Waltz (1975), the learning theory of Patrick
Winston (1970), the natural language understanding program of Terry Winograd (1972), and the
planner of Scott Fahlman (1974).
Early work building on the neural networks of McCulloch and Pitts also flourished. The
work of Winograd and Cowan (1963) showed how a large number of elements could collectively
represent an individual concept, with a corresponding increase in robustness and parallelism.
Hebb's learning methods were enhanced by Bernie Widrow (Widrow and Hoff, 1960; Widrow,
1962), who called his networks adalines, and by Frank Rosenblatt (1962) with his perceptrons.
--- PAGE 48 ---
Chapter 1.
Introduction
Figure 1.3 A scene from the blocks world. A task for the robot might be "Pick up a big red
block," expressed either in natural language or in a formal notation.
Rosenblatt proved the famous perceptron convergence theorem, showing that his learning
algorithm could adjust the connection strengths of a perceptron to match any input data, provided
such a match existed. These topics are covered in Section 19.3.
A dose of reality (1966-1974)
From the beginning, AI researchers were not shy in making predictions of their coming successes.
The following statement by Herbert Simon in 1957 is often quoted:
It is not my aim to surprise or shock you-but the simplest way I can summarize is to say
that there are now in the world machines that think, that learn and that create. Moreover, their
ability to do these things is going to increase rapidly until-in a visible future-the range of
problems they can handle will be coextensive with the range to which human mind has been
applied.
Although one might argue that terms such as "visible future" can be interpreted in various ways,
some of Simon's predictions were more concrete. In 1958, he predicted that within 10 years
a computer would be chess champion, and an important new mathematical theorem would be
proved by machine. Claims such as these turned out to be wildly optimistic. The barrier that
faced almost all AI research projects was that methods that sufficed for demonstrations on one or
two simple examples turned out to fail miserably when tried out on wider selections of problems
and on more difficult problems.
The first kind of difficulty arose because early programs often contained little or no
knowledge of their subject matter, and succeeded by means of simple syntactic manipulations.
Weizenbaum's ELIZA program (1965), which could apparently engage in serious conversation
--- PAGE 49 ---
Section 1.3. The History of Artificial Intelligence
MACHINE EVOLUTION
on any topic, actually just borrowed and manipulated the sentences typed into it by a human.
A typical story occurred in early machine translation efforts, which were generously funded by
the National Research Council in an attempt to speed up the translation of Russian scientific
papers in the wake of the Sputnik launch in 1957. It was thought initially that simple syntactic
transformations based on the grammars of Russian and English, and word replacement using
an electronic dictionary, would suffice to preserve the exact meanings of sentences. In fact,
translation requires general knowledge of the subject matter in order to resolve ambiguity and
establish the content of the sentence. The famous retranslation of "the spirit is willing but the
flesh is weak" as "the vodka is good but the meat is rotten" illustrates the difficulties encountered.
In 1966, a report by an advisory committee found that "there has been no machine translation
of general scientific text, and none is in immediate prospect." All U.S. government funding for
academic translation projects was cancelled.
The second kind of difficulty was the intractability of many of the problems that AI was
attempting to solve. Most of the early AI programs worked by representing the basic facts about
a problem and trying out a series of steps to solve it, combining different combinations of steps
until the right one was found. The early programs were feasible only because microworlds
contained very few objects. Before the theory of NP-completeness was developed, it was widely
thought that "scaling up" to larger problems was simply a matter of faster hardware and larger
memories. The optimism that accompanied the development of resolution theorem proving, for
example, was soon dampened when researchers failed to prove theorems involving more than a
few dozen facts. The fact that a program can find a solution in principle does not mean that the
program contains any of the mechanisms needed to find it in practice.
The illusion of unlimited computational power was not confined to problem-solving programs. Early experiments in machine evolution (now called genetic algorithms) (Friedberg,
1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by making an
appropriate series of small mutations to a machine code program, one can generate a program
with good performance for any particular simple task. The idea, then, was to try random mutations and then apply a selection process to preserve mutations that seemed to improve behavior.
Despite thousands of hours of CPU time, almost no progress was demonstrated.
Failure to come to grips with the "combinatorial explosion" was one of the main criticisms
of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for the decision
by the British government to end support for Al research in all but two universities. (Oral
tradition paints a somewhat different and more colorful picture, with political ambitions and
personal animosities that cannot be put in print.)
A third difficulty arose because of some fundamental limitations on the basic structures
being used to generate intelligent behavior. For example, in 1969, Minsky and Papert's book
Perceptrons (1969) proved that although perceptrons could be shown to learn anything they were
capable of representing, they could represent very little. In particular, a two-input perceptron
could not be trained to recognize when its two inputs were different. Although their results
did not apply to more complex, multilayer networks, research funding for neural net research
soon dwindled to almost nothing. Ironically, the new back-propagation learning algorithms for
multilayer networks that were to cause an enormous resurgence in neural net research in the late
1980s were actually discovered first in 1969 (Bryson and Ho, 1969).
--- PAGE 50 ---
WEAK METHODS
EXPERT SYSTEMS
Chapter 1. Introduction
Knowledge-based systems: The key to power? (1969-1979)
The picture of problem solving that had arisen during the first decade of AI research was of a
general-purpose search mechanism trying to string together elementary reasoning steps to find
complete solutions. Such approaches have been called weak methods, because they use weak
information about the domain. For many complex domains, it turns out that their performance is
also weak. The only way around this is to use knowledge more suited to making larger reasoning
steps and to solving typically occurring cases in narrow areas of expertise. One might say that to
solve a hard problem, you almost have to know the answer already.
The DENDRAL program (Buchanan et al., 1969) was an early example of this approach. It
was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon), Bruce
Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel laureate
geneticist) teamed up to solve the problem of inferring molecular structure from the information
provided by a mass spectrometer. The input to the program consists of the elementary formula of
the molecule (e.g., CoH13NO2), and the mass spectrum giving the masses of the various fragments
of the molecule generated when it is bombarded by an electron beam. For example, the mass
spectrum might contain a peak at m = 15 corresponding to the mass of a methyl (CH3) fragment.
The naive version of the program generated all possible structures consistent with the
formula, and then predicted what mass spectrum would be observed for each, comparing this
with the actual spectrum. As one might expect, this rapidly became intractable for decent-sized
molecules. The DENDRAL researchers consulted analytical chemists and found that they worked
by looking for well-known patterns of peaks in the spectrum that suggested common substructures
in the molecule. For example, the following rule is used to recognize a ketone (C=O) subgroup:
if there are two peaks at x₁ and x2 such that
(a) x1 +X2 = M + 28 (M is the mass of the whole molecule);
(b) x1 - 28 is a high peak;
(c) x2-28 is a high peak;
(d) At least one of x₁ and x2 is high.
then there is a ketone subgroup
Having recognized that the molecule contains a particular substructure, the number of possible
candidates is enormously reduced. The DENDRAL team concluded that the new system was
powerful because
All the relevant theoretical knowledge to solve these problems has been mapped over from its
general form in the [spectrum prediction component] ("first principles") to efficient special
forms ("cookbook recipes"). (Feigenbaum et al., 1971)
The significance of DENDRAL was that it was arguably the first successful knowledge-intensive
system: its expertise derived from large numbers of special-purpose rules. Later systems also
incorporated the main theme of McCarthy's Advice Taker approach- the clean separation of the
knowledge (in the form of rules) and the reasoning component.
With this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Programming Project (HPP), to investigate the extent to which the new methodology of expert systems
could be applied to other areas of human expertise. The next major effort was in the area of
medical diagnosis. Feigenbaum, Buchanan, and Dr. Edward Shortliffe developed MYCIN to
diagnose blood infections. With about 450 rules, MYCIN was able to perform as well as some
--- PAGE 51 ---
Section 1.3. The History of Artificial Intelligence
FRAMES
experts, and considerably better than junior doctors. It also contained two major differences from
DENDRAL. First, unlike the DENDRAL rules, no general theoretical model existed from which the
MYCIN rules could be deduced. They had to be acquired from extensive interviewing of experts,
who in turn acquired them from direct experience of cases. Second, the rules had to reflect the
uncertainty associated with medical knowledge. MYCIN incorporated a calculus of uncertainty
called certainty factors (see Chapter 14), which seemed (at the time) to fit well with how doctors
assessed the impact of evidence on the diagnosis.
Other approaches to medical diagnosis were also followed. At Rutgers University, Saul
Amarel's Computers in Biomedicine project began an ambitious attempt to diagnose diseases
based on explicit knowledge of the causal mechanisms of the disease process. Meanwhile, large
groups at MIT and the New England Medical Center were pursuing an approach to diagnosis and
treatment based on the theories of probability and utility. Their aim was to build systems that
gave provably optimal medical recommendations. In medicine, the Stanford approach using rules
provided by doctors proved more popular at first. But another probabilistic reasoning system,
PROSPECTOR (Duda et al., 1979), generated enormous publicity by recommending exploratory
drilling at a geological site that proved to contain a large molybdenum deposit.
The importance of domain knowledge was also apparent in the area of understanding
natural language. Although Winograd's SHRDLU system for understanding natural language had
engendered a good deal of excitement, its dependence on syntactic analysis caused some of
the same problems as occurred in the early machine translation work. It was able to overcome
ambiguity and understand pronoun references, but this was mainly because it was designed
specifically for one area-the blocks world. Several researchers, including Eugene Charniak,
a fellow graduate student of Winograd's at MIT, suggested that robust language understanding
would require general knowledge about the world and a general method for using that knowledge.
At Yale, the linguist-turned-AI-researcher Roger Schank emphasized this point by claiming,
"There is no such thing as syntax," which upset a lot of linguists, but did serve to start a useful
discussion. Schank and his students built a series of programs (Schank and Abelson, 1977;
Schank and Riesbeck, 1981; Dyer, 1983) that all had the task of understanding natural language.
The emphasis, however, was less on language per se and more on the problems of representing
and reasoning with the knowledge required for language understanding. The problems included
representing stereotypical situations (Cullingford, 1981), describing human memory organization
(Rieger, 1976; Kolodner, 1983), and understanding plans and goals (Wilensky, 1983). William
Woods (1973) built the LUNAR system, which allowed geologists to ask questions in English
about the rock samples brought back by the Apollo moon mission. LUNAR was the first natural
language program that was used by people other than the system's author to get real work done.
Since then, many natural language programs have been used as interfaces to databases.
The widespread growth of applications to real-world problems caused a concomitant increase in the demands for workable knowledge representation schemes. A large number of
different representation languages were developed. Some were based on logic-for example,
the Prolog language became popular in Europe, and the PLANNER family in the United States.
Others, following Minsky's idea of frames (1975), adopted a rather more structured approach,
collecting together facts about particular object and event types, and arranging the types into a
large taxonomic hierarchy analogous to a biological taxonomy.
--- PAGE 52 ---
Chapter 1.
Introduction
AI becomes an industry (1980-1988)
The first successful commercial expert system, R1, began operation at Digital Equipment Corporation (McDermott, 1982). The program helped configure orders for new computer systems,
and by 1986, it was saving the company an estimated $40 million a year. By 1988, DEC's AI
group had 40 deployed expert systems, with more on the way. Du Pont had 100 in use and 500 in
development, saving an estimated $10 million a year. Nearly every major U.S. corporation had
its own AI group and was either using or investigating expert system technology.
In 1981, the Japanese announced the "Fifth Generation" project, a 10-year plan to build
intelligent computers running Prolog in much the same way that ordinary computers run machine
code. The idea was that with the ability to make millions of inferences per second, computers
would be able to take advantage of vast stores of rules. The project proposed to achieve full-scale
natural language understanding, among other ambitious goals.
The Fifth Generation project fueled interest in AI, and by taking advantage of fears of
Japanese domination, researchers and corporations were able to generate support for a similar
investment in the United States. The Microelectronics and Computer Technology Corporation
(MCC) was formed as a research consortium to counter the Japanese project. In Britain, the
Alvey report reinstated the funding that was cut by the Lighthill report. 1 In both cases, AI was
part of a broad effort, including chip design and human-interface research.
The booming AI industry also included companies such as Carnegie Group, Inference,
Intellicorp, and Teknowledge that offered the software tools to build expert systems, and hardware companies such as Lisp Machines Inc., Texas Instruments, Symbolics, and Xerox that
were building workstations optimized for the development of Lisp programs. Over a hundred
companies built industrial robotic vision systems. Overall, the industry went from a few million
in sales in 1980 to $2 billion in 1988.
The return of neural networks (1986-present)
Although computer science had neglected the field of neural networks after Minsky and Papert's
Perceptrons book, work had continued in other fields, particularly physics. Large collections
of simple neurons could be understood in much the same way as large collections of atoms in
solids. Physicists such as Hopfield (1982) used techniques from statistical mechanics to analyze
the storage and optimization properties of networks, leading to significant cross-fertilization of
ideas. Psychologists including David Rumelhart and Geoff Hinton continued the study of neural
net models of memory. As we discuss in Chapter 19, the real impetus came in the mid-1980s
when at least four different groups reinvented the back-propagation learning algorithm first found
in 1969 by Bryson and Ho. The algorithm was applied to many learning problems in computer
science and psychology, and the widespread dissemination of the results in the collection Parallel
Distributed Processing (Rumelhart and McClelland, 1986) caused great excitement.
At about the same time, some disillusionment was occurring concerning the applicability
of the expert system technology derived from MYCIN-type systems. Many corporations and
16 To save embarrassment, a new field called IKBS (Intelligent Knowledge-Based Systems) was defined because Artificial
Intelligence had been officially cancelled.
--- PAGE 53 ---
Section 1.3. The History of Artificial Intelligence
research groups found that building a successful expert system involved much more than simply
buying a reasoning system and filling it with rules. Some predicted an "AI Winter" in which AI
funding would be squeezed severely. It was perhaps this fear, and the historical factors on the
neural network side, that led to a period in which neural networks and traditional AI were seen
as rival fields, rather than as mutually supporting approaches to the same problem.
Recent events (1987-present)
Recent years have seen a sea change in both the content and the methodology of research in
artificial intelligence. 17 It is now more common to build on existing theories than to propose
brand new ones, to base claims on rigorous theorems or hard experimental evidence rather than
on intuition, and to show relevance to real-world applications rather than toy examples.
The field of speech recognition illustrates the pattern. In the 1970s, a wide variety of
different architectures and approaches were tried. Many of these were rather ad hoc and fragile,
and were demonstrated on a few specially selected examples. In recent years, approaches based
on hidden Markov models (HMMs) have come to dominate the area. Two aspects of HMMs are
relevant to the present discussion. First, they are based on a rigorous mathematical theory. This
has allowed speech researchers to build on several decades of mathematical results developed in
other fields. Second, they are generated by a process of training on a large corpus of real speech
data. This ensures that the performance is robust, and in rigorous blind tests the HMMs have
been steadily improving their scores. Speech technology and the related field of handwritten
character recognition are already making the transition to widespread industrial and consumer
applications.
Another area that seems to have benefitted from formalization is planning. Early work by
Austin Tate (1977), followed up by David Chapman (1987), has resulted in an elegant synthesis
of existing planning programs into a simple framework. There have been a number of advances
that built upon each other rather than starting from scratch each time. The result is that planning
systems that were only good for microworlds in the 1970s are now used for scheduling of factory
work and space missions, among other things. See Chapters 11 and 12 for more details.
Judea Pearl's (1988) Probabilistic Reasoning in Intelligent Systems marked a new acсерtance of probability and decision theory in AI, following a resurgence of interest epitomized by
Peter Cheeseman's (1985) article "In Defense of Probability." The belief network formalism was
invented to allow efficient reasoning about the combination of uncertain evidence. This approach
largely overcomes the problems with probabilistic reasoning systems of the 1960s and 1970s,
and has come to dominate AI research on uncertain reasoning and expert systems. Work by
Judea Pearl (1982a) and by Eric Horvitz and David Heckerman (Horvitz and Heckerman, 1986;
Horvitz et al., 1986) promoted the idea of normative expert systems: ones that act rationally
according to the laws of decision theory and do not try to imitate human experts. Chapters 14 to
16 cover this area.
17 Some have characterized this change as a victory of the neats-those who think that AI theories should be grounded
in mathematical rigor-over the scruffies-those who would rather try out lots of ideas, write some programs, and then
assess what seems to be working. Both approaches are important. A shift toward increased neatness implies that the field
has reached a level of stability and maturity. (Whether that stability will be disrupted by a new scruffy idea is another
question.)
--- PAGE 54 ---
Chapter 1. Introduction
Similar gentle revolutions have occurred in robotics, computer vision, machine learning
(including neural networks), and knowledge representation. A better understanding of the problems and their complexity properties, combined with increased mathematical sophistication, has
led to workable research agendas and robust methods. Perhaps encouraged by the progress in
solving the subproblems of Al, researchers have also started to look at the "whole agent" problem
again. The work of Allen Newell, John Laird, and Paul Rosenbloom on SOAR (Newell, 1990;
Laird et al., 1987) is the best-known example of a complete agent architecture in AI. The so-called
"situated" movement aims to understand the workings of agents embedded in real environments
with continuous sensory inputs. Many interesting results are coming out of such work, including
the realization that the previously isolated subfields of AI may need to be reorganized somewhat
when their results are to be tied together into a single agent design.
1.4 THE STATE OF THE ART
International grandmaster Arnold Denker studies the pieces on the board in front of him. He
realizes there is no hope; he must resign the game. His opponent, HITECH, becomes the first
computer program to defeat a grandmaster in a game of chess (Berliner, 1989).
"I want to go from Boston to San Francisco," the traveller says into the microphone. "What
date will you be travelling on?" is the reply. The traveller explains she wants to go October 20th,
nonstop, on the cheapest available fare, returning on Sunday. A speech understanding program
named PEGASUS handles the whole transaction, which results in a confirmed reservation that
saves the traveller $894 over the regular coach fare. Even though the speech recognizer gets one
out of ten words wrong, 18 it is able to recover from these errors because of its understanding of
how dialogs are put together (Zue et al., 1994).
An analyst in the Mission Operations room of the Jet Propulsion Laboratory suddenly
starts paying attention. A red message has flashed onto the screen indicating an "anomaly" with
the Voyager spacecraft, which is somewhere in the vicinity of Neptune. Fortunately, the analyst
is able to correct the problem from the ground. Operations personnel believe the problem might
have been overlooked had it not been for MARVEL, a real-time expert system that monitors the
massive stream of data transmitted by the spacecraft, handling routine tasks and alerting the
analysts to more serious problems (Schwuttke, 1992).
Cruising the highway outside of Pittsburgh at a comfortable 55 mph, the man in the driver's
seat seems relaxed. He should be-for the past 90 miles, he has not had to touch the steering wheel,
brake, or accelerator. The real driver is a robotic system that gathers input from video cameras,
sonar, and laser range finders attached to the van. It combines these inputs with experience
learned from training runs and succesfully computes how to steer the vehicle (Pomerleau, 1993).
A leading expert on lymph-node pathology describes a fiendishly difficult case to the
expert system, and examines the system's diagnosis. He scoffs at the system's response. Only
slightly worried, the creators of the system suggest he ask the computer for an explanation of
18 Some other existing systems err only half as often on this task.
--- PAGE 55 ---
Section 1.5.
Summary
the diagnosis. The machine points out the major factors influencing its decision, and explains
the subtle interaction of several of the symptoms in this case. The expert admits his error,
eventually (Heckerman, 1991).
From a camera perched on a street light above the crossroads, the traffic monitor watches
the scene. If any humans were awake to read the main screen, they would see "Citröen 2CV
turning from Place de la Concorde into Champs Elysées," "Large truck of unknown make stopped
on Place de la Concorde," and so on into the night. And occasionally, "Major incident on Place
de la Concorde, speeding van collided with motorcyclist," and an automatic call to the emergency
services (King et al., 1993; Koller et al., 1994).
These are just a few examples of artificial intelligence systems that exist today. Not magic
or science fiction-but rather science, engineering, and mathematics, to which this book provides
an introduction.
1.5 SUMMARY
This chapter defines AI and establishes the cultural background against which it has developed.
Some of the important points are as follows:
Different people think of AI differently. Two important questions to ask are: Are you
concerned with thinking or behavior? Do you want to model humans, or work from an
ideal standard?
In this book, we adopt the view that intelligence is concerned mainly with rational action.
Ideally, an intelligent agent takes the best possible action in a situation. We will study the
problem of building agents that are intelligent in this sense.
Philosophers (going back to 400 B.C.) made AI conceivable by considering the ideas that
the mind is in some ways like a machine, that it operates on knowledge encoded in some
internal language, and that thought can be used to help arrive at the right actions to take.
Mathematicians provided the tools to manipulate statements of logical certainty as well
as uncertain, probabilistic statements. They also set the groundwork for reasoning about
algorithms.
Psychologists strengthened the idea that humans and other animals can be considered
information processing machines. Linguists showed that language use fits into this model.
Computer engineering provided the artifact that makes AI applications possible. AI programs tend to be large, and they could not work without the great advances in speed and
memory that the computer industry has provided.
The history of AI has had cycles of success, misplaced optimism, and resulting cutbacks
in enthusiasm and funding. There have also been cycles of introducing new creative
approaches and systematically refining the best ones.
Recent progress in understanding the theoretical basis for intelligence has gone hand in
hand with improvements in the capabilities of real systems.
--- PAGE 56 ---
Chapter 1. Introduction
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Daniel Crevier's (1993) Artificial Intelligence gives a complete history of the field, and Raymond
Kurzweil's (1990) Age of Intelligent Machines situates AI in the broader context of computer
science and intellectual history in general. Dianne Martin (1993) documents the degree to which
early computers were endowed by the media with mythical powers of intelligence.
The methodological status of artificial intelligence is discussed in The Sciences of the Artificial, by Herb Simon (1981), which discusses research areas concerned with complex artifacts.
It explains how AI can be viewed as both science and mathematics.
Artificial Intelligence: The Very Idea, by John Haugeland (1985) gives a readable account of
the philosophical and practical problems of AI. Cognitive science is well-described by JohnsonLaird's The Computer and the Mind: An Introduction to Cognitive Science. Baker (1989)
covers the syntactic part of modern linguistics, and Chierchia and McConnell-Ginet (1990) cover
semantics. Allen (1995) covers linguistics from the AI point of view.
Early AI work is covered in Feigenbaum and Feldman's Computers and Thought, Minsky's
Semantic Information Processing, and the Machine Intelligence series edited by Donald Michie.
A large number of influential papers are collected in Readings in Artificial Intelligence (Webber
and Nilsson, 1981). Early papers on neural networks are collected in Neurocomputing (Anderson
and Rosenfeld, 1988). The Encyclopedia of AI (Shapiro, 1992) contains survey articles on almost
every topic in AI. These articles usually provide a good entry point into the research literature on
each topic. The four-volume Handbook of Artificial Intelligence (Barr and Feigenbaum, 1981)
contains descriptions of almost every major AI system published before 1981.
The most recent work appears in the proceedings of the major AI conferences: the biennial
International Joint Conference on AI (IJCAI), and the annual National Conference on AI, more
often known as AAAI, after its sponsoring organization. The major journals for general AI are
Artificial Intelligence, Computational Intelligence, the IEEE Transactions on Pattern Analysis
and Machine Intelligence, and the electronic Journal of Artificial Intelligence Research. There
are also many journals devoted to specific areas, which we cover in the appropriate chapters.
Commercial products are covered in the magazines Al Expert and PC AI. The main professional
societies for AI are the American Association for Artificial Intelligence (AAAI), the ACM Special
Interest Group in Artificial Intelligence (SIGART), and the Society for Artificial Intelligence and
Simulation of Behaviour (AISB). AAAI's Al Magazine and the SIGART Bulletin contain many
topical and tutorial articles as well as announcements of conferences and workshops.
EXERCISES
These exercises are intended to stimulate discussion, and some might be set as term projects.
Alternatively, preliminary attempts can be made now, and these attempts can be reviewed after
completing the book.
1.1 Read Turing's original paper on AI (Turing, 1950). In the paper, he discusses several
potential objections to his proposed enterprise and his test for intelligence. Which objections
--- PAGE 57 ---
Section 1.5. Summary
STRONG A
WEAK AI
still carry some weight? Are his refutations valid? Can you think of new objections arising from
developments since he wrote the paper? In the paper, he predicts that by the year 2000, a computer
will have a 30% chance of passing a five-minute Turing Test with an unskilled interrogator. Do
you think this is reasonable?
1.2 We characterized the definitions of AI along two dimensions, human vs. ideal and thought
vs. action. But there are other dimensions that are worth considering. One dimension is whether
we are interested in theoretical results or in practical applications. Another is whether we intend
our intelligent computers to be conscious or not. Philosophers have had a lot to say about this
issue, and although most AI researchers are happy to leave the questions to the philosophers,
there has been heated debate. The claim that machines can be conscious is called the strong AI
claim; the weak AI position makes no such claim. Characterize the eight definitions on page
5 and the seven following definitions according to the four dimensions we have mentioned and
whatever other ones you feel are helpful.
Artificial intelligence is...
a. "a collection of algorithms that are computationally tractable, adequate approximations of
intractably specified problems" (Partridge, 1991)
b. "the enterprise of constructing a physical symbol system that can reliably pass the Turing
Test" (Ginsberg, 1993)
c. "the field of computer science that studies how machines can be made to act intelligently" (Jackson, 1986)
d. "a field of study that encompasses computational techniques for performing tasks that
apparently require intelligence when performed by humans" (Tanimoto, 1990)
e. "a very general investigation of the nature of intelligence and the principles and mechanisms
required for understanding or replicating it" (Sharples et al., 1989)
f. "the getting of computers to do things that seem to be intelligent" (Rowe, 1988).
1.3 There are well-known classes of problems that are intractably difficult for computers,
and other classes that are provably undecidable by any computer. Does this mean that Al is
impossible?
1.4 Suppose we extend Evans's ANALOGY program so that it can score 200 on a standard IQ
test. Would we then have a program more intelligent than a human? Explain.
1.5 Examine the AI literature to discover whether or not the following tasks can currently be
solved by computers:
a. Playing a decent game of table tennis (ping-pong).
b. Driving in the center of Cairo.
c. Playing a decent game of bridge at a competitive level.
d. Discovering and proving new mathematical theorems.
e. Writing an intentionally funny story.
f. Giving competent legal advice in a specialized area of law.
g. Translating spoken English into spoken Swedish in real time.
--- PAGE 58 ---
Chapter 1.
Introduction
For the currently infeasible tasks, try to find out what the difficulties are and estimate when they
will be overcome.
1.6 Find an article written by a lay person in a reputable newspaper or magazine claiming
the achievement of some intelligent capacity by a machine, where the claim is either wildly
exaggerated or false.
1.7 Fact, fiction, and forecast:
a. Find a claim in print by a reputable philosopher or scientist to the effect that a certain
capacity will never be exhibited by computers, where that capacity has now been exhibited.
b. Find a claim by a reputable computer scientist to the effect that a certain capacity would
be exhibited by a date that has since passed, without the appearance of that capacity.
c. Compare the accuracy of these predictions to predictions in other fields such as biomedicine,
fusion power, nanotechnology, transportation, or home electronics.
1.8 Some authors have claimed that perception and motor skills are the most important part of
intelligence, and that "higher-level" capacities are necessarily parasitic-simple add-ons to these
underlying facilities. Certainly, most of evolution and a large part of the brain have been devoted
to perception and motor skills, whereas AI has found tasks such as game playing and logical
inference to be easier, in many ways, than perceiving and acting in the real world. Do you think
that AI's traditional focus on higher-level cognitive abilities is misplaced?
1.9 "Surely computers cannot be intelligent-they can only do what their programmers tell
them." Is the latter statement true, and does it imply the former?
1.10 "Surely animals cannot be intelligent-they can only do what their genes tell them."
the latter statement true, and does it imply the former?
--- PAGE 59 ---
INTELLIGENT AGENTS
In whic t we discuss what an intelligent agent does, how it is related to its environment,
how it is evaluated, and how we might go about building one.
2.1 INTRODUCTION
An agent is anything that can be viewed as perceiving its environment through sensors and acting
upon that environment through effectors. A human agent has eyes, ears, and other organs for
sensors, and hands, legs, mouth, and other body parts for effectors. A robotic agent substitutes
cameras and infrared range finders for the sensors and various motors for the effectors.A
software agent has encoded bit strings as its percepts and actions. A generic agent is diagrammed
in Figure 2.1.
Our aim in this book is to design agents that do a good job of acting on their environment.
First, we will be a little more precise about what we mean by a good job. Then we will talk about
different designs for successful agents-filling in the question mark in Figure 2.1. We discuss
some of the general principles used in the design of agents throughout the book, chief among
which is the principle that agents should know things. Finally, we show how to couple an agent
to an environment and describe several kinds of environments.
2.2 HoW AGENTS SHOULD ACT
RATIONAL AGENT
A rational agent is one that does the right thing. Obviously, this is better than doing the wrong
thing, but what does it mean? As a first approximation, we will say that the right action is the
one that will cause the agent to be most successful. That leaves us with the problem of deciding
how and when to evaluate the agent's success.
--- PAGE 60 ---
Chapter 2.
Intelligent Agents
sensors
percepts
environment
agent
actions
effectors
PERFORMANCE
MEASURE
OMNISCIENCE
Figure 2.1 Agents interact with environments through sensors and effectors.
We use the term performance measure for the how-the criteria that determine how
successful an agent is. Obviously, there is not one fixed measure suitable for all agents. We
could ask the agent for a subjective opinion of how happy it is with its own performance, but
some agents would be unable to answer, and others would delude themselves. (Human agents in
particular are notorious for "sour grapes"-saying they did not really want something after they
are unsuccessful at getting it.) Therefore, we will insist on an objective performance measure
imposed by some authority. In other words, we as outside observers establish a standard of what
it means to be successful in an environment and use it to measure the performance of agents.
As an example, consider the case of an agent that is supposed to vacuum a dirty floor. A
plausible performance measure would be the amount of dirt cleaned up in a single eight-hour shift.
A more sophisticated performance measure would factor in the amount of electricity consumed
and the amount of noise generated as well. A third performance measure might give highest
marks to an agent that not only cleans the floor quietly and efficiently, but also finds time to go
windsurfing at the weekend.
The when of evaluating performance is also important. If we measured how much dirt the
agent had cleaned up in the first hour of the day, we would be rewarding those agents that start
fast (even if they do little or no work later on), and punishing those that work consistently. Thus,
we want to measure performance over the long run, be it an eight-hour shift or a lifetime.
We need to be careful to distinguish between rationality and omniscience. An omniscient
agent knows the actual outcome of its actions, and can act accordingly; but omniscience is
impossible in reality. Consider the following example: I am walking along the Champs Elysées
one day and I see an old friend across the street. There is no traffic nearby and I'm not otherwise
engaged, so, being rational, I start to cross the street. Meanwhile, at 33,000 feet, a cargo door
falls off a passing airliner,2 and before I make it to the other side of the street I am flattened. Was
I irrational to cross the street? It is unlikely that my obituary would read "Idiot attempts to cross
1 There is a danger here for those who establish performance measures: you often get what you ask for. That is, if
you measure success by the amount of dirt cleaned up, then some clever agent is bound to bring in a load of dirt each
morning, quickly clean it up, and get a good performance score. What you really want to measure is how clean the floor
is, but determining that is more difficult than just weighing the dirt cleaned up.
2 See N. Henderson, "New door latches urged for Boeing 747 jumbo jets," Washington Post, 8/24/89.
--- PAGE 61 ---
Section 2.2. How Agents Should Act
PERCEPT SEQUENCE
IDEAL RATIONAL
AGENT
street." Rather, this points out that rationality is concerned with expected success given what has
been perceived. Crossing the street was rational because most of the time the crossing would be
successful, and there was no way I could have foreseen the falling door. Note that another agent
that was equipped with radar for detecting falling doors or a steel cage strong enough to repel
them would be more successful, but it would not be any more rational.
In other words, we cannot blame an agent for failing to take into account something it could
not perceive, or for failing to take an action (such as repelling the cargo door) that it is incapable
of taking. But relaxing the requirement of perfection is not just a question of being fair to agents.
The point is that if we specify that an intelligent agent should always do what is actually the right
thing, it will be impossible to design an agent to fulfill this specification-unless we improve the
performance of crystal balls.
In summary, what is rational at any given time depends on four things:
The performance measure that defines degree of success.
Everything that the agent has perceived so far. We will call this complete perceptual history
the percept sequence.
What the agent knows about the environment.
The actions that the agent can perform.
This leads to a definition of an ideal rational agent: For each possible percept sequence, an
ideal rational agent should do whatever action is expected to maximize its performance measure,
on the basis of the evidence provided by the percept sequence and whatever built-in knowledge
the agent has.
We need to look carefully at this definition. At first glance, it might appear to allow an
agent to indulge in some decidedly underintelligent activities. For example, if an agent does not
look both ways before crossing a busy road, then its percept sequence will not tell it that there is
a large truck approaching at high speed. The definition seems to say that it would be OK for it to
cross the road. In fact, this interpretation is wrong on two counts. First, it would not be rational
to cross the road: the risk of crossing without looking is too great. Second, an ideal rational
agent would have chosen the "looking" action before stepping into the street, because looking
helps maximize the expected performance. Doing actions in order to obtain useful information
is an important part of rationality and is covered in depth in Chapter 16.
The notion of an agent is meant to be a tool for analyzing systems, not an absolute
characterization that divides the world into agents and non-agents. Consider a clock. It can be
thought of as just an inanimate object, or it can be thought of as a simple agent. As an agent,
most clocks always do the right action: moving their hands (or displaying digits) in the proper
fashion. Clocks are a kind of degenerate agent in that their percept sequence is empty; no matter
what happens outside, the clock's action should be unaffected.
Well, this is not quite true. If the clock and its owner take a trip from California to Australia,
the right thing for the clock to do would be to turn itself back six hours. We do not get upset at
our clocks for failing to do this because we realize that they are acting rationally, given their lack
of perceptual equipment.3
One of the authors still gets a small thrill when his computer successfully resets itself at daylight savings time.
--- PAGE 62 ---
MAPPING
IDEAL MAPPINGS
Chapter 2. Intelligent Agents
The ideal mapping from percept sequences to actions
Once we realize that an agent's behavior depends only on its percept sequence to date, then we can
describe any particular agent by making a table of the action it takes in response to each possible
percept sequence. (For most agents, this would be a very long list-infinite, in fact, unless we
place a bound on the length of percept sequences we want to consider.) Such a list is called
mapping from percept sequences to actions. We can, in principle, find out which mapping
correctly describes an agent by trying out all possible percept sequences and recording which
actions the agent does in response. (If the agent uses some randomization in its computations,
then we would have to try some percept sequences several times to get a good idea of the agent's
average behavior.) And if mappings describe agents, then ideal mappings describe ideal agents.
Specifying which action an agent ought to take in response to any given percept sequence provides
a design for an ideal agent.
This does not mean, of course, that we have to create an explicit table with an entry
for every possible percept sequence. It is possible to define a specification of the mapping
without exhaustively enumerating it. Consider a very simple agent: the square-root function
on a calculator. The percept sequence for this agent is a sequence of keystrokes representing a
number, and the action is to display a number on the display screen. The ideal mapping is that
when the percept is a positive number x, the right action is to display a positive number z such
that z2≈x, accurate to, say, 15 decimal places. This specification of the ideal mapping does
not require the designer to actually construct a table of square roots. Nor does the square-root
function have to use a table to behave correctly: Figure 2.2 shows part of the ideal mapping and
a simple program that implements the mapping using Newton's method.
The square-root example illustrates the relationship between the ideal mapping and an
ideal agent design, for a very restricted task. Whereas the table is very large, the agent is a nice,
compact program. It turns out that it is possible to design nice, compact agents that implement
Percept x
Action z
1.0
1.000000000000000
function SQRT(x)
1.1
1.048808848170152
2-1.0
/* initial guess */
1.2
1.095445115010332
repeat until |z2 - x| < 10-15
1.3
1.140175425099138
2-2-(2²- x)/(2z)
1.4
1.183215956619923
1.5
1.224744871391589
end
1.6
1.264911064067352
return z
1.7
1.303840481040530
1.8
1.341640786499874
1.9
1.378404875209022
Figure 2.2 Part of the ideal mapping for the square-root problem (accurate to 15 digits), and a
corresponding program that implements the ideal mapping.
--- PAGE 63 ---
Section 2.3. Structure of Intelligent Agents
AUTONOMY
the ideal mapping for much more general situations: agents that can solve a limitless variety of
tasks in a limitless variety of environments. Before we discuss how to do this, we need to look
at one more requirement that an intelligent agent ought to satisfy.
Autonomy
There is one more thing to deal with in the definition of an ideal rational agent: the "built-in
knowledge" part. If the agent's actions are based completely on built-in knowledge, such that it
need pay no attention to its percepts, then we say that the agent lacks autonomy. For example,
if the clock manufacturer was prescient enough to know that the clock's owner would be going
to Australia at some particular date, then a mechanism could be built in to adjust the hands
automatically by six hours at just the right time. This would certainly be successful behavior, but
the intelligence seems to belong to the clock's designer rather than to the clock itself.
An agent's behavior can be based on both its own experience and the built-in knowledge
used in constructing the agent for the particular environment in which it operates. A system is
autonomous to the extent that its behavior is determined by its own experience. It would be
too stringent, though, to require complete autonomy from the word go: when the agent has had
little or no experience, it would have to act randomly unless the designer gave some assistance.
So, just as evolution provides animals with enough built-in reflexes so that they can survive long
enough to learn for themselves, it would be reasonable to provide an artificial intelligent agent
with some initial knowledge as well as an ability to learn.
Autonomy not only fits in with our intuition, but it is an example of sound engineering
practices. An agent that operates on the basis of built-in assumptions will only operate successfully when those assumptions hold, and thus lacks flexibility. Consider, for example, the lowly
dung beetle. After digging its nest and laying its eggs, it fetches a ball of dung from a nearby heap
to plug the entrance; if the ball of dung is removed from its grasp en route, the beetle continues
on and pantomimes plugging the nest with the nonexistent dung ball, never noticing that it is
missing. Evolution has built an assumption into the beetle's behavior, and when it is violated,
unsuccessful behavior results. A truly autonomous intelligent agent should be able to operate
successfully in a wide variety of environments, given sufficient time to adapt.
2.3 STRUCTURE OF INTELLIGENT AGENTS
AGENT PROGRAM
ARCHITECTURE
So far we have talked about agents by describing their behavior-the action that is performed
after any given sequence of percepts. Now, we will have to bite the bullet and talk about how
the insides work. The job of AI is to design the agent program: a function that implements
the agent mapping from percepts to actions. We assume this program will run on some sort of
computing device, which we will call the architecture. Obviously, the program we choose has
4 The word "autonomous" has also come to mean something like "not under the immediate control of a human," as in
"autonomous land vehicle." We are using it in a stronger sense.
--- PAGE 64 ---
SOFTWARE AGENTS
SOFTBOTS
Chapter 2. Intelligent Agents
to be one that the architecture will accept and run. The architecture might be a plain computer, or
it might include special-purpose hardware for certain tasks, such as processing camera images or
filtering audio input. It might also include software that provides a degree of insulation between
the raw computer and the agent program, so that we can program at a higher level. In general,
the architecture makes the percepts from the sensors available to the program, runs the program,
and feeds the program's action choices to the effectors as they are generated. The relationship
among agents, architectures, and programs can be summed up as follows:
agent = architecture + program
Most of this book is about designing agent programs, although Chapters 24 and 25 deal directly
with the architecture.
Before we design an agent program, we must have a pretty good idea of the possible
percepts and actions, what goals or performance measure the agent is supposed to achieve, and
what sort of environment it will operate in.5 These come in a wide variety. Figure 2.3 shows the
basic elements for a selection of agent types.
It may come as a surprise to some readers that we include in our list of agent types some
programs that seem to operate in the entirely artificial environment defined by keyboard input
and character output on aa screen. "Surely," one might say, "this is not a real environment, is
it?" In fact, what matters is not the distinction between "real" and "artificial" environments,
but the complexity of the relationship among the behavior of the agent, the percept sequence
generated by the environment, and the goals that the agent is supposed to achieve. Some "real"
environments are actually quite simple. For example, a robot designed to inspect parts as they
come by on a conveyer belt can make use of a number of simplifying assumptions: that the
lighting is always just so, that the only thing on the conveyer belt will be parts of a certain kind,
and that there are only two actions--accept the part or mark it as a reject.
In contrast, some software agents (or software robots or softbots) exist in rich, unlimited
domains. Imagine a softbot designed to fly a flight simulator for a 747. The simulator is a
very detailed, complex environment, and the software agent must choose from a wide variety of
actions in real time. Or imagine a softbot designed to scan online news sources and show the
interesting items to its customers. To do well, it will need some natural language processing
abilities, it will need to learn what each customer is interested in, and it will need to dynamically
change its plans when, for example, the connection for one news source crashes or a new one
comes online.
Some environments blur the distinction between "real" and "artificial." In the ALIVE
environment (Maes et al., 1994), software agents are given as percepts a digitized camera image
of a room where a human walks about. The agent processes the camera image and chooses an
action. The environment also displays the camera
ra image on a large display screen that the human
can watch, and superimposes on the image a computer graphics rendering of the software agent.
One such image is a cartoon dog, which has been programmed to move toward the human (unless
he points to send the dog away) and to shake hands or jump up eagerly when the human makes
certain gestures.
For the acronymically minded, we call this the PAGE (Percepts, Actions, Goals, Environment) description. Note that
the goals do not necessarily have to be represented within the agent; they simply describe the performance measure by
which the agent design will be judged.
--- PAGE 65 ---
Section 2.3. Structure of Intelligent Agents
Agent Typе
Medical diagnosis
system
Percepts
Actions
Goals
Environment
Symptoms,
findings, patient's
answers
Questions, tests,
treatments
Healthy patient,
minimize costs
Patient, hospital
Satellite image
analysis system
Pixels of varying
intensity, color
Print a
categorization of
scene
Part-picking robot
Pixels of varying
intensity
Pick up parts and
sort into bins
Correct
categorization
Images from
orbiting satellite
Place parts in
correct bins
Conveyor belt
with parts
Refinery controller
Temperature,
pressure readings
Open, close
valves; adjust
temperature
Maximize purity,
yield, safety
Refinery
Interactive English
tutor
Typed words
Print exercises,
Maximize
Set of students
suggestions,
student's score on
corrections
test
Figure 2.3
Examples of agent types and their PAGE descriptions.
The most famous artificial environment is the Turing Test environment, in which the whole
point is that real and artificial agents are on equal footing, but the environment is challenging
enough that it is very difficult for a software agent to do as well as a human. Section 2.4 describes
in more detail the factors that make some environments more demanding than others.
Agent programs
We will be building intelligent agents throughout the book. They will all have the same skeleton,
namely, accepting percepts from an environment and generating actions. The early versions of
agent programs will have a very simple form (Figure 2.4). Each will use some internal data
structures that will be updated as new percepts arrive. These data structures are operated on by
the agent's decision-making procedures to generate an action choice, which is then passed to the
architecture to be executed.
There are two things to note about this skeleton program. First, even though we defined
the agent mapping as a function from percept sequences to actions, the agent program receives
only a single percept as its input. It is up to the agent to build up the percept sequence in memory,
if it so desires. In some environments, it is possible to be quite successful without storing
the percept sequence, and in complex domains, it is infeasible to store the complete sequence.
--- PAGE 66 ---
function SKELETON-AGENT(percept) returns action
static: memory, the agent's memory of the world
memory - UPDATE-MEMORY(memory, percept)
action - CHOOSE-BEST-ACTION(memory)
memory- UPDATE-MEMORY(memory, action)
return action
Chapter 2. Intelligent Agents
Figure 2.4 A skeleton agent. On each invocation, the agent's memory is updated to reflect
the new percept, the best action is chosen, and the fact that the action was taken is also stored in
memory. The memory persists from one invocation to the next.
Second, the goal or performance measure is not part of the skeleton program. This is because
the performance measure is applied externally to judge the behavior of the agent, and it is often
possible to achieve high performance without explicit knowledge of the performance measure
(see, e.g., the square-root agent).
Why not just look up the answers?
Let us start with the simplest possible way we can think of to write the agent program-a lookup
table. Figure 2.5 shows the agent program. It operates by keeping in memory its entire percept
sequence, and using it to index into table, which contains the appropriate action for all possible
percept sequences.
It is instructive to consider why this proposal is doomed to failure:
1. The table needed for something as simple as an agent that can only play chess would be
about 35100 entries.
2. It would take quite a long time for the designer to build the table.
3. The agent has no autonomy at all, because the calculation of best actions is entirely built-in.
So if the environment changed in some unexpected way, the agent would be lost.
function TABLE-DRIVEN-AGENT( percept) returns action
static: percepts, a sequence, initially empty
table, a table, indexed by percept sequences, initially fully specified
append percept to the end of percepts
action -LOOKUP( percepts, table)
return action
Figure 2.5 An agent based on a prespecified lookup table. It keeps track of the percept
sequence and just looks up the best action.
--- PAGE 67 ---
Section 2.3. Structure of Intelligent Agents
4. Even if we gave the agent a learning mechanism as well, so that it could have a degree of
autonomy, it would take forever to learn the right value for all the table entries.
Despite all this, TABLE-DRIVEN-AGENT does do what we want: it implements the desired agent
mapping. It is not enough to say, "It can't be intelligent;" the point is to understand why an agent
that reasons (as opposed to looking things up in a table) can do even better by avoiding the four
drawbacks listed here.
An example
At this point, it will be helpful to consider a particular environment, so that our discussion
can become more concrete. Mainly because of its familiarity, and because it involves a broad
range of skills, we will look at the job of designing an automated taxi driver. We should point
out, before the reader becomes alarmed, that such a system is currently somewhat beyond the
capabilities of existing technology, although most of the components are available in some form.6
The full driving task is extremely open-ended-there is no limit to the novel combinations of
circumstances that can arise (which is another reason why we chose it as a focus for discussion).
We must first think about the percepts, actions, goals and environment for the taxi. They
are summarized in Figure 2.6 and discussed in turn.
Agent Typе
Percepts
Actions
Goals
Environment
Taxi driver
Cameras,
speedometer, GPS,
sonar, microphone
Steer, accelerate,
brake, talk to
passenger
Safe, fast, legal,
comfortable trip,
maximize profits
Roads, other
traffic, pedestrians,
customers
Figure 2.6 The taxi driver agent type.
The taxi will need to know where it is, what else is on the road, and how fast it is going.
This information can be obtained from the percepts provided by one or more controllable TV
cameras, the speedometer, and odometer. To control the vehicle properly, especially on curves, it
should have an accelerometer; it will also need to know the mechanical state of the vehicle, so it
will need the usual array of engine and electrical system sensors. It might have instruments that
are not available to the average human driver: a satellite global positioning system (GPS) to give
it accurate position information with respect to an electronic map; or infrared or sonar sensors to
detect distances to other cars and obstacles. Finally, it will need a microphone or keyboard for
the passengers to tell it their destination.
The actions available to a taxi driver will be more or less the same ones available to a human
driver: control over the engine through the gas pedal and control over steering and braking. In
addition, it will need output to a screen or voice synthesizer to talk back to the passengers, and
perhaps some way to communicate with other vehicles.
See page 26 for a description of an existing driving robot, or look at the conference proceedings on Intelligent Vehicle
and Highway Systems (IVHS).
--- PAGE 68 ---
CONDITION-ACTION
RULE
Chapter 2.
Intelligent Agents
What performance measure would we like our automated driver to aspire to? Desirable
qualities include getting to the correct destination; minimizing fuel consumption and wear and
tear; minimizing the trip time and/or cost; minimizing violations of traffic laws and disturbances
to other drivers; maximizing safety and passenger comfort; maximizing profits. Obviously, some
of these goals conflict, so there will be trade-offs involved.
Finally, were this a real project, we would need to decide what kind of driving environment
the taxi will face. Should it operate on local roads, or also on freeways? Will it be in Southern
California, where snow is seldom a problem, or in Alaska, where it seldom is not? Will it always
be driving on the right, or might we want it to be flexible enough to drive on the left in case we
want to operate taxis in Britain or Japan? Obviously, the more restricted the environment, the
easier the design problem.
Now we have to decide how to build a real program to implement the mapping from
percepts to action. We will find that different aspects of driving suggest different types of agent
program. We will consider four types of agent program:
Simple reflex agents
Agents that keep track of the world
Goal-based agents
Utility-based agents
Simple reflex agents
The option of constructing an explicit lookup table is out of the question. The visual input from
a single camera comes in at the rate of 50 megabytes per second (25 frames per second, 1000 ×
1000 pixels with 8 bits of color and 8 bits of intensity information). So the lookup table for an
hour would be 260×60× 50M entries.
However, we can summarize portions of the table by noting certain commonly occurring
input/output associations. For example, if the car in front brakes, and its brake lights come on,
then the driver should notice this and initiate braking. In other words, some processing is done on
the visual input to establish the condition we call "The car in front is braking"; then this triggers
some established connection in the agent program to the action "initiate braking". We call such
a connection a condition-action rule7 written as
if car-in-front-is-braking then initiate-braking
Humans also have many such connections, some of which are learned responses (as for driving)
and some of which are innate reflexes (such as blinking when something approaches the eye).
In the course of the book, we will see several different ways in which such connections can be
learned and implemented.
Figure 2.7 gives the structure of a simple reflex agent in schematic form, showing how
the condition-action rules allow the agent to make the connection from percept to action. (Do
not worry if this seems trivial; it gets more interesting shortly.) We use rectangles to denote
7 Also called situation-action rules, productions, or if-then rules. The last term is also used by some authors for
logical implications, so we will avoid it altogether.
--- PAGE 69 ---
Section 2.3. Structure of Intelligent Agents
Figure 2.7
Agent
Sensors
What the world
is like now
Condition-action rules
What action I
should do now
Effectors
Schematic diagram of a simple reflex agent.
function SIMPLE-REFLEX-AGENT( percept) returns action
static: rules, a set of condition-action rules
state - INTERPRET-INPUT(percept)
rule- RULE-MATCH(state, rules)
action - RULE-ACTION[rule]
return action
Environment
Figure 2.8 A simple reflex agent. It works by finding a rule whose condition matches the
current situation (as defined by the percept) and then doing the action associated with that rule.
the current internal state of the agent's decision process, and ovals to represent the background
information used in the process. The agent program, which is also very simple, is shown in
Figure 2.8. The INTERPRET-INPUT function generates an abstracted description of the current
state from the percept, and the RULE-MATCH function returns the first rule in the set of rules that
matches the given state description. Although such agents can be implemented very efficiently
(see Chapter 10), their range of applicability is very narrow, as we shall see.
Agents that keep track of the world
The simple reflex agent described before will work only if the correct decision can be made
on the basis of the current percept. If the car in front is a recent model, and has the centrally
mounted brake light now required in the United States, then it will be possible to tell if it is
braking from a single image. Unfortunately, older models have different configurations of tail
--- PAGE 70 ---
INTERNAL STATE
Chapter 2.
Intelligent Agents
lights, brake lights, and turn-signal lights, and it is not always possible to tell if the car is braking.
Thus, even for the simple braking rule, our driver will have to maintain some sort of internal
state in order to choose an action. Here, the internal state is not too extensive-it just needs the
previous frame from the camera to detect when two red lights at the edge of the vehicle go on or
off simultaneously.
Consider the following more obvious case: from time to time, the driver looks in the
rear-view mirror to check on the locations of nearby vehicles. When the driver is not looking in
the mirror, the vehicles in the next lane are invisible (i.e., the states in which they are present and
absent are indistinguishable); but in order to decide on a lane-change maneuver, the driver needs
to know whether or not they are there.
The problem illustrated by this example arises because the sensors do not provide access
the complete state of the world. In such cases, the agent may need to maintain some internal state
information in order to distinguish between world states that generate the same perceptual input
but nonetheless are significantly different. Here, "significantly different" means that different
actions are appropriate in the two states.
Updating this internal state information as time goes by requires two kinds of knowledge to
be encoded in the agent program. First, we need some information about how the world evolves
independently of the agent-for example, that an overtaking car generally will be closer behind
than it was a moment ago. Second, we need some information about how the agent's own actions
affect the world-for example, that when the agent changes lanes to the right, there is a gap
least temporarily) in the lane it was in before, or that after driving for five minutes northbound
on the freeway one is usually about five miles north of where one was five minutes ago.
(at
Figure 2.9 gives the structure of the reflex agent, showing how the current percept is
combined with the old internal state to generate the updated description of the current state. The
agent program is shown in Figure 2.10. The interesting part is the function UPDATE-STATE, which
is responsible for creating the new internal state description. As well as interpreting the new
percept in the light of existing knowledge about the state, it uses information about how the world
evolves to keep track of the unseen parts of the world, and also must know about what the agent's
actions do to the state of the world. Detailed examples appear in Chapters 7 and 17.
GOAL
SEARCH
PLANNING
Goal-based agents
Knowing about the current state of the environment is not always enough to decide what to do.
For example, at a road junction, the taxi can turn left, right, or go straight on. The right decision
depends on where the taxi is trying to get to. In other words, as well as a current state description,
the agent needs some sort of goal information, which describes situations that are desirablefor example, being at the passenger's destination. The agent program can combine this with
information about the results of possible actions (the same information as was used to update
internal state in the reflex agent) in order to choose actions that achieve the goal. Sometimes
this will be simple, when goal satisfaction results immediately from a single action; sometimes,
it will be more tricky, when the agent has to consider long sequences of twists and turns to fiud
a way to achieve the goal. Search (Chapters 3 to 5) and planning (Chapters 11 to 13) are the
subfields of AI devoted to finding action sequences that do achieve the agent's goals.
--- PAGE 71 ---
Section 2.3. Structure of Intelligent Agents
State
How the world evolves
Sensors
What the world
is like now
What my actions do
Condition-action rules
Agent
Figure 2.9 A reflex agent with internal state.
What action I
should do now
Effectors
function REFLEX-AGENT-WITH-STATE( percept) returns action
static: state, a description of the current world state
rules, a set of condition-action rules
state - UPDATE-STATE(state, percept)
rule - RULE-MATCH(state, rules)
action-RULE-ACTION[rule]
state- UPDATE-STATE(state, action)
return action
Environment
Figure 2.10 A reflex agent with internal state. It works by finding a rule whose condition
matches the current situation (as defined by the percept and the stored internal state) and then
doing the action associated with that rule.
Notice that decision-making of this kind is fundamentally different from the conditionaction rules described earlier, in that it involves consideration of the future-both "What will
happen if I do such-and-such?" and "Will that make me happy?" In the reflex agent designs,
this information is not explicitly used, because the designer has precomputed the correct action
for various cases. The reflex agent brakes when it sees brake lights. A goal-based agent,
principle, could reason that if the car in front has its brake lights on, it will slow down. From
the way the world usually evolves, the only action that will achieve the goal of not hitting other
cars is to brake. Although the goal-based agent appears less efficient, it is far more flexible. If it
starts to rain, the agent can update its knowledge of how effectively its brakes will operate; this
will automatically cause all of the relevant behaviors to be altered to suit the new conditions. For
the reflex agent, on the other hand, we would have to rewrite a large number of condition-action
--- PAGE 72 ---
Chapter 2. Intelligent Agents
rules. Of course, the goal-based agent is also more flexible with respect to reaching different
destinations. Simply by specifying a new destination, we can get the goal-based agent to come
up with a new behavior. The reflex agent's rules for when to turn and when to go straight will
only work for a single destination; they must all be replaced to go somewhere new.
Figure 2.11 shows the goal-based agent's structure. Chapter 13 contains detailed agent
programs for goal-based agents.
State
How the world evolves
What my actions do
Sensors
What the world
is like now
What it will be like
if I do action A
Goals
Agent
Figure 2.11
An agent with explicit goals.
What action I
should do now
Effectors
Environment
UTILITY
Utility-based agents
Goals alone are not really enough to generate high-quality behavior. For example, there are many
action sequences that will get the taxi to its destination, thereby achieving the goal, but some
are quicker, safer, more reliable, or cheaper than others. Goals just provide a crude distinction
between "happy" and "unhappy" states, whereas a more general performance measure should
allow a comparison of different world states (or sequences of states) according to exactly how
happy they would make the agent if they could be achieved. Because "happy" does not sound
very scientific, the customary terminology is to say that if one world state is preferred to another,
then it has higher utility for the agent.8
Utility is therefore a function that maps a state onto a real number, which describes the
associated degree of happiness. A complete specification of the utility function allows rational
decisions in two kinds of cases where goals have trouble. First, when there are conflicting goals,
only some of which can be achieved (for example, speed and safety), the utility function specifies
the appropriate trade-off. Second, when there are several goals that the agent can aim for, none
The word "utility" here refers to "the quality of being useful," not to the electric company or water works.
Or sequence of states, if we are measuring the utility of an agent over the long run.
--- PAGE 73 ---
Section 2.4. Environments
of which can be achieved with certainty, utility provides a way in which the likelihood of success
can be weighed up against the importance of the goals.
In Chapter 16, we show that any rational agent can be described as possessing a utility
function. An agent that possesses an explicit utility function therefore can make rational decisions,
but may have to compare the utilities achieved by different courses of actions. Goals, although
cruder, enable the agent to pick an action right away if it satisfies the goal. In some cases,
moreover, a utility function can be translated into a set of goals, such that the decisions made by
a goal-based agent using those goals are identical to those made by the utility-based agent.
The overall utility-based agent structure appears in Figure 2.12. Actual utility-based agent
programs appear in Chapter 5, where we examine game-playing programs that must make fine
distinctions among various board positions; and in Chapter 17, where we tackle the general
problem of designing decision-making agents.
State
How the world evolves
What my actions do
Utility
Sensors
What the world
is like now
What it will be like
if I do action A
How happy I will be
in such a state
What action I
should do now
Agent
Figure 2.12
A complete utility-based agent.
2.4 ENVIRONMENTS
Effectors
Environment
In this section and in the exercises at the end of the chapter, you will see how to couple an agent
to an environment. Section 2.3 introduced several different kinds of agents and environments.
In all cases, however, the nature of the connection between them is the same: actions are done
by the agent on the environment, which in turn provides percepts to the agent. First, we will
describe the different types of environments and how they affect the design of agents. Then we
will describe environment programs that can be used as testbeds for agent programs.
--- PAGE 74 ---
Chapter 2. Intelligent Agents
ACCESSIBLE
DETERMINISTIC
EPISODIC
STATIC
SEMIDYNAMIC
DISCRETE
Properties of environments
Environments come in several flavors. The principal distinctions to be made are as follows:
Accessible vs. inaccessible.
If an agent's sensory apparatus gives it access to the complete state of the environment,
then we say that the environment is accessible to that agent. An environment is effectively
accessible if the sensors detect all aspects that are relevant to the choice of action. An
accessible environment is convenient because the agent need not maintain any internal state
to keep track of the world.
◆Deterministic vs. nondeterministic.
If the next state of the environment is completely determined by the current state and the
actions selected by the agents, then we say the environment is deterministic. In principle,
an agent need not worry about uncertainty in an accessible, deterministic environment. If
the environment is inaccessible, however, then it may appear to be nondeterministic. This
is particularly true if the environment is complex, making it hard to keep track of all the
inaccessible aspects. Thus, it is often better to think of an environment as deterministic or
nondeterministic from the point of view of the agent.
<◇ Episodic vs. nonepisodic.
In an episodic environment, the agent's experience is divided into "episodes." Each episode
consists of the agent perceiving and then acting. The quality of its action depends just on
the episode itself, because subsequent episodes do not depend on what actions occur in
previous episodes. Episodic environments are much simpler because the agent does not
need to think ahead.
Static vs. dynamic.
If the environment can change while an agent is deliberating, then we say the environment
is dynamic for that agent; otherwise it is static. Static environments are easy to deal with
because the agent need not keep looking at the world while it is deciding on an action,
nor need it worry about the passage of time. If the environment does not change with the
passage of time but the agent's performance score does, then we say the environment is
semidynamic.
Discrete vs. continuous.
If there are a limited number of distinct, clearly defined percepts and actions we say that
the environment is discrete. Chess is discrete-there are a fixed number of possible moves
on each turn. Taxi driving is continuous-the speed and location of the taxi and the other
vehicles sweep through a range of continuous values.10
We will see that different environment types require somewhat different agent programs to deal
with them effectively. It will turn out, as you might expect, that the hardest case is inaccessible,
nonepisodic, dynamic, and continuous. It also turns out that most real situations are so complex
that whether they are really deterministic is a moot point; for practical purposes, they must be
treated as nondeterministic.
10 At a fine enough level of granularity, even the taxi driving environment is discrete, because the camera image is
digitized to yield discrete pixel values. But any sensible agent program would have to abstract above this level, up to a
level of granularity that is continuous.
--- PAGE 75 ---
Section 2.4.
Environments
Figure 2.13 lists the properties of a number of familiar environments. Note that the answers
can change depending on how you conceptualize the environments and agents. For example,
poker is deterministic if the agent can keep track of the order of cards in the deck, but it is
nondeterministic if it cannot. Also, many environments are episodic at higher levels than the
agent's individual actions. For example, a chess tournament consists of a sequence of games;
each game is an episode, because (by and large) the contribution of the moves in one game to the
agent's overall performance is not affected by the moves in its next game. On the other hand,
moves within a single game certainly interact, so the agent needs to look ahead several moves.
Environment
Accessible
Deterministic
Episodic
Static
Discrete
Chess with a clock
Yes
Yes
Semi
Yes
Chess without a clock
Yes
Yes
Yes
Yes
Poker
Yes
Yes
Backgammon
Yes
Yes
Yes
Taxi driving
Medical diagnosis system
Image-analysis system
Yes
Yes
Yes
Semi
Part-picking robot
Yes
Refinery controller
Interactive English tutor
Yes
Figure 2.13 Examples of environments and their characteristics.
Environment programs
The generic environment program in Figure 2.14 illustrates the basic relationship between agents
and environments. In this book, we will find it convenient for many of the examples and exercises
use an environment simulator that follows this program structure. The simulator takes one or
more agents as input and arranges to repeatedly give each agent the right percepts and receive back
an action. The simulator then updates the environment based on the actions, and possibly other
dynamic processes in the environment that are not considered to be agents (rain, for example).
The environment is therefore defined by the initial state and the update function. Of course, an
agent that works in a simulator ought also to work in a real environment that provides the same
kinds of percepts and accepts the same kinds of actions.
The RUN-ENVIRONMENT procedure correctly exercises the agents in an environment. For
some kinds of agents, such as those that engage in natural language dialogue, it may be sufficient
simply to observe their behavior. To get more detailed information about agent performance, we
insert some performance measurement code. The function RUN-EVAL-ENVIRONMENT, shown in
Figure 2.15, does this; it applies a performance measure to each agent and returns a list of the
resulting scores. The scores variable keeps track of each agent's score.
In general, the performance measure can depend on the entire sequence of environment
states generated during the operation of the program. Usually, however, the performance measure
--- PAGE 76 ---
Chapter 2. Intelligent Agents
procedure RUN-ENVIRONMENT(state, UPDATE-FN, agents, termination)
inputs: state, the initial state of the environment
UPDATE-FN, function to modify the environment
agents, a set of agents
termination, a predicate to test when we are done
repeat
for each agent in agents do
PERCEPT[agent] - GET-PERCEPT(agent, state)
end
for each agent in agents do
ACTION[agent] - PROGRAM[agent](PERCEPT[agent])
end
state - UPDATE-FN(actions, agents, state)
until termination(state)
Figure 2.14 The basic environment simulator program. It gives each agent its percept, gets an
action from each agent, and then updates the environment.
function RUN-EVAL-ENVIRONMENT(state, UPDATE-FN, agents,
termination, PERFORMANCE-FN) returns scores
local variables: scores, a vector the same size as agents, all 0
repeat
for each agent in agents do
PERCEPT[agent] - GET-PERCEPT(agent, state)
end
for each agent in agents do
ACTION[agent] - PROGRAM[agent](PERCEPT[agent])
end
state-UPDATE-FN(actions, agents, state)
scores- PERFORMANCE-FN(scores, agents, state)
until termination(state)
return scores
/* change * /
Figure 2.15 An environment simulator program that keeps track of the performance measure
for each agent.
works by a simple accumulation using either summation, averaging, or taking a maximum. For
example, if the performance measure for a vacuum-cleaning agent is the total amount of dirt
cleaned in a shift, scores will just keep track of how much dirt has been cleaned up so far.
RUN-EVAL-ENVIRONMENT returns the performance measure for a a single environment,
defined by a single initial state and a particular update function. Usually, an agent is designed to
--- PAGE 77 ---
Section 2.5. Summary
ENVIRONMENT
CLASS
work in an environment class, a whole set of different environments. For example, we design
a chess program to play against any of a wide collection of human and machine opponents. If
we designed it for a single opponent, we might be able to take advantage of specific weaknesses
in that opponent, but that would not give us a good program for general play. Strictly speaking,
in order to measure the performance of an agent, we need to have an environment generator
that selects particular environments (with certain likelihoods) in which to run the agent. We are
then interested in the agent's average performance over the environment class. This is fairly
straightforward to implement for a simulated environment, and Exercises 2.5 to 2.11 take you
through the entire development of an environment and the associated measurement process.
A possible confusion arises between the state variable in the environment simulator and
the state variable in the agent itself (see REFLEX-AGENT-WITH-STATE). As a programmer implementing both the environment simulator and the agent, it is tempting to allow the agent to peek
at the environment simulator's state variable. This temptation must be resisted at all costs! The
agent's version of the state must be constructed from its percepts alone, without access to the
complete state information.
2.5 SUMMARY
This chapter has been something of a whirlwind tour of AI, which we have conceived of as the
science of agent design. The major points to recall are as follows:
An agent is something that perceives and acts in an environment. We split an agent into
an architecture and an agent program.
An ideal agent is one that always takes the action that is expected to maximize its performance measure, given the percept sequence it has seen so far.
An agent is autonomous to the extent that its action choices depend on its own experience,
rather than on knowledge of the environment that has been built-in by the designer.
An agent program maps from a percept to an action, while updating an internal state.
There exists a variety of basic agent program designs, depending on the kind of information
made explicit and used in the decision process. The designs vary in efficiency, compactness,
and flexibility. The appropriate design of the agent program depends on the percepts,
actions, goals, and environment.
Reflex agents respond immediately to percepts, goal-based agents act so that they will
achieve their goal(s), and utility-based agents try to maximize their own "happiness."
The process of making decisions by reasoning with knowledge is central to Al and to
successful agent design. This means that representing knowledge is important.
Some environments are more demanding than others. Environments that are inaccessible,
nondeterministic, nonepisodic, dynamic, and continuous are the most challenging.
--- PAGE 78 ---
Chapter 2. Intelligent Agents
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The analysis of rational agency as a mapping from percept sequences to actions probably stems
ultimately from the effort to identify rational behavior in the realm of economics and other forms
of reasoning under uncertainty (covered in later chapters) and from the efforts of psychological
behaviorists such as Skinner (1953) to reduce the psychology of organisms strictly to input/output
or stimulus/response mappings. The advance from behaviorism to functionalism in psychology,
which was at least partly driven by the application of the computer metaphor to agents (Putnam,
1960; Lewis, 1966), introduced the internal state of the agent into the picture. The philosopher
Daniel Dennett (1969; 1978b) helped to synthesize these viewpoints into a coherent "intentional
stance" toward agents. A high-level, abstract perspective on agency is also taken within the world
of AI in (McCarthy and Hayes, 1969). Jon Doyle (1983) proposed that rational agent design is
the core of AI, and would remain as its mission while other topics in AI would spin off to form
new disciplines. Horvitz et al. (1988) specifically suggest the use of rationality conceived as the
maximization of expected utility as a basis for AI.
The AI researcher and Nobel-prize-winningeconomist Herb Simon drew a clear distinction
between rationality under resource limitations (procedural rationality) and rationality as making
the objectively rational choice (substantive rationality) (Simon, 1958). Cherniak (1986) explores
the minimal level of rationality needed to qualify an entity as an agent. Russell and Wefald (1991)
deal explicitly with the possibility of using a variety of agent architectures. Dung Beetle Ecology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the behavior
of dung beetles.
EXERCISES
2.1 What is the difference between a performance measure and a utility function?
2.2 For each of the environments in Figure 2.3, determine what type of agent architecture is
most appropriate (table lookup, simple reflex, goal-based or utility-based).
2.3 Choose a domain that you are familiar with, and write a PAGE description of an agent
for the environment. Characterize the environment as being accessible, deterministic, episodic,
static, and continuous or not. What agent architecture is best for this domain?
2.4 While driving, which is the best policy?
a. Always put your directional blinker on before turning,
b. Never use your blinker,
c. Look in your mirrors and use your blinker only if you observe a car that can observe you?
What kind of reasoning did you need to do to arrive at this policy (logical, goal-based, or utilitybased)? What kind of agent design is necessary to carry out the policy (reflex, goal-based, or
utility-based)?
--- PAGE 79 ---
Section 2.5. Summary
The following exercises all concern the implementation of an environment and set of agents in
the vacuum-cleaner world.
2.5 Implement a performance-measuring environment simulator for the vacuum-cleaner world.
This world can be described as follows:
Percepts: Each vacuum-cleaner agent gets a three-element percept vector on each turn.
The first element, a touch sensor, should be a 1 if the machine has bumped into something
and a 0 otherwise. The second comes from a photosensor under the machine, which emits
a 1 if there is dirt there and a 0 otherwise. The third comes from an infrared sensor, which
emits a 1 when the agent is in its home location, and a 0 otherwise.
Actions: There are five actions available: go forward, turn right by 90°, turn left by 90°,
suck up dirt, and turn off.
Goals: The goal for each agent is to clean up and go home. To be precise, the performance
measure will be 100 points for each piece of dirt vacuumed up, minus 1 point for each
action taken, and minus 1000 points if it is not in the home location when it turns itself off.
Environment: The environment consists of a grid of squares. Some squares contain
obstacles (walls and furniture) and other squares are open space. Some of the open squares
contain dirt. Each "go forward" action moves one square unless there is an obstacle in that
square, in which case the agent stays where it is, but the touch sensor goes on. A "suck up
dirt" action always cleans up the dirt. A "turn off" command ends the simulation.
We can vary the complexity of the environment along three dimensions:
◇Room shape: In the simplest case, the room is an n × n square, for some fixed n. We can
make it more difficult by changing to a rectangular, L-shaped, or irregularly shaped room,
or a series of rooms connected by corridors.
Furniture: Placing furniture in the room makes it more complex than an empty room. To
the vacuum-cleaning agent, a piece of furniture cannot be distinguished from a wall by
perception; both appear as a 1 on the touch sensor.
Dirt placement: In the simplest case, dirt is distributed uniformly around the room. But
it is more realistic for the dirt to predominate in certain locations, such as along a heavily
travelled path to the next room, or in front of the couch.
2.6 Implement a table-lookup agent for the special case of the vacuum-cleaner world consisting
of a 2 x 2 grid of open squares, in which at most two squares will contain dirt. The agent starts
in the upper left corner, facing to the right. Recall that a table-lookup agent consists of a table of
actions indexed by a percept sequence. In this environment, the agent can always complete its
task in nine or fewer actions (four moves, three turns, and two suck-ups), so the table only needs
entries for percept sequences up to length nine. At each turn, there are eight possible percept
vectors, so the table will be of size 89 = 134, 217,728. Fortunately, we can cut this down by
realizing that the touch sensor and home sensor inputs are not needed; we can arrange so that
the agent never bumps into a wall and knows when it has returned home. Then there are only
two relevant percept vectors, ?0? and ?1?, and the size of the table is at most 29 = 512. Run the
environment simulator on the table-lookup agent in all possible worlds (how many are there?).
Record its performance score for each world and its overall average score.
--- PAGE 80 ---
Chapter
Intelligent Agents
2.7 Implement an environment for an xm rectangular room, where each square has a 5% chance
of containing dirt, and n and m are chosen at random from the range 8 to 15, inclusive.
2.8 Design and implement a pure reflex agent for the environment of Exercise 2.7, ignoring
the requirement of returning home, and measure its performance. Explain why it is impossible
to have a reflex agent that returns home and shuts itself off. Speculate on what the best possible
reflex agent could do. What prevents a reflex agent from doing very well?
2.9 Design and implement several agents with internal state. Measure their performance. How
close do they come to the ideal agent for this environment?
2.10 Calculate the size of the table for a table-lookup agent in the domain of Exercise 2.7.
Explain your calculation. You need not fill in the entries for the table.
2.11 Experiment with changing the shape and dirt placement of the room, and with adding
furniture. Measure your agents in these new environments. Discuss how their performance
might be improved to handle more complex geographies.
--- PAGE 81 ---
Part II
PROBLEM-SOLVING
In this part we show how an agent can act by establishing goals and considering
sequences of actions that might achieve those goals. A goal and a set of means
for achieving the goal is called a problem, and the process of exploring what the
means can do is called search. We show what search can do, how it must be
modified to account for adversaries, and what its limitations are.
--- PAGE 82 ---
--- PAGE 83 ---
SOLVING PROBLEMS BY
SEARCHING
In which we look at how an agent can decide what to do by systematically considering
the outcomes of various sequences of actions that it might take.
PROBLEM-SOLVING
AGENT
In Chapter 2, we saw that simple reflex agents are unable to plan ahead. They are limited in what
they can do because their actions are determined only by the current percept. Furthermore, they
have no knowledge of what their actions do nor of what they are trying to achieve.
In this chapter, we describe one kind of goal-based agent called a problem-solving agent.
Problem-solving agents decide what to do by finding sequences of actions that lead to desirable
states. We discuss informally how the agent can formulate an appropriate view of the problem it
faces. The problem type that results from the formulation process will depend on the knowledge
available to the agent: principally, whether it knows the current state and the outcomes of actions.
We then define more precisely the elements that constitute a "problem" and its "solution," and
give several examples to illustrate these definitions. Given precise definitions of problems,
is relatively straightforward to construct a search process for finding solutions. We cover six
different search strategies and show how they can be applied to a variety of problems. Chapter 4
will then cover search strategies that make use of more information about the problem to improve
the efficiency of the search process.
This chapter uses concepts from the analysis of algorithms. Readers unfamiliar with the
concepts of asymptotic complexity and NP-completeness should consult Appendix A.
3.1 PROBLEM-SOLVING AGENTS
Intelligent agents are supposed to act in such a way that the environment goes through a sequence
of states that maximizes the performance measure. In its full generality, this specification is
difficult to translate into a successful agent design. As we mentioned in Chapter 2, the task is
somewhat simplified if the agent can adopt a goal and aim to satisfy it. Let us first look at how
and why an agent might do this.
--- PAGE 84 ---
GOAL FORMULATION
PROBLEM
FORMULATION
SEARCH
SOLUTION
Chapter 3. Solving Problems by Searching
Imagine our agent in the city of Arad, Romania, toward the end of a touring holiday. The
agent has a ticket to fly out of Bucharest the following day. The ticket is nonrefundable, the
agent's visa is about to expire, and after tomorrow, there are no seats available for six weeks. Now
the agent's performance measure contains many other factors besides the cost of the ticket and
the undesirability of being arrested and deported. For example, it wants to improve its suntan,
improve its Romanian, take in the sights, and so on. All these factors might suggest any of a vast
array of possible actions. Given the seriousness of the situation, however, it should adopt the
goal of driving to Bucharest. Actions that result in a failure to reach Bucharest on time can be
rejected without further consideration. Goals such as this help organize behavior by limiting the
objectives that the agent is trying to achieve. Goal formulation, based on the current situation,
is the first step in problem solving. As well as formulating a goal, the agent may wish to decide
on some other factors that affect the desirability of different ways of achieving the goal.
For the purposes of this chapter, we will consider a goal to be a set of world states-just
those states in which the goal is satisfied. Actions can be viewed as causing transitions between
world states, so obviously the agent has to find out which actions will get it to a goal state. Before
it can do this, it needs to decide what sorts of actions and states to consider. If it were to try
to consider actions at the level of "move the left foot forward 18 inches" or "turn the steering
wheel six degrees left," it would never find its way out of the parking lot, let alone to Bucharest,
because constructing a solution at that level of detail would be an intractable problem. Problem
formulation is the process of deciding what actions and states to consider, and follows goal
formulation. We will discuss this process in more detail. For now, let us assume that the agent
will consider actions at the level of driving from one major town to another. The states it will
consider therefore correspond to being in a particular town.
Our agent has now adopted the goal of driving to Bucharest, and is considering which town
to drive to from Arad. There are three roads out of Arad, one toward Sibiu, one to Timisoara,
and one to Zerind. None of these achieves the goal, so unless the agent is very familiar with the
geography of Romania, it will not know which road to follow.2 In other words, the agent will not
know which of its possible actions is best, because it does not know enough about the state that
results from taking each action. If the agent has no additional knowledge, then it is stuck. The
best it can do is choose one of the actions at random.
But suppose the agent has a map of Romania, either on paper or in its memory. The point
of a map is to provide the agent with information about the states it might get itself into, and
the actions it can take. The agent can use this information to consider subsequent stages of a
hypothetical journey through each of the three towns, to try to find a journey that eventually gets
to Bucharest. Once it has found a path on the map from Arad to Bucharest, it can achieve its goal
by carrying out the driving actions that correspond to the legs of the journey. In general, then, an
agent with several immediate options of unknown value can decide what to do by first examining
different possible sequences of actions that lead to states of known value, and then choosing the
best one. This process of looking for such a sequence is called search. A search algorithm takes
a problem as input and returns a solution in the form of an action sequence. Once a solution is
1 Notice that these states actually correspond to large sets of world states, because a world state specifies every aspect
of reality. It is important to keep in mind the distinction between states in problem solving and world states.
2 We are assuming that most readers are in the same position, and can easily imagine themselves as clueless as our
agent. We apologize to Romanian readers who are unable to take advantage of this pedagogical device.
--- PAGE 85 ---
Section 3.2. Formulating Problems
EXECUTION
found, the actions it recommends can be carried out. This is called the execution phase. Thus,
we have a simple "formulate, search, execute" design for the agent, as shown in Figure 3.1. After
formulating a goal and a problem to solve, the agent calls a search procedure to solve it. It then
uses the solution to guide its actions, doing whatever the solution recommends as the next thing
to do, and then removing that step from the sequence. Once the solution has been executed, the
agent will find a new goal.
function SIMPLE-PROBLEM-SOLVING-AGENT(p) returns an action
inputs: p, a percept
static: s, an action sequence, initially empty
state, some description of the current world state
g, a goal, initially null
problem, a problem formulation
state - UPDATE-STATE(state, p)
if s is empty then
g- FORMULATE-GOAL(state)
problem- FORMULATE-PROBLEM(state, g)
S- SEARCH( probleт)
action - RECOMMENDATION(S, state)
S-REMAINDER(s, state)
return action
Figure 3.1 A simple problem-solving agent.
We will not discuss the UPDATE-STATE and FORMULATE-GOAL functions further in this
chapter. The next two sections describe the process of problem formulation, and then the
remainder of the chapter is devoted to various versions of the SEARCH function. The execution
phase is usually straightforward for a simple problem-solving agent: RECOMMENDATION just
takes the first action in the sequence, and REMAINDER returns the rest.
3.2 FORMULATING PROBLEMS
In this section, we will consider the problem formulation process in more detail. First, we will
look at the different amounts of knowledge that an agent can have concerning its actions and the
state that it is in. This depends on how the agent is connected to its environment through its
percepts and actions. We find that there are four essentially different types of problems-singlestate problems, multiple-state problems, contingency problems, and exploration problems. We
will define these types precisely, in preparation for later sections that address the solution process.
--- PAGE 86 ---
Knowledge and problem types
Chapter 3. Solving Problems by Searching
Let us consider an environment somewhat different from Romania: the vacuum world from
Exercises 2.5 to 2.11 in Chapter 2. We will simplify it even further for the sake of exposition. Let
the world contain just two locations. Each location may or may not contain dirt, and the agent
may be in one location or the other. There are 8 possible world states, as shown in Figure 3.2.
The agent has three possible actions in this version of the vacuum world: Left, Right, and Suck.
Assume, for the moment, that sucking is 100% effective. The goal is to clean up all the dirt. That
is, the goal is equivalent to the state set {7,8}.
SINGLE-STATE
PROBLEM
MULTIPLE-STATE
PROBLEM
Figure 3.2
The eight possible states of the simplified vacuum world.
First, suppose that the agent's sensors give it enough information to tell exactly which state
it is in (i.e., the world is accessible); and suppose that it knows exactly what each of its actions
does. Then it can calculate exactly which state it will be in after any sequence of actions. For
example, if its initial state is 5, then it can calculate that the action sequence [Right, Suck] will get
to a goal state. This is the simplest case, which we call a single-state problem.
Second, suppose that the agent knows all the effects of its actions, but has limited access
to the world state. For example, in the extreme case, it may have no sensors at all. In that case,
it knows only that its initial state is one of the set {1,2, 3, 4, 5, 6, 7, 8}. One might suppose that
the agent's predicament is hopeless, but in fact it can do quite well. Because it knows what its
actions do, it can, for example, calculate that the action Right will cause it to be in one of the
states {2,4, 6, 8}. In fact, the agent can discover that the action sequence [Right,Suck,Left,Suck]
is guaranteed to reach a goal state no matter what the start state. To summarize: when the world
is not fully accessible, the agent must reason about sets of states that it might get to, rather than
single states. We call this a multiple-state problem.
--- PAGE 87 ---
Section 3.2. Formulating Problems
CONTINGENCY
PROBLEM
INTERLEAVING
EXPLORATION
PROBLEM
Although it might seem different, the case of ignorance about the effects of actions can be
treated similarly. Suppose, for example, that the environment appears to be nondeterministic in
that it obeys Murphy's Law: the so-called Suck action sometimes deposits dirt on the carpet but
only if there is no dirt there already.3 For example, if the agent knows it is in state 4, then it
knows that if it sucks, it will reach one of the states {2, 4}. For any known initial state, however,
there is an action sequence that is guaranteed to reach a goal state (see Exercise 3.2).
Sometimes ignorance prevents the agent from finding a guaranteed solution sequence.
Suppose, for example, that the agent is in the Murphy's Law world, and that it has a position
sensor and a local dirt sensor, but no sensor capable of detecting dirt in other squares. Suppose
further that the sensors tell it that it is in one of the states {1,3}. The agent might formulate the
action sequence [Suck, Right, Suck]. Sucking would change the state to one of {5, 7}, and moving
right would then change the state to one of {6, 8}. If it is in fact state 6, then the action sequence
will succeed, but if it is state 8, the plan will fail. If the agent had chosen the simpler action
sequence [Suck], it would also succeed some of the time, but not always. It turns out there is no
fixed action sequence that guarantees a solution to this problem.
Obviously, the agent does have a way to solve the problem starting from one of {1, 3}: first
suck, then move right, then suck only if there is dirt there. Thus, solving this problem requires
sensing during the execution phase. Notice that the agent must now calculate a whole tree of
actions, rather than a single action sequence. In general, each branch of the tree deals with a
possible contingency that might arise. For this reason, we call this a contingency problem.
Many problems in the real, physical world are contingency problems, because exact prediction is
impossible. For this reason, many people keep their eyes open while walking around or driving.
Single-state and multiple-state problems can be handled by similar search techniques,
which are covered in this chapter and the next. Contingency problems, on the other hand,
require more complex algorithms, which we cover in Chapter 13. They also lend themselves to a
somewhat different agent design, in which the agent can act before it has found a guaranteed plan.
This is useful because rather than considering in advance every possible contingency that might
arise during execution, it is often better to actually start executing and see which contingencies
do arise. The agent can then continue to solve the problem given the additional information. This
type of interleaving of search and execution is also covered in Chapter 13, and for the limited
case of two-player games, in Chapter 5. For the remainder of this chapter, we will only consider
cases where guaranteed solutions consist of a single sequence of actions.
Finally, consider the plight of an agent that has no information about the effects of its
actions. This is somewhat equivalent to being lost in a strange country with no map at all, and is
the hardest task faced by an intelligent agent. The agent must experiment, gradually discovering
what its actions do and what sorts of states exist. This is a kind of search, but a search in the
real world rather than in a model thereof. Taking a step in the real world, rather than in a model,
may involve significant danger for an ignorant agent. If it survives, the agent learns a "map" of
the environment, which it can then use to solve subsequent problems. We discuss this kind of
exploration problem in Chapter 20.
3 We assume that most readers face similar problems, and can imagine themselves as frustrated as our agent. We
apologize to owners of modern, efficient home appliances who cannot take advantage of this pedagogical device.
4 It is also the task faced by newborn babies.
--- PAGE 88 ---
PROBLEM
INITIAL STATE
OPERATOR
SUCCESSOR
FUNCTION
STATE SPACE
PATH
GOAL TEST
PATH COST
SOLUTION
STATE SET SPACE
Well-defined problems and solutions
Chapter 3. Solving Problems by Searching
A problem is really a collection of information that the agent will use to decide what to do. We
will begin by specifying the information needed to define a single-state problem.
We have seen that the basic elements of a problem definition are the states and actions. To
capture these formally, we need the following:
• The initial state that the agent knows itself to be in.
• The set of possible actions available to the agent. The term operator is used to denote
the description of an action in terms of which state will be reached by carrying out the
action in a particular state. (An alternate formulation uses a successor function S. Given
a particular state x, S(x) returns the set of states reachable from x by any single action.)
Together, these define the state space of the problem: the set of all states reachable from the
initial state by any sequence of actions. A path in the state space is simply any sequence of
actions leading from one state to another. The next element of a problem is the following:
• The goal test, which the agent can apply to a single state description to determine if it is
a goal state. Sometimes there is an explicit set of possible goal states, and the test simply
checks to see if we have reached one of them. Sometimes the goal is specified by an
abstract property rather than an explicitly enumerated set of states. For example, in chess,
the goal is to reach a state called "checkmate," where the opponent's king can be captured
on the next move no matter what the opponent does.
Finally, it may be the case that one solution is preferable to another, even though they both reach
the goal. For example, we might prefer paths with fewer or less costly actions.
A path cost function is a function that assigns a cost to a path. In all cases we will consider,
the cost of a path is the sum of the costs of the individual actions along the path. The path
cost function is often denoted by g.
Together, the initial state, operator set, goal test, and path cost function define a problem.
Naturally, we can then define a datatype with which to represent problems:
datatype PROBLEM
components: INITIAL-STATE, OPERATORS, GOAL-TEST, PATH-COST-FUNCTION
Instances of this datatype will be the input to our search algorithms. The output of a search
algorithm is a solution, that is, a path from the initial state to a state that satisfies the goal test.
To deal with multiple-state problems, we need to make only minor modifications: a problem
consists of an initial state set; a set of operators specifying for each action the set of states reached
from any given state; and a goal test and path cost function as before. An operator is applied to
state set by unioning the results of applying the operator to each state in the set. A path now
connects sets of states, and a solution is now a path that leads to a set of states all of which are
goal states. The state space is replaced by the state set space (see Figure 3.7 for an example).
Problems of both types are illustrated in Section 3.3.
--- PAGE 89 ---
Section 3.2. Formulating Problems
SEARCH COST
TOTAL COST
Measuring problem-solving performance
The effectiveness of a search can be measured in at least three ways. First, does it find a solution
at all? Second, is it a good solution (one with a low path cost)? Third, what is the search cost
associated with the time and memory required to find a solution? The total cost of the search is
the sum of the path cost and the search cost.5
For the problem of finding a route from Arad to Bucharest, the path cost might be proportional to the total mileage of the path, perhaps with something thrown in for wear and tear
on different road surfaces. The search cost will depend on the circumstances. In a static environment, it will be zero because the performance measure is independent of time. If there is
some urgency to get to Bucharest, the environment is semidynamic because deliberating longer
will cost more. In this case, the search cost might vary approximately linearly with computation
time (at least for small amounts of time). Thus, to compute the total cost, it would appear that
we have to add miles and milliseconds. This is not always easy, because there is no "official
exchange rate" between the two. The agent must somehow decide what resources to devote to
search and what resources to devote to execution. For problems with very small state spaces,
is easy to find the solution with the lowest path cost. But for large, complicated problems, there
is a trade-off to be made-the agent can search for a very long time to get an optimal solution,
or the agent can search for a shorter time and get a solution with a slightly larger path cost. The
issue of allocating resources will be taken up again in Chapter 16; for now, we concentrate on
the search itself.
Choosing states and actions
Now that we have the definitions out of the way, let us start our investigation of problems with
an easy one: "Drive from Arad to Bucharest using the roads in the map in Figure 3.3." An
appropriate state space has 20 states, where each state is defined solely by location, specified as
a city. Thus, the initial state is "in Arad" and the goal test is "is this Bucharest?" The operators
correspond to driving along the roads between cities.
One solution is the path Arad to Sibiu to Rimnicu Vilcea to Pitesti to Bucharest. There are
lots of other paths that are also solutions, for example, via Lugoj and Craiova. To decide which
of these solutions is better, we need to know what the path cost function is measuring: it could
be the total mileage, or the expected travel time. Because our current map does not specify either
of these. we will use the number of steps as the cost function. That means that the path through
Sibiu and Fagaras, with a path cost of 3, is the best possible solution.
The real art of problem solving is in deciding what goes into the description of the states
and operators and what is left out. Compare the simple state description we have chosen, "in
Arad," to an actual cross-country trip, where the state of the world includes so many things: the
travelling companions, what is on the radio, what there is to look at out of the window, the vehicle
being used for the trip, how fast it is going, whether there are any law enforcement officers nearby,
what time it is, whether the driver is hungry or tired or running out of gas, how far it is to the next
5 In theoretical computer science and in robotics, the search cost (the part you do before interacting with the environment)
is called the offline cost and the path cost is called the online cost.
--- PAGE 90 ---
Arad
Oradea
Zerind
Chapter 3. Solving Problems by Searching
Sibiu
Fagaras
Timisoara
Lugoj
Dobreta ☐
Rimnicu Vilcea
Pitesti
Mehadia
Craiova
Figure 3.3 A simplified road map of Romania.
Neamt
lasi
Vaslui
☐ Hirsova
Urziceni
Bucharest
Giurgiu
Eforie
ABSTRACTION
rest stop, the condition of the road, the weather, and so on. All these considerations are left out
of state descriptions because they are irrelevant to the problem of finding a route to Bucharest.
The process of removing detail from a representation is called abstraction.
As well as abstracting the state description, we must abstract the actions themselves. An
action-let us say a car trip from Arad to Zerind-has many effects. Besides changing the
location of the vehicle and its occupants, it takes up time, consumes fuel, generates pollution, and
changes the agent (as they say, travel is broadening). In our formulation, we take into account
only the change in location. Also, there are many actions that we will omit altogether: turning
on the radio, looking out of the window, slowing down for law enforcement officers, and so on.
Can we be more precise about defining the appropriate level of abstraction? Think of the
states and actions we have chosen as corresponding to sets of detailed world states and sets of
detailed action sequences. Now consider a solution to the abstract problem: for example, the
path Arad to Sibiu to Rimnicu Vilcea to Pitesti to Bucharest. This solution corresponds to a large
number of more detailed paths. For example, we could drive with the radio on between Sibiu
and Rimnicu Vilcea, and then switch it off for the rest of the trip. Each of these more detailed
paths is still a solution to the goal, so the abstraction is valid. The abstraction is also useful,
because carrying out each of the actions in the solution, such as driving from Pitesti to Bucharest,
is somewhat easier than the original problem. The choice of a good abstraction thus involves
removing as much detail as possible while retaining validity and ensuring that the abstract actions
are easy to carry out. Were it not for the ability to construct useful abstractions, intelligent agents
would be completely swamped by the real world.
--- PAGE 91 ---
Section 3.3.
Example Problems
3.3 EXAMPLE PROBLEMS
TOY PROBLEMS
REAL-WORLD
PROBLEMS
8-PUZZLE
SLIDING-BLOCK
PUZZLES
The range of task environments that can be characterized by well-defined problems is vast.
can distinguish between so-called toy problems, which are intended to illustrate or exercise
various problem-solving methods, and so-called real-world problems, which tend to be more
difficult and whose solutions people actually care about. In this section, we will give examples of
both. By nature, toy problems can be given a concise, exact description. This means that they can
be easily used by different researchers to compare the performance of algorithms. Real-world
problems, on the other hand, tend not to have a single agreed-upon description, but we will
attempt to give the general flavor of their formulations.
Toy problems
The 8-puzzle
The 8-puzzle, an instance of which is shown in Figure 3.4, consists of a 3×3 board with eight
numbered tiles and a blank space. A tile adjacent to the blank space can slide into the space.
The object is to reach the configuration shown on the right of the figure. One important trick is
to notice that rather than use operators such as "move the 4 tile into the blank space," it is more
sensible to have operators such as "the blank space changes places with the tile to its left." This is
because there are fewer of the latter kind of operator. This leads us to the following formulation:
States: a state description specifies the location of each of the eight tiles in one of the nine
squares. For efficiency, it is useful to include the location of the blank.
Operators: blank moves left, right, up, or down.
Goal test: state matches the goal configuration shown in Figure 3.4.
Path cost: each step costs 1, so the path cost is just the length of the path.
The 8-puzzle belongs to the family of sliding-block puzzles. This general class is known
to be NP-complete, so one does not expect to find methods significantly better than the search
Start State
Goal State
Figure 3.4
A typical instance of the 8-puzzle.
--- PAGE 92 ---
Chapter 3. Solving Problems by Searching
algorithms described in this chapter and the next. The 8-puzzle and its larger cousin, the 15puzzle, are the standard test problems for new search algorithms in AI.
The 8-queens problem
The goal of the 8-queens problem is to place eight queens on a chessboard such that no queen
attacks any other. (A queen attacks any piece in the same row, column or diagonal.) Figure 3.5
shows an attempted solution that fails: the queen in the rightmost column is attacked by the queen
at top left.
Figure 3.5 Almost a solution to the 8-queens problem. (Solution is left as an exercise.)
Although efficient special-purpose algorithms exist for this problem and the whole nqueens family, it remains an interesting test problem for search algorithms. There are two main
kinds of formulation. The incremental formulation involves placing queens one by one, whereas
the complete-state formulation starts with all 8 queens on the board and moves them around. In
either case, the path cost is of no interest because only the final state counts; algorithms are thus
compared only on search cost. Thus, we have the following goal test and path cost:
Goal test: 8 queens on board, none attacked.
Path cost: zero.
There are also different possible states and operators. Consider the following simple-minded
formulation:
States: any arrangement of 0 to 8 queens on board.
Operators: add a queen to any square.
In this formulation, we have 648 possible sequences to investigate. A more sensible choice would
use the fact that placing a queen where it is already attacked cannot work, because subsequent
placings of other queens will not undo the attack. So we might try the following:
--- PAGE 93 ---
Section 3.3.
Example Problems
States: arrangements of 0 to 8 queens with none attacked.
◇ Operators: place a queen in the left-most empty column such that it is not attacked by any
other queen.
It is easy to see that the actions given can generate only states with no attacks; but sometimes
no actions will be possible. For example, after making the first seven choices (left-to-right) in
Figure 3.5, there is no action available in this formulation. The search process must try another
choice. A quick calculation shows that there are only 2057 possible sequences to investigate. The
right formulation makes a big difference to the size of the search space. Similar considerations
apply for a complete-state formulation. For example, we could set the problem up as follows:
States: arrangements of 8 queens, one in each column.
Operators: move any attacked queen to another square in the same column.
This formulation would allow the algorithm to find a solution eventually, but it would be better
move to an unattacked square if possible.
Cryptarithmetic
In cryptarithmetic problems, letters stand for digits and the aim is to find a substitution of digits
for letters such that the resulting sum is arithmetically correct. Usually, each letter must stand
for a different digit. The following is a well-known example:
FORTY
Solution:
29786
F=2, O=9, R=7, etc.
TEN
TEN
SIXTY
31486
The following formulation is probably the simplest:
States: a cryptarithmetic puzzle with some letters replaced by digits.
Operators: replace all occurrences of a letter with a digit not already appearing in the
puzzle.
Goal test: puzzle contains only digits, and represents a correct sum.
Path cost: zero. All solutions equally valid.
A moment's thought shows that replacing E by 6 then F by 7 is the same thing as replacing F by
7 then E by 6-order does not matter to correctness, so we want to avoid trying permutations of
the same substitutions. One way to do this is to adopt a fixed order, e.g., alphabetical order. А
better choice is to do whichever is the most constrained substitution, that is, the letter that has
the fewest legal possibilities given the constraints of the puzzle.
The vacuum world
Here we will define the simplified vacuum world from Figure 3.2, rather than the full version
from Chapter 2. The latter is dealt with in Exercise 3.17.
--- PAGE 94 ---
Chapter 3. Solving Problems by Searching
First, let us review the single-state case with complete information. We assume that the
agent knows its location and the locations of all the pieces of dirt, and that the suction is still in
good working order.
States: one of the eight states shown in Figure 3.2 (or Figure 3.6).
Operators: move left, move right, suck.
Goal test: no dirt left in any square.
Path cost: each action costs 1.
Figure 3.6 Diagram of the simplified vacuum state space. Arcs denote actions. L = move left,
R = move right, S = suck.
Figure 3.6 shows the complete state space showing all the possible paths. Solving the
problem from any starting state is simply a matter of following arrows to a goal state. This is the
case for all problems, of course, but in most, the state space is vastly larger and more tangled.
Now let us consider the case where the agent has no sensors, but still has to clean up all
the dirt. Because this is a multiple-state problem, we will have the following:
어State sets: subsets of states 1-8 shown in Figure 3.2 (or Figure 3.6).
Operators: move left, move right, suck.
Goal test: all states in state set have no dirt.
Path cost: each action costs 1.
The start state set is the set of all states, because the agent has no sensors. A solution is any
sequence leading from the start state set to a set of states with no dirt (see Figure 3.7). Similar
state set spaces can be constructed for the case of uncertainty about actions and uncertainty about
both states and actions.
--- PAGE 95 ---
Section 3.3. Example Problems
Figure 3.7 State set space for the simplified vacuum world with no sensors. Each dashed-line
box encloses a set of states. At any given point, the agent is within a state set but does not know
which state of that set it is in. The initial state set (complete ignorance) is the top center boх.
Actions are represented by labelled arcs. Self-loops are omitted for clarity.
Missionaries and cannibals
The missionaries and cannibals problem is usually stated as follows. Three missionaries and
three cannibals are on one side of a river, along with a boat that can hold one or two people. Find
a way to get everyone to the other side, without ever leaving a group of missionaries in one place
outnumbered by the cannibals in that place.
This problem is famous in AI because it was the subject of the first paper that approached
problem formulation from an analytical viewpoint (Amarel, 1968). As with travelling in Romania,
the real-life problem must be greatly abstracted before we can apply a problem-solving strategy.
--- PAGE 96 ---
--- PAGE 97 ---
--- PAGE 98 ---
--- PAGE 99 ---
--- PAGE 100 ---
--- PAGE 101 ---
--- PAGE 102 ---
--- PAGE 103 ---
--- PAGE 104 ---
--- PAGE 105 ---
--- PAGE 106 ---
--- PAGE 107 ---
--- PAGE 108 ---
--- PAGE 109 ---
--- PAGE 110 ---
--- PAGE 111 ---
--- PAGE 112 ---
--- PAGE 113 ---
--- PAGE 114 ---
--- PAGE 115 ---
--- PAGE 116 ---
--- PAGE 117 ---
--- PAGE 118 ---
--- PAGE 119 ---
--- PAGE 120 ---
--- PAGE 121 ---
--- PAGE 122 ---
--- PAGE 123 ---
--- PAGE 124 ---
--- PAGE 125 ---
--- PAGE 126 ---
--- PAGE 127 ---
--- PAGE 128 ---
--- PAGE 129 ---
--- PAGE 130 ---
--- PAGE 131 ---
--- PAGE 132 ---
--- PAGE 133 ---
--- PAGE 134 ---
--- PAGE 135 ---
--- PAGE 136 ---
--- PAGE 137 ---
--- PAGE 138 ---
--- PAGE 139 ---
--- PAGE 140 ---
--- PAGE 141 ---
--- PAGE 142 ---
--- PAGE 143 ---
--- PAGE 144 ---
--- PAGE 145 ---
--- PAGE 146 ---
--- PAGE 147 ---
--- PAGE 148 ---
--- PAGE 149 ---
--- PAGE 150 ---
--- PAGE 151 ---
--- PAGE 152 ---
--- PAGE 153 ---
--- PAGE 154 ---
--- PAGE 155 ---
--- PAGE 156 ---
--- PAGE 157 ---
--- PAGE 158 ---
--- PAGE 159 ---
--- PAGE 160 ---
--- PAGE 161 ---
--- PAGE 162 ---
--- PAGE 163 ---
--- PAGE 164 ---
--- PAGE 165 ---
--- PAGE 166 ---
--- PAGE 167 ---
--- PAGE 168 ---
--- PAGE 169 ---
--- PAGE 170 ---
--- PAGE 171 ---
--- PAGE 172 ---
--- PAGE 173 ---
--- PAGE 174 ---
--- PAGE 175 ---
--- PAGE 176 ---
--- PAGE 177 ---
--- PAGE 178 ---
--- PAGE 179 ---
--- PAGE 180 ---
--- PAGE 181 ---
--- PAGE 182 ---
--- PAGE 183 ---
--- PAGE 184 ---
--- PAGE 185 ---
--- PAGE 186 ---
--- PAGE 187 ---
--- PAGE 188 ---
--- PAGE 189 ---
--- PAGE 190 ---
--- PAGE 191 ---
--- PAGE 192 ---
--- PAGE 193 ---
--- PAGE 194 ---
--- PAGE 195 ---
--- PAGE 196 ---
--- PAGE 197 ---
--- PAGE 198 ---
--- PAGE 199 ---
--- PAGE 200 ---
--- PAGE 201 ---
--- PAGE 202 ---
--- PAGE 203 ---
--- PAGE 204 ---
--- PAGE 205 ---
--- PAGE 206 ---
--- PAGE 207 ---
--- PAGE 208 ---
--- PAGE 209 ---
--- PAGE 210 ---
--- PAGE 211 ---
--- PAGE 212 ---
--- PAGE 213 ---
--- PAGE 214 ---
--- PAGE 215 ---
--- PAGE 216 ---
--- PAGE 217 ---
--- PAGE 218 ---
--- PAGE 219 ---
--- PAGE 220 ---
--- PAGE 221 ---
--- PAGE 222 ---
--- PAGE 223 ---
--- PAGE 224 ---
--- PAGE 225 ---
--- PAGE 226 ---
--- PAGE 227 ---
--- PAGE 228 ---
--- PAGE 229 ---
--- PAGE 230 ---
--- PAGE 231 ---
--- PAGE 232 ---
--- PAGE 233 ---
--- PAGE 234 ---
--- PAGE 235 ---
--- PAGE 236 ---
--- PAGE 237 ---
--- PAGE 238 ---
--- PAGE 239 ---
--- PAGE 240 ---
--- PAGE 241 ---
--- PAGE 242 ---
--- PAGE 243 ---
--- PAGE 244 ---
--- PAGE 245 ---
--- PAGE 246 ---
--- PAGE 247 ---
--- PAGE 248 ---
--- PAGE 249 ---
--- PAGE 250 ---
--- PAGE 251 ---
--- PAGE 252 ---
--- PAGE 253 ---
--- PAGE 254 ---
--- PAGE 255 ---
--- PAGE 256 ---
--- PAGE 257 ---
--- PAGE 258 ---
--- PAGE 259 ---
--- PAGE 260 ---
--- PAGE 261 ---
--- PAGE 262 ---
--- PAGE 263 ---
--- PAGE 264 ---
--- PAGE 265 ---
--- PAGE 266 ---
--- PAGE 267 ---
--- PAGE 268 ---
--- PAGE 269 ---
--- PAGE 270 ---
--- PAGE 271 ---
--- PAGE 272 ---
--- PAGE 273 ---
--- PAGE 274 ---
--- PAGE 275 ---
--- PAGE 276 ---
--- PAGE 277 ---
--- PAGE 278 ---
--- PAGE 279 ---
--- PAGE 280 ---
--- PAGE 281 ---
--- PAGE 282 ---
--- PAGE 283 ---
--- PAGE 284 ---
--- PAGE 285 ---
--- PAGE 286 ---
--- PAGE 287 ---
--- PAGE 288 ---
--- PAGE 289 ---
--- PAGE 290 ---
--- PAGE 291 ---
--- PAGE 292 ---
--- PAGE 293 ---
--- PAGE 294 ---
--- PAGE 295 ---
--- PAGE 296 ---
--- PAGE 297 ---
--- PAGE 298 ---
--- PAGE 299 ---
--- PAGE 300 ---
--- PAGE 301 ---
--- PAGE 302 ---
--- PAGE 303 ---
--- PAGE 304 ---
--- PAGE 305 ---
--- PAGE 306 ---
--- PAGE 307 ---
--- PAGE 308 ---
--- PAGE 309 ---
--- PAGE 310 ---
--- PAGE 311 ---
--- PAGE 312 ---
--- PAGE 313 ---
--- PAGE 314 ---
--- PAGE 315 ---
--- PAGE 316 ---
--- PAGE 317 ---
--- PAGE 318 ---
--- PAGE 319 ---
--- PAGE 320 ---
--- PAGE 321 ---
--- PAGE 322 ---
--- PAGE 323 ---
--- PAGE 324 ---
--- PAGE 325 ---
--- PAGE 326 ---
--- PAGE 327 ---
--- PAGE 328 ---
--- PAGE 329 ---
--- PAGE 330 ---
--- PAGE 331 ---
--- PAGE 332 ---
--- PAGE 333 ---
--- PAGE 334 ---
--- PAGE 335 ---
--- PAGE 336 ---
--- PAGE 337 ---
--- PAGE 338 ---
--- PAGE 339 ---
--- PAGE 340 ---
--- PAGE 341 ---
--- PAGE 342 ---
--- PAGE 343 ---
--- PAGE 344 ---
--- PAGE 345 ---
--- PAGE 346 ---
--- PAGE 347 ---
--- PAGE 348 ---
--- PAGE 349 ---
--- PAGE 350 ---
--- PAGE 351 ---
--- PAGE 352 ---
--- PAGE 353 ---
--- PAGE 354 ---
--- PAGE 355 ---
--- PAGE 356 ---
--- PAGE 357 ---
--- PAGE 358 ---
--- PAGE 359 ---
--- PAGE 360 ---
--- PAGE 361 ---
--- PAGE 362 ---
--- PAGE 363 ---
--- PAGE 364 ---
--- PAGE 365 ---
--- PAGE 366 ---
--- PAGE 367 ---
--- PAGE 368 ---
--- PAGE 369 ---
--- PAGE 370 ---
--- PAGE 371 ---
--- PAGE 372 ---
--- PAGE 373 ---
--- PAGE 374 ---
--- PAGE 375 ---
--- PAGE 376 ---
--- PAGE 377 ---
--- PAGE 378 ---
--- PAGE 379 ---
--- PAGE 380 ---
--- PAGE 381 ---
--- PAGE 382 ---
--- PAGE 383 ---
--- PAGE 384 ---
--- PAGE 385 ---
--- PAGE 386 ---
--- PAGE 387 ---
--- PAGE 388 ---
--- PAGE 389 ---
--- PAGE 390 ---
--- PAGE 391 ---
--- PAGE 392 ---
--- PAGE 393 ---
--- PAGE 394 ---
--- PAGE 395 ---
--- PAGE 396 ---
--- PAGE 397 ---
--- PAGE 398 ---
--- PAGE 399 ---
--- PAGE 400 ---
--- PAGE 401 ---
--- PAGE 402 ---
--- PAGE 403 ---
--- PAGE 404 ---
--- PAGE 405 ---
--- PAGE 406 ---
--- PAGE 407 ---
--- PAGE 408 ---
--- PAGE 409 ---
--- PAGE 410 ---
--- PAGE 411 ---
--- PAGE 412 ---
--- PAGE 413 ---
--- PAGE 414 ---
--- PAGE 415 ---
--- PAGE 416 ---
--- PAGE 417 ---
--- PAGE 418 ---
--- PAGE 419 ---
--- PAGE 420 ---
--- PAGE 421 ---
--- PAGE 422 ---
--- PAGE 423 ---
--- PAGE 424 ---
--- PAGE 425 ---
--- PAGE 426 ---
--- PAGE 427 ---
--- PAGE 428 ---
--- PAGE 429 ---
--- PAGE 430 ---
--- PAGE 431 ---
--- PAGE 432 ---
--- PAGE 433 ---
--- PAGE 434 ---
--- PAGE 435 ---
--- PAGE 436 ---
--- PAGE 437 ---
--- PAGE 438 ---
--- PAGE 439 ---
--- PAGE 440 ---
--- PAGE 441 ---
--- PAGE 442 ---
--- PAGE 443 ---
--- PAGE 444 ---
--- PAGE 445 ---
--- PAGE 446 ---
--- PAGE 447 ---
--- PAGE 448 ---
--- PAGE 449 ---
--- PAGE 450 ---
--- PAGE 451 ---
--- PAGE 452 ---
--- PAGE 453 ---
--- PAGE 454 ---
--- PAGE 455 ---
--- PAGE 456 ---
--- PAGE 457 ---
--- PAGE 458 ---
--- PAGE 459 ---
--- PAGE 460 ---
--- PAGE 461 ---
--- PAGE 462 ---
--- PAGE 463 ---
--- PAGE 464 ---
--- PAGE 465 ---
--- PAGE 466 ---
--- PAGE 467 ---
--- PAGE 468 ---
--- PAGE 469 ---
--- PAGE 470 ---
--- PAGE 471 ---
--- PAGE 472 ---
--- PAGE 473 ---
--- PAGE 474 ---
--- PAGE 475 ---
--- PAGE 476 ---
--- PAGE 477 ---
--- PAGE 478 ---
--- PAGE 479 ---
--- PAGE 480 ---
--- PAGE 481 ---
--- PAGE 482 ---
--- PAGE 483 ---
--- PAGE 484 ---
--- PAGE 485 ---
--- PAGE 486 ---
--- PAGE 487 ---
--- PAGE 488 ---
--- PAGE 489 ---
--- PAGE 490 ---
--- PAGE 491 ---
--- PAGE 492 ---
--- PAGE 493 ---
--- PAGE 494 ---
--- PAGE 495 ---
--- PAGE 496 ---
--- PAGE 497 ---
--- PAGE 498 ---
--- PAGE 499 ---
--- PAGE 500 ---
--- PAGE 501 ---
--- PAGE 502 ---
--- PAGE 503 ---
--- PAGE 504 ---
--- PAGE 505 ---
--- PAGE 506 ---
--- PAGE 507 ---
--- PAGE 508 ---
--- PAGE 509 ---
--- PAGE 510 ---
--- PAGE 511 ---
--- PAGE 512 ---
--- PAGE 513 ---
--- PAGE 514 ---
--- PAGE 515 ---
--- PAGE 516 ---
--- PAGE 517 ---
--- PAGE 518 ---
--- PAGE 519 ---
--- PAGE 520 ---
--- PAGE 521 ---
--- PAGE 522 ---
--- PAGE 523 ---
--- PAGE 524 ---
--- PAGE 525 ---
--- PAGE 526 ---
--- PAGE 527 ---
--- PAGE 528 ---
--- PAGE 529 ---
--- PAGE 530 ---
--- PAGE 531 ---
--- PAGE 532 ---
--- PAGE 533 ---
--- PAGE 534 ---
--- PAGE 535 ---
--- PAGE 536 ---
--- PAGE 537 ---
--- PAGE 538 ---
--- PAGE 539 ---
--- PAGE 540 ---
--- PAGE 541 ---
--- PAGE 542 ---
--- PAGE 543 ---
--- PAGE 544 ---
--- PAGE 545 ---
--- PAGE 546 ---
--- PAGE 547 ---
--- PAGE 548 ---
--- PAGE 549 ---
--- PAGE 550 ---
--- PAGE 551 ---
--- PAGE 552 ---
--- PAGE 553 ---
--- PAGE 554 ---
--- PAGE 555 ---
--- PAGE 556 ---
--- PAGE 557 ---
--- PAGE 558 ---
--- PAGE 559 ---
--- PAGE 560 ---
--- PAGE 561 ---
--- PAGE 562 ---
--- PAGE 563 ---
--- PAGE 564 ---
--- PAGE 565 ---
--- PAGE 566 ---
--- PAGE 567 ---
--- PAGE 568 ---
--- PAGE 569 ---
--- PAGE 570 ---
--- PAGE 571 ---
--- PAGE 572 ---
--- PAGE 573 ---
--- PAGE 574 ---
--- PAGE 575 ---
--- PAGE 576 ---
--- PAGE 577 ---
--- PAGE 578 ---
--- PAGE 579 ---
--- PAGE 580 ---
--- PAGE 581 ---
--- PAGE 582 ---
--- PAGE 583 ---
--- PAGE 584 ---
--- PAGE 585 ---
--- PAGE 586 ---
--- PAGE 587 ---
--- PAGE 588 ---
--- PAGE 589 ---
--- PAGE 590 ---
--- PAGE 591 ---
--- PAGE 592 ---
--- PAGE 593 ---
--- PAGE 594 ---
--- PAGE 595 ---
--- PAGE 596 ---
--- PAGE 597 ---
--- PAGE 598 ---
--- PAGE 599 ---
--- PAGE 600 ---
--- PAGE 601 ---
--- PAGE 602 ---
--- PAGE 603 ---
--- PAGE 604 ---
--- PAGE 605 ---
--- PAGE 606 ---
--- PAGE 607 ---
--- PAGE 608 ---
--- PAGE 609 ---
--- PAGE 610 ---
--- PAGE 611 ---
--- PAGE 612 ---
--- PAGE 613 ---
--- PAGE 614 ---
--- PAGE 615 ---
--- PAGE 616 ---
--- PAGE 617 ---
--- PAGE 618 ---
--- PAGE 619 ---
--- PAGE 620 ---
--- PAGE 621 ---
--- PAGE 622 ---
--- PAGE 623 ---
--- PAGE 624 ---
--- PAGE 625 ---
--- PAGE 626 ---
--- PAGE 627 ---
--- PAGE 628 ---
--- PAGE 629 ---
--- PAGE 630 ---
--- PAGE 631 ---
--- PAGE 632 ---
--- PAGE 633 ---
--- PAGE 634 ---
--- PAGE 635 ---
--- PAGE 636 ---
--- PAGE 637 ---
--- PAGE 638 ---
--- PAGE 639 ---
--- PAGE 640 ---
--- PAGE 641 ---
--- PAGE 642 ---
--- PAGE 643 ---
--- PAGE 644 ---
--- PAGE 645 ---
--- PAGE 646 ---
--- PAGE 647 ---
--- PAGE 648 ---
--- PAGE 649 ---
--- PAGE 650 ---
--- PAGE 651 ---
--- PAGE 652 ---
--- PAGE 653 ---
--- PAGE 654 ---
--- PAGE 655 ---
--- PAGE 656 ---
--- PAGE 657 ---
--- PAGE 658 ---
Document generated by Anna’s Archive around 2023-2024 as part of the DuXiu collection
(https://annas-blog.org/duxiu-exclusive.html).
Images have been losslessly embedded. Information about the original file can be found in PDF attachments. Some stats (more in the
PDF attachments):
 "filename": "NDA1MzIxMzEuemlw",
 "filename_decoded": "40532131.zip",
 "filesize": 211291146,
 "md5": "0fbfcf62d73fdd63e68b460eb959297b",
 "header_md5": "8b37b6069aebbca86bdee7a0a1c11b58",
 "sha1": "f7972be6db8334dec6c96c292055a413b6079724",
 "sha256": "e976a47a48bc9d2bb45d7603c86fa62761c8bda10eb1208a959464157a7b9276",
 "crc32": 3234451669,
 "zip_password": "",
 "uncompressed_size": 211124246,
 "pdg_dir_name": "40532131",
 "pdg_main_pages_found": 629,
 "pdg_main_pages_max": 629,
 "total_pages": 657,
 "total_pixels": 3848353320,
 "pdf_generation_missing_pages": false